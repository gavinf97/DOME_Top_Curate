{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139277be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aef963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FILTERING AND UPDATING JSON METADATA (Block 5.0)\n",
      "============================================================\n",
      "Reading TSV: Positive_PMC_TSV_Files/positive_entries_status.tsv\n",
      "Found 1012 JSON files in Copilot_1000_v0_Processed_2026-01-15\n",
      "Entries matching JSON files: 1012\n",
      "Saved filtered TSV to: Positive_PMC_TSV_Files/positive_entries_pmid_pmcid_filtered.tsv\n",
      "Updating JSON files with publication IDs (checking order)...\n",
      "Successfully updated 1012 JSON files.\n"
     ]
    }
   ],
   "source": [
    "# Block 5.0: Filter IDs based on Copilot Processed folder and update JSONs\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILTERING AND UPDATING JSON METADATA (Block 5.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Define paths\n",
    "input_tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "json_folder_path = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "filtered_tsv_output = 'Positive_PMC_TSV_Files/positive_entries_pmid_pmcid_filtered.tsv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(input_tsv_path) and os.path.exists(json_folder_path):\n",
    "        # 2. Read TSV\n",
    "        print(f\"Reading TSV: {input_tsv_path}\")\n",
    "        df = pd.read_csv(input_tsv_path, sep='\\t')\n",
    "        \n",
    "        # 3. Extract and Clean IDs\n",
    "        # We need PMCID and PMID. Note: PMID might be float from previous steps\n",
    "        # Create a simplified copy\n",
    "        df_ids = df[['PMID', 'PMCID']].copy()\n",
    "        \n",
    "        def clean_pmid(val):\n",
    "            if pd.isna(val) or val == '':\n",
    "                return None\n",
    "            try:\n",
    "                # Convert to float then int to drop decimal, then string\n",
    "                return str(int(float(val)))\n",
    "            except:\n",
    "                return str(val)\n",
    "\n",
    "        df_ids['PMID'] = df_ids['PMID'].apply(clean_pmid)\n",
    "        df_ids['PMCID'] = df_ids['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        # 4. Get list of JSON files to filter against\n",
    "        json_files = [f for f in os.listdir(json_folder_path) if f.endswith('.json')]\n",
    "        # Create a set of PMCIDs from filenames (remove .json extension)\n",
    "        # Assuming filenames are like \"PMC12345.json\"\n",
    "        json_pmcids = set(f.replace('.json', '') for f in json_files)\n",
    "        \n",
    "        print(f\"Found {len(json_pmcids)} JSON files in {json_folder_path}\")\n",
    "        \n",
    "        # 5. Filter the DataFrame\n",
    "        # Keep row if its PMCID matches one in the folder\n",
    "        df_filtered = df_ids[df_ids['PMCID'].isin(json_pmcids)].copy()\n",
    "        \n",
    "        count = len(df_filtered)\n",
    "        print(f\"Entries matching JSON files: {count}\")\n",
    "        \n",
    "        # 6. Save the filtered TSV\n",
    "        df_filtered.to_csv(filtered_tsv_output, sep='\\t', index=False)\n",
    "        print(f\"Saved filtered TSV to: {filtered_tsv_output}\")\n",
    "        \n",
    "        # 7. Update JSON files\n",
    "        print(\"Updating JSON files with publication IDs (checking order)...\")\n",
    "        updated_count = 0\n",
    "        \n",
    "        for index, row in df_filtered.iterrows():\n",
    "            pmcid = row['PMCID']\n",
    "            pmid = row['PMID']\n",
    "            \n",
    "            if not pmcid:\n",
    "                continue\n",
    "                \n",
    "            json_file_path = os.path.join(json_folder_path, f\"{pmcid}.json\")\n",
    "            \n",
    "            if os.path.exists(json_file_path):\n",
    "                try:\n",
    "                    with open(json_file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # Prepare new data dict to preserve/enforce order\n",
    "                    # Target order: ..., publication/year, publication/pmid, publication/pmcid, publication/doi ...\n",
    "                    new_data = {}\n",
    "                    inserted = False\n",
    "                    \n",
    "                    pmid_val = pmid if pmid else \"\"\n",
    "                    pmcid_val = pmcid\n",
    "                    \n",
    "                    # If pmid/pmcid keys already exist in data, skip them during iteration\n",
    "                    keys_to_skip = ['publication/pmid', 'publication/pmcid']\n",
    "                    \n",
    "                    for key, value in data.items():\n",
    "                        if key in keys_to_skip:\n",
    "                            continue\n",
    "                            \n",
    "                        new_data[key] = value\n",
    "                        \n",
    "                        # Insert new keys immediately after publication/year\n",
    "                        if key == 'publication/year':\n",
    "                            new_data['publication/pmid'] = pmid_val\n",
    "                            new_data['publication/pmcid'] = pmcid_val\n",
    "                            inserted = True\n",
    "                            \n",
    "                    # Fallback: if 'publication/year' was not found, add them at the end\n",
    "                    if not inserted:\n",
    "                        new_data['publication/pmid'] = pmid_val\n",
    "                        new_data['publication/pmcid'] = pmcid_val\n",
    "                    \n",
    "                    with open(json_file_path, 'w') as f:\n",
    "                        json.dump(new_data, f, indent=2)\n",
    "                        \n",
    "                    updated_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating {pmcid}.json: {e}\")\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        print(f\"Successfully updated {updated_count} JSON files.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Input file or folder not found.\")\n",
    "        print(f\"TSV: {os.path.exists(input_tsv_path)}\")\n",
    "        print(f\"Folder: {os.path.exists(json_folder_path)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e9c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c938f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:185: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:186: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:185: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:186: SyntaxWarning: invalid escape sequence '\\|'\n",
      "/tmp/ipykernel_102433/3177539443.py:185: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  j_t = m['JSON'].replace('|', '\\|')\n",
      "/tmp/ipykernel_102433/3177539443.py:186: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  t_t = m['TSV'].replace('|', '\\|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYZING METADATA DIFFERENCES & GENERATING REPORT (Block 6.0)\n",
      "============================================================\n",
      "Created report directory: Metadata_Analysis_Reports/Report_2026-01-15_15-18-53\n",
      "Loading TSV...\n",
      "Comparing files...\n",
      "Detailed mismatch CSV saved to: Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/all_mismatches.csv\n",
      "\n",
      "Analysis Complete!\n",
      "Report and Visualizations generated in folder: Metadata_Analysis_Reports/Report_2026-01-15_15-18-53\n",
      "  - Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/Analysis_Report.md\n",
      "  - Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/mismatch_counts.png\n",
      "  - Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/title_similarity.png\n"
     ]
    }
   ],
   "source": [
    "# Block 6.0: Analyze Metadata Differences (TSV vs JSON) - Enhanced Reporting\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import difflib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYZING METADATA DIFFERENCES & GENERATING REPORT (Block 6.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Paths\n",
    "tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "\n",
    "# Create Report Folder\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "report_folder = f\"Metadata_Analysis_Reports/Report_{timestamp}\"\n",
    "\n",
    "# Field mapping\n",
    "field_map = {\n",
    "    'publication/title': 'Title',\n",
    "    'publication/authors': 'Authors',\n",
    "    'publication/journal': 'Journal',\n",
    "    'publication/year': 'Year',\n",
    "    'publication/doi': 'DOI'\n",
    "}\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(report_folder):\n",
    "        os.makedirs(report_folder)\n",
    "        print(f\"Created report directory: {report_folder}\")\n",
    "\n",
    "    if os.path.exists(tsv_path) and os.path.exists(json_folder):\n",
    "        print(\"Loading TSV...\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        # Metrics\n",
    "        total_jsons = 0\n",
    "        matched_jsons = 0\n",
    "        diff_counts = {k: 0 for k in field_map.keys()}\n",
    "        \n",
    "        # Title specific\n",
    "        title_stats = {\n",
    "            'exact': 0,\n",
    "            'case_only': 0,\n",
    "            'minor_diffs': 0, # > 0.8\n",
    "            'major_diffs': 0  # < 0.8\n",
    "        }\n",
    "        \n",
    "        # Detailed mismatch logs\n",
    "        major_title_diffs = []\n",
    "        all_field_mismatches = [] # store dicts of {pmcid, field, json_val, tsv_val}\n",
    "\n",
    "        print(\"Comparing files...\")\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_jsons = len(json_files)\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            row = df[df['PMCID_clean'] == pmcid]\n",
    "            \n",
    "            if len(row) == 0: continue\n",
    "            matched_jsons += 1\n",
    "            row = row.iloc[0]\n",
    "            \n",
    "            with open(os.path.join(json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            for json_key, tsv_col in field_map.items():\n",
    "                j_val = str(data.get(json_key, \"\")).strip()\n",
    "                t_val = row[tsv_col]\n",
    "                \n",
    "                # Check TSV nan\n",
    "                if pd.isna(t_val): t_val = \"\"\n",
    "                \n",
    "                # Clean Year\n",
    "                if tsv_col == 'Year' and t_val != \"\":\n",
    "                    try: t_val = str(int(float(t_val)))\n",
    "                    except: t_val = str(t_val)\n",
    "                else:\n",
    "                    t_val = str(t_val).strip()\n",
    "                \n",
    "                if j_val != t_val:\n",
    "                    diff_counts[json_key] += 1\n",
    "                    \n",
    "                    # Log mismatch\n",
    "                    all_field_mismatches.append({\n",
    "                        'PMCID': pmcid,\n",
    "                        'Field': json_key,\n",
    "                        'JSON_Value': j_val,\n",
    "                        'TSV_Value': t_val\n",
    "                    })\n",
    "                    \n",
    "                    if json_key == 'publication/title':\n",
    "                         if j_val.lower() == t_val.lower():\n",
    "                            title_stats['case_only'] += 1\n",
    "                         else:\n",
    "                            ratio = difflib.SequenceMatcher(None, j_val, t_val).ratio()\n",
    "                            if ratio > 0.8:\n",
    "                                title_stats['minor_diffs'] += 1\n",
    "                            else:\n",
    "                                title_stats['major_diffs'] += 1\n",
    "                                major_title_diffs.append({\n",
    "                                    'PMCID': pmcid, \n",
    "                                    'JSON': j_val, \n",
    "                                    'TSV': t_val, \n",
    "                                    'Sim': f\"{ratio:.2f}\"\n",
    "                                })\n",
    "                else:\n",
    "                     if json_key == 'publication/title':\n",
    "                        title_stats['exact'] += 1\n",
    "                        \n",
    "        # --- GENERATE OUTPUTS ---\n",
    "        \n",
    "        # 1. Visualization\n",
    "        # Mismatch Counts Bar Chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        keys = list(diff_counts.keys())\n",
    "        vals = list(diff_counts.values())\n",
    "        colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#c2c2f0']\n",
    "        \n",
    "        plt.bar(keys, vals, color=colors[:len(keys)])\n",
    "        plt.title(f'Metadata Mismatches per Field (N={matched_jsons})')\n",
    "        plt.xlabel('Field')\n",
    "        plt.ylabel('Count of Mismatches')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(report_folder, 'mismatch_counts.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Title Breakdown Pie Chart\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        labels = [f\"Exact ({title_stats['exact']})\", \n",
    "                  f\"Case Only ({title_stats['case_only']})\", \n",
    "                  f\"Minor >0.8 ({title_stats['minor_diffs']})\",\n",
    "                  f\"Major <0.8 ({title_stats['major_diffs']})\"]\n",
    "        sizes = [title_stats['exact'], title_stats['case_only'], title_stats['minor_diffs'], title_stats['major_diffs']]\n",
    "        \n",
    "        # Filter zero slices to avoid cluttered legend\n",
    "        pie_data = [(l, s) for l, s in zip(labels, sizes) if s > 0]\n",
    "        if pie_data:\n",
    "            p_labels, p_sizes = zip(*pie_data)\n",
    "            plt.pie(p_sizes, labels=p_labels, autopct='%1.1f%%', startangle=140)\n",
    "            plt.title('Title Similarity Breakdown')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(report_folder, 'title_similarity.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Detailed Report File (Markdown)\n",
    "        report_path = os.path.join(report_folder, 'Analysis_Report.md')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"# Metadata Analysis Report\\n\")\n",
    "            f.write(f\"**Date:** {timestamp}\\n\\n\")\n",
    "            f.write(f\"**Source JSON Folder:** `{json_folder}`\\n\")\n",
    "            f.write(f\"**Source TSV:** `{tsv_path}`\\n\")\n",
    "            f.write(f\"**Total Files Scanned:** {total_jsons}\\n\")\n",
    "            f.write(f\"**Files Matched to TSV:** {matched_jsons}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 1. Field Mismatch Summary\\n\")\n",
    "            f.write(\"| Field | Mismatches | %\\n\")\n",
    "            f.write(\"|---|---|---|\\n\")\n",
    "            for k, v in diff_counts.items():\n",
    "                pct = (v/matched_jsons)*100 if matched_jsons else 0\n",
    "                f.write(f\"| {k} | {v} | {pct:.1f}% |\\n\")\n",
    "            \n",
    "            f.write(\"\\n![Mismatch Counts](mismatch_counts.png)\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 2. Title Similarity Breakdown\\n\")\n",
    "            f.write(f\"- **Exact Matches:** {title_stats['exact']}\\n\")\n",
    "            f.write(f\"- **Case-only Differences:** {title_stats['case_only']}\\n\")\n",
    "            f.write(f\"- **Minor Differences (>80%):** {title_stats['minor_diffs']}\\n\")\n",
    "            f.write(f\"- **Major Differences (<80%):** {title_stats['major_diffs']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n![Title Breakdown](title_similarity.png)\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 3. Major Title Differences (Low Similarity)\\n\")\n",
    "            if major_title_diffs:\n",
    "                f.write(\"| PMCID | JSON Title | TSV Title | Similarity |\\n\")\n",
    "                f.write(\"|---|---|---|---|\\n\")\n",
    "                for m in major_title_diffs:\n",
    "                    # Escape pipes for markdown table\n",
    "                    j_t = m['JSON'].replace('|', '\\|')\n",
    "                    t_t = m['TSV'].replace('|', '\\|')\n",
    "                    f.write(f\"| {m['PMCID']} | {j_t} | {t_t} | {m['Sim']} |\\n\")\n",
    "            else:\n",
    "                f.write(\"No major title differences found.\\n\")\n",
    "                \n",
    "        # 3. CSV export of all mismatches\n",
    "        if all_field_mismatches:\n",
    "            csv_path = os.path.join(report_folder, 'all_mismatches.csv')\n",
    "            pd.DataFrame(all_field_mismatches).to_csv(csv_path, index=False)\n",
    "            print(f\"Detailed mismatch CSV saved to: {csv_path}\")\n",
    "\n",
    "        print(f\"\\nAnalysis Complete!\")\n",
    "        print(f\"Report and Visualizations generated in folder: {report_folder}\")\n",
    "        print(f\"  - {os.path.join(report_folder, 'Analysis_Report.md')}\")\n",
    "        print(f\"  - {os.path.join(report_folder, 'mismatch_counts.png')}\")\n",
    "        print(f\"  - {os.path.join(report_folder, 'title_similarity.png')}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Input files not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e67b6758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING COVERAGE ANALYSIS (Block 9.0)\n",
      "============================================================\n",
      "Created report directory: Coverage_Analysis_Reports/Report_2026-01-15_15-27-46\n",
      "Scanning 1012 files for missing information markers...\n",
      "Scan complete. Generating analysis...\n",
      "\n",
      "Analysis Successfully Completed.\n",
      "Outputs saved to: Coverage_Analysis_Reports/Report_2026-01-15_15-27-46/\n",
      "  - Report: Coverage_Analysis_Report.md\n",
      "  - Plots: Category_Summary.png, Field_Level_Analysis.png\n",
      "  - Data: coverage_stats.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Block 9.0: \"Not Enough Information\" Coverage Analysis\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING COVERAGE ANALYSIS (Block 9.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Setup\n",
    "json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "target_phrase = \"Not enough information is available\" # We'll match this loosely (starts with)\n",
    "\n",
    "# Create Output Folder\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Coverage_Analysis_Reports/Report_{timestamp}\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created report directory: {output_dir}\")\n",
    "\n",
    "# Categories Mapping\n",
    "def get_category(key):\n",
    "    if key.startswith('publication'): return 'Publication'\n",
    "    if key.startswith('dataset'): return 'Data'\n",
    "    if key.startswith('optimization'): return 'Optimisation'\n",
    "    if key.startswith('model'): return 'Model'\n",
    "    if key.startswith('evaluation'): return 'Evaluation'\n",
    "    return 'Other'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(json_folder):\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        print(f\"Scanning {total_files} files for missing information markers...\")\n",
    "        \n",
    "        # Structure to hold counts\n",
    "        # counts[field] = num_missing\n",
    "        field_counts = {}\n",
    "        all_fields = set()\n",
    "        \n",
    "        # 2. Scan Files\n",
    "        for json_file in json_files:\n",
    "            with open(os.path.join(json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            for key, val in data.items():\n",
    "                all_fields.add(key)\n",
    "                if key not in field_counts:\n",
    "                    field_counts[key] = 0\n",
    "                \n",
    "                # Check for target phrase\n",
    "                val_str = str(val).strip()\n",
    "                if val_str.startswith(target_phrase):\n",
    "                    field_counts[key] += 1\n",
    "\n",
    "        print(\"Scan complete. Generating analysis...\")\n",
    "\n",
    "        # 3. Process Data into DataFrame\n",
    "        data_list = []\n",
    "        for key in all_fields:\n",
    "            missing_count = field_counts.get(key, 0)\n",
    "            category = get_category(key)\n",
    "            pct_missing = (missing_count / total_files) * 100\n",
    "            \n",
    "            data_list.append({\n",
    "                'Field': key,\n",
    "                'Category': category,\n",
    "                'Missing_Count': missing_count,\n",
    "                'Total_Files': total_files,\n",
    "                'Missing_Percentage': pct_missing\n",
    "            })\n",
    "            \n",
    "        df_stats = pd.DataFrame(data_list)\n",
    "        \n",
    "        # Sort for consistency\n",
    "        df_stats = df_stats.sort_values(by=['Category', 'Field'])\n",
    "        \n",
    "        # 4. Generate Visualizations\n",
    "        \n",
    "        # A. Main Category Aggregation (Mean % Missing per category)\n",
    "        category_stats = df_stats.groupby('Category')['Missing_Percentage'].mean().reset_index()\n",
    "        # Custom sort order\n",
    "        cat_order = ['Publication', 'Data', 'Optimisation', 'Model', 'Evaluation']\n",
    "        category_stats['Category'] = pd.Categorical(category_stats['Category'], categories=cat_order, ordered=True)\n",
    "        category_stats = category_stats.sort_values('Category')\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(category_stats['Category'], category_stats['Missing_Percentage'], color='#4c72b0')\n",
    "        plt.title('Average Information Gap by Main Category\\n(% of fields marked \"Not enough information is available\")', fontsize=12)\n",
    "        plt.ylabel('Avg. Missing %', fontsize=10)\n",
    "        plt.xlabel('Category', fontsize=10)\n",
    "        plt.ylim(0, 100)\n",
    "        \n",
    "        # Add labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                     f'{height:.1f}%', ha='center', va='bottom')\n",
    "                     \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'Category_Summary.png'), dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        # B. Granular Subfields (Grouped Bar Chart)\n",
    "        # We'll make subplots for each category to ensure readability\n",
    "        unique_cats = [c for c in cat_order if c in df_stats['Category'].unique()]\n",
    "        \n",
    "        # Figure size depends on number of fields\n",
    "        fig, axes = plt.subplots(nrows=len(unique_cats), ncols=1, figsize=(12, 4 * len(unique_cats)), constrained_layout=True)\n",
    "        if len(unique_cats) == 1: axes = [axes] # Handle single category case\n",
    "        \n",
    "        for i, cat in enumerate(unique_cats):\n",
    "            ax = axes[i]\n",
    "            subset = df_stats[df_stats['Category'] == cat].sort_values('Missing_Percentage', ascending=False)\n",
    "            \n",
    "            # Simple barh\n",
    "            y_pos = np.arange(len(subset))\n",
    "            ax.barh(y_pos, subset['Missing_Percentage'], align='center', color='#55a868')\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(subset['Field'])\n",
    "            ax.invert_yaxis() # labels read top-to-bottom\n",
    "            ax.set_xlabel('% Coverage Gap')\n",
    "            ax.set_title(f'Category: {cat}')\n",
    "            ax.set_xlim(0, 100)\n",
    "            \n",
    "            # Add text labels\n",
    "            for j, v in enumerate(subset['Missing_Percentage']):\n",
    "                ax.text(v + 1, j, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "\n",
    "        plt.suptitle(f'Detailed Gap Analysis by Field (Total Files: {total_files})', fontsize=16)\n",
    "        plt.savefig(os.path.join(output_dir, 'Field_Level_Analysis.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # 5. Generate Report Document\n",
    "        report_file = os.path.join(output_dir, 'Coverage_Analysis_Report.md')\n",
    "        \n",
    "        with open(report_file, 'w') as r:\n",
    "            r.write(f\"# Metadata Coverage Analysis Report\\n\")\n",
    "            r.write(f\"**Date:** {timestamp}\\n\")\n",
    "            r.write(f\"**Dataset:** {total_files} JSON files from `{json_folder}`\\n\")\n",
    "            r.write(f\"**Target Phrase:** \\\"{target_phrase}...\\\"\\n\\n\")\n",
    "            \n",
    "            r.write(\"## 1. Executive Summary\\n\")\n",
    "            r.write(\"The following chart shows the average percentage of fields marked as 'Not enough information is available' within each main category.\\n\\n\")\n",
    "            r.write(\"![Category Summary](Category_Summary.png)\\n\\n\")\n",
    "            \n",
    "            r.write(\"## 2. Category Statistics\\n\")\n",
    "            r.write(\"| Category | Avg Missing % | Max Missing Field | Min Missing Field |\\n\")\n",
    "            r.write(\"|---|---|---|---|\\n\")\n",
    "            \n",
    "            for cat in unique_cats:\n",
    "                subset = df_stats[df_stats['Category'] == cat]\n",
    "                avg = subset['Missing_Percentage'].mean()\n",
    "                max_f = subset.loc[subset['Missing_Percentage'].idxmax()]\n",
    "                min_f = subset.loc[subset['Missing_Percentage'].idxmin()]\n",
    "                \n",
    "                r.write(f\"| **{cat}** | {avg:.1f}% | {max_f['Field']} ({max_f['Missing_Percentage']:.1f}%) | {min_f['Field']} ({min_f['Missing_Percentage']:.1f}%) |\\n\")\n",
    "                \n",
    "            r.write(\"\\n## 3. Detailed Field Breakdown\\n\")\n",
    "            r.write(\"![Field Level Analysis](Field_Level_Analysis.png)\\n\\n\")\n",
    "            \n",
    "            r.write(\"### Full Data Table\\n\")\n",
    "            r.write(\"| Category | Field | Missing Count | Missing % |\\n\")\n",
    "            r.write(\"|---|---|---|---|\\n\")\n",
    "            \n",
    "            # Re-sort nicely for table\n",
    "            df_table = df_stats.sort_values(by=['Category', 'Missing_Percentage'], ascending=[True, False])\n",
    "            \n",
    "            for _, row in df_table.iterrows():\n",
    "                r.write(f\"| {row['Category']} | {row['Field']} | {row['Missing_Count']} | {row['Missing_Percentage']:.1f}% |\\n\")\n",
    "        \n",
    "        # 6. Save CSV Data\n",
    "        df_stats.to_csv(os.path.join(output_dir, 'coverage_stats.csv'), index=False)\n",
    "        \n",
    "        print(f\"\\nAnalysis Successfully Completed.\")\n",
    "        print(f\"Outputs saved to: {output_dir}/\")\n",
    "        print(f\"  - Report: Coverage_Analysis_Report.md\")\n",
    "        print(f\"  - Plots: Category_Summary.png, Field_Level_Analysis.png\")\n",
    "        print(f\"  - Data: coverage_stats.csv\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Folder {json_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba70532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FOR MISSING TITLES IN JSON (Block 6.5)\n",
      "============================================================\n",
      "Scanning 1012 JSON files...\n",
      "----------------------------------------\n",
      "Files with title 'Not enough information is available': 25\n",
      "----------------------------------------\n",
      "Files listed:\n",
      "  - PMC8080676.json\n",
      "  - PMC11148103.json\n",
      "  - PMC4368063.json\n",
      "  - PMC3205469.json\n",
      "  - PMC9420706.json\n",
      "  - PMC7988437.json\n",
      "  - PMC7874964.json\n",
      "  - PMC10052279.json\n",
      "  - PMC10365090.json\n",
      "  - PMC6992687.json\n",
      "  - PMC7821214.json\n",
      "  - PMC10785655.json\n",
      "  - PMC9086604.json\n",
      "  - PMC10046420.json\n",
      "  - PMC11140654.json\n",
      "  - PMC10239131.json\n",
      "  - PMC10791584.json\n",
      "  - PMC2846370.json\n",
      "  - PMC11110913.json\n",
      "  - PMC11127166.json\n",
      "  - PMC10235219.json\n",
      "  - PMC6924628.json\n",
      "  - PMC10060474.json\n",
      "  - PMC5685313.json\n",
      "  - PMC9869541.json\n"
     ]
    }
   ],
   "source": [
    "# Block 6.5: Check for \"Not enough information is available\" in Title\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING FOR MISSING TITLES IN JSON (Block 6.5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "json_folder_path = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "target_string = \"Not enough information is available\"\n",
    "missing_title_count = 0\n",
    "missing_title_files = []\n",
    "\n",
    "try:\n",
    "    if os.path.exists(json_folder_path):\n",
    "        json_files = [f for f in os.listdir(json_folder_path) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        \n",
    "        print(f\"Scanning {total_files} JSON files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            file_path = os.path.join(json_folder_path, json_file)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                title = data.get('publication/title', '')\n",
    "                \n",
    "                # Check for specific phrase\n",
    "                if title == target_string:\n",
    "                    missing_title_count += 1\n",
    "                    missing_title_files.append(json_file)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {json_file}: {e}\")\n",
    "                \n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Files with title '{target_string}': {missing_title_count}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if missing_title_count > 0:\n",
    "            print(\"Files listed:\")\n",
    "            for f in missing_title_files:\n",
    "                print(f\"  - {f}\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"Folder not found: {json_folder_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469b3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING UPDATED JSONS IN NEW FOLDER (Block 7.0)\n",
      "============================================================\n",
      "Created directory: Copilot_1000_v0_Processed_2026-01-15_Updated_Metadata\n",
      "Loading TSV reference data...\n",
      "Processing 1012 files...\n",
      "Completed.\n",
      "Total files processed: 1012\n",
      "Files with metadata updates: 1012\n",
      "All files saved to: Copilot_1000_v0_Processed_2026-01-15_Updated_Metadata\n"
     ]
    }
   ],
   "source": [
    "# Block 7.0: Create Updated JSONs with Corrected Metadata\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING UPDATED JSONS IN NEW FOLDER (Block 7.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Paths\n",
    "source_json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "target_json_folder = 'Copilot_1000_v0_Processed_2026-01-15_Updated_Metadata'\n",
    "tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "\n",
    "# Field mapping: JSON key -> TSV column\n",
    "field_map = {\n",
    "    'publication/title': 'Title',\n",
    "    'publication/authors': 'Authors',\n",
    "    'publication/journal': 'Journal',\n",
    "    'publication/year': 'Year',\n",
    "    'publication/doi': 'DOI'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 1. Create target directory\n",
    "    if not os.path.exists(target_json_folder):\n",
    "        os.makedirs(target_json_folder)\n",
    "        print(f\"Created directory: {target_json_folder}\")\n",
    "    else:\n",
    "        print(f\"Directory exists: {target_json_folder}\")\n",
    "\n",
    "    if os.path.exists(tsv_path) and os.path.exists(source_json_folder):\n",
    "        print(\"Loading TSV reference data...\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        updated_files_count = 0\n",
    "        \n",
    "        json_files = [f for f in os.listdir(source_json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        \n",
    "        print(f\"Processing {total_files} files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            \n",
    "            # Find TSV row\n",
    "            row = df[df['PMCID_clean'] == pmcid]\n",
    "            \n",
    "            # Load original JSON\n",
    "            source_path = os.path.join(source_json_folder, json_file)\n",
    "            target_path = os.path.join(target_json_folder, json_file)\n",
    "            \n",
    "            with open(source_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # If we have TSV data, update fields if different\n",
    "            if len(row) > 0:\n",
    "                row = row.iloc[0]\n",
    "                \n",
    "                changes_made = False\n",
    "                \n",
    "                for json_key, tsv_col in field_map.items():\n",
    "                    current_val = str(data.get(json_key, \"\")).strip()\n",
    "                    \n",
    "                    # Prepare new value\n",
    "                    new_val_raw = row[tsv_col]\n",
    "                    if pd.isna(new_val_raw):\n",
    "                        new_val = \"\"\n",
    "                    else:\n",
    "                        if tsv_col == 'Year':\n",
    "                            try:\n",
    "                                new_val = str(int(float(new_val_raw)))\n",
    "                            except:\n",
    "                                new_val = str(new_val_raw)\n",
    "                        else:\n",
    "                            new_val = str(new_val_raw).strip()\n",
    "                    \n",
    "                    # Check difference\n",
    "                    if current_val != new_val:\n",
    "                        data[json_key] = new_val\n",
    "                        changes_made = True\n",
    "                \n",
    "                if changes_made:\n",
    "                    updated_files_count += 1\n",
    "            \n",
    "            # Save to new location (either updated or original copy)\n",
    "            with open(target_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "                \n",
    "        print(f\"Completed.\")\n",
    "        print(f\"Total files processed: {total_files}\")\n",
    "        print(f\"Files with metadata updates: {updated_files_count}\")\n",
    "        print(f\"All files saved to: {target_json_folder}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Source data not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a353c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adafe302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Low Similarity (<=20%) | Similarity: 0.19"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #007acc; font-family: sans-serif;\">\n",
       "                <span style=\"font-weight: bold; margin-right: 10px;\">IDS:</span>\n",
       "                <a href=\"https://europepmc.org/search?query=PMC8350610\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; margin-right: 20px; font-size: 1.1em;\">PMC8350610 ↗</a>\n",
       "                <a href=\"https://europepmc.org/search?query=34401383\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; font-size: 1.1em;\">PMID:34401383 ↗</a>\n",
       "            </div>\n",
       "            <br>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ [Title]\n",
      "  JSON: Internet Interventions 25 (2021) 100424\n",
      "  TSV : Predicting acute suicidal ideation on Instagram using ensemble machine learning models.\n",
      "------------------------------------------------------------\n",
      "❌ [Authors]\n",
      "  JSON: The authors who contributed to this article are Damien Lekkas, Robert J. Klein, and Nicholas C. Jacobson. Damien Lekkas was involved in conceptualization, methodology, software, formal analysis, writing the original draft, reviewing and editing, and visualization. Robert J. Klein contributed to writing the original draft and reviewing and editing. Nicholas C. Jacobson assisted with methodology and formal analysis, as well as reviewing and editing the paper.\n",
      "  TSV : Lekkas D, Klein RJ, Jacobson NC\n",
      "------------------------------------------------------------\n",
      "❌ [Journal]\n",
      "  JSON: Internet Interventions\n",
      "  TSV : Internet interventions\n",
      "------------------------------------------------------------\n",
      "✅ [Year]\n",
      "  2021\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>✅ [DOI]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&nbsp;&nbsp;<a href=\"https://doi.org/10.1016/j.invent.2021.100424\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">10.1016/j.invent.2021.100424 ↗</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <br>\n",
       "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
       "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
       "                        ▶ Show Remaining JSON Data (24 fields)\n",
       "                    </summary>\n",
       "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{\n",
       "  &quot;publication/pmid&quot;: &quot;34401383&quot;,\n",
       "  &quot;publication/pmcid&quot;: &quot;PMC8350610&quot;,\n",
       "  &quot;publication/tags&quot;: &quot;- Suicidal Ideation\\n- Machine Learning\\n- Social Media\\n- Natural Language Processing\\n- Predictive Modeling\\n- Mental Health\\n- Instagram\\n- Risk Factors\\n- Ensemble Learning\\n- Data Mining&quot;,\n",
       "  &quot;dataset/provenance&quot;: &quot;The dataset utilized in this study was derived from a previous investigation that explored the connections between acute suicidality, language use, and Instagram activity. Specifically, a subset of German adolescents was selected from a larger study that examined non-suicidal self-injury on Instagram. Public Instagram user data and post content were collected from these subjects over a four-week period prior to a personal interview conducted via Instagram messenger.\\n\\nThe dataset consists of a randomized subset of 52 study participants. The participants had a mean age of 16.6 years, with a median age of 16 years. The majority of the participants were female, accounting for 78.8% of the sample. Most participants were attending high school (76.9%), while a smaller portion were attending university or professional school (13.5%), and a few were unemployed (3.8%). All participants reported a lifetime history of suicidal ideation.\\n\\nThe data collection period of four weeks was chosen because it provided a sufficient amount of data for machine learning models to detect patterns, while also being recent enough to be relevant to the participants&#x27; current suicidal ideation. This dataset was used to build upon previous research, aiming to improve the prediction of acute suicidal ideation using machine learning techniques.&quot;,\n",
       "  &quot;dataset/splits&quot;: &quot;The dataset was analyzed using a ten-fold repeated cross-validation framework. This means that the data was split into ten different subsets, or folds. Each fold was used once as a validation set while the remaining nine folds formed the training set. This process was repeated ten times, with each of the ten folds used exactly once as the validation data.\\n\\nThe distribution of data points in each split was balanced, ensuring that each fold contained an approximately equal number of data points. This approach helps to ensure that the model&#x27;s performance is evaluated on a diverse range of data, reducing the risk of overfitting and providing a more robust estimate of the model&#x27;s generalizability.\\n\\nThe specific number of data points in each split was not explicitly stated, but the use of ten-fold cross-validation implies that the data was divided into ten roughly equal parts. This method is commonly used to provide a comprehensive evaluation of model performance by ensuring that each data point is used for both training and validation across different iterations.&quot;,\n",
       "  &quot;dataset/redundancy&quot;: &quot;In our study, we utilized a dataset consisting of a randomized subset of 52 participants, with a mean age of 16.6 years, predominantly female (78.8%), and mostly attending high school (76.9%). The dataset included both interview data and Instagram activity data collected over a four-week period prior to the interviews. This timeframe was chosen to balance the need for sufficient data to detect patterns while ensuring the relevance of the data to the participants&#x27; current mental state.\\n\\nTo ensure the robustness and generalizability of our models, we employed a repeated, ten-fold cross-validation framework. This approach involves splitting the dataset into ten subsets, or folds, and training the model on nine of these folds while testing it on the remaining fold. This process is repeated ten times, with each fold serving as the test set once. This method ensures that every data point is used for both training and testing, providing a more reliable estimate of the model&#x27;s performance.\\n\\nThe cross-validation framework also enforces independence between the training and test sets in each iteration. This is crucial for evaluating the model&#x27;s ability to generalize to unseen data, which is a key aspect of our study&#x27;s hypotheses. By using this approach, we aimed to address the potential overfitting issues observed in previous in-sample analyses, where the same data was used for both training and testing.\\n\\nRegarding the distribution of our dataset, it is important to note that it is characterized by a small sample size and a sparse set of informative predictors, which is typical for studies involving sensitive topics like suicidal ideation. However, the variables used in our models are generalizable to other online settings, suggesting that our approach could be effective with larger, more feature-rich datasets across different social media platforms. The specific detection of acute suicidal ideation among individuals with a lifetime history of suicidal ideation adds a layer of complexity to our classification task, but it also highlights the potential utility of our ensemble approach in similar contexts.&quot;,\n",
       "  &quot;dataset/availability&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;optimization/algorithm&quot;: &quot;The machine-learning algorithm class used in this study is ensemble learning, specifically a consensus ensemble model. This approach combines the predictions from multiple individual models to improve overall performance. The individual models used include Extreme Gradient Boosted Trees (xgboost), boosted logistic decision trees (logitboost), generalized linear models via penalized maximum likelihood (glmnet), k-nearest neighbors (knn), feed-forward neural networks (nnet), aggregated and averaged random seed neural nets (avnnet), and a naive Bayes classifier (naiveBayes).\\n\\nThese algorithms are not new; they are well-established in the field of machine learning. The choice to use these specific algorithms was driven by their proven effectiveness in various predictive tasks. The ensemble approach was selected to leverage the strengths of multiple models, thereby enhancing the predictive accuracy and robustness of the final consensus model.\\n\\nThe focus of this study is on the application of these machine-learning techniques to predict acute suicidal ideation using Instagram activity and language use data. The algorithms were chosen for their ability to handle complex datasets and provide reliable predictions. The results demonstrate that the ensemble approach significantly improves predictive performance compared to traditional logistic regression models.\\n\\nThe decision to publish this work in a journal focused on internet interventions rather than a machine-learning journal is due to the specific application and context of the study. The primary goal is to highlight the potential of machine learning in predicting acute suicidal ideation using social media data, which is a critical area of research in mental health and internet interventions. The study aims to contribute to the development of more effective tools for identifying individuals at risk of acute suicidal thoughts, leveraging the unique insights provided by social media activity.&quot;,\n",
       "  &quot;optimization/meta&quot;: &quot;The model employed in this study is indeed a meta-predictor, leveraging data from multiple machine-learning algorithms as input. Specifically, the prediction probabilities from seven different lower-level models were used as features for the meta-predictor. These models include logitboost, a generalized linear model via penalized maximum likelihood (glmnet), k-nearest neighbors (knn), a three-layer feed-forward neural network (nnet), aggregated and averaged random seed neural nets (avnnet), and a naive Bayes classifier (naiveBayes).\\n\\nThe meta-predictor itself consists of five ensemble learning models: xgboost, logitboost, knn, nnet, and avnnet. Each of these models was run within a ten-fold repeated, cross-validated framework with grid search hyperparameter tuning for maximum accuracy. The final consensus prediction for the acute suicidal ideation binary classification task was obtained by averaging the predictions across these five stacked ensemble models.\\n\\nRegarding the independence of the training data, the use of a ten-fold repeated, cross-validated framework ensures that the data used for training and validation is independent at each fold. This approach helps to mitigate data leakage and overestimation of model performance, providing a more robust evaluation of the model&#x27;s predictive accuracy. The independence of the training data is further supported by the fact that no hyperparameter tuning was performed on the seven lower-level models, with the exception of glmnet, which followed hyperparameter recommendations from a separate meta-analytical study. This ensures that the lower-level models were not overfitted to the training data, maintaining the independence of the data used for the meta-predictor.&quot;,\n",
       "  &quot;optimization/encoding&quot;: &quot;In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms could effectively learn from the data. We began by collecting a total of 15 features, which included both linguistic text analysis features and Instagram-specific metadata. The linguistic features were derived from the average sentence length and the average number of syllables per word, yielding six features in total. Additionally, we gathered Instagram user-specific metadata, including the number of total followers, number following, number of pictures posted within the last month, average number of comments per picture within the last month, and average number of likes per picture within the past month.\\n\\nTo enhance the capture of user activity on Instagram, we implemented additional feature engineering. This involved creating four new features: follow ratio, engagement, sum of average comments and average likes per follower, and average comments-to-average-likes ratio. These engineered features provided a more holistic view of user behavior on the platform.\\n\\nPrior to model training, all 15 features were standardized to have a mean of 0 and a standard deviation of 1. This standardization process ensured that each feature contributed equally to the model, preventing any single feature from dominating due to its scale. Subjects for whom Instagram user data was not available were removed from the analysis, resulting in a clean and consistent dataset.\\n\\nThe preprocessing steps were essential for preparing the data for the machine-learning pipeline, which was built and run in R using the caret package. This pipeline included a variety of models, such as Extreme Gradient Boosted Trees, boosted logistic decision trees, generalized linear models via penalized maximum likelihood, k-nearest neighbors, neural networks, and naive Bayes classifiers. The standardized and engineered features served as inputs to these models, enabling them to make accurate predictions about the presence or absence of acute suicidal thoughts.&quot;,\n",
       "  &quot;optimization/parameters&quot;: &quot;In our study, we utilized a total of 15 features as input parameters for our models. These features encompassed both linguistic text analysis variables and Instagram-specific metadata. The linguistic features were derived from interview transcripts, focusing on aspects like negative emotion and readability indices. The Instagram features included metrics such as the number of followers, following, posts, comments, and likes, along with additional engineered features like follow ratio, engagement, and comments-to-likes ratio.\\n\\nThe selection of these parameters was guided by previous research and the goal of capturing a comprehensive profile of user activity and linguistic patterns. No explicit feature selection method was employed beyond this initial choice, as the machine learning pipeline was designed to evaluate the predictive utility of all included features. The models were built and run using the caret package in R, ensuring a systematic and reproducible approach to parameter handling.&quot;,\n",
       "  &quot;optimization/features&quot;: &quot;In the optimization process, a total of 15 features were used as input. These features encompassed both linguistic text analysis features and Instagram-specific metadata. The linguistic features were derived from the average sentence length and the average number of syllables per word. The Instagram features included the number of total followers, number following, number of pictures posted within the last month, average number of comments per picture within the last month, and average number of likes per picture within the past month.\\n\\nAdditionally, four derived features were engineered from the Instagram data to capture user activity more holistically. These included the follow ratio, engagement, the sum of average comments and average likes per follower, and the average comments-to-average-likes ratio.\\n\\nFeature selection was performed using a stepwise logistic regression model within a repeated, ten-fold cross-validated framework. This approach ensured that the selection process was done using the training set only, thereby mitigating the risk of data leakage and overfitting. The final model included only the most significant features, namely negative emotion in interviews and the number of followers on Instagram. This rigorous selection process helped in identifying the most relevant predictors for acute suicidal ideation.&quot;,\n",
       "  &quot;optimization/fitting&quot;: &quot;In our study, we employed a variety of machine learning models to predict acute suicidal ideation, ensuring that we addressed both overfitting and underfitting concerns. We utilized seven different models, including decision tree-based methods like Extreme Gradient Boosted Trees (xgboost) and boosted logistic decision trees (logitboost), as well as other classifiers such as k-nearest neighbors (knn), neural networks (nnet and avnnet), and a naive Bayes classifier (naiveBayes). These models were run at default hyperparameter values to mitigate data leakage and overestimation of model performance due to hyperparameter tuning.\\n\\nTo further enhance the robustness of our predictions, we implemented an ensemble learning approach. The prediction probabilities from each of the seven lower-level models were used as features for five ensemble learning models: xgboost, logitboost, knn, nnet, and avnnet. Each of these ensemble models was run within a ten-fold repeated, cross-validated framework with grid search hyperparameter tuning for maximum accuracy. This cross-validation technique helped in ensuring that our models generalized well to unseen data, thereby ruling out overfitting.\\n\\nThe final consensus prediction was obtained by averaging the predictions across these five stacked ensemble models. This ensemble approach not only improved the predictive performance but also helped in mitigating the risk of underfitting by leveraging the strengths of multiple models.\\n\\nAdditionally, we used the SHAP (SHapley Additive exPlanations) framework to explain the predictions of our consensus ensemble model. SHAP values provided insights into the relative influence of each feature, ensuring that our models were not underfitting by capturing the important predictors effectively.\\n\\nIn summary, our approach involved using a diverse set of models, cross-validation techniques, and ensemble learning to balance the trade-off between overfitting and underfitting, resulting in a robust and generalizable predictive model for acute suicidal ideation.&quot;,\n",
       "  &quot;optimization/regularization&quot;: &quot;In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach was the use of cross-validation, specifically a ten-fold repeated cross-validation framework. This method helps to assess the model&#x27;s performance on different subsets of the data, providing a more reliable estimate of its generalization capability.\\n\\nAdditionally, we utilized regularization techniques within our models. For instance, the generalized linear model via penalized maximum likelihood (glmnet) incorporates regularization paths to prevent overfitting by adding a penalty to the loss function. This helps in reducing the complexity of the model and improving its ability to generalize to new data.\\n\\nFurthermore, we avoided hyperparameter tuning for the seven lower-level models to mitigate data leakage and overestimation of model performance. This decision was made to ensure that the models were not overly tailored to the specific dataset, thereby enhancing their generalizability.\\n\\nIn the meta-layer of stacked models, we employed grid search hyperparameter tuning within the cross-validated framework. This approach helps in finding the optimal hyperparameters that minimize overfitting while maximizing model accuracy.\\n\\nOverall, these techniques collectively contributed to the prevention of overfitting and ensured that our models were robust and generalizable.&quot;,\n",
       "  &quot;optimization/config&quot;: &quot;In our study, we focused on ensuring transparency and reproducibility by detailing the configurations and parameters used in our machine learning models. The hyper-parameter configurations for the seven lower-level models were primarily set to their default package values, with the exception of the generalized linear model via penalized maximum likelihood (glmnet), which followed recommendations from a separate meta-analytical study. This approach was chosen to mitigate data leakage and overestimation of model performance due to hyper-parameter tuning.\\n\\nFor the ensemble learning models, we employed a ten-fold repeated, cross-validated framework with grid search hyper-parameter tuning for maximum accuracy using the automatic grid search feature in caret. This method ensured that each of the five ensemble models (xgboost, logitboost, knn, nnet, and avnnet) was optimized within a robust validation scheme.\\n\\nThe specific model files and optimization parameters are not explicitly provided in the text, but the methods and packages used (such as caret, glmnet, and xgboost) are well-documented and publicly available. Researchers can replicate our findings by following the described procedures and using the specified packages in R. The use of standard packages and well-established methods ensures that the configurations and optimization schedules are accessible and can be implemented by others in the field.\\n\\nThe data and code used in this study are not explicitly mentioned as being available under a specific license, but the methodologies and tools employed are standard in the machine learning community. Researchers interested in replicating or building upon our work can refer to the cited literature and use the same packages and techniques described. This approach promotes transparency and facilitates further research in the prediction of acute suicidal ideation using machine learning techniques.&quot;,\n",
       "  &quot;model/interpretability&quot;: &quot;Machine learning models have often been criticized for their lack of transparency, often referred to as \\&quot;black-box\\&quot; models. This opacity makes it challenging to understand how these models arrive at their predictions. However, recent advancements have addressed this limitation, providing methods to explain model predictions at both global and local levels.\\n\\nOne such method is SHAP (SHapley Additive exPlanations), which is based on Shapley values from game theory. SHAP equates feature values in a prediction task to players in a cooperative game, calculating each feature&#x27;s contribution to the prediction. This results in values that indicate the relative influence of features on prediction outcomes.\\n\\nIn our work, we utilized the SHAP framework to decompose the ensemble consensus machine learning model. This allowed us to investigate the relative influence of each variable used to predict the status of acute suicidal ideation. The SHAP analysis provided notable insights, revealing that four of the top five most influential predictors were associated with social media use behavior rather than interview-related linguistic content. This highlights the potential benefits of leveraging discrete behaviors such as \\&quot;liking\\&quot; and \\&quot;following\\&quot; in addition to natural language processing strategies.\\n\\nThe SHAP values were visualized using the SHAPforxgboost R package, offering a clear and intuitive representation of feature importance. This visualization helps in understanding which features are most influential in predicting acute suicidal ideation, thereby enhancing the transparency of the model.\\n\\nMoreover, the SHAP framework is model-agnostic, meaning it can be applied across various model types, including linear, tree-based, and neural network models. This versatility makes it a powerful tool for interpreting complex machine learning models.\\n\\nIn summary, while traditional machine learning models have suffered from a lack of transparency, methods like SHAP offer a way to explain model predictions. By using SHAP, we were able to gain insights into the features that most influence the prediction of acute suicidal ideation, making our model more interpretable and transparent.&quot;,\n",
       "  &quot;model/output&quot;: &quot;The model developed is a classification model. Specifically, it is designed to predict the presence or absence of acute suicidal thoughts, which is a binary classification task. The model leverages various features, including baseline and derived features from Instagram activity and language use, to make these predictions. The performance of the model is evaluated using metrics such as accuracy, AUC, sensitivity, and specificity, which are commonly used in classification tasks. The final consensus model achieved an accuracy of 70.2% and an AUC of 0.755, indicating its effectiveness in classifying individuals with acute suicidal ideation. The model&#x27;s predictions are based on an ensemble of different machine learning algorithms, including decision tree-based methods, neural networks, and probabilistic classifiers, which work together to improve predictive performance.&quot;,\n",
       "  &quot;model/duration&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;model/availability&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;evaluation/method&quot;: &quot;The evaluation method employed in this study was designed to ensure statistical rigor and generalizability. Initially, a ten-fold repeated cross-validation framework was used to recapitulate the logistic regression model from previous research. This approach helped to assess the model&#x27;s predictive performance more accurately than the original in-sample analysis.\\n\\nFor the machine learning models, a similar ten-fold repeated cross-validation strategy was applied. This involved splitting the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process was repeated ten times, with each subset serving as the validation set once. This method helps to mitigate overfitting and provides a more reliable estimate of the model&#x27;s performance.\\n\\nIn addition to accuracy, other metrics such as the Kappa score, Area Under the Receiver Operating Characteristic Curve (AUROC), specificity, sensitivity, and F1 score were reported. These metrics offer a comprehensive view of the model&#x27;s performance, including its ability to correctly identify both positive and negative cases.\\n\\nThe consensus ensemble model, which averaged predictions from five different ensemble models, was also evaluated using the same cross-validation framework. This approach aimed to leverage the strengths of multiple models to improve overall predictive performance.\\n\\nThe results indicated that the ensemble machine learning approach achieved a final accuracy of 70.2%, which is comparable to the previously reported in-sample analysis accuracy of 69.0% and significantly higher than the out-of-sample logistic regression approach. The AUROC of 0.755 further demonstrated the model&#x27;s ability to discriminate between individuals with acute suicidal ideation and those without.\\n\\nOverall, the evaluation method focused on ensuring that the models were robust, generalizable, and capable of providing reliable predictions in an out-of-sample paradigm. The use of cross-validation and multiple performance metrics helped to validate the effectiveness of the machine learning approach in predicting acute suicidal ideation.&quot;,\n",
       "  &quot;evaluation/measure&quot;: &quot;In the evaluation of our models, several performance metrics were reported to provide a comprehensive assessment of their predictive capabilities. The primary metrics included accuracy, Kappa, the Area Under the Receiver Operating Characteristic Curve (AUROC), specificity, sensitivity (or recall), and the F1 score. These metrics were chosen to offer a well-rounded view of model performance, covering aspects such as overall correctness, agreement beyond chance, the trade-off between true positive and false positive rates, and the balance between precision and recall.\\n\\nAccuracy measures the proportion of correctly predicted instances out of the total instances. It provides a straightforward measure of how often the model is correct. Kappa, on the other hand, adjusts accuracy for the agreement that could be expected by chance, offering a more robust measure of model performance, especially in imbalanced datasets.\\n\\nThe AUROC is a critical metric that evaluates the model&#x27;s ability to distinguish between the positive and negative classes across all possible classification thresholds. It provides a single scalar value that summarizes the model&#x27;s performance across all thresholds, making it a valuable metric for comparing different models.\\n\\nSpecificity and sensitivity are complementary metrics that focus on the model&#x27;s performance in predicting the negative and positive classes, respectively. Specificity measures the proportion of true negatives correctly identified, while sensitivity (or recall) measures the proportion of true positives correctly identified. These metrics are particularly important in the context of predicting acute suicidal ideation, where the costs of false positives and false negatives may differ significantly.\\n\\nThe F1 score is the harmonic mean of precision and recall, providing a single metric that balances these two important aspects of model performance. Precision measures the proportion of true positives among the predicted positives, while recall (or sensitivity) measures the proportion of true positives among the actual positives. The F1 score is especially useful when dealing with imbalanced datasets, as it provides a more nuanced view of model performance than accuracy alone.\\n\\nThe reported metrics are representative of those commonly used in the literature for evaluating machine learning models, particularly in the context of classification tasks. They provide a comprehensive view of model performance, covering aspects such as overall correctness, agreement beyond chance, the trade-off between true positive and false positive rates, and the balance between precision and recall. This set of metrics allows for a thorough evaluation of the models&#x27; predictive capabilities and facilitates comparison with other studies in the field.&quot;,\n",
       "  &quot;evaluation/comparison&quot;: &quot;A comparison to simpler baselines was indeed performed. The research began by replicating the logistic regression model from previous studies by Brown et al. (2019a) and Brown et al. (2019b). This replication was done using a ten-fold repeated cross-validation framework, which is a more rigorous statistical approach than the original in-sample analysis. The predictive performance of this baseline model was found to be lower than what was previously reported, with an accuracy of 55.6% and an AUC of 0.560.\\n\\nIn addition to this baseline comparison, the study also employed a machine learning approach using ensemble models. These models utilized the output from seven lower-level models as predictors. The ensemble models included Extreme Gradient Boosted Trees, boosted logistic decision trees, generalized linear models via penalized maximum likelihood, k-nearest neighbors, feed-forward neural networks, aggregated and averaged random seed neural nets, and a naive Bayes classifier.\\n\\nThe performance of these ensemble models was evaluated and compared to the baseline model. The neural net-based models performed the best among the ensemble models, while the k-nearest neighbors algorithm performed the worst. The final consensus predictions, which averaged the output predictions from the five ensemble models, achieved a superior accuracy of 70.2% and an AUC of 0.755. This ensemble machine learning approach demonstrated better predictive performance than the baseline model.\\n\\nThe comparison to simpler baselines and the use of ensemble models provided a comprehensive evaluation of the predictive performance for acute suicidal ideation. The results highlighted the advantages of using more complex machine learning models over traditional logistic regression in this context.&quot;,\n",
       "  &quot;evaluation/confidence&quot;: &quot;The evaluation of the models in this study involved a rigorous cross-validation approach to ensure the robustness and generalizability of the results. The performance metrics reported include accuracy, Kappa score, AUC (Area Under the Receiver Operating Characteristic Curve), specificity, sensitivity/recall, and F1 score. These metrics were derived from a ten-fold repeated cross-validation framework, which helps in assessing the model&#x27;s performance across different subsets of the data.\\n\\nThe consensus ensemble model, which averaged the predictions from five different ensemble models, achieved an accuracy of 70.2%, an AUC of 0.755, and an F1 score of 0.741. These metrics indicate a statistically significant improvement in predictive performance compared to the baseline logistic regression model, which had an accuracy of approximately 54% and an AUC of 0.560. The improvement in the AUC from 0.560 to 0.755 is particularly noteworthy, as it reflects a better balance between sensitivity and specificity.\\n\\nThe statistical significance of the results was assessed, and the improvement in model predictive ability was found to be statistically significant (p &lt; 0.05). This significance level provides confidence that the observed improvements are not due to random chance but rather reflect a genuine enhancement in the model&#x27;s performance.\\n\\nWhile specific confidence intervals for the performance metrics were not explicitly mentioned, the use of repeated cross-validation and the reported statistical significance suggest a high level of confidence in the results. The cross-validation process helps in estimating the variability of the performance metrics and ensures that the models are not overfitting to the training data. The significant improvement in AUC and other metrics further supports the claim that the ensemble approach is superior to the baseline logistic regression model.&quot;,\n",
       "  &quot;evaluation/availability&quot;: &quot;Not enough information is available.&quot;\n",
       "}</pre>\n",
       "                </details>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Run cell again [Ctrl+Enter] to flip category next time)\n"
     ]
    }
   ],
   "source": [
    "# Block 8.0: Manual Visual Inspection Interface (Alternating High/Low Sim)\n",
    "# Run this cell repeatedly (Ctrl+Enter) to view entries.\n",
    "# It will alternate between High Similarity mismatches (>=80%) and Low Similarity (<=20%).\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import difflib\n",
    "import random\n",
    "import html\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Global toggle for alternation (persists across cell runs)\n",
    "# Initialize only if not present\n",
    "if 'inspection_mode_high' not in globals():\n",
    "    inspection_mode_high = True # True=High, False=Low\n",
    "\n",
    "source_json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(tsv_path) and os.path.exists(source_json_folder):\n",
    "        # Load Data\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        # Helper to clean numeric strings\n",
    "        def clean_val(v):\n",
    "            if pd.isna(v) or v == '': return \"\"\n",
    "            try: return str(int(float(v)))\n",
    "            except: return str(v).strip()\n",
    "\n",
    "        high_sim = [] \n",
    "        low_sim = []\n",
    "        \n",
    "        json_files = [f for f in os.listdir(source_json_folder) if f.endswith('.json')]\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            row = df[df['PMCID_clean'] == pmcid]\n",
    "            if len(row) == 0: continue\n",
    "            row = row.iloc[0]\n",
    "            \n",
    "            with open(os.path.join(source_json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            j_title = str(data.get('publication/title', \"\")).strip()\n",
    "            t_title = str(row['Title']).strip() if pd.notna(row['Title']) else \"\"\n",
    "            \n",
    "            if j_title != t_title:\n",
    "                ratio = difflib.SequenceMatcher(None, j_title, t_title).ratio()\n",
    "                \n",
    "                entry = {\n",
    "                    'pmcid': pmcid,\n",
    "                    'json': data,\n",
    "                    'tsv': row,\n",
    "                    'ratio': ratio\n",
    "                }\n",
    "                \n",
    "                if ratio >= 0.8: high_sim.append(entry)\n",
    "                elif ratio <= 0.2: low_sim.append(entry)\n",
    "\n",
    "        # Toggle Selection Logic\n",
    "        target_pool = []\n",
    "        mode_str = \"\"\n",
    "        \n",
    "        # Try to respect toggle, but fallback if one pool is empty\n",
    "        if inspection_mode_high:\n",
    "            if high_sim: \n",
    "                target_pool = high_sim\n",
    "                mode_str = \"High Similarity (>=80%)\"\n",
    "            elif low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Low Similarity (<=20%) [High list empty]\"\n",
    "        else:\n",
    "            if low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Low Similarity (<=20%)\"\n",
    "            elif high_sim:\n",
    "                target_pool = high_sim\n",
    "                mode_str = \"High Similarity (>=80%) [Low list empty]\"\n",
    "        \n",
    "        # Flip toggle for next run\n",
    "        inspection_mode_high = not inspection_mode_high\n",
    "        \n",
    "        if not target_pool:\n",
    "            print(\"No mismatches found in either category.\")\n",
    "        else:\n",
    "            item = random.choice(target_pool)\n",
    "            \n",
    "            # Prepare IDs for Link\n",
    "            curr_pmcid = item['pmcid']\n",
    "            curr_pmid = clean_val(item['tsv']['PMID'])\n",
    "            \n",
    "            # --- DISPLAY SECTION ---\n",
    "            \n",
    "            # Using display() ensures rich output isn't hidden/truncated easily by text buffer limits\n",
    "            display(Markdown(f\"### {mode_str} | Similarity: {item['ratio']:.2f}\"))\n",
    "            \n",
    "            # Create HTML links to Europe PMC Search\n",
    "            url_pmcid = f\"https://europepmc.org/search?query={curr_pmcid}\"\n",
    "            url_pmid = f\"https://europepmc.org/search?query={curr_pmid}\" if curr_pmid else \"#\"\n",
    "            \n",
    "            # Render Links\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #007acc; font-family: sans-serif;\">\n",
    "                <span style=\"font-weight: bold; margin-right: 10px;\">IDS:</span>\n",
    "                <a href=\"{url_pmcid}\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; margin-right: 20px; font-size: 1.1em;\">{curr_pmcid} ↗</a>\n",
    "                <a href=\"{url_pmid}\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; font-size: 1.1em;\">PMID:{curr_pmid} ↗</a>\n",
    "            </div>\n",
    "            <br>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Comparison Loop\n",
    "            fields = [\n",
    "                ('Title', 'publication/title', 'Title'),\n",
    "                ('Authors', 'publication/authors', 'Authors'),\n",
    "                ('Journal', 'publication/journal', 'Journal'),\n",
    "                ('Year', 'publication/year', 'Year'),\n",
    "                ('DOI', 'publication/doi', 'DOI')\n",
    "            ]\n",
    "            \n",
    "            for label, k_json, k_tsv in fields:\n",
    "                v_json = str(item['json'].get(k_json, \"\")).strip()\n",
    "                v_tsv = item['tsv'][k_tsv]\n",
    "                \n",
    "                # Special clean for display\n",
    "                if k_tsv == 'Year': v_tsv = clean_val(v_tsv)\n",
    "                else: v_tsv = str(v_tsv).strip() if pd.notna(v_tsv) else \"\"\n",
    "                \n",
    "                match = v_json == v_tsv\n",
    "                symbol = \"✅\" if match else \"❌\"\n",
    "                \n",
    "                if label == 'DOI':\n",
    "                    # Special HTML handling for clickable DOI\n",
    "                    def make_doi_link(v):\n",
    "                        if not v: return \"<em>(empty)</em>\"\n",
    "                        # Simple cleanup if formatted strangely, but usually just the DOI string\n",
    "                        return f'<a href=\"https://doi.org/{v}\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">{v} ↗</a>'\n",
    "                    \n",
    "                    display(HTML(f\"<strong>{symbol} [{label}]</strong>\"))\n",
    "                    if not match:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;JSON: {make_doi_link(v_json)}\"))\n",
    "                        display(HTML(f\"&nbsp;&nbsp;TSV : {make_doi_link(v_tsv)}\"))\n",
    "                    else:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;{make_doi_link(v_json)}\"))\n",
    "                    print(\"-\" * 60)\n",
    "                else:\n",
    "                    # Use print for content to avoid HTML rendering issues with weird chars\n",
    "                    print(f\"{symbol} [{label}]\")\n",
    "                    if not match:\n",
    "                        print(f\"  JSON: {v_json}\")\n",
    "                        print(f\"  TSV : {v_tsv}\")\n",
    "                    else:\n",
    "                        print(f\"  {v_json}\")\n",
    "                    print(\"-\" * 60)\n",
    "            \n",
    "            # --- EXPANDABLE REST OF DATA ---\n",
    "            # Identify keys already shown\n",
    "            shown_keys = [f[1] for f in fields]\n",
    "            \n",
    "            # Collect remaining data\n",
    "            remaining_data = {k: v for k, v in item['json'].items() if k not in shown_keys}\n",
    "            \n",
    "            if remaining_data:\n",
    "                # Format to JSON string and escape for HTML\n",
    "                json_str = json.dumps(remaining_data, indent=2)\n",
    "                safe_json_str = html.escape(json_str)\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                <br>\n",
    "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
    "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
    "                        ▶ Show Remaining JSON Data ({len(remaining_data)} fields)\n",
    "                    </summary>\n",
    "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{safe_json_str}</pre>\n",
    "                </details>\n",
    "                \"\"\"))\n",
    "            \n",
    "            print(\"\\n(Run cell again [Ctrl+Enter] to flip category next time)\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Files not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2a8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "785a5562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING UNIFIED ANALYSIS ON REGISTRY DATASET (Copilot_v0_Processed_2025-12-04)\n",
      "================================================================================\n",
      "Output Directory: Registry_Analysis_Reports/Report_2026-01-15_16-42-24\n",
      "Loading metadata from: DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv\n",
      "Dropped 10 duplicate PMCIDs from metadata.\n",
      "Loaded 280 rows. Found 237 unique mapped PMCIDs.\n",
      "Found 231 JSON files in Copilot_v0_Processed_2025-12-04/registry_v0\n",
      "Processing files...\n",
      "Processing complete.\n",
      "Analysis Complete.\n",
      "Report saved to: Registry_Analysis_Reports/Report_2026-01-15_16-42-24/Registry_Analysis_Report.md\n"
     ]
    }
   ],
   "source": [
    "# Block 10.0: Unified Analysis on Copilot_v0 (Older Batch) with Registry Metadata\n",
    "# This block repeats:\n",
    "# 1. Title Mismatch Analysis\n",
    "# 2. \"No Information\" Coverage Analysis & Visualisation\n",
    "# 3. Uses 'PMCIDs_DOME_Registry_Contents_2026-01-09.tsv' because it contains 'mapped_pmcid'\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import difflib\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING UNIFIED ANALYSIS ON REGISTRY DATASET (Copilot_v0_Processed_2025-12-04)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Updated path to point to the inner directory containing JSONs\n",
    "json_folder = 'Copilot_v0_Processed_2025-12-04/registry_v0'\n",
    "# User requested 'flattened_...' but that file lacks 'mapped_pmcid'. \n",
    "# Using 'PMCIDs_...' which contains the mapping column.\n",
    "tsv_path = 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Registry_Analysis_Reports/Report_{timestamp}\"\n",
    "target_phrase = \"Not enough information is available\"\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output Directory: {output_dir}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_category(key):\n",
    "    if key.startswith('publication'): return 'Publication'\n",
    "    if key.startswith('dataset'): return 'Data'\n",
    "    if key.startswith('optimization'): return 'Optimisation'\n",
    "    if key.startswith('model'): return 'Model'\n",
    "    if key.startswith('evaluation'): return 'Evaluation'\n",
    "    return 'Other'\n",
    "\n",
    "try:\n",
    "    # 1. Load Metadata\n",
    "    print(f\"Loading metadata from: {tsv_path}\")\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # Filter for rows that have a mapped PMCID\n",
    "    df_mapped = df[df['mapped_pmcid'].notna()].copy()\n",
    "    df_mapped['clean_pmcid'] = df_mapped['mapped_pmcid'].apply(lambda x: str(x).strip())\n",
    "    \n",
    "    # Handle duplicates by taking the first occurrence\n",
    "    initial_len = len(df_mapped)\n",
    "    df_mapped = df_mapped.drop_duplicates(subset=['clean_pmcid'], keep='first')\n",
    "    if len(df_mapped) < initial_len:\n",
    "        print(f\"Dropped {initial_len - len(df_mapped)} duplicate PMCIDs from metadata.\")\n",
    "    \n",
    "    # Create lookup dictionary\n",
    "    meta_lookup = df_mapped.set_index('clean_pmcid').to_dict('index')\n",
    "    print(f\"Loaded {len(df)} rows. Found {len(df_mapped)} unique mapped PMCIDs.\")\n",
    "\n",
    "    # 2. Scan JSON Files\n",
    "    if os.path.exists(json_folder):\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        print(f\"Found {total_files} JSON files in {json_folder}\")\n",
    "        \n",
    "        if total_files == 0:\n",
    "            print(\"Warning: No JSON files found. Checking path...\")\n",
    "            print(f\"Path contents: {os.listdir(json_folder)}\")\n",
    "        \n",
    "        # Stats Containers\n",
    "        field_counts = {}\n",
    "        all_fields = set()\n",
    "        \n",
    "        # Mismatch Containers\n",
    "        mismatches = []\n",
    "        matched_count = 0\n",
    "        missing_title_in_json_count = 0\n",
    "        \n",
    "        print(\"Processing files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            file_path = os.path.join(json_folder, json_file)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # --- Analysis A: Coverage (\"Not enough info\") ---\n",
    "            for key, val in data.items():\n",
    "                all_fields.add(key)\n",
    "                if key not in field_counts: field_counts[key] = 0\n",
    "                \n",
    "                if str(val).strip().startswith(target_phrase):\n",
    "                    field_counts[key] += 1\n",
    "            \n",
    "            # --- Analysis B: Title Mismatches & Metadata Validation ---\n",
    "            # Check if JSON title says \"Not enough info\"\n",
    "            json_title = str(data.get('publication/title', '')).strip()\n",
    "            if json_title == target_phrase:\n",
    "                missing_title_in_json_count += 1\n",
    "            \n",
    "            # Compare with Metadata\n",
    "            if pmcid in meta_lookup:\n",
    "                matched_count += 1\n",
    "                row = meta_lookup[pmcid]\n",
    "                \n",
    "                # Metadata Title\n",
    "                tsv_title = str(row.get('publication_title', '')).strip()\n",
    "                \n",
    "                # Calculate Similarity\n",
    "                # Ignore if JSON title is missing/not-enough-info\n",
    "                if json_title and json_title != target_phrase:\n",
    "                    if json_title != tsv_title:\n",
    "                        ratio = difflib.SequenceMatcher(None, json_title, tsv_title).ratio() \n",
    "                        if ratio < 1.0: # Keep all diffs for report\n",
    "                            mismatches.append({\n",
    "                                'pmcid': pmcid,\n",
    "                                'json_title': json_title,\n",
    "                                'tsv_title': tsv_title,\n",
    "                                'ratio': ratio\n",
    "                            })\n",
    "\n",
    "        print(\"Processing complete.\")\n",
    "        \n",
    "        if total_files > 0:\n",
    "            # --- 3. Generate Coverage Report (Same as Block 9.0) ---\n",
    "            data_list = []\n",
    "            for key in all_fields:\n",
    "                missing_count = field_counts.get(key, 0)\n",
    "                category = get_category(key)\n",
    "                pct_missing = (missing_count / total_files) * 100 if total_files > 0 else 0\n",
    "                \n",
    "                data_list.append({\n",
    "                    'Field': key,\n",
    "                    'Category': category,\n",
    "                    'Missing_Count': missing_count,\n",
    "                    'Total_Files': total_files,\n",
    "                    'Missing_Percentage': pct_missing\n",
    "                })\n",
    "                \n",
    "            df_stats = pd.DataFrame(data_list)\n",
    "            df_stats = df_stats.sort_values(by=['Category', 'Field'])\n",
    "            \n",
    "            # -- Plotting --\n",
    "            # 1. Category Summary\n",
    "            category_stats = df_stats.groupby('Category')['Missing_Percentage'].mean().reset_index()\n",
    "            cat_order = ['Publication', 'Data', 'Optimisation', 'Model', 'Evaluation']\n",
    "            category_stats['Category'] = pd.Categorical(category_stats['Category'], categories=cat_order, ordered=True)\n",
    "            category_stats = category_stats.sort_values('Category')\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(category_stats['Category'], category_stats['Missing_Percentage'], color='#4c72b0')\n",
    "            plt.title('Average Information Gap by Category (Registry Dataset)', fontsize=12)\n",
    "            plt.ylabel('Avg. Missing %')\n",
    "            plt.ylim(0, 100)\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 1, f'{height:.1f}%', ha='center', va='bottom')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'Registry_Category_Summary.png'), dpi=150)\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Field Level\n",
    "            unique_cats = [c for c in cat_order if c in df_stats['Category'].unique()]\n",
    "            if not unique_cats:\n",
    "                print(\"No categories found to plot.\")\n",
    "            else:\n",
    "                fig, axes = plt.subplots(nrows=len(unique_cats), ncols=1, figsize=(12, 4 * len(unique_cats)), constrained_layout=True)\n",
    "                if len(unique_cats) == 1: axes = [axes]\n",
    "                \n",
    "                for i, cat in enumerate(unique_cats):\n",
    "                    if i < len(axes): # Safety check\n",
    "                        ax = axes[i]\n",
    "                        subset = df_stats[df_stats['Category'] == cat].sort_values('Missing_Percentage', ascending=False)\n",
    "                        y_pos = np.arange(len(subset))\n",
    "                        ax.barh(y_pos, subset['Missing_Percentage'], align='center', color='#55a868')\n",
    "                        ax.set_yticks(y_pos)\n",
    "                        ax.set_yticklabels(subset['Field'])\n",
    "                        ax.invert_yaxis()\n",
    "                        ax.set_xlabel('% Coverage Gap')\n",
    "                        ax.set_title(f'Category: {cat}')\n",
    "                        ax.set_xlim(0, 100)\n",
    "                        for j, v in enumerate(subset['Missing_Percentage']):\n",
    "                            ax.text(v + 1, j, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "                    \n",
    "                plt.suptitle(f'Detailed Gap Analysis (Registry Data, n={total_files})', fontsize=16)\n",
    "                plt.savefig(os.path.join(output_dir, 'Registry_Field_Analysis.png'), dpi=150)\n",
    "                plt.close()\n",
    "            \n",
    "            # --- 4. Validation Report ---\n",
    "            \n",
    "            report_file = os.path.join(output_dir, 'Registry_Analysis_Report.md')\n",
    "            with open(report_file, 'w') as r:\n",
    "                r.write(f\"# DOME Registry Data Analysis Report\\n\")\n",
    "                r.write(f\"**Date:** {timestamp}\\n\")\n",
    "                r.write(f\"**JSON Dataset:** `{json_folder}` ({total_files} files)\\n\")\n",
    "                r.write(f\"**Metadata:** `{tsv_path}`\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 1. Metadata Linking\\n\")\n",
    "                r.write(f\"- Total JSON Files: {total_files}\\n\")\n",
    "                r.write(f\"- Matched to Registry Metadata: {matched_count} ({(matched_count/total_files)*100:.1f}%)\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 2. Title Analysis\\n\")\n",
    "                r.write(f\"- JSONs with '{target_phrase}' as Title: {missing_title_in_json_count}\\n\")\n",
    "                r.write(f\"- Title Mismatches (vs Metadata): {len(mismatches)}\\n\\n\")\n",
    "                \n",
    "                if mismatches:\n",
    "                    r.write(\"### Low Similarity Title Mismatches (Ratio < 0.5)\\n\")\n",
    "                    r.write(\"| PMCID | JSON Title | Metadata Title | Similarity |\\n\")\n",
    "                    r.write(\"|---|---|---|---|\\n\")\n",
    "                    severe_mismatches = [m for m in mismatches if m['ratio'] < 0.5]\n",
    "                    for m in severe_mismatches[:20]: # Show top 20\n",
    "                        r.write(f\"| {m['pmcid']} | {m['json_title'][:50]}... | {m['tsv_title'][:50]}... | {m['ratio']:.2f} |\\n\")\n",
    "                    if len(severe_mismatches) > 20:\n",
    "                        r.write(f\"| ... | ... | ... | ... |\\n\")\n",
    "                \n",
    "                r.write(\"\\n## 3. Information Coverage\\n\")\n",
    "                r.write(\"![Category Summary](Registry_Category_Summary.png)\\n\\n\")\n",
    "                \n",
    "                r.write(\"| Category | Avg Missing % |\\n\")\n",
    "                r.write(\"|---|---|\\n\")\n",
    "                for _, row in category_stats.iterrows():\n",
    "                    r.write(f\"| {row['Category']} | {row['Missing_Percentage']:.1f}% |\\n\")\n",
    "                    \n",
    "            print(f\"Analysis Complete.\")\n",
    "            print(f\"Report saved to: {report_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: JSON folder {json_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Critical Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db68fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72c649f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING METADATA UPDATE VIA EUROPE PMC API (Block 11.0)\n",
      "================================================================================\n",
      "Created target directory: Copilot_v0_Processed_2025-12-04_Updated_Metadata\n",
      "Found 231 files. Starting metadata fetch...\n",
      "Successfully fetched metadata for 231 entries.\n",
      "Saved remediation TSV to: DOME_Registry_Remediation/registry_metadata_remediation.tsv\n",
      "Updating JSONs in Copilot_v0_Processed_2025-12-04_Updated_Metadata...\n",
      "Update Process Complete.\n",
      "Total files written: 231\n",
      "Files updated with API data: 231\n"
     ]
    }
   ],
   "source": [
    "# Block 11.0: Update Registry JSONs via Europe PMC API (Fetch & Apply)\n",
    "# 1. Scans JSONs in 'Copilot_v0_Processed_2025-12-04/registry_v0' to get PMCIDs.\n",
    "# 2. Queries Europe PMC API for metadata (Title, Authors, Journal, Year, DOI).\n",
    "# 3. Saves fetched metadata to 'DOME_Registry_Remediation/registry_metadata_remediation.tsv'.\n",
    "# 4. Updates JSONs with this new metadata and saves them to 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING METADATA UPDATE VIA EUROPE PMC API (Block 11.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Configuration ---\n",
    "source_folder = 'Copilot_v0_Processed_2025-12-04/registry_v0'\n",
    "target_folder = 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "remediation_tsv = 'DOME_Registry_Remediation/registry_metadata_remediation.tsv'\n",
    "api_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(os.path.dirname(remediation_tsv), exist_ok=True)\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "    print(f\"Created target directory: {target_folder}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def fetch_metadata(pmcids):\n",
    "    \"\"\"\n",
    "    Fetches metadata for a list of PMCIDs in batches.\n",
    "    Returns a dictionary keyed by PMCID.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    batch_size = 50 # Europe PMC handles largish queries, but keep it safe\n",
    "    \n",
    "    # Chunk the PMCIDs\n",
    "    for i in range(0, len(pmcids), batch_size):\n",
    "        batch = pmcids[i:i + batch_size]\n",
    "        query = \" OR \".join([f'PMCID:{pid}' for pid in batch])\n",
    "        \n",
    "        params = {\n",
    "            'query': query,\n",
    "            'format': 'json',\n",
    "            'resultType': 'core',\n",
    "            'pageSize': batch_size\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(api_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for item in data.get('resultList', {}).get('result', []):\n",
    "                pmcid = item.get('pmcid')\n",
    "                if pmcid:\n",
    "                    # Extract fields\n",
    "                    # Authors list to string\n",
    "                    author_list = item.get('authorList', {}).get('author', [])\n",
    "                    authors_str = \", \".join([f\"{a.get('lastName', '')} {a.get('firstName', '')}\".strip() for a in author_list])\n",
    "                    \n",
    "                    results[pmcid] = {\n",
    "                        'PMCID': pmcid,\n",
    "                        'Title': item.get('title', ''),\n",
    "                        'Authors': authors_str,\n",
    "                        'Journal': item.get('journalInfo', {}).get('journal', {}).get('title', ''),\n",
    "                        'Year': item.get('pubYear', ''),\n",
    "                        'DOI': item.get('doi', '')\n",
    "                    }\n",
    "            \n",
    "            # Be polite to the API\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch starting with {batch[0]}: {e}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "try:\n",
    "    # 1. Identify PMCIDs from files\n",
    "    if os.path.exists(source_folder):\n",
    "        json_files = [f for f in os.listdir(source_folder) if f.endswith('.json')]\n",
    "        pmcids_to_fetch = [f.replace('.json', '') for f in json_files]\n",
    "        total_files = len(pmcids_to_fetch)\n",
    "        \n",
    "        print(f\"Found {total_files} files. Starting metadata fetch...\")\n",
    "        \n",
    "        # 2. Fetch Metadata\n",
    "        metadata_map = fetch_metadata(pmcids_to_fetch)\n",
    "        print(f\"Successfully fetched metadata for {len(metadata_map)} entries.\")\n",
    "        \n",
    "        # 3. Save to TSV\n",
    "        df_rem = pd.DataFrame(list(metadata_map.values()))\n",
    "        # Ensure column order\n",
    "        cols = ['PMCID', 'Title', 'Authors', 'Journal', 'Year', 'DOI']\n",
    "        df_rem = df_rem[cols] if not df_rem.empty else pd.DataFrame(columns=cols)\n",
    "        \n",
    "        df_rem.to_csv(remediation_tsv, sep='\\t', index=False)\n",
    "        print(f\"Saved remediation TSV to: {remediation_tsv}\")\n",
    "        \n",
    "        # 4. Update JSONs\n",
    "        print(f\"Updating JSONs in {target_folder}...\")\n",
    "        updated_count = 0\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            source_path = os.path.join(source_folder, json_file)\n",
    "            target_path = os.path.join(target_folder, json_file)\n",
    "            \n",
    "            # Read Source\n",
    "            with open(source_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check if we have new data\n",
    "            if pmcid in metadata_map:\n",
    "                meta = metadata_map[pmcid]\n",
    "                \n",
    "                # Apply updates (always overwrite with fresh API data)\n",
    "                data['publication/title'] = meta['Title']\n",
    "                data['publication/authors'] = meta['Authors']\n",
    "                data['publication/journal'] = meta['Journal']\n",
    "                data['publication/year'] = meta['Year']\n",
    "                data['publication/doi'] = meta['DOI']\n",
    "                \n",
    "                updated_count += 1\n",
    "            \n",
    "            # Write to Target\n",
    "            with open(target_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "                \n",
    "        print(f\"Update Process Complete.\")\n",
    "        print(f\"Total files written: {total_files}\")\n",
    "        print(f\"Files updated with API data: {updated_count}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Source folder {source_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Critical Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af51080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "414b39eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING ANALYSIS ON UPDATED REGISTRY DATASET\n",
      "================================================================================\n",
      "Output Directory: Registry_Analysis_Reports/Report_Updated_2026-01-15_16-53-53\n",
      "Loading metadata from: DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv\n",
      "Dropped 10 duplicate PMCIDs from metadata.\n",
      "Loaded 280 rows. Found 237 unique mapped PMCIDs.\n",
      "Found 231 JSON files in Copilot_v0_Processed_2025-12-04_Updated_Metadata\n",
      "Processing files...\n",
      "Processing complete.\n",
      "Analysis Complete.\n",
      "Report saved to: Registry_Analysis_Reports/Report_Updated_2026-01-15_16-53-53/Updated_Registry_Analysis_Report.md\n"
     ]
    }
   ],
   "source": [
    "# Block 12.0: Unified Analysis on UPDATED Registry JSONs vs Registry Metadata\n",
    "# This performs the same cross-check as Block 10.0, but on the newly remediated JSONs\n",
    "# folder: 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import difflib\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING ANALYSIS ON UPDATED REGISTRY DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Pointing to the new folder created in Block 11.0\n",
    "json_folder = 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "tsv_path = 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Registry_Analysis_Reports/Report_Updated_{timestamp}\"\n",
    "target_phrase = \"Not enough information is available\"\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output Directory: {output_dir}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_category(key):\n",
    "    if key.startswith('publication'): return 'Publication'\n",
    "    if key.startswith('dataset'): return 'Data'\n",
    "    if key.startswith('optimization'): return 'Optimisation'\n",
    "    if key.startswith('model'): return 'Model'\n",
    "    if key.startswith('evaluation'): return 'Evaluation'\n",
    "    return 'Other'\n",
    "\n",
    "try:\n",
    "    # 1. Load Metadata\n",
    "    print(f\"Loading metadata from: {tsv_path}\")\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # Filter for rows that have a mapped PMCID\n",
    "    df_mapped = df[df['mapped_pmcid'].notna()].copy()\n",
    "    df_mapped['clean_pmcid'] = df_mapped['mapped_pmcid'].apply(lambda x: str(x).strip())\n",
    "    \n",
    "    # Handle duplicates by taking the first occurrence\n",
    "    initial_len = len(df_mapped)\n",
    "    df_mapped = df_mapped.drop_duplicates(subset=['clean_pmcid'], keep='first')\n",
    "    if len(df_mapped) < initial_len:\n",
    "        print(f\"Dropped {initial_len - len(df_mapped)} duplicate PMCIDs from metadata.\")\n",
    "    \n",
    "    # Create lookup dictionary\n",
    "    meta_lookup = df_mapped.set_index('clean_pmcid').to_dict('index')\n",
    "    print(f\"Loaded {len(df)} rows. Found {len(df_mapped)} unique mapped PMCIDs.\")\n",
    "\n",
    "    # 2. Scan JSON Files\n",
    "    if os.path.exists(json_folder):\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        print(f\"Found {total_files} JSON files in {json_folder}\")\n",
    "        \n",
    "        if total_files == 0:\n",
    "            print(\"Warning: No JSON files found.\")\n",
    "        \n",
    "        # Stats Containers\n",
    "        field_counts = {}\n",
    "        all_fields = set()\n",
    "        \n",
    "        # Mismatch Containers\n",
    "        mismatches = []\n",
    "        matched_count = 0\n",
    "        missing_title_in_json_count = 0\n",
    "        \n",
    "        print(\"Processing files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            file_path = os.path.join(json_folder, json_file)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # --- Analysis A: Coverage (\"Not enough info\") ---\n",
    "            for key, val in data.items():\n",
    "                all_fields.add(key)\n",
    "                if key not in field_counts: field_counts[key] = 0\n",
    "                \n",
    "                if str(val).strip().startswith(target_phrase):\n",
    "                    field_counts[key] += 1\n",
    "            \n",
    "            # --- Analysis B: Title Mismatches & Metadata Validation ---\n",
    "            # Check if JSON title says \"Not enough info\"\n",
    "            json_title = str(data.get('publication/title', '')).strip()\n",
    "            if json_title == target_phrase:\n",
    "                missing_title_in_json_count += 1\n",
    "            \n",
    "            # Compare with Metadata\n",
    "            if pmcid in meta_lookup:\n",
    "                matched_count += 1\n",
    "                row = meta_lookup[pmcid]\n",
    "                \n",
    "                # Metadata Title\n",
    "                tsv_title = str(row.get('publication_title', '')).strip()\n",
    "                \n",
    "                # Calculate Similarity\n",
    "                # Ignore if JSON title is missing/not-enough-info\n",
    "                if json_title and json_title != target_phrase:\n",
    "                    if json_title != tsv_title:\n",
    "                        ratio = difflib.SequenceMatcher(None, json_title, tsv_title).ratio() \n",
    "                        if ratio < 1.0: # Keep all diffs for report\n",
    "                            mismatches.append({\n",
    "                                'pmcid': pmcid,\n",
    "                                'json_title': json_title,\n",
    "                                'tsv_title': tsv_title,\n",
    "                                'ratio': ratio\n",
    "                            })\n",
    "\n",
    "        print(\"Processing complete.\")\n",
    "        \n",
    "        if total_files > 0:\n",
    "            # --- 3. Generate Coverage Report ---\n",
    "            data_list = []\n",
    "            for key in all_fields:\n",
    "                missing_count = field_counts.get(key, 0)\n",
    "                category = get_category(key)\n",
    "                pct_missing = (missing_count / total_files) * 100 if total_files > 0 else 0\n",
    "                \n",
    "                data_list.append({\n",
    "                    'Field': key,\n",
    "                    'Category': category,\n",
    "                    'Missing_Count': missing_count,\n",
    "                    'Total_Files': total_files,\n",
    "                    'Missing_Percentage': pct_missing\n",
    "                })\n",
    "                \n",
    "            df_stats = pd.DataFrame(data_list)\n",
    "            df_stats = df_stats.sort_values(by=['Category', 'Field'])\n",
    "            \n",
    "            # -- Plotting --\n",
    "            # 1. Category Summary\n",
    "            category_stats = df_stats.groupby('Category')['Missing_Percentage'].mean().reset_index()\n",
    "            cat_order = ['Publication', 'Data', 'Optimisation', 'Model', 'Evaluation']\n",
    "            category_stats['Category'] = pd.Categorical(category_stats['Category'], categories=cat_order, ordered=True)\n",
    "            category_stats = category_stats.sort_values('Category')\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(category_stats['Category'], category_stats['Missing_Percentage'], color='#4c72b0')\n",
    "            plt.title('Average Information Gap by Category (Updated Dataset)', fontsize=12)\n",
    "            plt.ylabel('Avg. Missing %')\n",
    "            plt.ylim(0, 100)\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 1, f'{height:.1f}%', ha='center', va='bottom')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'Updated_Registry_Category_Summary.png'), dpi=150)\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Field Level\n",
    "            unique_cats = [c for c in cat_order if c in df_stats['Category'].unique()]\n",
    "            if not unique_cats:\n",
    "                 print(\"No categories found to plot.\")\n",
    "            else:\n",
    "                fig, axes = plt.subplots(nrows=len(unique_cats), ncols=1, figsize=(12, 4 * len(unique_cats)), constrained_layout=True)\n",
    "                if len(unique_cats) == 1: axes = [axes]\n",
    "                \n",
    "                for i, cat in enumerate(unique_cats):\n",
    "                    if i < len(axes):\n",
    "                        ax = axes[i]\n",
    "                        subset = df_stats[df_stats['Category'] == cat].sort_values('Missing_Percentage', ascending=False)\n",
    "                        y_pos = np.arange(len(subset))\n",
    "                        ax.barh(y_pos, subset['Missing_Percentage'], align='center', color='#55a868')\n",
    "                        ax.set_yticks(y_pos)\n",
    "                        ax.set_yticklabels(subset['Field'])\n",
    "                        ax.invert_yaxis()\n",
    "                        ax.set_xlabel('% Coverage Gap')\n",
    "                        ax.set_title(f'Category: {cat}')\n",
    "                        ax.set_xlim(0, 100)\n",
    "                        for j, v in enumerate(subset['Missing_Percentage']):\n",
    "                            ax.text(v + 1, j, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "                        \n",
    "                plt.suptitle(f'Detailed Gap Analysis (Updated Registry Data, n={total_files})', fontsize=16)\n",
    "                plt.savefig(os.path.join(output_dir, 'Updated_Registry_Field_Analysis.png'), dpi=150)\n",
    "                plt.close()\n",
    "            \n",
    "            # --- 4. Validation Report ---\n",
    "            \n",
    "            report_file = os.path.join(output_dir, 'Updated_Registry_Analysis_Report.md')\n",
    "            with open(report_file, 'w') as r:\n",
    "                r.write(f\"# Updated DOME Registry Data Analysis Report\\n\")\n",
    "                r.write(f\"**Date:** {timestamp}\\n\")\n",
    "                r.write(f\"**JSON Dataset:** `{json_folder}` ({total_files} files)\\n\")\n",
    "                r.write(f\"**Metadata:** `{tsv_path}`\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 1. Metadata Linking\\n\")\n",
    "                r.write(f\"- Total JSON Files: {total_files}\\n\")\n",
    "                r.write(f\"- Matched to Registry Metadata: {matched_count} ({(matched_count/total_files)*100:.1f}%)\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 2. Title Analysis\\n\")\n",
    "                r.write(f\"- JSONs with '{target_phrase}' as Title: {missing_title_in_json_count}\\n\")\n",
    "                r.write(f\"- Title Mismatches (vs Metadata): {len(mismatches)}\\n\\n\")\n",
    "                \n",
    "                if mismatches:\n",
    "                    r.write(\"### Low Similarity Title Mismatches (Ratio < 0.5)\\n\")\n",
    "                    r.write(\"| PMCID | JSON Title | Metadata Title | Similarity |\\n\")\n",
    "                    r.write(\"|---|---|---|---|\\n\")\n",
    "                    severe_mismatches = [m for m in mismatches if m['ratio'] < 0.5]\n",
    "                    for m in severe_mismatches[:20]: # Show top 20\n",
    "                        r.write(f\"| {m['pmcid']} | {m['json_title'][:50]}... | {m['tsv_title'][:50]}... | {m['ratio']:.2f} |\\n\")\n",
    "                    if len(severe_mismatches) > 20:\n",
    "                        r.write(f\"| ... | ... | ... | ... |\\n\")\n",
    "                \n",
    "                r.write(\"\\n## 3. Information Coverage\\n\")\n",
    "                r.write(\"![Category Summary](Updated_Registry_Category_Summary.png)\\n\\n\")\n",
    "                \n",
    "                r.write(\"| Category | Avg Missing % |\\n\")\n",
    "                r.write(\"|---|---|\\n\")\n",
    "                for _, row in category_stats.iterrows():\n",
    "                    r.write(f\"| {row['Category']} | {row['Missing_Percentage']:.1f}% |\\n\")\n",
    "                    \n",
    "            print(f\"Analysis Complete.\")\n",
    "            print(f\"Report saved to: {report_file}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Error: JSON folder {json_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Critical Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57831269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89818551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### High Similarity (>=80%) | Similarity: 0.89"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #28a745; font-family: sans-serif;\">\n",
       "                <span style=\"font-weight: bold; margin-right: 10px;\">IDS:</span>\n",
       "                <a href=\"https://europepmc.org/search?query=PMC11299106\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #1e7e34; margin-right: 20px; font-size: 1.1em;\">PMC11299106 ↗</a>\n",
       "                <a href=\"https://europepmc.org/search?query=39101782\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #1e7e34; font-size: 1.1em;\">PMID:39101782 ↗</a>\n",
       "            </div>\n",
       "            <br>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ [Title]\n",
      "  JSON: MOBFinder: a tool for mobilization typing of plasmid metagenomic fragments based on a language model.\n",
      "  TSV : MOBFinder: a tool for MOB typing for plasmid metagenomic fragments based on language model\n",
      "------------------------------------------------------------\n",
      "❌ [Authors]\n",
      "  JSON: Feng Tao, Wu Shufang, Zhou Hongwei, Fang Zhencheng\n",
      "  TSV : Tao Feng, Shufang Wu, Hongwei Zhou, and Zhencheng Fang\n",
      "------------------------------------------------------------\n",
      "✅ [Journal]\n",
      "  GigaScience\n",
      "------------------------------------------------------------\n",
      "✅ [Year]\n",
      "  2024\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>✅ [DOI]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&nbsp;&nbsp;<a href=\"https://doi.org/10.1093/gigascience/giae047\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">10.1093/gigascience/giae047 ↗</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <br>\n",
       "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
       "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
       "                        ▶ Show Remaining JSON Data (22 fields)\n",
       "                    </summary>\n",
       "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{\n",
       "  &quot;publication/tags&quot;: &quot;- MOBFinder\\n- Plasmid metagenomic fragments\\n- MOB typing\\n- Skip-gram algorithm\\n- Word embeddings\\n- Language models\\n- Metagenomic data analysis\\n- Bioinformatics tools\\n- Plasmid relaxases\\n- Horizontal gene transfer\\n- Type 2 diabetes\\n- Antibiotic resistance genes\\n- Plasmid genomes\\n- Machine learning in genomics\\n- DNA sequence analysis&quot;,\n",
       "  &quot;dataset/provenance&quot;: &quot;The dataset used in our study primarily consists of metagenomic sequencing data retrieved from the NCBI Short Read Archive (SRA) database. Specifically, we utilized datasets SRA045646 and SRA050230. These datasets were chosen to investigate the presence and characteristics of plasmids within different MOB (Mobility) classes in the context of type 2 diabetes (T2D) metagenomic data.\\n\\nThe number of data points in our study is substantial, as we generated simulated metagenomic contigs from complete plasmid genomes that had been MOB typed. For the training dataset, we created 90,000 artificial contigs for each MOB class within specific length ranges: 100\\u2013400 bp, 401\\u2013800 bp, 801\\u20131,200 bp, and 1,201\\u20131,600 bp. For the test dataset, we generated 500 fragments for each MOB class in four length groups: 801\\u20131,200 bp, 1,201\\u20131,600 bp, 3,000\\u20134,000 bp, and 5,000\\u201310,000 bp. This approach ensured a comprehensive evaluation of MOBFinder&#x27;s performance across various fragment lengths.\\n\\nThe data used in our study builds upon previous research and community efforts in metagenomics. For instance, our analysis of T2D metagenomic sequencing data aligns with findings from a metagenome-wide association study of gut microbiota in type 2 diabetes, which was published in Nature. Additionally, our work leverages established bioinformatics tools and databases, such as the nonredundant (NR) database and the NCBI plasmid database, to develop and validate our models. This integration of existing data and methodologies ensures that our findings are robust and relevant to the broader scientific community.&quot;,\n",
       "  &quot;dataset/splits&quot;: &quot;In the development of MOBFinder, two primary data splits were created: the training dataset and the test dataset. For the training dataset, plasmid genomes in each MOB category were randomly split at a proportion of 70% and 30%. This means that 70% of the classified plasmid genomes were used for training, while the remaining 30% were reserved for testing.\\n\\nFor the training dataset, contigs of different length ranges were generated to predict plasmid fragments with varying lengths. The length ranges included 100\\u2013400 bp, 401\\u2013800 bp, 801\\u20131,200 bp, and 1,201\\u20131,600 bp. For each MOB class within these length ranges, 90,000 artificial contigs were randomly generated. Plasmid fragments longer than 1,600 bp were segmented into shorter contigs and predicted using models designed for the corresponding lengths.\\n\\nThe test dataset was constructed to assess the performance of MOBFinder on plasmid fragments of different lengths. Four length groups were created: group A with a length range of 801\\u20131,200 bp, group B with a length range of 1,201\\u20131,600 bp, group C with a length range of 3,000\\u20134,000 bp, and group D with a length range of 5,000\\u201310,000 bp. For each MOB class in these four groups, 500 fragments were randomly extracted. This distribution allowed for a comprehensive evaluation of MOBFinder&#x27;s performance across different fragment lengths.&quot;,\n",
       "  &quot;dataset/redundancy&quot;: &quot;To ensure the robustness and independence of our datasets, we employed a rigorous splitting strategy. For classified plasmid genomes in each MOB category, we randomly divided them into training and test sets at a proportion of 70% and 30%, respectively. This split was designed to maintain the independence of the training and test sets, ensuring that the model&#x27;s performance could be accurately evaluated on unseen data.\\n\\nThe training dataset was further processed to generate contigs of varying lengths: 100\\u2013400 bp, 401\\u2013800 bp, 801\\u20131,200 bp, and 1,201\\u20131,600 bp. For each MOB class within these length ranges, we randomly generated 90,000 artificial contigs. This approach allowed us to predict plasmid fragments of different lengths effectively. Plasmid fragments longer than 1,600 bp were segmented into shorter contigs and predicted using models designed for the corresponding lengths.\\n\\nFor the test dataset, we created four length groups to assess the performance of MOBFinder: group A (801\\u20131,200 bp), group B (1,201\\u20131,600 bp), group C (3,000\\u20134,000 bp), and group D (5,000\\u201310,000 bp). For each MOB class in these groups, 500 fragments were randomly extracted. This strategy ensured that the test set included a diverse range of fragment lengths, providing a comprehensive evaluation of the model&#x27;s performance.\\n\\nThe distribution of our datasets aligns with common practices in machine learning, where a significant portion of the data is used for training, and a smaller, independent portion is reserved for testing. This approach helps in evaluating the model&#x27;s generalization capability and ensures that the results are not biased by overfitting to the training data. By maintaining independent training and test sets, we enforced the robustness of our model and its ability to perform accurately on new, unseen data.&quot;,\n",
       "  &quot;dataset/availability&quot;: &quot;The data used in the development and testing of MOBFinder, including the simulated datasets, are not explicitly mentioned as being released in a public forum. The development process involved generating simulated datasets through specific steps, such as splitting classified plasmid genomes into training and test datasets, and creating contigs of different length ranges. However, there is no information provided about the public availability of these datasets or the specific data splits used.\\n\\nThe focus of the publication is on the methodology and performance of MOBFinder, rather than the public release of the datasets. The simulated datasets were constructed to serve as a benchmark for evaluating the tool&#x27;s performance, given the lack of real metagenomic data for this purpose. The construction of these datasets involved using complete plasmid genomes from the NCBI and applying a 4-mer language model to generate word vectors for training and testing.\\n\\nThe publication does not detail any enforcement mechanisms for the release of the datasets, as the primary emphasis is on the technical approach and the results obtained from the simulated data. Therefore, it is not possible to provide specific information about where the data can be accessed or under what license it might be available.&quot;,\n",
       "  &quot;optimization/algorithm&quot;: &quot;The optimization algorithm employed in our work utilizes a well-established machine-learning approach, specifically the random forest algorithm. This algorithm is not new and has been extensively used and validated in various fields, including bioinformatics.\\n\\nRandom forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This method is chosen for its robustness, ability to handle high-dimensional data, and effectiveness in reducing overfitting.\\n\\nThe decision to use random forest was driven by its proven performance in similar classification tasks and its suitability for handling the complex and high-dimensional data generated from plasmid sequences. The algorithm&#x27;s ability to capture intricate patterns and interactions within the data makes it an ideal choice for our purposes.\\n\\nGiven that random forest is a widely recognized and established algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this algorithm to the specific problem of MOB typing in plasmid fragments, which is a novel contribution in the field of bioinformatics.&quot;,\n",
       "  &quot;optimization/meta&quot;: &quot;The model, MOBFinder, employs a meta-predictor approach that integrates multiple machine-learning methods to enhance its predictive accuracy. Specifically, it uses random forest algorithms as the core classification models. These models are trained on simulated metagenomic contigs, which are encoded into word vectors using a language model. The word vectors are generated through a skip-gram algorithm, which transforms overlapping 4-mers into numerical representations.\\n\\nMOBFinder trains four separate random forest models, each tailored to different length ranges of plasmid fragments: 100\\u2013400 bp, 401\\u2013800 bp, 801\\u20131,200 bp, and 1,201\\u20131,600 bp. These models are then ensembled to make more accurate predictions. For fragments shorter than 100 bp, the model designed for the 100\\u2013400 bp range is used. For fragments longer than 1,600 bp, the fragments are segmented into shorter contigs, and predictions are made using the corresponding models. The final prediction is determined by aggregating and calculating the weighted average scores for each MOB class, with the highest score indicating the predicted MOB type.\\n\\nThe training data for these models is constructed from classified plasmid genomes, which are split into training and test datasets. The training dataset includes contigs of various length ranges, with 90,000 artificial contigs generated for each MOB class within these ranges. The test dataset includes longer fragments to assess the model&#x27;s performance across different length groups.\\n\\nThe independence of the training data is ensured through the random splitting of classified plasmid genomes into training and test sets, with a 70% and 30% proportion, respectively. This approach helps in evaluating the model&#x27;s performance on unseen data, ensuring that the training data is independent and representative of real-world scenarios.&quot;,\n",
       "  &quot;optimization/encoding&quot;: &quot;In the development of MOBFinder, data encoding was a crucial step to transform DNA sequences into a format suitable for machine learning algorithms. The process began with the generation of k-mers, specifically 4-mers, from the DNA sequences using a sliding window approach. This method segmented the sequences into overlapping substrings of length 4, such as \\&quot;ATCG\\&quot; from the sequence \\&quot;ATCGCTGA\\&quot;. These 4-mers served as the basic units, or \\&quot;words,\\&quot; for the subsequent encoding process.\\n\\nEach 4-mer was initially assigned a random vector. These vectors were then refined using a skip-gram language model, which is a type of neural network architecture. The model was trained to predict the context of each 4-mer within the sequence, effectively learning the relationships between different 4-mers. This training process involved optimizing the vectors through backpropagation over multiple epochs, resulting in a set of 100-dimensional word vectors that captured the semantic information of the 4-mers.\\n\\nFor a given DNA fragment, the average of all 4-mer word vectors was computed to create a feature vector representing the entire fragment. This feature vector served as the input for the random forest algorithm, which was used to classify the DNA fragments into different MOB types. The use of 4-mers was chosen based on empirical observations that this length provided a good balance between capturing sequence information and computational efficiency. Longer k-mers did not significantly improve performance but increased computational complexity. This encoding process allowed MOBFinder to effectively handle and classify metagenomic data, providing insights into the transmission mechanisms of plasmid-mediated antibiotic resistance genes and virulence factors.&quot;,\n",
       "  &quot;optimization/parameters&quot;: &quot;In the development of MOBFinder, the model utilizes word vectors generated through a skip-gram language model. The dimensionality of these word vectors is a critical parameter. Initially, each word (4-mer) is assigned a random vector with a dimension of 100. This dimension was chosen based on previous studies and default settings, ensuring that the word vectors can effectively capture the underlying features and patterns of the plasmid fragments.\\n\\nThe choice of a 4-mer length for generating these word vectors was determined through comparative analysis. Models with k-mer lengths ranging from 2 to 8 were evaluated. It was observed that a k-mer length of 4 provided a balanced accuracy, harmonic mean, F1-score, and AUC values across different MOB types. Increasing the k-mer length beyond 4 did not significantly improve these metrics but did increase the runtime. Therefore, a k-mer length of 4 was selected for training the word vectors and developing MOBFinder.\\n\\nThe random forest algorithm, used for classification, also involves parameters such as the number of trees. In this case, the number of trees was set to 500 to generate predictive models. This setting was chosen to ensure robust and accurate predictions.\\n\\nIn summary, the model parameters include the dimensionality of the word vectors (100) and the k-mer length (4), both selected based on empirical evidence and comparative analysis. The random forest algorithm uses 500 trees to enhance the model&#x27;s predictive performance.&quot;,\n",
       "  &quot;optimization/features&quot;: &quot;The input features for MOBFinder are derived from the sequence data of plasmid fragments. Specifically, a 4-mer sliding window is used to generate overlapping words from the DNA sequences. These words are then transformed into numerical word vectors using trained word embeddings. The dimension of these word vectors is 100, as defined by the skip-gram language model employed.\\n\\nFor each contig in the training dataset, all the word vectors generated from the 4-mer sliding window are summed to compute their average. This average vector serves as the input feature for the random forest classifier. Therefore, each contig is represented by a single 100-dimensional feature vector.\\n\\nFeature selection was not explicitly performed in the traditional sense. Instead, the use of word embeddings inherently selects relevant features by capturing the contextual importance of nucleotide sequences. This approach provides a more sophisticated analysis compared to simple k-mer frequency methods, as it considers the biochemical characteristics and contextual relevance of the sequences.\\n\\nThe training of the word embeddings was conducted using a large dataset of plasmid genomes, ensuring that the feature extraction process is robust and generalizable. The word vectors were generated through an unsupervised learning process, which involved training a skip-gram model on the plasmid sequences. This process did not involve the test dataset, ensuring that the feature extraction is independent of the evaluation data.&quot;,\n",
       "  &quot;optimization/fitting&quot;: &quot;In the development of MOBFinder, we employed a random forest algorithm to train predictive models using simulated datasets. The training process involved generating contigs of varying lengths from classified plasmid genomes, ensuring a comprehensive representation of different MOB types. To handle the potential issue of high dimensionality, we utilized word vector models, specifically the skip-gram algorithm, to transform DNA sequences into lower-dimensional word vectors. This approach helped in compressing high-dimensional initial vectors into more manageable word vectors, effectively avoiding dimensionality issues during supervised training.\\n\\nThe random forest algorithm was chosen for its robustness and ability to handle large datasets with numerous features. We trained four separate models for different length ranges of contigs (100\\u2013400 bp, 401\\u2013800 bp, 801\\u20131,200 bp, and 1,201\\u20131,600 bp), each with 500 trees to ensure a diverse and thorough exploration of the feature space. This ensemble method helped in mitigating overfitting by averaging the predictions of multiple models, thereby reducing the variance and improving the generalization capability of MOBFinder.\\n\\nTo further ensure that overfitting was not an issue, we employed cross-validation techniques during the training process. The datasets were split into training and test sets, with the training set used to build the models and the test set reserved for evaluating their performance. Additionally, we generated simulated datasets with varying lengths to assess the performance of MOBFinder across different scenarios, ensuring that the models could generalize well to unseen data.\\n\\nUnderfitting was addressed by carefully selecting the features and hyperparameters. The use of word vectors allowed us to capture the underlying patterns and features of the DNA sequences more effectively than traditional methods like k-mer frequency models or one-hot encoding. The random forest algorithm&#x27;s ability to handle non-linear relationships and interactions between features further helped in building models that could capture the complexity of the data without being too simplistic.\\n\\nIn summary, the fitting method for MOBFinder involved the use of word vector models to manage dimensionality, random forest algorithms for robust training, and cross-validation to ensure generalization. These steps collectively helped in ruling out both overfitting and underfitting, resulting in a model that performs well across different MOB types and contig lengths.&quot;,\n",
       "  &quot;optimization/regularization&quot;: &quot;In the development of MOBFinder, several techniques were employed to prevent overfitting and ensure robust performance. One of the key methods used was the random forest algorithm, which inherently helps to reduce overfitting by averaging multiple decision trees. This ensemble method provides a more stable and accurate prediction by mitigating the risk of any single tree overfitting the training data.\\n\\nAdditionally, the use of word vectors generated through a skip-gram language model played a crucial role in dimensionality reduction. By transforming high-dimensional k-mer sequences into lower-dimensional word vectors, the model effectively compressed the feature space, thereby reducing the complexity and potential for overfitting. This approach allowed the model to capture essential sequence characteristics while avoiding the noise associated with high-dimensional data.\\n\\nFurthermore, the training process involved splitting the classified plasmid genomes into training and test datasets, with a 70% and 30% proportion respectively. This ensured that the model was trained on a diverse set of data and evaluated on a separate, unseen dataset, which helps in assessing the model&#x27;s generalization capability and preventing overfitting.\\n\\nThe model was also trained using an optimization algorithm that minimized the loss function through backpropagation over 10 epochs. This iterative process helped in fine-tuning the model parameters and ensuring that the model did not memorize the training data but rather learned generalizable patterns.\\n\\nIn summary, the combination of random forest, word vector dimensionality reduction, and careful dataset splitting contributed to the prevention of overfitting in MOBFinder, resulting in a model that performs well on both training and test datasets.&quot;,\n",
       "  &quot;optimization/config&quot;: &quot;The hyper-parameter configurations and optimization schedule used in the development of MOBFinder are not explicitly detailed in the provided information. However, it is mentioned that a skip-gram language model was employed to generate word vectors, with specific settings such as a 100-dimensional word vector and a context window size of 20. The model was trained for 10 epochs using backpropagation.\\n\\nThe random forest algorithm was used for classification, with the number of trees set to 500. Different models were trained for various length ranges of plasmid fragments: 100\\u2013400 bp, 401\\u2013800 bp, 801\\u20131,200 bp, and 1,201\\u20131,600 bp. These models were then ensembled to handle fragments of different lengths.\\n\\nRegarding the availability of model files and optimization parameters, there is no specific mention of where these can be accessed or under what license. The focus of the provided information is on the methodology and performance evaluation of MOBFinder, rather than the technical details of the implementation and availability of the model files.&quot;,\n",
       "  &quot;model/interpretability&quot;: &quot;The model employed in MOBFinder is not entirely a black box, as it leverages language models to provide some level of interpretability. The use of word embeddings, specifically 4-mers, allows for a more nuanced understanding of the sequence features. These 4-mers are analogous to words in a language, and the longer sequences of DNA are treated as sentences. This approach enables the model to capture contextual information, which is crucial for understanding the biochemical complexities of nucleotide sequences.\\n\\nThe language model used in MOBFinder generates numerical word vectors through a skip-gram algorithm. This process involves creating a probability distribution over context words, which helps in characterizing the sequence features of different MOB categories. The model is trained using a neural network with two layers: the first layer converts initialized vectors into a 100-dimensional word vector representation, and the second layer computes the probability of correct context words. This training process allows the model to map characters with similar contexts to similar feature spaces, thereby compressing high-dimensional initial vectors into lower-dimensional word vectors.\\n\\nThe use of 4-mers as the basic unit in the model provides a clear example of interpretability. By segmenting DNA sequences into overlapping 4-mers, the model can identify patterns and features that are specific to each MOB type. This segmentation allows for a more detailed analysis of the sequence characteristics, which is essential for accurate MOB typing. The model&#x27;s ability to stabilize metrics such as balanced accuracy, harmonic mean, F1-score, and AUC values across different MOB types further demonstrates its interpretability and effectiveness.\\n\\nIn summary, while MOBFinder utilizes advanced machine learning techniques, the incorporation of language models and word embeddings provides a level of transparency. The use of 4-mers as the basic unit allows for a detailed and contextually rich analysis of DNA sequences, making the model&#x27;s predictions more interpretable.&quot;,\n",
       "  &quot;model/output&quot;: &quot;The model, MOBFinder, is a classification model designed to predict the mobility (MOB) types of plasmid fragments and bins from metagenomic data. It categorizes input sequences into specific MOB classes, such as MOBB, MOBC, MOBF, MOBH, MOBL, MOBM, MOBP, MOBQ, MOBT, MOBV, and non-MOB. The output of MOBFinder provides the predicted MOB class for each input fragment or bin, along with the scores for different MOB types. For plasmid metagenomic bins, the output includes the plasmid bin ID, the predicted MOB class, and the MOB scores for various MOB types. The model uses an ensemble of random forest classifiers trained on different length ranges of plasmid fragments to make these predictions. The final output is the MOB type with the highest score for the input fragment or bin.&quot;,\n",
       "  &quot;model/duration&quot;: &quot;The execution time of MOBFinder varied depending on the length of the DNA fragments being analyzed. For shorter fragments, the model could process them relatively quickly. However, for longer fragments, the execution time increased due to the need to segment them into shorter contigs and process each segment individually. The model was designed to handle fragments of different lengths efficiently, with specific models trained for different length ranges (100\\u2013400 bp, 401\\u2013800 bp, 801\\u20131,200 bp, and 1,201\\u20131,600 bp). For fragments longer than 1,600 bp, the model segmented them into shorter contigs and processed each segment using the corresponding model, which added to the overall execution time. The use of a random forest algorithm with 500 trees also contributed to the execution time, as it required multiple iterations to generate predictive models. Overall, the execution time was optimized to balance accuracy and efficiency, ensuring that MOBFinder could handle large metagenomic datasets within a reasonable timeframe.&quot;,\n",
       "  &quot;model/availability&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;evaluation/method&quot;: &quot;The evaluation of MOBFinder involved several key steps and metrics to ensure its performance and accuracy. The harmonic mean was used to provide an overall evaluation of the model&#x27;s performance, with sensitivity and specificity being crucial metrics. Sensitivity, or the true positive rate, measures the proportion of actual positives correctly identified by the model, while specificity measures the proportion of actual negatives correctly identified.\\n\\nThe F1-score was also employed, which combines precision and recall to offer a balanced measure of the model&#x27;s performance. Precision indicates the number of correct positive predictions out of all positive predictions, and recall measures the number of correct positive predictions out of all actual positives.\\n\\nTo visualize the performance of MOBFinder in predicting each MOB category, a receiver operating characteristic (ROC) curve was utilized. The ROC curve plots the false-positive rate against the true-positive rate, with plots closer to the top-left corner indicating better performance. For each MOB class, the area under the curve (AUC) value was calculated to quantify the model&#x27;s performance. An AUC value between 0.5 and 1 indicates that the model performs better than random chance, with higher values signifying better prediction capability.\\n\\nSimulated metagenomic contigs were constructed from complete genomes that had been MOB typed, serving as a benchmark dataset. These contigs were encoded into word vectors and used to train a random forest algorithm. The trained model was then used to predict the MOB typing of corresponding DNA fragments based on their word vectors.\\n\\nThe evaluation process included generating simulated datasets by randomly splitting classified plasmid genomes into training and test datasets. For the training dataset, contigs of varying lengths were generated to predict plasmid fragments of different sizes. The test dataset included longer fragments to assess the model&#x27;s performance on real metagenomic data.\\n\\nFour classification models were trained on different length ranges of the training dataset, and these models were ensembled to make more accurate predictions. For fragments shorter than 100 base pairs, a model designed for 100\\u2013400 base pairs was used. For longer fragments, they were segmented into shorter contigs and predicted using the corresponding models. The final prediction result for an input fragment was determined by aggregating and calculating the weighted average scores for each MOB class, with the highest score indicating the predicted MOB type.\\n\\nAdditionally, MOBFinder was designed to perform MOB typing on both plasmid contigs and plasmid bins. For plasmid bins, the model predicts the likelihood of each MOB class for fragments within the bin, aggregating the scores of each sequence within the bin to calculate the weighted average scores based on sequence length.&quot;,\n",
       "  &quot;evaluation/measure&quot;: &quot;In the evaluation of MOBFinder, several performance metrics were reported to provide a comprehensive assessment of the tool&#x27;s effectiveness. These metrics include overall accuracy, kappa, run time, balanced accuracy, harmonic mean, F1-score, and area under the curve (AUC) values.\\n\\nOverall accuracy measures the proportion of correct predictions made by the model. Kappa, on the other hand, assesses the consistency between the predicted classes and the true classes, accounting for the possibility of random prediction. Run time was recorded to evaluate the efficiency of the tool.\\n\\nBalanced accuracy was used to measure the average accuracy of each MOB category, which is particularly important given the class imbalance within the training dataset. This metric considers both the true-positive rate (TPR) and the true-negative rate (TNR).\\n\\nThe harmonic mean provides an overall evaluation of the model\\u2019s performance, combining sensitivity (Sn) and specificity (Sp). Sensitivity is the ratio of true positives to the sum of true positives and false negatives, while specificity is the ratio of true negatives to the sum of true negatives and false positives.\\n\\nThe F1-score combines precision and recall to offer a balanced measure of the model\\u2019s performance. Precision is the ratio of true positives to the sum of true positives and false positives, and recall is the ratio of true positives to the sum of true positives and false negatives.\\n\\nAUC values were calculated for each MOB class to quantify the performance of MOBFinder in distinguishing between positive and negative samples. An AUC value between 0.5 and 1 indicates that the model performs better than random chance, with higher values signifying better prediction capability.\\n\\nThese metrics are representative of standard evaluation practices in the field, ensuring that the performance of MOBFinder can be compared with other tools and models in the literature. The use of multiple metrics provides a thorough assessment of the tool&#x27;s accuracy, consistency, and efficiency, making it a reliable choice for MOB typing of plasmid fragments and bins from metagenomic data.&quot;,\n",
       "  &quot;evaluation/comparison&quot;: &quot;In the evaluation of MOBFinder, a comparison was conducted with other publicly available methods to assess its performance. Specifically, MOBFinder was compared to MOB-suite and MOBscan. These comparisons were performed using a test dataset to evaluate the accuracy, kappa, and run time of each method. MOBscan, which predicts MOB types using plasmid protein sequences rather than DNA sequences, required an additional step of annotating proteins in the plasmid fragments using Prokka before predictions could be made. The overall accuracy, kappa, and run time were calculated by comparing the predicted classes with the true classes. The overall accuracy was determined by the proportion of accurate predictions, while kappa assessed the overall consistency between predictions and true classes, accounting for the possibility of random prediction. The run time was recorded using the Linux command \\&quot;time.\\&quot;\\n\\nAdditionally, simpler baselines were not explicitly mentioned as part of the comparison. The focus was primarily on comparing MOBFinder with established methods in the field. The performance metrics used included overall accuracy, kappa, balanced accuracy, harmonic mean, and F1-score. These metrics provided a comprehensive evaluation of MOBFinder&#x27;s performance in predicting MOB types from metagenomic data. The balanced accuracy was particularly useful for measuring the average accuracy of each MOB category, considering the class imbalance within the training dataset. The harmonic mean offered an overall evaluation of the model&#x27;s performance, combining sensitivity and specificity. The F1-score provided a balanced measure of the model&#x27;s performance by combining precision and recall.&quot;,\n",
       "  &quot;evaluation/confidence&quot;: &quot;The evaluation of MOBFinder&#x27;s performance was conducted using several metrics, including accuracy, kappa, balanced accuracy, harmonic mean, F1-score, and AUC values. These metrics provide a comprehensive assessment of the tool&#x27;s effectiveness in predicting MOB categories.\\n\\nConfidence intervals for the performance metrics were not explicitly mentioned. However, the statistical significance of the results was addressed through the use of the Wilcoxon rank-sum test and the Benjamini-Hochberg method for adjusting P values. These methods ensure that the differences observed between groups, such as the T2D group and the control group, are statistically significant and not due to random chance.\\n\\nThe performance of MOBFinder was compared to other tools like MOB-suite and MOBscan. The results showed that MOBFinder consistently outperformed these tools across various metrics. For instance, MOBFinder achieved higher accuracy and kappa values, indicating better overall performance and consistency in predictions. The AUC values for MOBFinder were also greater than 0.8 for all MOB classes, with most values exceeding 0.9, which demonstrates the tool&#x27;s effectiveness in distinguishing between positive and negative samples.\\n\\nThe statistical analyses conducted using R further support the robustness of the findings. The use of the Tukey honest significant difference test to compare identified resistance genes among different MOB classes adds another layer of confidence in the results. Overall, the evaluation provides strong evidence that MOBFinder is a superior tool for MOB typing, with statistically significant improvements over existing methods.&quot;,\n",
       "  &quot;evaluation/availability&quot;: &quot;Not enough information is available.&quot;\n",
       "}</pre>\n",
       "                </details>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Run cell again [Ctrl+Enter] to flip category next time)\n"
     ]
    }
   ],
   "source": [
    "# Block 13.0: Manual Visual Inspection Interface for Remediated Registry JSONs\n",
    "# Comparison:\n",
    "# - JSONs: Copilot_v0_Processed_2025-12-04_Updated_Metadata (Updated via API)\n",
    "# - TSV: DOME_Registry_Remediation/registry_metadata_remediation.tsv (Source of truth from API)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import difflib\n",
    "import random\n",
    "import html\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Global toggle for alternation (persists across cell runs)\n",
    "if 'remediation_inspection_mode_high' not in globals():\n",
    "    remediation_inspection_mode_high = True # True=High, False=Low\n",
    "\n",
    "source_json_folder = 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "tsv_path = 'DOME_Registry_Remediation/registry_metadata_remediation.tsv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(tsv_path) and os.path.exists(source_json_folder):\n",
    "        # Load Data\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        \n",
    "        # Prepare TSV data \n",
    "        # Remediation TSV uses 'PMCID' as key\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        df_mapped = df.drop_duplicates(subset=['PMCID_clean'], keep='first')\n",
    "        \n",
    "        # Helper to clean numeric strings\n",
    "        def clean_val(v):\n",
    "            if pd.isna(v) or v == '': return \"\"\n",
    "            try: return str(int(float(v)))\n",
    "            except: return str(v).strip()\n",
    "\n",
    "        high_sim = [] \n",
    "        low_sim = []\n",
    "        \n",
    "        json_files = [f for f in os.listdir(source_json_folder) if f.endswith('.json')]\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            \n",
    "            # Find matching TSV row\n",
    "            row = df_mapped[df_mapped['PMCID_clean'] == pmcid]\n",
    "            if len(row) == 0: continue\n",
    "            row = row.iloc[0]\n",
    "            \n",
    "            with open(os.path.join(source_json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            j_title = str(data.get('publication/title', \"\")).strip()\n",
    "            # TSV column is 'Title' in the remediation file\n",
    "            t_title = str(row['Title']).strip() if pd.notna(row['Title']) else \"\"\n",
    "            \n",
    "            # We expect them to be identical if update worked, but let's check\n",
    "            if j_title != t_title:\n",
    "                ratio = difflib.SequenceMatcher(None, j_title, t_title).ratio()\n",
    "            else:\n",
    "                ratio = 1.0\n",
    "                \n",
    "            entry = {\n",
    "                'pmcid': pmcid,\n",
    "                'json': data,\n",
    "                'tsv': row,\n",
    "                'ratio': ratio\n",
    "            }\n",
    "            \n",
    "            if ratio >= 0.8: high_sim.append(entry)\n",
    "            elif ratio <= 0.8: low_sim.append(entry)\n",
    "\n",
    "        # Toggle Selection Logic\n",
    "        target_pool = []\n",
    "        mode_str = \"\"\n",
    "        \n",
    "        if remediation_inspection_mode_high:\n",
    "            if high_sim: \n",
    "                target_pool = high_sim\n",
    "                mode_str = \"Exact Match / High Similarity (Sampling verified entries)\"\n",
    "            elif low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Mismatches (Sampling despite preference)\"\n",
    "        else:\n",
    "            if low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Mismatches / Differences\"\n",
    "            elif high_sim:\n",
    "                target_pool = high_sim\n",
    "                mode_str = \"Exact Matches (No mismatches found)\"\n",
    "        \n",
    "        remediation_inspection_mode_high = not remediation_inspection_mode_high\n",
    "        \n",
    "        if not target_pool:\n",
    "            print(\"No entries found.\")\n",
    "        else:\n",
    "            item = random.choice(target_pool)\n",
    "            \n",
    "            # Prepare IDs for Link\n",
    "            curr_pmcid = item['pmcid']\n",
    "            curr_pmid = '' # Not in this TSV usually, unless we want to fetch it? The remediation TSV in Block 11 didn't save PMID.\n",
    "            \n",
    "            # --- DISPLAY SECTION ---\n",
    "            \n",
    "            display(Markdown(f\"### {mode_str} | Similarity: {item['ratio']:.2f}\"))\n",
    "            \n",
    "            # HTML Links\n",
    "            url_pmcid = f\"https://europepmc.org/search?query={curr_pmcid}\"\n",
    "            \n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #6f42c1; font-family: sans-serif;\">\n",
    "                <span style=\"font-weight: bold; margin-right: 10px;\">ID:</span>\n",
    "                <a href=\"{url_pmcid}\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #6610f2; margin-right: 20px; font-size: 1.1em;\">{curr_pmcid} ↗</a>\n",
    "            </div>\n",
    "            <br>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Comparison Loop for Remediation Fields ('Title', 'Authors', 'Journal', 'Year', 'DOI')\n",
    "            fields = [\n",
    "                ('Title', 'publication/title', 'Title'),\n",
    "                ('Authors', 'publication/authors', 'Authors'),\n",
    "                ('Journal', 'publication/journal', 'Journal'),\n",
    "                ('Year', 'publication/year', 'Year'),\n",
    "                ('DOI', 'publication/doi', 'DOI')\n",
    "            ]\n",
    "            \n",
    "            for label, k_json, k_tsv in fields:\n",
    "                v_json = str(item['json'].get(k_json, \"\")).strip()\n",
    "                v_tsv = item['tsv'].get(k_tsv, \"\")\n",
    "                \n",
    "                if k_tsv == 'Year': v_tsv = clean_val(v_tsv)\n",
    "                else: v_tsv = str(v_tsv).strip() if pd.notna(v_tsv) else \"\"\n",
    "                \n",
    "                match = v_json == v_tsv\n",
    "                symbol = \"✅\" if match else \"❌\"\n",
    "                \n",
    "                if label == 'DOI':\n",
    "                    def make_doi_link(v):\n",
    "                        if not v: return \"<em>(empty)</em>\"\n",
    "                        return f'<a href=\"https://doi.org/{v}\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">{v} ↗</a>'\n",
    "                    \n",
    "                    display(HTML(f\"<strong>{symbol} [{label}]</strong>\"))\n",
    "                    if not match:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;JSON: {make_doi_link(v_json)}\"))\n",
    "                        display(HTML(f\"&nbsp;&nbsp;TSV : {make_doi_link(v_tsv)}\"))\n",
    "                    else:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;{make_doi_link(v_json)}\"))\n",
    "                    print(\"-\" * 60)\n",
    "                else:\n",
    "                    print(f\"{symbol} [{label}]\")\n",
    "                    if not match:\n",
    "                        print(f\"  JSON: {v_json}\")\n",
    "                        print(f\"  TSV : {v_tsv}\")\n",
    "                    else:\n",
    "                        if len(v_json) > 100:\n",
    "                            print(f\"  {v_json[:100]}...\")\n",
    "                        else:\n",
    "                            print(f\"  {v_json}\")\n",
    "                    print(\"-\" * 60)\n",
    "            \n",
    "            # --- EXPANDABLE REST OF DATA ---\n",
    "            shown_keys = [f[1] for f in fields]\n",
    "            remaining_data = {k: v for k, v in item['json'].items() if k not in shown_keys}\n",
    "            \n",
    "            if remaining_data:\n",
    "                json_str = json.dumps(remaining_data, indent=2)\n",
    "                safe_json_str = html.escape(json_str)\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                <br>\n",
    "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
    "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
    "                        ▶ Show Remaining JSON Data ({len(remaining_data)} fields)\n",
    "                    </summary>\n",
    "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{safe_json_str}</pre>\n",
    "                </details>\n",
    "                \"\"\"))\n",
    "            \n",
    "            print(\"\\n(Run cell again [Ctrl+Enter] to flip between Matches and Mismatches)\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Files not found.\")\n",
    "        print(f\"TSV Exists: {os.path.exists(tsv_path)}\")\n",
    "        print(f\"JSON Folder Exists: {os.path.exists(source_json_folder)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
