{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139277be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aef963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FILTERING AND UPDATING JSON METADATA (Block 5.0)\n",
      "============================================================\n",
      "Reading TSV: Positive_PMC_TSV_Files/positive_entries_status.tsv\n",
      "Found 1012 JSON files in Copilot_1000_v0_Processed_2026-01-15\n",
      "Entries matching JSON files: 1012\n",
      "Saved filtered TSV to: Positive_PMC_TSV_Files/positive_entries_pmid_pmcid_filtered.tsv\n",
      "Updating JSON files with publication IDs (checking order)...\n",
      "Successfully updated 1012 JSON files.\n"
     ]
    }
   ],
   "source": [
    "# Block 5.0: Filter IDs based on Copilot Processed folder and update JSONs\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILTERING AND UPDATING JSON METADATA (Block 5.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Define paths\n",
    "input_tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "json_folder_path = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "filtered_tsv_output = 'Positive_PMC_TSV_Files/positive_entries_pmid_pmcid_filtered.tsv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(input_tsv_path) and os.path.exists(json_folder_path):\n",
    "        # 2. Read TSV\n",
    "        print(f\"Reading TSV: {input_tsv_path}\")\n",
    "        df = pd.read_csv(input_tsv_path, sep='\\t')\n",
    "        \n",
    "        # 3. Extract and Clean IDs\n",
    "        # We need PMCID and PMID. Note: PMID might be float from previous steps\n",
    "        # Create a simplified copy\n",
    "        df_ids = df[['PMID', 'PMCID']].copy()\n",
    "        \n",
    "        def clean_pmid(val):\n",
    "            if pd.isna(val) or val == '':\n",
    "                return None\n",
    "            try:\n",
    "                # Convert to float then int to drop decimal, then string\n",
    "                return str(int(float(val)))\n",
    "            except:\n",
    "                return str(val)\n",
    "\n",
    "        df_ids['PMID'] = df_ids['PMID'].apply(clean_pmid)\n",
    "        df_ids['PMCID'] = df_ids['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        # 4. Get list of JSON files to filter against\n",
    "        json_files = [f for f in os.listdir(json_folder_path) if f.endswith('.json')]\n",
    "        # Create a set of PMCIDs from filenames (remove .json extension)\n",
    "        # Assuming filenames are like \"PMC12345.json\"\n",
    "        json_pmcids = set(f.replace('.json', '') for f in json_files)\n",
    "        \n",
    "        print(f\"Found {len(json_pmcids)} JSON files in {json_folder_path}\")\n",
    "        \n",
    "        # 5. Filter the DataFrame\n",
    "        # Keep row if its PMCID matches one in the folder\n",
    "        df_filtered = df_ids[df_ids['PMCID'].isin(json_pmcids)].copy()\n",
    "        \n",
    "        count = len(df_filtered)\n",
    "        print(f\"Entries matching JSON files: {count}\")\n",
    "        \n",
    "        # 6. Save the filtered TSV\n",
    "        df_filtered.to_csv(filtered_tsv_output, sep='\\t', index=False)\n",
    "        print(f\"Saved filtered TSV to: {filtered_tsv_output}\")\n",
    "        \n",
    "        # 7. Update JSON files\n",
    "        print(\"Updating JSON files with publication IDs (checking order)...\")\n",
    "        updated_count = 0\n",
    "        \n",
    "        for index, row in df_filtered.iterrows():\n",
    "            pmcid = row['PMCID']\n",
    "            pmid = row['PMID']\n",
    "            \n",
    "            if not pmcid:\n",
    "                continue\n",
    "                \n",
    "            json_file_path = os.path.join(json_folder_path, f\"{pmcid}.json\")\n",
    "            \n",
    "            if os.path.exists(json_file_path):\n",
    "                try:\n",
    "                    with open(json_file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # Prepare new data dict to preserve/enforce order\n",
    "                    # Target order: ..., publication/year, publication/pmid, publication/pmcid, publication/doi ...\n",
    "                    new_data = {}\n",
    "                    inserted = False\n",
    "                    \n",
    "                    pmid_val = pmid if pmid else \"\"\n",
    "                    pmcid_val = pmcid\n",
    "                    \n",
    "                    # If pmid/pmcid keys already exist in data, skip them during iteration\n",
    "                    keys_to_skip = ['publication/pmid', 'publication/pmcid']\n",
    "                    \n",
    "                    for key, value in data.items():\n",
    "                        if key in keys_to_skip:\n",
    "                            continue\n",
    "                            \n",
    "                        new_data[key] = value\n",
    "                        \n",
    "                        # Insert new keys immediately after publication/year\n",
    "                        if key == 'publication/year':\n",
    "                            new_data['publication/pmid'] = pmid_val\n",
    "                            new_data['publication/pmcid'] = pmcid_val\n",
    "                            inserted = True\n",
    "                            \n",
    "                    # Fallback: if 'publication/year' was not found, add them at the end\n",
    "                    if not inserted:\n",
    "                        new_data['publication/pmid'] = pmid_val\n",
    "                        new_data['publication/pmcid'] = pmcid_val\n",
    "                    \n",
    "                    with open(json_file_path, 'w') as f:\n",
    "                        json.dump(new_data, f, indent=2)\n",
    "                        \n",
    "                    updated_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating {pmcid}.json: {e}\")\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        print(f\"Successfully updated {updated_count} JSON files.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Input file or folder not found.\")\n",
    "        print(f\"TSV: {os.path.exists(input_tsv_path)}\")\n",
    "        print(f\"Folder: {os.path.exists(json_folder_path)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e9c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c938f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:185: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:186: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:185: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:186: SyntaxWarning: invalid escape sequence '\\|'\n",
      "/tmp/ipykernel_102433/3177539443.py:185: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  j_t = m['JSON'].replace('|', '\\|')\n",
      "/tmp/ipykernel_102433/3177539443.py:186: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  t_t = m['TSV'].replace('|', '\\|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYZING METADATA DIFFERENCES & GENERATING REPORT (Block 6.0)\n",
      "============================================================\n",
      "Created report directory: Metadata_Analysis_Reports/Report_2026-01-15_15-18-53\n",
      "Loading TSV...\n",
      "Comparing files...\n",
      "Detailed mismatch CSV saved to: Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/all_mismatches.csv\n",
      "\n",
      "Analysis Complete!\n",
      "Report and Visualizations generated in folder: Metadata_Analysis_Reports/Report_2026-01-15_15-18-53\n",
      "  - Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/Analysis_Report.md\n",
      "  - Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/mismatch_counts.png\n",
      "  - Metadata_Analysis_Reports/Report_2026-01-15_15-18-53/title_similarity.png\n"
     ]
    }
   ],
   "source": [
    "# Block 6.0: Analyze Metadata Differences (TSV vs JSON) - Enhanced Reporting\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import difflib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYZING METADATA DIFFERENCES & GENERATING REPORT (Block 6.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Paths\n",
    "tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "\n",
    "# Create Report Folder\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "report_folder = f\"Metadata_Analysis_Reports/Report_{timestamp}\"\n",
    "\n",
    "# Field mapping\n",
    "field_map = {\n",
    "    'publication/title': 'Title',\n",
    "    'publication/authors': 'Authors',\n",
    "    'publication/journal': 'Journal',\n",
    "    'publication/year': 'Year',\n",
    "    'publication/doi': 'DOI'\n",
    "}\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(report_folder):\n",
    "        os.makedirs(report_folder)\n",
    "        print(f\"Created report directory: {report_folder}\")\n",
    "\n",
    "    if os.path.exists(tsv_path) and os.path.exists(json_folder):\n",
    "        print(\"Loading TSV...\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        # Metrics\n",
    "        total_jsons = 0\n",
    "        matched_jsons = 0\n",
    "        diff_counts = {k: 0 for k in field_map.keys()}\n",
    "        \n",
    "        # Title specific\n",
    "        title_stats = {\n",
    "            'exact': 0,\n",
    "            'case_only': 0,\n",
    "            'minor_diffs': 0, # > 0.8\n",
    "            'major_diffs': 0  # < 0.8\n",
    "        }\n",
    "        \n",
    "        # Detailed mismatch logs\n",
    "        major_title_diffs = []\n",
    "        all_field_mismatches = [] # store dicts of {pmcid, field, json_val, tsv_val}\n",
    "\n",
    "        print(\"Comparing files...\")\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_jsons = len(json_files)\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            row = df[df['PMCID_clean'] == pmcid]\n",
    "            \n",
    "            if len(row) == 0: continue\n",
    "            matched_jsons += 1\n",
    "            row = row.iloc[0]\n",
    "            \n",
    "            with open(os.path.join(json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            for json_key, tsv_col in field_map.items():\n",
    "                j_val = str(data.get(json_key, \"\")).strip()\n",
    "                t_val = row[tsv_col]\n",
    "                \n",
    "                # Check TSV nan\n",
    "                if pd.isna(t_val): t_val = \"\"\n",
    "                \n",
    "                # Clean Year\n",
    "                if tsv_col == 'Year' and t_val != \"\":\n",
    "                    try: t_val = str(int(float(t_val)))\n",
    "                    except: t_val = str(t_val)\n",
    "                else:\n",
    "                    t_val = str(t_val).strip()\n",
    "                \n",
    "                if j_val != t_val:\n",
    "                    diff_counts[json_key] += 1\n",
    "                    \n",
    "                    # Log mismatch\n",
    "                    all_field_mismatches.append({\n",
    "                        'PMCID': pmcid,\n",
    "                        'Field': json_key,\n",
    "                        'JSON_Value': j_val,\n",
    "                        'TSV_Value': t_val\n",
    "                    })\n",
    "                    \n",
    "                    if json_key == 'publication/title':\n",
    "                         if j_val.lower() == t_val.lower():\n",
    "                            title_stats['case_only'] += 1\n",
    "                         else:\n",
    "                            ratio = difflib.SequenceMatcher(None, j_val, t_val).ratio()\n",
    "                            if ratio > 0.8:\n",
    "                                title_stats['minor_diffs'] += 1\n",
    "                            else:\n",
    "                                title_stats['major_diffs'] += 1\n",
    "                                major_title_diffs.append({\n",
    "                                    'PMCID': pmcid, \n",
    "                                    'JSON': j_val, \n",
    "                                    'TSV': t_val, \n",
    "                                    'Sim': f\"{ratio:.2f}\"\n",
    "                                })\n",
    "                else:\n",
    "                     if json_key == 'publication/title':\n",
    "                        title_stats['exact'] += 1\n",
    "                        \n",
    "        # --- GENERATE OUTPUTS ---\n",
    "        \n",
    "        # 1. Visualization\n",
    "        # Mismatch Counts Bar Chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        keys = list(diff_counts.keys())\n",
    "        vals = list(diff_counts.values())\n",
    "        colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#c2c2f0']\n",
    "        \n",
    "        plt.bar(keys, vals, color=colors[:len(keys)])\n",
    "        plt.title(f'Metadata Mismatches per Field (N={matched_jsons})')\n",
    "        plt.xlabel('Field')\n",
    "        plt.ylabel('Count of Mismatches')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(report_folder, 'mismatch_counts.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Title Breakdown Pie Chart\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        labels = [f\"Exact ({title_stats['exact']})\", \n",
    "                  f\"Case Only ({title_stats['case_only']})\", \n",
    "                  f\"Minor >0.8 ({title_stats['minor_diffs']})\",\n",
    "                  f\"Major <0.8 ({title_stats['major_diffs']})\"]\n",
    "        sizes = [title_stats['exact'], title_stats['case_only'], title_stats['minor_diffs'], title_stats['major_diffs']]\n",
    "        \n",
    "        # Filter zero slices to avoid cluttered legend\n",
    "        pie_data = [(l, s) for l, s in zip(labels, sizes) if s > 0]\n",
    "        if pie_data:\n",
    "            p_labels, p_sizes = zip(*pie_data)\n",
    "            plt.pie(p_sizes, labels=p_labels, autopct='%1.1f%%', startangle=140)\n",
    "            plt.title('Title Similarity Breakdown')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(report_folder, 'title_similarity.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Detailed Report File (Markdown)\n",
    "        report_path = os.path.join(report_folder, 'Analysis_Report.md')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"# Metadata Analysis Report\\n\")\n",
    "            f.write(f\"**Date:** {timestamp}\\n\\n\")\n",
    "            f.write(f\"**Source JSON Folder:** `{json_folder}`\\n\")\n",
    "            f.write(f\"**Source TSV:** `{tsv_path}`\\n\")\n",
    "            f.write(f\"**Total Files Scanned:** {total_jsons}\\n\")\n",
    "            f.write(f\"**Files Matched to TSV:** {matched_jsons}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 1. Field Mismatch Summary\\n\")\n",
    "            f.write(\"| Field | Mismatches | %\\n\")\n",
    "            f.write(\"|---|---|---|\\n\")\n",
    "            for k, v in diff_counts.items():\n",
    "                pct = (v/matched_jsons)*100 if matched_jsons else 0\n",
    "                f.write(f\"| {k} | {v} | {pct:.1f}% |\\n\")\n",
    "            \n",
    "            f.write(\"\\n![Mismatch Counts](mismatch_counts.png)\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 2. Title Similarity Breakdown\\n\")\n",
    "            f.write(f\"- **Exact Matches:** {title_stats['exact']}\\n\")\n",
    "            f.write(f\"- **Case-only Differences:** {title_stats['case_only']}\\n\")\n",
    "            f.write(f\"- **Minor Differences (>80%):** {title_stats['minor_diffs']}\\n\")\n",
    "            f.write(f\"- **Major Differences (<80%):** {title_stats['major_diffs']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n![Title Breakdown](title_similarity.png)\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 3. Major Title Differences (Low Similarity)\\n\")\n",
    "            if major_title_diffs:\n",
    "                f.write(\"| PMCID | JSON Title | TSV Title | Similarity |\\n\")\n",
    "                f.write(\"|---|---|---|---|\\n\")\n",
    "                for m in major_title_diffs:\n",
    "                    # Escape pipes for markdown table\n",
    "                    j_t = m['JSON'].replace('|', '\\|')\n",
    "                    t_t = m['TSV'].replace('|', '\\|')\n",
    "                    f.write(f\"| {m['PMCID']} | {j_t} | {t_t} | {m['Sim']} |\\n\")\n",
    "            else:\n",
    "                f.write(\"No major title differences found.\\n\")\n",
    "                \n",
    "        # 3. CSV export of all mismatches\n",
    "        if all_field_mismatches:\n",
    "            csv_path = os.path.join(report_folder, 'all_mismatches.csv')\n",
    "            pd.DataFrame(all_field_mismatches).to_csv(csv_path, index=False)\n",
    "            print(f\"Detailed mismatch CSV saved to: {csv_path}\")\n",
    "\n",
    "        print(f\"\\nAnalysis Complete!\")\n",
    "        print(f\"Report and Visualizations generated in folder: {report_folder}\")\n",
    "        print(f\"  - {os.path.join(report_folder, 'Analysis_Report.md')}\")\n",
    "        print(f\"  - {os.path.join(report_folder, 'mismatch_counts.png')}\")\n",
    "        print(f\"  - {os.path.join(report_folder, 'title_similarity.png')}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Input files not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e67b6758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING COVERAGE ANALYSIS (Block 9.0)\n",
      "============================================================\n",
      "Created report directory: Coverage_Analysis_Reports/Report_2026-01-15_15-27-46\n",
      "Scanning 1012 files for missing information markers...\n",
      "Scan complete. Generating analysis...\n",
      "\n",
      "Analysis Successfully Completed.\n",
      "Outputs saved to: Coverage_Analysis_Reports/Report_2026-01-15_15-27-46/\n",
      "  - Report: Coverage_Analysis_Report.md\n",
      "  - Plots: Category_Summary.png, Field_Level_Analysis.png\n",
      "  - Data: coverage_stats.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Block 9.0: \"Not Enough Information\" Coverage Analysis\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING COVERAGE ANALYSIS (Block 9.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Setup\n",
    "json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "target_phrase = \"Not enough information is available\" # We'll match this loosely (starts with)\n",
    "\n",
    "# Create Output Folder\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Coverage_Analysis_Reports/Report_{timestamp}\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created report directory: {output_dir}\")\n",
    "\n",
    "# Categories Mapping\n",
    "def get_category(key):\n",
    "    if key.startswith('publication'): return 'Publication'\n",
    "    if key.startswith('dataset'): return 'Data'\n",
    "    if key.startswith('optimization'): return 'Optimisation'\n",
    "    if key.startswith('model'): return 'Model'\n",
    "    if key.startswith('evaluation'): return 'Evaluation'\n",
    "    return 'Other'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(json_folder):\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        print(f\"Scanning {total_files} files for missing information markers...\")\n",
    "        \n",
    "        # Structure to hold counts\n",
    "        # counts[field] = num_missing\n",
    "        field_counts = {}\n",
    "        all_fields = set()\n",
    "        \n",
    "        # 2. Scan Files\n",
    "        for json_file in json_files:\n",
    "            with open(os.path.join(json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            for key, val in data.items():\n",
    "                all_fields.add(key)\n",
    "                if key not in field_counts:\n",
    "                    field_counts[key] = 0\n",
    "                \n",
    "                # Check for target phrase\n",
    "                val_str = str(val).strip()\n",
    "                if val_str.startswith(target_phrase):\n",
    "                    field_counts[key] += 1\n",
    "\n",
    "        print(\"Scan complete. Generating analysis...\")\n",
    "\n",
    "        # 3. Process Data into DataFrame\n",
    "        data_list = []\n",
    "        for key in all_fields:\n",
    "            missing_count = field_counts.get(key, 0)\n",
    "            category = get_category(key)\n",
    "            pct_missing = (missing_count / total_files) * 100\n",
    "            \n",
    "            data_list.append({\n",
    "                'Field': key,\n",
    "                'Category': category,\n",
    "                'Missing_Count': missing_count,\n",
    "                'Total_Files': total_files,\n",
    "                'Missing_Percentage': pct_missing\n",
    "            })\n",
    "            \n",
    "        df_stats = pd.DataFrame(data_list)\n",
    "        \n",
    "        # Sort for consistency\n",
    "        df_stats = df_stats.sort_values(by=['Category', 'Field'])\n",
    "        \n",
    "        # 4. Generate Visualizations\n",
    "        \n",
    "        # A. Main Category Aggregation (Mean % Missing per category)\n",
    "        category_stats = df_stats.groupby('Category')['Missing_Percentage'].mean().reset_index()\n",
    "        # Custom sort order\n",
    "        cat_order = ['Publication', 'Data', 'Optimisation', 'Model', 'Evaluation']\n",
    "        category_stats['Category'] = pd.Categorical(category_stats['Category'], categories=cat_order, ordered=True)\n",
    "        category_stats = category_stats.sort_values('Category')\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(category_stats['Category'], category_stats['Missing_Percentage'], color='#4c72b0')\n",
    "        plt.title('Average Information Gap by Main Category\\n(% of fields marked \"Not enough information is available\")', fontsize=12)\n",
    "        plt.ylabel('Avg. Missing %', fontsize=10)\n",
    "        plt.xlabel('Category', fontsize=10)\n",
    "        plt.ylim(0, 100)\n",
    "        \n",
    "        # Add labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                     f'{height:.1f}%', ha='center', va='bottom')\n",
    "                     \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'Category_Summary.png'), dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        # B. Granular Subfields (Grouped Bar Chart)\n",
    "        # We'll make subplots for each category to ensure readability\n",
    "        unique_cats = [c for c in cat_order if c in df_stats['Category'].unique()]\n",
    "        \n",
    "        # Figure size depends on number of fields\n",
    "        fig, axes = plt.subplots(nrows=len(unique_cats), ncols=1, figsize=(12, 4 * len(unique_cats)), constrained_layout=True)\n",
    "        if len(unique_cats) == 1: axes = [axes] # Handle single category case\n",
    "        \n",
    "        for i, cat in enumerate(unique_cats):\n",
    "            ax = axes[i]\n",
    "            subset = df_stats[df_stats['Category'] == cat].sort_values('Missing_Percentage', ascending=False)\n",
    "            \n",
    "            # Simple barh\n",
    "            y_pos = np.arange(len(subset))\n",
    "            ax.barh(y_pos, subset['Missing_Percentage'], align='center', color='#55a868')\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(subset['Field'])\n",
    "            ax.invert_yaxis() # labels read top-to-bottom\n",
    "            ax.set_xlabel('% Coverage Gap')\n",
    "            ax.set_title(f'Category: {cat}')\n",
    "            ax.set_xlim(0, 100)\n",
    "            \n",
    "            # Add text labels\n",
    "            for j, v in enumerate(subset['Missing_Percentage']):\n",
    "                ax.text(v + 1, j, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "\n",
    "        plt.suptitle(f'Detailed Gap Analysis by Field (Total Files: {total_files})', fontsize=16)\n",
    "        plt.savefig(os.path.join(output_dir, 'Field_Level_Analysis.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # 5. Generate Report Document\n",
    "        report_file = os.path.join(output_dir, 'Coverage_Analysis_Report.md')\n",
    "        \n",
    "        with open(report_file, 'w') as r:\n",
    "            r.write(f\"# Metadata Coverage Analysis Report\\n\")\n",
    "            r.write(f\"**Date:** {timestamp}\\n\")\n",
    "            r.write(f\"**Dataset:** {total_files} JSON files from `{json_folder}`\\n\")\n",
    "            r.write(f\"**Target Phrase:** \\\"{target_phrase}...\\\"\\n\\n\")\n",
    "            \n",
    "            r.write(\"## 1. Executive Summary\\n\")\n",
    "            r.write(\"The following chart shows the average percentage of fields marked as 'Not enough information is available' within each main category.\\n\\n\")\n",
    "            r.write(\"![Category Summary](Category_Summary.png)\\n\\n\")\n",
    "            \n",
    "            r.write(\"## 2. Category Statistics\\n\")\n",
    "            r.write(\"| Category | Avg Missing % | Max Missing Field | Min Missing Field |\\n\")\n",
    "            r.write(\"|---|---|---|---|\\n\")\n",
    "            \n",
    "            for cat in unique_cats:\n",
    "                subset = df_stats[df_stats['Category'] == cat]\n",
    "                avg = subset['Missing_Percentage'].mean()\n",
    "                max_f = subset.loc[subset['Missing_Percentage'].idxmax()]\n",
    "                min_f = subset.loc[subset['Missing_Percentage'].idxmin()]\n",
    "                \n",
    "                r.write(f\"| **{cat}** | {avg:.1f}% | {max_f['Field']} ({max_f['Missing_Percentage']:.1f}%) | {min_f['Field']} ({min_f['Missing_Percentage']:.1f}%) |\\n\")\n",
    "                \n",
    "            r.write(\"\\n## 3. Detailed Field Breakdown\\n\")\n",
    "            r.write(\"![Field Level Analysis](Field_Level_Analysis.png)\\n\\n\")\n",
    "            \n",
    "            r.write(\"### Full Data Table\\n\")\n",
    "            r.write(\"| Category | Field | Missing Count | Missing % |\\n\")\n",
    "            r.write(\"|---|---|---|---|\\n\")\n",
    "            \n",
    "            # Re-sort nicely for table\n",
    "            df_table = df_stats.sort_values(by=['Category', 'Missing_Percentage'], ascending=[True, False])\n",
    "            \n",
    "            for _, row in df_table.iterrows():\n",
    "                r.write(f\"| {row['Category']} | {row['Field']} | {row['Missing_Count']} | {row['Missing_Percentage']:.1f}% |\\n\")\n",
    "        \n",
    "        # 6. Save CSV Data\n",
    "        df_stats.to_csv(os.path.join(output_dir, 'coverage_stats.csv'), index=False)\n",
    "        \n",
    "        print(f\"\\nAnalysis Successfully Completed.\")\n",
    "        print(f\"Outputs saved to: {output_dir}/\")\n",
    "        print(f\"  - Report: Coverage_Analysis_Report.md\")\n",
    "        print(f\"  - Plots: Category_Summary.png, Field_Level_Analysis.png\")\n",
    "        print(f\"  - Data: coverage_stats.csv\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Folder {json_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba70532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FOR MISSING TITLES IN JSON (Block 6.5)\n",
      "============================================================\n",
      "Scanning 1012 JSON files...\n",
      "----------------------------------------\n",
      "Files with title 'Not enough information is available': 25\n",
      "----------------------------------------\n",
      "Files listed:\n",
      "  - PMC8080676.json\n",
      "  - PMC11148103.json\n",
      "  - PMC4368063.json\n",
      "  - PMC3205469.json\n",
      "  - PMC9420706.json\n",
      "  - PMC7988437.json\n",
      "  - PMC7874964.json\n",
      "  - PMC10052279.json\n",
      "  - PMC10365090.json\n",
      "  - PMC6992687.json\n",
      "  - PMC7821214.json\n",
      "  - PMC10785655.json\n",
      "  - PMC9086604.json\n",
      "  - PMC10046420.json\n",
      "  - PMC11140654.json\n",
      "  - PMC10239131.json\n",
      "  - PMC10791584.json\n",
      "  - PMC2846370.json\n",
      "  - PMC11110913.json\n",
      "  - PMC11127166.json\n",
      "  - PMC10235219.json\n",
      "  - PMC6924628.json\n",
      "  - PMC10060474.json\n",
      "  - PMC5685313.json\n",
      "  - PMC9869541.json\n"
     ]
    }
   ],
   "source": [
    "# Block 6.5: Check for \"Not enough information is available\" in Title\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING FOR MISSING TITLES IN JSON (Block 6.5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "json_folder_path = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "target_string = \"Not enough information is available\"\n",
    "missing_title_count = 0\n",
    "missing_title_files = []\n",
    "\n",
    "try:\n",
    "    if os.path.exists(json_folder_path):\n",
    "        json_files = [f for f in os.listdir(json_folder_path) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        \n",
    "        print(f\"Scanning {total_files} JSON files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            file_path = os.path.join(json_folder_path, json_file)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                title = data.get('publication/title', '')\n",
    "                \n",
    "                # Check for specific phrase\n",
    "                if title == target_string:\n",
    "                    missing_title_count += 1\n",
    "                    missing_title_files.append(json_file)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {json_file}: {e}\")\n",
    "                \n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Files with title '{target_string}': {missing_title_count}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if missing_title_count > 0:\n",
    "            print(\"Files listed:\")\n",
    "            for f in missing_title_files:\n",
    "                print(f\"  - {f}\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"Folder not found: {json_folder_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469b3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING UPDATED JSONS IN NEW FOLDER (Block 7.0)\n",
      "============================================================\n",
      "Created directory: Copilot_1000_v0_Processed_2026-01-15_Updated_Metadata\n",
      "Loading TSV reference data...\n",
      "Processing 1012 files...\n",
      "Completed.\n",
      "Total files processed: 1012\n",
      "Files with metadata updates: 1012\n",
      "All files saved to: Copilot_1000_v0_Processed_2026-01-15_Updated_Metadata\n"
     ]
    }
   ],
   "source": [
    "# Block 7.0: Create Updated JSONs with Corrected Metadata\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING UPDATED JSONS IN NEW FOLDER (Block 7.0)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Paths\n",
    "source_json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "target_json_folder = 'Copilot_1000_v0_Processed_2026-01-15_Updated_Metadata'\n",
    "tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "\n",
    "# Field mapping: JSON key -> TSV column\n",
    "field_map = {\n",
    "    'publication/title': 'Title',\n",
    "    'publication/authors': 'Authors',\n",
    "    'publication/journal': 'Journal',\n",
    "    'publication/year': 'Year',\n",
    "    'publication/doi': 'DOI'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 1. Create target directory\n",
    "    if not os.path.exists(target_json_folder):\n",
    "        os.makedirs(target_json_folder)\n",
    "        print(f\"Created directory: {target_json_folder}\")\n",
    "    else:\n",
    "        print(f\"Directory exists: {target_json_folder}\")\n",
    "\n",
    "    if os.path.exists(tsv_path) and os.path.exists(source_json_folder):\n",
    "        print(\"Loading TSV reference data...\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        updated_files_count = 0\n",
    "        \n",
    "        json_files = [f for f in os.listdir(source_json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        \n",
    "        print(f\"Processing {total_files} files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            \n",
    "            # Find TSV row\n",
    "            row = df[df['PMCID_clean'] == pmcid]\n",
    "            \n",
    "            # Load original JSON\n",
    "            source_path = os.path.join(source_json_folder, json_file)\n",
    "            target_path = os.path.join(target_json_folder, json_file)\n",
    "            \n",
    "            with open(source_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # If we have TSV data, update fields if different\n",
    "            if len(row) > 0:\n",
    "                row = row.iloc[0]\n",
    "                \n",
    "                changes_made = False\n",
    "                \n",
    "                for json_key, tsv_col in field_map.items():\n",
    "                    current_val = str(data.get(json_key, \"\")).strip()\n",
    "                    \n",
    "                    # Prepare new value\n",
    "                    new_val_raw = row[tsv_col]\n",
    "                    if pd.isna(new_val_raw):\n",
    "                        new_val = \"\"\n",
    "                    else:\n",
    "                        if tsv_col == 'Year':\n",
    "                            try:\n",
    "                                new_val = str(int(float(new_val_raw)))\n",
    "                            except:\n",
    "                                new_val = str(new_val_raw)\n",
    "                        else:\n",
    "                            new_val = str(new_val_raw).strip()\n",
    "                    \n",
    "                    # Check difference\n",
    "                    if current_val != new_val:\n",
    "                        data[json_key] = new_val\n",
    "                        changes_made = True\n",
    "                \n",
    "                if changes_made:\n",
    "                    updated_files_count += 1\n",
    "            \n",
    "            # Save to new location (either updated or original copy)\n",
    "            with open(target_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "                \n",
    "        print(f\"Completed.\")\n",
    "        print(f\"Total files processed: {total_files}\")\n",
    "        print(f\"Files with metadata updates: {updated_files_count}\")\n",
    "        print(f\"All files saved to: {target_json_folder}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Source data not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a353c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adafe302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Low Similarity (<=20%) | Similarity: 0.19"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #007acc; font-family: sans-serif;\">\n",
       "                <span style=\"font-weight: bold; margin-right: 10px;\">IDS:</span>\n",
       "                <a href=\"https://europepmc.org/search?query=PMC8350610\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; margin-right: 20px; font-size: 1.1em;\">PMC8350610 ↗</a>\n",
       "                <a href=\"https://europepmc.org/search?query=34401383\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; font-size: 1.1em;\">PMID:34401383 ↗</a>\n",
       "            </div>\n",
       "            <br>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ [Title]\n",
      "  JSON: Internet Interventions 25 (2021) 100424\n",
      "  TSV : Predicting acute suicidal ideation on Instagram using ensemble machine learning models.\n",
      "------------------------------------------------------------\n",
      "❌ [Authors]\n",
      "  JSON: The authors who contributed to this article are Damien Lekkas, Robert J. Klein, and Nicholas C. Jacobson. Damien Lekkas was involved in conceptualization, methodology, software, formal analysis, writing the original draft, reviewing and editing, and visualization. Robert J. Klein contributed to writing the original draft and reviewing and editing. Nicholas C. Jacobson assisted with methodology and formal analysis, as well as reviewing and editing the paper.\n",
      "  TSV : Lekkas D, Klein RJ, Jacobson NC\n",
      "------------------------------------------------------------\n",
      "❌ [Journal]\n",
      "  JSON: Internet Interventions\n",
      "  TSV : Internet interventions\n",
      "------------------------------------------------------------\n",
      "✅ [Year]\n",
      "  2021\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>✅ [DOI]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&nbsp;&nbsp;<a href=\"https://doi.org/10.1016/j.invent.2021.100424\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">10.1016/j.invent.2021.100424 ↗</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <br>\n",
       "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
       "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
       "                        ▶ Show Remaining JSON Data (24 fields)\n",
       "                    </summary>\n",
       "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{\n",
       "  &quot;publication/pmid&quot;: &quot;34401383&quot;,\n",
       "  &quot;publication/pmcid&quot;: &quot;PMC8350610&quot;,\n",
       "  &quot;publication/tags&quot;: &quot;- Suicidal Ideation\\n- Machine Learning\\n- Social Media\\n- Natural Language Processing\\n- Predictive Modeling\\n- Mental Health\\n- Instagram\\n- Risk Factors\\n- Ensemble Learning\\n- Data Mining&quot;,\n",
       "  &quot;dataset/provenance&quot;: &quot;The dataset utilized in this study was derived from a previous investigation that explored the connections between acute suicidality, language use, and Instagram activity. Specifically, a subset of German adolescents was selected from a larger study that examined non-suicidal self-injury on Instagram. Public Instagram user data and post content were collected from these subjects over a four-week period prior to a personal interview conducted via Instagram messenger.\\n\\nThe dataset consists of a randomized subset of 52 study participants. The participants had a mean age of 16.6 years, with a median age of 16 years. The majority of the participants were female, accounting for 78.8% of the sample. Most participants were attending high school (76.9%), while a smaller portion were attending university or professional school (13.5%), and a few were unemployed (3.8%). All participants reported a lifetime history of suicidal ideation.\\n\\nThe data collection period of four weeks was chosen because it provided a sufficient amount of data for machine learning models to detect patterns, while also being recent enough to be relevant to the participants&#x27; current suicidal ideation. This dataset was used to build upon previous research, aiming to improve the prediction of acute suicidal ideation using machine learning techniques.&quot;,\n",
       "  &quot;dataset/splits&quot;: &quot;The dataset was analyzed using a ten-fold repeated cross-validation framework. This means that the data was split into ten different subsets, or folds. Each fold was used once as a validation set while the remaining nine folds formed the training set. This process was repeated ten times, with each of the ten folds used exactly once as the validation data.\\n\\nThe distribution of data points in each split was balanced, ensuring that each fold contained an approximately equal number of data points. This approach helps to ensure that the model&#x27;s performance is evaluated on a diverse range of data, reducing the risk of overfitting and providing a more robust estimate of the model&#x27;s generalizability.\\n\\nThe specific number of data points in each split was not explicitly stated, but the use of ten-fold cross-validation implies that the data was divided into ten roughly equal parts. This method is commonly used to provide a comprehensive evaluation of model performance by ensuring that each data point is used for both training and validation across different iterations.&quot;,\n",
       "  &quot;dataset/redundancy&quot;: &quot;In our study, we utilized a dataset consisting of a randomized subset of 52 participants, with a mean age of 16.6 years, predominantly female (78.8%), and mostly attending high school (76.9%). The dataset included both interview data and Instagram activity data collected over a four-week period prior to the interviews. This timeframe was chosen to balance the need for sufficient data to detect patterns while ensuring the relevance of the data to the participants&#x27; current mental state.\\n\\nTo ensure the robustness and generalizability of our models, we employed a repeated, ten-fold cross-validation framework. This approach involves splitting the dataset into ten subsets, or folds, and training the model on nine of these folds while testing it on the remaining fold. This process is repeated ten times, with each fold serving as the test set once. This method ensures that every data point is used for both training and testing, providing a more reliable estimate of the model&#x27;s performance.\\n\\nThe cross-validation framework also enforces independence between the training and test sets in each iteration. This is crucial for evaluating the model&#x27;s ability to generalize to unseen data, which is a key aspect of our study&#x27;s hypotheses. By using this approach, we aimed to address the potential overfitting issues observed in previous in-sample analyses, where the same data was used for both training and testing.\\n\\nRegarding the distribution of our dataset, it is important to note that it is characterized by a small sample size and a sparse set of informative predictors, which is typical for studies involving sensitive topics like suicidal ideation. However, the variables used in our models are generalizable to other online settings, suggesting that our approach could be effective with larger, more feature-rich datasets across different social media platforms. The specific detection of acute suicidal ideation among individuals with a lifetime history of suicidal ideation adds a layer of complexity to our classification task, but it also highlights the potential utility of our ensemble approach in similar contexts.&quot;,\n",
       "  &quot;dataset/availability&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;optimization/algorithm&quot;: &quot;The machine-learning algorithm class used in this study is ensemble learning, specifically a consensus ensemble model. This approach combines the predictions from multiple individual models to improve overall performance. The individual models used include Extreme Gradient Boosted Trees (xgboost), boosted logistic decision trees (logitboost), generalized linear models via penalized maximum likelihood (glmnet), k-nearest neighbors (knn), feed-forward neural networks (nnet), aggregated and averaged random seed neural nets (avnnet), and a naive Bayes classifier (naiveBayes).\\n\\nThese algorithms are not new; they are well-established in the field of machine learning. The choice to use these specific algorithms was driven by their proven effectiveness in various predictive tasks. The ensemble approach was selected to leverage the strengths of multiple models, thereby enhancing the predictive accuracy and robustness of the final consensus model.\\n\\nThe focus of this study is on the application of these machine-learning techniques to predict acute suicidal ideation using Instagram activity and language use data. The algorithms were chosen for their ability to handle complex datasets and provide reliable predictions. The results demonstrate that the ensemble approach significantly improves predictive performance compared to traditional logistic regression models.\\n\\nThe decision to publish this work in a journal focused on internet interventions rather than a machine-learning journal is due to the specific application and context of the study. The primary goal is to highlight the potential of machine learning in predicting acute suicidal ideation using social media data, which is a critical area of research in mental health and internet interventions. The study aims to contribute to the development of more effective tools for identifying individuals at risk of acute suicidal thoughts, leveraging the unique insights provided by social media activity.&quot;,\n",
       "  &quot;optimization/meta&quot;: &quot;The model employed in this study is indeed a meta-predictor, leveraging data from multiple machine-learning algorithms as input. Specifically, the prediction probabilities from seven different lower-level models were used as features for the meta-predictor. These models include logitboost, a generalized linear model via penalized maximum likelihood (glmnet), k-nearest neighbors (knn), a three-layer feed-forward neural network (nnet), aggregated and averaged random seed neural nets (avnnet), and a naive Bayes classifier (naiveBayes).\\n\\nThe meta-predictor itself consists of five ensemble learning models: xgboost, logitboost, knn, nnet, and avnnet. Each of these models was run within a ten-fold repeated, cross-validated framework with grid search hyperparameter tuning for maximum accuracy. The final consensus prediction for the acute suicidal ideation binary classification task was obtained by averaging the predictions across these five stacked ensemble models.\\n\\nRegarding the independence of the training data, the use of a ten-fold repeated, cross-validated framework ensures that the data used for training and validation is independent at each fold. This approach helps to mitigate data leakage and overestimation of model performance, providing a more robust evaluation of the model&#x27;s predictive accuracy. The independence of the training data is further supported by the fact that no hyperparameter tuning was performed on the seven lower-level models, with the exception of glmnet, which followed hyperparameter recommendations from a separate meta-analytical study. This ensures that the lower-level models were not overfitted to the training data, maintaining the independence of the data used for the meta-predictor.&quot;,\n",
       "  &quot;optimization/encoding&quot;: &quot;In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms could effectively learn from the data. We began by collecting a total of 15 features, which included both linguistic text analysis features and Instagram-specific metadata. The linguistic features were derived from the average sentence length and the average number of syllables per word, yielding six features in total. Additionally, we gathered Instagram user-specific metadata, including the number of total followers, number following, number of pictures posted within the last month, average number of comments per picture within the last month, and average number of likes per picture within the past month.\\n\\nTo enhance the capture of user activity on Instagram, we implemented additional feature engineering. This involved creating four new features: follow ratio, engagement, sum of average comments and average likes per follower, and average comments-to-average-likes ratio. These engineered features provided a more holistic view of user behavior on the platform.\\n\\nPrior to model training, all 15 features were standardized to have a mean of 0 and a standard deviation of 1. This standardization process ensured that each feature contributed equally to the model, preventing any single feature from dominating due to its scale. Subjects for whom Instagram user data was not available were removed from the analysis, resulting in a clean and consistent dataset.\\n\\nThe preprocessing steps were essential for preparing the data for the machine-learning pipeline, which was built and run in R using the caret package. This pipeline included a variety of models, such as Extreme Gradient Boosted Trees, boosted logistic decision trees, generalized linear models via penalized maximum likelihood, k-nearest neighbors, neural networks, and naive Bayes classifiers. The standardized and engineered features served as inputs to these models, enabling them to make accurate predictions about the presence or absence of acute suicidal thoughts.&quot;,\n",
       "  &quot;optimization/parameters&quot;: &quot;In our study, we utilized a total of 15 features as input parameters for our models. These features encompassed both linguistic text analysis variables and Instagram-specific metadata. The linguistic features were derived from interview transcripts, focusing on aspects like negative emotion and readability indices. The Instagram features included metrics such as the number of followers, following, posts, comments, and likes, along with additional engineered features like follow ratio, engagement, and comments-to-likes ratio.\\n\\nThe selection of these parameters was guided by previous research and the goal of capturing a comprehensive profile of user activity and linguistic patterns. No explicit feature selection method was employed beyond this initial choice, as the machine learning pipeline was designed to evaluate the predictive utility of all included features. The models were built and run using the caret package in R, ensuring a systematic and reproducible approach to parameter handling.&quot;,\n",
       "  &quot;optimization/features&quot;: &quot;In the optimization process, a total of 15 features were used as input. These features encompassed both linguistic text analysis features and Instagram-specific metadata. The linguistic features were derived from the average sentence length and the average number of syllables per word. The Instagram features included the number of total followers, number following, number of pictures posted within the last month, average number of comments per picture within the last month, and average number of likes per picture within the past month.\\n\\nAdditionally, four derived features were engineered from the Instagram data to capture user activity more holistically. These included the follow ratio, engagement, the sum of average comments and average likes per follower, and the average comments-to-average-likes ratio.\\n\\nFeature selection was performed using a stepwise logistic regression model within a repeated, ten-fold cross-validated framework. This approach ensured that the selection process was done using the training set only, thereby mitigating the risk of data leakage and overfitting. The final model included only the most significant features, namely negative emotion in interviews and the number of followers on Instagram. This rigorous selection process helped in identifying the most relevant predictors for acute suicidal ideation.&quot;,\n",
       "  &quot;optimization/fitting&quot;: &quot;In our study, we employed a variety of machine learning models to predict acute suicidal ideation, ensuring that we addressed both overfitting and underfitting concerns. We utilized seven different models, including decision tree-based methods like Extreme Gradient Boosted Trees (xgboost) and boosted logistic decision trees (logitboost), as well as other classifiers such as k-nearest neighbors (knn), neural networks (nnet and avnnet), and a naive Bayes classifier (naiveBayes). These models were run at default hyperparameter values to mitigate data leakage and overestimation of model performance due to hyperparameter tuning.\\n\\nTo further enhance the robustness of our predictions, we implemented an ensemble learning approach. The prediction probabilities from each of the seven lower-level models were used as features for five ensemble learning models: xgboost, logitboost, knn, nnet, and avnnet. Each of these ensemble models was run within a ten-fold repeated, cross-validated framework with grid search hyperparameter tuning for maximum accuracy. This cross-validation technique helped in ensuring that our models generalized well to unseen data, thereby ruling out overfitting.\\n\\nThe final consensus prediction was obtained by averaging the predictions across these five stacked ensemble models. This ensemble approach not only improved the predictive performance but also helped in mitigating the risk of underfitting by leveraging the strengths of multiple models.\\n\\nAdditionally, we used the SHAP (SHapley Additive exPlanations) framework to explain the predictions of our consensus ensemble model. SHAP values provided insights into the relative influence of each feature, ensuring that our models were not underfitting by capturing the important predictors effectively.\\n\\nIn summary, our approach involved using a diverse set of models, cross-validation techniques, and ensemble learning to balance the trade-off between overfitting and underfitting, resulting in a robust and generalizable predictive model for acute suicidal ideation.&quot;,\n",
       "  &quot;optimization/regularization&quot;: &quot;In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach was the use of cross-validation, specifically a ten-fold repeated cross-validation framework. This method helps to assess the model&#x27;s performance on different subsets of the data, providing a more reliable estimate of its generalization capability.\\n\\nAdditionally, we utilized regularization techniques within our models. For instance, the generalized linear model via penalized maximum likelihood (glmnet) incorporates regularization paths to prevent overfitting by adding a penalty to the loss function. This helps in reducing the complexity of the model and improving its ability to generalize to new data.\\n\\nFurthermore, we avoided hyperparameter tuning for the seven lower-level models to mitigate data leakage and overestimation of model performance. This decision was made to ensure that the models were not overly tailored to the specific dataset, thereby enhancing their generalizability.\\n\\nIn the meta-layer of stacked models, we employed grid search hyperparameter tuning within the cross-validated framework. This approach helps in finding the optimal hyperparameters that minimize overfitting while maximizing model accuracy.\\n\\nOverall, these techniques collectively contributed to the prevention of overfitting and ensured that our models were robust and generalizable.&quot;,\n",
       "  &quot;optimization/config&quot;: &quot;In our study, we focused on ensuring transparency and reproducibility by detailing the configurations and parameters used in our machine learning models. The hyper-parameter configurations for the seven lower-level models were primarily set to their default package values, with the exception of the generalized linear model via penalized maximum likelihood (glmnet), which followed recommendations from a separate meta-analytical study. This approach was chosen to mitigate data leakage and overestimation of model performance due to hyper-parameter tuning.\\n\\nFor the ensemble learning models, we employed a ten-fold repeated, cross-validated framework with grid search hyper-parameter tuning for maximum accuracy using the automatic grid search feature in caret. This method ensured that each of the five ensemble models (xgboost, logitboost, knn, nnet, and avnnet) was optimized within a robust validation scheme.\\n\\nThe specific model files and optimization parameters are not explicitly provided in the text, but the methods and packages used (such as caret, glmnet, and xgboost) are well-documented and publicly available. Researchers can replicate our findings by following the described procedures and using the specified packages in R. The use of standard packages and well-established methods ensures that the configurations and optimization schedules are accessible and can be implemented by others in the field.\\n\\nThe data and code used in this study are not explicitly mentioned as being available under a specific license, but the methodologies and tools employed are standard in the machine learning community. Researchers interested in replicating or building upon our work can refer to the cited literature and use the same packages and techniques described. This approach promotes transparency and facilitates further research in the prediction of acute suicidal ideation using machine learning techniques.&quot;,\n",
       "  &quot;model/interpretability&quot;: &quot;Machine learning models have often been criticized for their lack of transparency, often referred to as \\&quot;black-box\\&quot; models. This opacity makes it challenging to understand how these models arrive at their predictions. However, recent advancements have addressed this limitation, providing methods to explain model predictions at both global and local levels.\\n\\nOne such method is SHAP (SHapley Additive exPlanations), which is based on Shapley values from game theory. SHAP equates feature values in a prediction task to players in a cooperative game, calculating each feature&#x27;s contribution to the prediction. This results in values that indicate the relative influence of features on prediction outcomes.\\n\\nIn our work, we utilized the SHAP framework to decompose the ensemble consensus machine learning model. This allowed us to investigate the relative influence of each variable used to predict the status of acute suicidal ideation. The SHAP analysis provided notable insights, revealing that four of the top five most influential predictors were associated with social media use behavior rather than interview-related linguistic content. This highlights the potential benefits of leveraging discrete behaviors such as \\&quot;liking\\&quot; and \\&quot;following\\&quot; in addition to natural language processing strategies.\\n\\nThe SHAP values were visualized using the SHAPforxgboost R package, offering a clear and intuitive representation of feature importance. This visualization helps in understanding which features are most influential in predicting acute suicidal ideation, thereby enhancing the transparency of the model.\\n\\nMoreover, the SHAP framework is model-agnostic, meaning it can be applied across various model types, including linear, tree-based, and neural network models. This versatility makes it a powerful tool for interpreting complex machine learning models.\\n\\nIn summary, while traditional machine learning models have suffered from a lack of transparency, methods like SHAP offer a way to explain model predictions. By using SHAP, we were able to gain insights into the features that most influence the prediction of acute suicidal ideation, making our model more interpretable and transparent.&quot;,\n",
       "  &quot;model/output&quot;: &quot;The model developed is a classification model. Specifically, it is designed to predict the presence or absence of acute suicidal thoughts, which is a binary classification task. The model leverages various features, including baseline and derived features from Instagram activity and language use, to make these predictions. The performance of the model is evaluated using metrics such as accuracy, AUC, sensitivity, and specificity, which are commonly used in classification tasks. The final consensus model achieved an accuracy of 70.2% and an AUC of 0.755, indicating its effectiveness in classifying individuals with acute suicidal ideation. The model&#x27;s predictions are based on an ensemble of different machine learning algorithms, including decision tree-based methods, neural networks, and probabilistic classifiers, which work together to improve predictive performance.&quot;,\n",
       "  &quot;model/duration&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;model/availability&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;evaluation/method&quot;: &quot;The evaluation method employed in this study was designed to ensure statistical rigor and generalizability. Initially, a ten-fold repeated cross-validation framework was used to recapitulate the logistic regression model from previous research. This approach helped to assess the model&#x27;s predictive performance more accurately than the original in-sample analysis.\\n\\nFor the machine learning models, a similar ten-fold repeated cross-validation strategy was applied. This involved splitting the data into ten subsets, training the model on nine subsets, and validating it on the remaining subset. This process was repeated ten times, with each subset serving as the validation set once. This method helps to mitigate overfitting and provides a more reliable estimate of the model&#x27;s performance.\\n\\nIn addition to accuracy, other metrics such as the Kappa score, Area Under the Receiver Operating Characteristic Curve (AUROC), specificity, sensitivity, and F1 score were reported. These metrics offer a comprehensive view of the model&#x27;s performance, including its ability to correctly identify both positive and negative cases.\\n\\nThe consensus ensemble model, which averaged predictions from five different ensemble models, was also evaluated using the same cross-validation framework. This approach aimed to leverage the strengths of multiple models to improve overall predictive performance.\\n\\nThe results indicated that the ensemble machine learning approach achieved a final accuracy of 70.2%, which is comparable to the previously reported in-sample analysis accuracy of 69.0% and significantly higher than the out-of-sample logistic regression approach. The AUROC of 0.755 further demonstrated the model&#x27;s ability to discriminate between individuals with acute suicidal ideation and those without.\\n\\nOverall, the evaluation method focused on ensuring that the models were robust, generalizable, and capable of providing reliable predictions in an out-of-sample paradigm. The use of cross-validation and multiple performance metrics helped to validate the effectiveness of the machine learning approach in predicting acute suicidal ideation.&quot;,\n",
       "  &quot;evaluation/measure&quot;: &quot;In the evaluation of our models, several performance metrics were reported to provide a comprehensive assessment of their predictive capabilities. The primary metrics included accuracy, Kappa, the Area Under the Receiver Operating Characteristic Curve (AUROC), specificity, sensitivity (or recall), and the F1 score. These metrics were chosen to offer a well-rounded view of model performance, covering aspects such as overall correctness, agreement beyond chance, the trade-off between true positive and false positive rates, and the balance between precision and recall.\\n\\nAccuracy measures the proportion of correctly predicted instances out of the total instances. It provides a straightforward measure of how often the model is correct. Kappa, on the other hand, adjusts accuracy for the agreement that could be expected by chance, offering a more robust measure of model performance, especially in imbalanced datasets.\\n\\nThe AUROC is a critical metric that evaluates the model&#x27;s ability to distinguish between the positive and negative classes across all possible classification thresholds. It provides a single scalar value that summarizes the model&#x27;s performance across all thresholds, making it a valuable metric for comparing different models.\\n\\nSpecificity and sensitivity are complementary metrics that focus on the model&#x27;s performance in predicting the negative and positive classes, respectively. Specificity measures the proportion of true negatives correctly identified, while sensitivity (or recall) measures the proportion of true positives correctly identified. These metrics are particularly important in the context of predicting acute suicidal ideation, where the costs of false positives and false negatives may differ significantly.\\n\\nThe F1 score is the harmonic mean of precision and recall, providing a single metric that balances these two important aspects of model performance. Precision measures the proportion of true positives among the predicted positives, while recall (or sensitivity) measures the proportion of true positives among the actual positives. The F1 score is especially useful when dealing with imbalanced datasets, as it provides a more nuanced view of model performance than accuracy alone.\\n\\nThe reported metrics are representative of those commonly used in the literature for evaluating machine learning models, particularly in the context of classification tasks. They provide a comprehensive view of model performance, covering aspects such as overall correctness, agreement beyond chance, the trade-off between true positive and false positive rates, and the balance between precision and recall. This set of metrics allows for a thorough evaluation of the models&#x27; predictive capabilities and facilitates comparison with other studies in the field.&quot;,\n",
       "  &quot;evaluation/comparison&quot;: &quot;A comparison to simpler baselines was indeed performed. The research began by replicating the logistic regression model from previous studies by Brown et al. (2019a) and Brown et al. (2019b). This replication was done using a ten-fold repeated cross-validation framework, which is a more rigorous statistical approach than the original in-sample analysis. The predictive performance of this baseline model was found to be lower than what was previously reported, with an accuracy of 55.6% and an AUC of 0.560.\\n\\nIn addition to this baseline comparison, the study also employed a machine learning approach using ensemble models. These models utilized the output from seven lower-level models as predictors. The ensemble models included Extreme Gradient Boosted Trees, boosted logistic decision trees, generalized linear models via penalized maximum likelihood, k-nearest neighbors, feed-forward neural networks, aggregated and averaged random seed neural nets, and a naive Bayes classifier.\\n\\nThe performance of these ensemble models was evaluated and compared to the baseline model. The neural net-based models performed the best among the ensemble models, while the k-nearest neighbors algorithm performed the worst. The final consensus predictions, which averaged the output predictions from the five ensemble models, achieved a superior accuracy of 70.2% and an AUC of 0.755. This ensemble machine learning approach demonstrated better predictive performance than the baseline model.\\n\\nThe comparison to simpler baselines and the use of ensemble models provided a comprehensive evaluation of the predictive performance for acute suicidal ideation. The results highlighted the advantages of using more complex machine learning models over traditional logistic regression in this context.&quot;,\n",
       "  &quot;evaluation/confidence&quot;: &quot;The evaluation of the models in this study involved a rigorous cross-validation approach to ensure the robustness and generalizability of the results. The performance metrics reported include accuracy, Kappa score, AUC (Area Under the Receiver Operating Characteristic Curve), specificity, sensitivity/recall, and F1 score. These metrics were derived from a ten-fold repeated cross-validation framework, which helps in assessing the model&#x27;s performance across different subsets of the data.\\n\\nThe consensus ensemble model, which averaged the predictions from five different ensemble models, achieved an accuracy of 70.2%, an AUC of 0.755, and an F1 score of 0.741. These metrics indicate a statistically significant improvement in predictive performance compared to the baseline logistic regression model, which had an accuracy of approximately 54% and an AUC of 0.560. The improvement in the AUC from 0.560 to 0.755 is particularly noteworthy, as it reflects a better balance between sensitivity and specificity.\\n\\nThe statistical significance of the results was assessed, and the improvement in model predictive ability was found to be statistically significant (p &lt; 0.05). This significance level provides confidence that the observed improvements are not due to random chance but rather reflect a genuine enhancement in the model&#x27;s performance.\\n\\nWhile specific confidence intervals for the performance metrics were not explicitly mentioned, the use of repeated cross-validation and the reported statistical significance suggest a high level of confidence in the results. The cross-validation process helps in estimating the variability of the performance metrics and ensures that the models are not overfitting to the training data. The significant improvement in AUC and other metrics further supports the claim that the ensemble approach is superior to the baseline logistic regression model.&quot;,\n",
       "  &quot;evaluation/availability&quot;: &quot;Not enough information is available.&quot;\n",
       "}</pre>\n",
       "                </details>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Run cell again [Ctrl+Enter] to flip category next time)\n"
     ]
    }
   ],
   "source": [
    "# Block 8.0: Manual Visual Inspection Interface (Alternating High/Low Sim)\n",
    "# Run this cell repeatedly (Ctrl+Enter) to view entries.\n",
    "# It will alternate between High Similarity mismatches (>=80%) and Low Similarity (<=20%).\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import difflib\n",
    "import random\n",
    "import html\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Global toggle for alternation (persists across cell runs)\n",
    "# Initialize only if not present\n",
    "if 'inspection_mode_high' not in globals():\n",
    "    inspection_mode_high = True # True=High, False=Low\n",
    "\n",
    "source_json_folder = 'Copilot_1000_v0_Processed_2026-01-15'\n",
    "tsv_path = 'Positive_PMC_TSV_Files/positive_entries_status.tsv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(tsv_path) and os.path.exists(source_json_folder):\n",
    "        # Load Data\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        \n",
    "        # Helper to clean numeric strings\n",
    "        def clean_val(v):\n",
    "            if pd.isna(v) or v == '': return \"\"\n",
    "            try: return str(int(float(v)))\n",
    "            except: return str(v).strip()\n",
    "\n",
    "        high_sim = [] \n",
    "        low_sim = []\n",
    "        \n",
    "        json_files = [f for f in os.listdir(source_json_folder) if f.endswith('.json')]\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            row = df[df['PMCID_clean'] == pmcid]\n",
    "            if len(row) == 0: continue\n",
    "            row = row.iloc[0]\n",
    "            \n",
    "            with open(os.path.join(source_json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            j_title = str(data.get('publication/title', \"\")).strip()\n",
    "            t_title = str(row['Title']).strip() if pd.notna(row['Title']) else \"\"\n",
    "            \n",
    "            if j_title != t_title:\n",
    "                ratio = difflib.SequenceMatcher(None, j_title, t_title).ratio()\n",
    "                \n",
    "                entry = {\n",
    "                    'pmcid': pmcid,\n",
    "                    'json': data,\n",
    "                    'tsv': row,\n",
    "                    'ratio': ratio\n",
    "                }\n",
    "                \n",
    "                if ratio >= 0.8: high_sim.append(entry)\n",
    "                elif ratio <= 0.2: low_sim.append(entry)\n",
    "\n",
    "        # Toggle Selection Logic\n",
    "        target_pool = []\n",
    "        mode_str = \"\"\n",
    "        \n",
    "        # Try to respect toggle, but fallback if one pool is empty\n",
    "        if inspection_mode_high:\n",
    "            if high_sim: \n",
    "                target_pool = high_sim\n",
    "                mode_str = \"High Similarity (>=80%)\"\n",
    "            elif low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Low Similarity (<=20%) [High list empty]\"\n",
    "        else:\n",
    "            if low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Low Similarity (<=20%)\"\n",
    "            elif high_sim:\n",
    "                target_pool = high_sim\n",
    "                mode_str = \"High Similarity (>=80%) [Low list empty]\"\n",
    "        \n",
    "        # Flip toggle for next run\n",
    "        inspection_mode_high = not inspection_mode_high\n",
    "        \n",
    "        if not target_pool:\n",
    "            print(\"No mismatches found in either category.\")\n",
    "        else:\n",
    "            item = random.choice(target_pool)\n",
    "            \n",
    "            # Prepare IDs for Link\n",
    "            curr_pmcid = item['pmcid']\n",
    "            curr_pmid = clean_val(item['tsv']['PMID'])\n",
    "            \n",
    "            # --- DISPLAY SECTION ---\n",
    "            \n",
    "            # Using display() ensures rich output isn't hidden/truncated easily by text buffer limits\n",
    "            display(Markdown(f\"### {mode_str} | Similarity: {item['ratio']:.2f}\"))\n",
    "            \n",
    "            # Create HTML links to Europe PMC Search\n",
    "            url_pmcid = f\"https://europepmc.org/search?query={curr_pmcid}\"\n",
    "            url_pmid = f\"https://europepmc.org/search?query={curr_pmid}\" if curr_pmid else \"#\"\n",
    "            \n",
    "            # Render Links\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #007acc; font-family: sans-serif;\">\n",
    "                <span style=\"font-weight: bold; margin-right: 10px;\">IDS:</span>\n",
    "                <a href=\"{url_pmcid}\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; margin-right: 20px; font-size: 1.1em;\">{curr_pmcid} ↗</a>\n",
    "                <a href=\"{url_pmid}\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #0066cc; font-size: 1.1em;\">PMID:{curr_pmid} ↗</a>\n",
    "            </div>\n",
    "            <br>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Comparison Loop\n",
    "            fields = [\n",
    "                ('Title', 'publication/title', 'Title'),\n",
    "                ('Authors', 'publication/authors', 'Authors'),\n",
    "                ('Journal', 'publication/journal', 'Journal'),\n",
    "                ('Year', 'publication/year', 'Year'),\n",
    "                ('DOI', 'publication/doi', 'DOI')\n",
    "            ]\n",
    "            \n",
    "            for label, k_json, k_tsv in fields:\n",
    "                v_json = str(item['json'].get(k_json, \"\")).strip()\n",
    "                v_tsv = item['tsv'][k_tsv]\n",
    "                \n",
    "                # Special clean for display\n",
    "                if k_tsv == 'Year': v_tsv = clean_val(v_tsv)\n",
    "                else: v_tsv = str(v_tsv).strip() if pd.notna(v_tsv) else \"\"\n",
    "                \n",
    "                match = v_json == v_tsv\n",
    "                symbol = \"✅\" if match else \"❌\"\n",
    "                \n",
    "                if label == 'DOI':\n",
    "                    # Special HTML handling for clickable DOI\n",
    "                    def make_doi_link(v):\n",
    "                        if not v: return \"<em>(empty)</em>\"\n",
    "                        # Simple cleanup if formatted strangely, but usually just the DOI string\n",
    "                        return f'<a href=\"https://doi.org/{v}\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">{v} ↗</a>'\n",
    "                    \n",
    "                    display(HTML(f\"<strong>{symbol} [{label}]</strong>\"))\n",
    "                    if not match:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;JSON: {make_doi_link(v_json)}\"))\n",
    "                        display(HTML(f\"&nbsp;&nbsp;TSV : {make_doi_link(v_tsv)}\"))\n",
    "                    else:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;{make_doi_link(v_json)}\"))\n",
    "                    print(\"-\" * 60)\n",
    "                else:\n",
    "                    # Use print for content to avoid HTML rendering issues with weird chars\n",
    "                    print(f\"{symbol} [{label}]\")\n",
    "                    if not match:\n",
    "                        print(f\"  JSON: {v_json}\")\n",
    "                        print(f\"  TSV : {v_tsv}\")\n",
    "                    else:\n",
    "                        print(f\"  {v_json}\")\n",
    "                    print(\"-\" * 60)\n",
    "            \n",
    "            # --- EXPANDABLE REST OF DATA ---\n",
    "            # Identify keys already shown\n",
    "            shown_keys = [f[1] for f in fields]\n",
    "            \n",
    "            # Collect remaining data\n",
    "            remaining_data = {k: v for k, v in item['json'].items() if k not in shown_keys}\n",
    "            \n",
    "            if remaining_data:\n",
    "                # Format to JSON string and escape for HTML\n",
    "                json_str = json.dumps(remaining_data, indent=2)\n",
    "                safe_json_str = html.escape(json_str)\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                <br>\n",
    "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
    "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
    "                        ▶ Show Remaining JSON Data ({len(remaining_data)} fields)\n",
    "                    </summary>\n",
    "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{safe_json_str}</pre>\n",
    "                </details>\n",
    "                \"\"\"))\n",
    "            \n",
    "            print(\"\\n(Run cell again [Ctrl+Enter] to flip category next time)\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Files not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2a8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "785a5562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING UNIFIED ANALYSIS ON REGISTRY DATASET (Copilot_v0_Processed_2025-12-04)\n",
      "================================================================================\n",
      "Output Directory: Registry_Analysis_Reports/Report_2026-01-15_16-42-24\n",
      "Loading metadata from: DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv\n",
      "Dropped 10 duplicate PMCIDs from metadata.\n",
      "Loaded 280 rows. Found 237 unique mapped PMCIDs.\n",
      "Found 231 JSON files in Copilot_v0_Processed_2025-12-04/registry_v0\n",
      "Processing files...\n",
      "Processing complete.\n",
      "Analysis Complete.\n",
      "Report saved to: Registry_Analysis_Reports/Report_2026-01-15_16-42-24/Registry_Analysis_Report.md\n"
     ]
    }
   ],
   "source": [
    "# Block 10.0: Unified Analysis on Copilot_v0 (Older Batch) with Registry Metadata\n",
    "# This block repeats:\n",
    "# 1. Title Mismatch Analysis\n",
    "# 2. \"No Information\" Coverage Analysis & Visualisation\n",
    "# 3. Uses 'PMCIDs_DOME_Registry_Contents_2026-01-09.tsv' because it contains 'mapped_pmcid'\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import difflib\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING UNIFIED ANALYSIS ON REGISTRY DATASET (Copilot_v0_Processed_2025-12-04)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Updated path to point to the inner directory containing JSONs\n",
    "json_folder = 'Copilot_v0_Processed_2025-12-04/registry_v0'\n",
    "# User requested 'flattened_...' but that file lacks 'mapped_pmcid'. \n",
    "# Using 'PMCIDs_...' which contains the mapping column.\n",
    "tsv_path = 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Registry_Analysis_Reports/Report_{timestamp}\"\n",
    "target_phrase = \"Not enough information is available\"\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output Directory: {output_dir}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_category(key):\n",
    "    if key.startswith('publication'): return 'Publication'\n",
    "    if key.startswith('dataset'): return 'Data'\n",
    "    if key.startswith('optimization'): return 'Optimisation'\n",
    "    if key.startswith('model'): return 'Model'\n",
    "    if key.startswith('evaluation'): return 'Evaluation'\n",
    "    return 'Other'\n",
    "\n",
    "try:\n",
    "    # 1. Load Metadata\n",
    "    print(f\"Loading metadata from: {tsv_path}\")\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # Filter for rows that have a mapped PMCID\n",
    "    df_mapped = df[df['mapped_pmcid'].notna()].copy()\n",
    "    df_mapped['clean_pmcid'] = df_mapped['mapped_pmcid'].apply(lambda x: str(x).strip())\n",
    "    \n",
    "    # Handle duplicates by taking the first occurrence\n",
    "    initial_len = len(df_mapped)\n",
    "    df_mapped = df_mapped.drop_duplicates(subset=['clean_pmcid'], keep='first')\n",
    "    if len(df_mapped) < initial_len:\n",
    "        print(f\"Dropped {initial_len - len(df_mapped)} duplicate PMCIDs from metadata.\")\n",
    "    \n",
    "    # Create lookup dictionary\n",
    "    meta_lookup = df_mapped.set_index('clean_pmcid').to_dict('index')\n",
    "    print(f\"Loaded {len(df)} rows. Found {len(df_mapped)} unique mapped PMCIDs.\")\n",
    "\n",
    "    # 2. Scan JSON Files\n",
    "    if os.path.exists(json_folder):\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        print(f\"Found {total_files} JSON files in {json_folder}\")\n",
    "        \n",
    "        if total_files == 0:\n",
    "            print(\"Warning: No JSON files found. Checking path...\")\n",
    "            print(f\"Path contents: {os.listdir(json_folder)}\")\n",
    "        \n",
    "        # Stats Containers\n",
    "        field_counts = {}\n",
    "        all_fields = set()\n",
    "        \n",
    "        # Mismatch Containers\n",
    "        mismatches = []\n",
    "        matched_count = 0\n",
    "        missing_title_in_json_count = 0\n",
    "        \n",
    "        print(\"Processing files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            file_path = os.path.join(json_folder, json_file)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # --- Analysis A: Coverage (\"Not enough info\") ---\n",
    "            for key, val in data.items():\n",
    "                all_fields.add(key)\n",
    "                if key not in field_counts: field_counts[key] = 0\n",
    "                \n",
    "                if str(val).strip().startswith(target_phrase):\n",
    "                    field_counts[key] += 1\n",
    "            \n",
    "            # --- Analysis B: Title Mismatches & Metadata Validation ---\n",
    "            # Check if JSON title says \"Not enough info\"\n",
    "            json_title = str(data.get('publication/title', '')).strip()\n",
    "            if json_title == target_phrase:\n",
    "                missing_title_in_json_count += 1\n",
    "            \n",
    "            # Compare with Metadata\n",
    "            if pmcid in meta_lookup:\n",
    "                matched_count += 1\n",
    "                row = meta_lookup[pmcid]\n",
    "                \n",
    "                # Metadata Title\n",
    "                tsv_title = str(row.get('publication_title', '')).strip()\n",
    "                \n",
    "                # Calculate Similarity\n",
    "                # Ignore if JSON title is missing/not-enough-info\n",
    "                if json_title and json_title != target_phrase:\n",
    "                    if json_title != tsv_title:\n",
    "                        ratio = difflib.SequenceMatcher(None, json_title, tsv_title).ratio() \n",
    "                        if ratio < 1.0: # Keep all diffs for report\n",
    "                            mismatches.append({\n",
    "                                'pmcid': pmcid,\n",
    "                                'json_title': json_title,\n",
    "                                'tsv_title': tsv_title,\n",
    "                                'ratio': ratio\n",
    "                            })\n",
    "\n",
    "        print(\"Processing complete.\")\n",
    "        \n",
    "        if total_files > 0:\n",
    "            # --- 3. Generate Coverage Report (Same as Block 9.0) ---\n",
    "            data_list = []\n",
    "            for key in all_fields:\n",
    "                missing_count = field_counts.get(key, 0)\n",
    "                category = get_category(key)\n",
    "                pct_missing = (missing_count / total_files) * 100 if total_files > 0 else 0\n",
    "                \n",
    "                data_list.append({\n",
    "                    'Field': key,\n",
    "                    'Category': category,\n",
    "                    'Missing_Count': missing_count,\n",
    "                    'Total_Files': total_files,\n",
    "                    'Missing_Percentage': pct_missing\n",
    "                })\n",
    "                \n",
    "            df_stats = pd.DataFrame(data_list)\n",
    "            df_stats = df_stats.sort_values(by=['Category', 'Field'])\n",
    "            \n",
    "            # -- Plotting --\n",
    "            # 1. Category Summary\n",
    "            category_stats = df_stats.groupby('Category')['Missing_Percentage'].mean().reset_index()\n",
    "            cat_order = ['Publication', 'Data', 'Optimisation', 'Model', 'Evaluation']\n",
    "            category_stats['Category'] = pd.Categorical(category_stats['Category'], categories=cat_order, ordered=True)\n",
    "            category_stats = category_stats.sort_values('Category')\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(category_stats['Category'], category_stats['Missing_Percentage'], color='#4c72b0')\n",
    "            plt.title('Average Information Gap by Category (Registry Dataset)', fontsize=12)\n",
    "            plt.ylabel('Avg. Missing %')\n",
    "            plt.ylim(0, 100)\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 1, f'{height:.1f}%', ha='center', va='bottom')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'Registry_Category_Summary.png'), dpi=150)\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Field Level\n",
    "            unique_cats = [c for c in cat_order if c in df_stats['Category'].unique()]\n",
    "            if not unique_cats:\n",
    "                print(\"No categories found to plot.\")\n",
    "            else:\n",
    "                fig, axes = plt.subplots(nrows=len(unique_cats), ncols=1, figsize=(12, 4 * len(unique_cats)), constrained_layout=True)\n",
    "                if len(unique_cats) == 1: axes = [axes]\n",
    "                \n",
    "                for i, cat in enumerate(unique_cats):\n",
    "                    if i < len(axes): # Safety check\n",
    "                        ax = axes[i]\n",
    "                        subset = df_stats[df_stats['Category'] == cat].sort_values('Missing_Percentage', ascending=False)\n",
    "                        y_pos = np.arange(len(subset))\n",
    "                        ax.barh(y_pos, subset['Missing_Percentage'], align='center', color='#55a868')\n",
    "                        ax.set_yticks(y_pos)\n",
    "                        ax.set_yticklabels(subset['Field'])\n",
    "                        ax.invert_yaxis()\n",
    "                        ax.set_xlabel('% Coverage Gap')\n",
    "                        ax.set_title(f'Category: {cat}')\n",
    "                        ax.set_xlim(0, 100)\n",
    "                        for j, v in enumerate(subset['Missing_Percentage']):\n",
    "                            ax.text(v + 1, j, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "                    \n",
    "                plt.suptitle(f'Detailed Gap Analysis (Registry Data, n={total_files})', fontsize=16)\n",
    "                plt.savefig(os.path.join(output_dir, 'Registry_Field_Analysis.png'), dpi=150)\n",
    "                plt.close()\n",
    "            \n",
    "            # --- 4. Validation Report ---\n",
    "            \n",
    "            report_file = os.path.join(output_dir, 'Registry_Analysis_Report.md')\n",
    "            with open(report_file, 'w') as r:\n",
    "                r.write(f\"# DOME Registry Data Analysis Report\\n\")\n",
    "                r.write(f\"**Date:** {timestamp}\\n\")\n",
    "                r.write(f\"**JSON Dataset:** `{json_folder}` ({total_files} files)\\n\")\n",
    "                r.write(f\"**Metadata:** `{tsv_path}`\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 1. Metadata Linking\\n\")\n",
    "                r.write(f\"- Total JSON Files: {total_files}\\n\")\n",
    "                r.write(f\"- Matched to Registry Metadata: {matched_count} ({(matched_count/total_files)*100:.1f}%)\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 2. Title Analysis\\n\")\n",
    "                r.write(f\"- JSONs with '{target_phrase}' as Title: {missing_title_in_json_count}\\n\")\n",
    "                r.write(f\"- Title Mismatches (vs Metadata): {len(mismatches)}\\n\\n\")\n",
    "                \n",
    "                if mismatches:\n",
    "                    r.write(\"### Low Similarity Title Mismatches (Ratio < 0.5)\\n\")\n",
    "                    r.write(\"| PMCID | JSON Title | Metadata Title | Similarity |\\n\")\n",
    "                    r.write(\"|---|---|---|---|\\n\")\n",
    "                    severe_mismatches = [m for m in mismatches if m['ratio'] < 0.5]\n",
    "                    for m in severe_mismatches[:20]: # Show top 20\n",
    "                        r.write(f\"| {m['pmcid']} | {m['json_title'][:50]}... | {m['tsv_title'][:50]}... | {m['ratio']:.2f} |\\n\")\n",
    "                    if len(severe_mismatches) > 20:\n",
    "                        r.write(f\"| ... | ... | ... | ... |\\n\")\n",
    "                \n",
    "                r.write(\"\\n## 3. Information Coverage\\n\")\n",
    "                r.write(\"![Category Summary](Registry_Category_Summary.png)\\n\\n\")\n",
    "                \n",
    "                r.write(\"| Category | Avg Missing % |\\n\")\n",
    "                r.write(\"|---|---|\\n\")\n",
    "                for _, row in category_stats.iterrows():\n",
    "                    r.write(f\"| {row['Category']} | {row['Missing_Percentage']:.1f}% |\\n\")\n",
    "                    \n",
    "            print(f\"Analysis Complete.\")\n",
    "            print(f\"Report saved to: {report_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: JSON folder {json_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Critical Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db68fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72c649f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING METADATA UPDATE VIA EUROPE PMC API (Block 11.0)\n",
      "================================================================================\n",
      "Created target directory: Copilot_v0_Processed_2025-12-04_Updated_Metadata\n",
      "Found 231 files. Starting metadata fetch...\n",
      "Successfully fetched metadata for 231 entries.\n",
      "Saved remediation TSV to: DOME_Registry_Remediation/registry_metadata_remediation.tsv\n",
      "Updating JSONs in Copilot_v0_Processed_2025-12-04_Updated_Metadata...\n",
      "Update Process Complete.\n",
      "Total files written: 231\n",
      "Files updated with API data: 231\n"
     ]
    }
   ],
   "source": [
    "# Block 11.0: Update Registry JSONs via Europe PMC API (Fetch & Apply)\n",
    "# 1. Scans JSONs in 'Copilot_v0_Processed_2025-12-04/registry_v0' to get PMCIDs.\n",
    "# 2. Queries Europe PMC API for metadata (Title, Authors, Journal, Year, DOI).\n",
    "# 3. Saves fetched metadata to 'DOME_Registry_Remediation/registry_metadata_remediation.tsv'.\n",
    "# 4. Updates JSONs with this new metadata and saves them to 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING METADATA UPDATE VIA EUROPE PMC API (Block 11.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Configuration ---\n",
    "source_folder = 'Copilot_v0_Processed_2025-12-04/registry_v0'\n",
    "target_folder = 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "remediation_tsv = 'DOME_Registry_Remediation/registry_metadata_remediation.tsv'\n",
    "api_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(os.path.dirname(remediation_tsv), exist_ok=True)\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "    print(f\"Created target directory: {target_folder}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def fetch_metadata(pmcids):\n",
    "    \"\"\"\n",
    "    Fetches metadata for a list of PMCIDs in batches.\n",
    "    Returns a dictionary keyed by PMCID.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    batch_size = 50 # Europe PMC handles largish queries, but keep it safe\n",
    "    \n",
    "    # Chunk the PMCIDs\n",
    "    for i in range(0, len(pmcids), batch_size):\n",
    "        batch = pmcids[i:i + batch_size]\n",
    "        query = \" OR \".join([f'PMCID:{pid}' for pid in batch])\n",
    "        \n",
    "        params = {\n",
    "            'query': query,\n",
    "            'format': 'json',\n",
    "            'resultType': 'core',\n",
    "            'pageSize': batch_size\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(api_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for item in data.get('resultList', {}).get('result', []):\n",
    "                pmcid = item.get('pmcid')\n",
    "                if pmcid:\n",
    "                    # Extract fields\n",
    "                    # Authors list to string\n",
    "                    author_list = item.get('authorList', {}).get('author', [])\n",
    "                    authors_str = \", \".join([f\"{a.get('lastName', '')} {a.get('firstName', '')}\".strip() for a in author_list])\n",
    "                    \n",
    "                    results[pmcid] = {\n",
    "                        'PMCID': pmcid,\n",
    "                        'Title': item.get('title', ''),\n",
    "                        'Authors': authors_str,\n",
    "                        'Journal': item.get('journalInfo', {}).get('journal', {}).get('title', ''),\n",
    "                        'Year': item.get('pubYear', ''),\n",
    "                        'DOI': item.get('doi', '')\n",
    "                    }\n",
    "            \n",
    "            # Be polite to the API\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch starting with {batch[0]}: {e}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "try:\n",
    "    # 1. Identify PMCIDs from files\n",
    "    if os.path.exists(source_folder):\n",
    "        json_files = [f for f in os.listdir(source_folder) if f.endswith('.json')]\n",
    "        pmcids_to_fetch = [f.replace('.json', '') for f in json_files]\n",
    "        total_files = len(pmcids_to_fetch)\n",
    "        \n",
    "        print(f\"Found {total_files} files. Starting metadata fetch...\")\n",
    "        \n",
    "        # 2. Fetch Metadata\n",
    "        metadata_map = fetch_metadata(pmcids_to_fetch)\n",
    "        print(f\"Successfully fetched metadata for {len(metadata_map)} entries.\")\n",
    "        \n",
    "        # 3. Save to TSV\n",
    "        df_rem = pd.DataFrame(list(metadata_map.values()))\n",
    "        # Ensure column order\n",
    "        cols = ['PMCID', 'Title', 'Authors', 'Journal', 'Year', 'DOI']\n",
    "        df_rem = df_rem[cols] if not df_rem.empty else pd.DataFrame(columns=cols)\n",
    "        \n",
    "        df_rem.to_csv(remediation_tsv, sep='\\t', index=False)\n",
    "        print(f\"Saved remediation TSV to: {remediation_tsv}\")\n",
    "        \n",
    "        # 4. Update JSONs\n",
    "        print(f\"Updating JSONs in {target_folder}...\")\n",
    "        updated_count = 0\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            source_path = os.path.join(source_folder, json_file)\n",
    "            target_path = os.path.join(target_folder, json_file)\n",
    "            \n",
    "            # Read Source\n",
    "            with open(source_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check if we have new data\n",
    "            if pmcid in metadata_map:\n",
    "                meta = metadata_map[pmcid]\n",
    "                \n",
    "                # Apply updates (always overwrite with fresh API data)\n",
    "                data['publication/title'] = meta['Title']\n",
    "                data['publication/authors'] = meta['Authors']\n",
    "                data['publication/journal'] = meta['Journal']\n",
    "                data['publication/year'] = meta['Year']\n",
    "                data['publication/doi'] = meta['DOI']\n",
    "                \n",
    "                updated_count += 1\n",
    "            \n",
    "            # Write to Target\n",
    "            with open(target_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "                \n",
    "        print(f\"Update Process Complete.\")\n",
    "        print(f\"Total files written: {total_files}\")\n",
    "        print(f\"Files updated with API data: {updated_count}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Source folder {source_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Critical Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af51080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "414b39eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING ANALYSIS ON UPDATED REGISTRY DATASET\n",
      "================================================================================\n",
      "Output Directory: Registry_Analysis_Reports/Report_Updated_2026-01-15_16-53-53\n",
      "Loading metadata from: DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv\n",
      "Dropped 10 duplicate PMCIDs from metadata.\n",
      "Loaded 280 rows. Found 237 unique mapped PMCIDs.\n",
      "Found 231 JSON files in Copilot_v0_Processed_2025-12-04_Updated_Metadata\n",
      "Processing files...\n",
      "Processing complete.\n",
      "Analysis Complete.\n",
      "Report saved to: Registry_Analysis_Reports/Report_Updated_2026-01-15_16-53-53/Updated_Registry_Analysis_Report.md\n"
     ]
    }
   ],
   "source": [
    "# Block 12.0: Unified Analysis on UPDATED Registry JSONs vs Registry Metadata\n",
    "# This performs the same cross-check as Block 10.0, but on the newly remediated JSONs\n",
    "# folder: 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import difflib\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING ANALYSIS ON UPDATED REGISTRY DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Pointing to the new folder created in Block 11.0\n",
    "json_folder = 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "tsv_path = 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2026-01-09.tsv'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Registry_Analysis_Reports/Report_Updated_{timestamp}\"\n",
    "target_phrase = \"Not enough information is available\"\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Output Directory: {output_dir}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_category(key):\n",
    "    if key.startswith('publication'): return 'Publication'\n",
    "    if key.startswith('dataset'): return 'Data'\n",
    "    if key.startswith('optimization'): return 'Optimisation'\n",
    "    if key.startswith('model'): return 'Model'\n",
    "    if key.startswith('evaluation'): return 'Evaluation'\n",
    "    return 'Other'\n",
    "\n",
    "try:\n",
    "    # 1. Load Metadata\n",
    "    print(f\"Loading metadata from: {tsv_path}\")\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # Filter for rows that have a mapped PMCID\n",
    "    df_mapped = df[df['mapped_pmcid'].notna()].copy()\n",
    "    df_mapped['clean_pmcid'] = df_mapped['mapped_pmcid'].apply(lambda x: str(x).strip())\n",
    "    \n",
    "    # Handle duplicates by taking the first occurrence\n",
    "    initial_len = len(df_mapped)\n",
    "    df_mapped = df_mapped.drop_duplicates(subset=['clean_pmcid'], keep='first')\n",
    "    if len(df_mapped) < initial_len:\n",
    "        print(f\"Dropped {initial_len - len(df_mapped)} duplicate PMCIDs from metadata.\")\n",
    "    \n",
    "    # Create lookup dictionary\n",
    "    meta_lookup = df_mapped.set_index('clean_pmcid').to_dict('index')\n",
    "    print(f\"Loaded {len(df)} rows. Found {len(df_mapped)} unique mapped PMCIDs.\")\n",
    "\n",
    "    # 2. Scan JSON Files\n",
    "    if os.path.exists(json_folder):\n",
    "        json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
    "        total_files = len(json_files)\n",
    "        print(f\"Found {total_files} JSON files in {json_folder}\")\n",
    "        \n",
    "        if total_files == 0:\n",
    "            print(\"Warning: No JSON files found.\")\n",
    "        \n",
    "        # Stats Containers\n",
    "        field_counts = {}\n",
    "        all_fields = set()\n",
    "        \n",
    "        # Mismatch Containers\n",
    "        mismatches = []\n",
    "        matched_count = 0\n",
    "        missing_title_in_json_count = 0\n",
    "        \n",
    "        print(\"Processing files...\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            file_path = os.path.join(json_folder, json_file)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # --- Analysis A: Coverage (\"Not enough info\") ---\n",
    "            for key, val in data.items():\n",
    "                all_fields.add(key)\n",
    "                if key not in field_counts: field_counts[key] = 0\n",
    "                \n",
    "                if str(val).strip().startswith(target_phrase):\n",
    "                    field_counts[key] += 1\n",
    "            \n",
    "            # --- Analysis B: Title Mismatches & Metadata Validation ---\n",
    "            # Check if JSON title says \"Not enough info\"\n",
    "            json_title = str(data.get('publication/title', '')).strip()\n",
    "            if json_title == target_phrase:\n",
    "                missing_title_in_json_count += 1\n",
    "            \n",
    "            # Compare with Metadata\n",
    "            if pmcid in meta_lookup:\n",
    "                matched_count += 1\n",
    "                row = meta_lookup[pmcid]\n",
    "                \n",
    "                # Metadata Title\n",
    "                tsv_title = str(row.get('publication_title', '')).strip()\n",
    "                \n",
    "                # Calculate Similarity\n",
    "                # Ignore if JSON title is missing/not-enough-info\n",
    "                if json_title and json_title != target_phrase:\n",
    "                    if json_title != tsv_title:\n",
    "                        ratio = difflib.SequenceMatcher(None, json_title, tsv_title).ratio() \n",
    "                        if ratio < 1.0: # Keep all diffs for report\n",
    "                            mismatches.append({\n",
    "                                'pmcid': pmcid,\n",
    "                                'json_title': json_title,\n",
    "                                'tsv_title': tsv_title,\n",
    "                                'ratio': ratio\n",
    "                            })\n",
    "\n",
    "        print(\"Processing complete.\")\n",
    "        \n",
    "        if total_files > 0:\n",
    "            # --- 3. Generate Coverage Report ---\n",
    "            data_list = []\n",
    "            for key in all_fields:\n",
    "                missing_count = field_counts.get(key, 0)\n",
    "                category = get_category(key)\n",
    "                pct_missing = (missing_count / total_files) * 100 if total_files > 0 else 0\n",
    "                \n",
    "                data_list.append({\n",
    "                    'Field': key,\n",
    "                    'Category': category,\n",
    "                    'Missing_Count': missing_count,\n",
    "                    'Total_Files': total_files,\n",
    "                    'Missing_Percentage': pct_missing\n",
    "                })\n",
    "                \n",
    "            df_stats = pd.DataFrame(data_list)\n",
    "            df_stats = df_stats.sort_values(by=['Category', 'Field'])\n",
    "            \n",
    "            # -- Plotting --\n",
    "            # 1. Category Summary\n",
    "            category_stats = df_stats.groupby('Category')['Missing_Percentage'].mean().reset_index()\n",
    "            cat_order = ['Publication', 'Data', 'Optimisation', 'Model', 'Evaluation']\n",
    "            category_stats['Category'] = pd.Categorical(category_stats['Category'], categories=cat_order, ordered=True)\n",
    "            category_stats = category_stats.sort_values('Category')\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(category_stats['Category'], category_stats['Missing_Percentage'], color='#4c72b0')\n",
    "            plt.title('Average Information Gap by Category (Updated Dataset)', fontsize=12)\n",
    "            plt.ylabel('Avg. Missing %')\n",
    "            plt.ylim(0, 100)\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 1, f'{height:.1f}%', ha='center', va='bottom')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'Updated_Registry_Category_Summary.png'), dpi=150)\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Field Level\n",
    "            unique_cats = [c for c in cat_order if c in df_stats['Category'].unique()]\n",
    "            if not unique_cats:\n",
    "                 print(\"No categories found to plot.\")\n",
    "            else:\n",
    "                fig, axes = plt.subplots(nrows=len(unique_cats), ncols=1, figsize=(12, 4 * len(unique_cats)), constrained_layout=True)\n",
    "                if len(unique_cats) == 1: axes = [axes]\n",
    "                \n",
    "                for i, cat in enumerate(unique_cats):\n",
    "                    if i < len(axes):\n",
    "                        ax = axes[i]\n",
    "                        subset = df_stats[df_stats['Category'] == cat].sort_values('Missing_Percentage', ascending=False)\n",
    "                        y_pos = np.arange(len(subset))\n",
    "                        ax.barh(y_pos, subset['Missing_Percentage'], align='center', color='#55a868')\n",
    "                        ax.set_yticks(y_pos)\n",
    "                        ax.set_yticklabels(subset['Field'])\n",
    "                        ax.invert_yaxis()\n",
    "                        ax.set_xlabel('% Coverage Gap')\n",
    "                        ax.set_title(f'Category: {cat}')\n",
    "                        ax.set_xlim(0, 100)\n",
    "                        for j, v in enumerate(subset['Missing_Percentage']):\n",
    "                            ax.text(v + 1, j, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "                        \n",
    "                plt.suptitle(f'Detailed Gap Analysis (Updated Registry Data, n={total_files})', fontsize=16)\n",
    "                plt.savefig(os.path.join(output_dir, 'Updated_Registry_Field_Analysis.png'), dpi=150)\n",
    "                plt.close()\n",
    "            \n",
    "            # --- 4. Validation Report ---\n",
    "            \n",
    "            report_file = os.path.join(output_dir, 'Updated_Registry_Analysis_Report.md')\n",
    "            with open(report_file, 'w') as r:\n",
    "                r.write(f\"# Updated DOME Registry Data Analysis Report\\n\")\n",
    "                r.write(f\"**Date:** {timestamp}\\n\")\n",
    "                r.write(f\"**JSON Dataset:** `{json_folder}` ({total_files} files)\\n\")\n",
    "                r.write(f\"**Metadata:** `{tsv_path}`\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 1. Metadata Linking\\n\")\n",
    "                r.write(f\"- Total JSON Files: {total_files}\\n\")\n",
    "                r.write(f\"- Matched to Registry Metadata: {matched_count} ({(matched_count/total_files)*100:.1f}%)\\n\\n\")\n",
    "                \n",
    "                r.write(\"## 2. Title Analysis\\n\")\n",
    "                r.write(f\"- JSONs with '{target_phrase}' as Title: {missing_title_in_json_count}\\n\")\n",
    "                r.write(f\"- Title Mismatches (vs Metadata): {len(mismatches)}\\n\\n\")\n",
    "                \n",
    "                if mismatches:\n",
    "                    r.write(\"### Low Similarity Title Mismatches (Ratio < 0.5)\\n\")\n",
    "                    r.write(\"| PMCID | JSON Title | Metadata Title | Similarity |\\n\")\n",
    "                    r.write(\"|---|---|---|---|\\n\")\n",
    "                    severe_mismatches = [m for m in mismatches if m['ratio'] < 0.5]\n",
    "                    for m in severe_mismatches[:20]: # Show top 20\n",
    "                        r.write(f\"| {m['pmcid']} | {m['json_title'][:50]}... | {m['tsv_title'][:50]}... | {m['ratio']:.2f} |\\n\")\n",
    "                    if len(severe_mismatches) > 20:\n",
    "                        r.write(f\"| ... | ... | ... | ... |\\n\")\n",
    "                \n",
    "                r.write(\"\\n## 3. Information Coverage\\n\")\n",
    "                r.write(\"![Category Summary](Updated_Registry_Category_Summary.png)\\n\\n\")\n",
    "                \n",
    "                r.write(\"| Category | Avg Missing % |\\n\")\n",
    "                r.write(\"|---|---|\\n\")\n",
    "                for _, row in category_stats.iterrows():\n",
    "                    r.write(f\"| {row['Category']} | {row['Missing_Percentage']:.1f}% |\\n\")\n",
    "                    \n",
    "            print(f\"Analysis Complete.\")\n",
    "            print(f\"Report saved to: {report_file}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Error: JSON folder {json_folder} not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Critical Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57831269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89818551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Exact Match / High Similarity (Sampling verified entries) | Similarity: 1.00"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #6f42c1; font-family: sans-serif;\">\n",
       "                <span style=\"font-weight: bold; margin-right: 10px;\">ID:</span>\n",
       "                <a href=\"https://europepmc.org/search?query=PMC12532322\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #6610f2; margin-right: 20px; font-size: 1.1em;\">PMC12532322 ↗</a>\n",
       "            </div>\n",
       "            <br>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [Title]\n",
      "  Cross-modal contrastive learning decodes developmental regulatory features through chromatin potenti...\n",
      "------------------------------------------------------------\n",
      "✅ [Authors]\n",
      "  Yang Yueyuxiao, Xie Chenxi, He Qiushun, Yang Meng\n",
      "------------------------------------------------------------\n",
      "✅ [Journal]\n",
      "  GigaScience\n",
      "------------------------------------------------------------\n",
      "✅ [Year]\n",
      "  2025\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>✅ [DOI]</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&nbsp;&nbsp;<a href=\"https://doi.org/10.1093/gigascience/giaf053\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">10.1093/gigascience/giaf053 ↗</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <br>\n",
       "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
       "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
       "                        ▶ Show Remaining JSON Data (22 fields)\n",
       "                    </summary>\n",
       "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{\n",
       "  &quot;publication/tags&quot;: &quot;- Cross-modal contrastive learning\\n- Differential expression analysis\\n- Gene ontology enrichment\\n- Pseudotime inference\\n- Chromatin accessibility\\n- Single-cell RNA sequencing\\n- Single-cell ATAC sequencing\\n- Regulatory network analysis\\n- Motif analysis\\n- Bioinformatics\\n- Machine learning\\n- Deep learning\\n- Data integration\\n- Multimodal data analysis\\n- Gene expression patterns&quot;,\n",
       "  &quot;dataset/provenance&quot;: &quot;The datasets utilized in this study are publicly accessible, ensuring reproducibility and accessibility for the scientific community. The scRNA-seq and scATAC-seq data for peripheral blood mononuclear cells (PBMC) were obtained from 10x Genomics. This dataset includes 11,909 cells and has been used extensively in the community for various single-cell multiomics studies.\\n\\nAnother key dataset is the SHARE-seq dataset, which includes data from mouse skin in the late anagen stage. This dataset, comprising 32,231 cells, can be accessed via the NCBI Gene Expression Omnibus (GEO) under the accession number GSE140203.\\n\\nThe NeurIPS dataset, which includes data from human bone marrow, is also publicly available on GEO under the accession number GSE194122. This dataset has been used in previous competitions and studies to benchmark single-cell multiomics integration methods.\\n\\nAdditionally, the Greenleaf 2021 dataset, derived from the human brain cortex, is provided through GEO under the accession number GSE162170. This dataset has been utilized in various studies focusing on brain development and disease.\\n\\nAll datasets utilized in this study are also available via Figshare, including the Attune 10x multiome dataset and the SHARE-seq TAC dataset. These datasets can be accessed through their respective DOIs.\\n\\nFurthermore, a complete list of all datasets and additional information can be found in Supplementary Table S1. Additional supporting data are available via the GigaScience database, GigaDB. The DOME-ML (Data, Optimization, Model, and Evaluation in Machine Learning) annotations are available via the DOME registry under the accession q1loj0zi07. These resources provide comprehensive access to the data and methodologies used in this study, facilitating further research and validation by the scientific community.&quot;,\n",
       "  &quot;dataset/splits&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;dataset/redundancy&quot;: &quot;Not enough information is available.&quot;,\n",
       "  &quot;dataset/availability&quot;: &quot;All datasets utilized in this study are publicly accessible. The scRNA-seq and scATAC-seq data for PBMC are available from 10x Genomics. The SHARE-seq dataset, which includes data from mouse skin in the late anagen stage, can be accessed via NCBI GEO (accession GSE140203). The NeurIPS dataset, comprising data from human bone marrow, is publicly available on GEO (accession GSE194122). The Greenleaf 2021 dataset, derived from human brain cortex, is provided through GEO (accession GSE162170). Data utilized in this study are also available via Figshare (Attune 10x multiome dataset and SHARE-seq TAC dataset). A complete list of all datasets and additional information can be found in Supplementary Table S1. Additional supporting data are available via the GigaScience database, GigaDB. DOME-ML (Data, Optimization, Model and Evaluation in Machine Learning) annotations are available via the DOME registry (accession q1loj0zi07). The data is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.&quot;,\n",
       "  &quot;optimization/algorithm&quot;: &quot;The optimization algorithm employed in our study leverages a transformer-based architecture, which is a well-established class of machine-learning algorithms known for their effectiveness in handling sequential and structured data. The transformer model used here is not entirely new but has been adapted and fine-tuned specifically for our multimodal integration tasks.\\n\\nThe decision to use a transformer was driven by its ability to capture complex regulatory interactions between different layers of genetic information. The cross-attention mechanism within the transformer is particularly adept at revealing these interactions, making it an ideal choice for our objectives.\\n\\nThe reason this work was published in a scientific journal rather than a machine-learning journal is that the primary focus of our research is on biological applications and the integration of multimodal data in genomics. While the transformer architecture is a key component, the innovation lies in its application to biological data and the specific adaptations made to enhance its performance in this context.\\n\\nThe optimization process involves fine-tuning the pretrained model using a multilayer perceptron (MLP) for modality prediction tasks. The learning rate for contrastive pretraining varies from 1 \\u00d7 10\\u22124 to 1 \\u00d7 10\\u22126, using the Adam optimizer. The model is trained for 20 epochs for contrastive pretraining, 5 epochs for the transformer, and 40 epochs for the modality prediction network. The temperature coefficient in the NT-Xent loss is set to 0.1, with a mini-batch size of 32 and an embedding dimension of 128.\\n\\nIn summary, while the transformer is a well-known machine-learning algorithm, its application and fine-tuning for multimodal genomic data integration represent a significant contribution to the field of bioinformatics.&quot;,\n",
       "  &quot;optimization/meta&quot;: &quot;The meta-predictor in our study leverages a modality prediction network that is fine-tuned on a pretrained model. This network is designed to predict RNA expression levels using a multilayer perceptron (MLP) regression model. The input to this network is the embedding from the ATAC modality, denoted as ZATAC. The network has a single hidden layer with 1,000 units and an output layer with units equal to the number of genes, G. The output of the network is the predicted gene counts, Xpred_counts.\\n\\nThe training process for the modality prediction network involves using a mean squared error (MSE) loss function, which measures the difference between the predicted gene counts and the actual gene counts. The learning rate for this network varies from 1 \\u00d7 10\\u22124 to 1 \\u00d7 10\\u22126 using the Adam optimizer, and it is trained for 40 epochs. The temperature coefficient in the NT-Xent loss is set to 0.1, the mini-batch size is 32, and the dimension of the embedding is 128.\\n\\nThe meta-predictor does not directly use data from other machine-learning algorithms as input. Instead, it relies on the embeddings generated by the pretrained model, which has been trained using cross-modal contrastive learning. This approach ensures that the training data for the modality prediction network is independent of the data used to train the pretrained model. The embeddings capture the regulatory interactions between genes and peaks, which are then used to predict RNA expression levels.\\n\\nThe overall architecture of the meta-predictor is designed to integrate multimodal data effectively. The use of a pretrained model followed by fine-tuning with an MLP regression model allows for the capture of complex regulatory interactions and the accurate prediction of RNA expression levels. The independent training of the pretrained model and the modality prediction network ensures that the meta-predictor can generalize well to new data.&quot;,\n",
       "  &quot;optimization/encoding&quot;: &quot;The data encoding process for the machine-learning algorithm involved several key steps. Initially, expression and accessibility matrices from matched multimodal single-cell RNA sequencing (scRNA-seq) and single-cell ATAC sequencing (scATAC-seq) data were used as input. For scRNA-seq data, genes expressed in fewer than 5% of cells were filtered out. Each cell count was normalized to 10,000 read counts before applying a log-arithm transformation. Additionally, sex chromosome genes were removed, and 2,000 highly variable genes (HVGs) were selected to balance performance, computational cost, and model stability. For scATAC-seq data, peaks accessed in fewer than 5% of cells were filtered out, and peaks from sex chromosomes were also removed.\\n\\nThe normalized expression and accessibility matrices were then encoded in the TensorFlow Record (TF-record) format. The scRNA-seq data were encapsulated in one TF-record file, with fields for \\&quot;gene index\\&quot; and \\&quot;gene count,\\&quot; while the scATAC-seq data were encapsulated in another file, with fields for \\&quot;peak index\\&quot; and \\&quot;peak count.\\&quot; This encoding scheme facilitated the efficient processing and integration of multimodal data within the machine-learning framework.&quot;,\n",
       "  &quot;optimization/parameters&quot;: &quot;In the optimization process of our model, several input parameters play crucial roles. The model utilizes a cross-attention weight matrix with dimensions defined by the product of the number of genes (G) and the number of peaks (P). This matrix is derived from the dot product of two attention weight vectors: one for RNA with dimensions N*(P+1) and another for ATAC with dimensions N*(G+1). Here, N represents the number of cells.\\n\\nThe selection of the number of genes and peaks is a critical aspect of our model&#x27;s performance. Through extensive experimentation, we determined that using 2,000 highly variable genes (HVGs) provides a reasonable compromise between computational efficiency and model performance. This choice was validated through benchmarks that assessed the integration performance under different gene and peak settings. The size of the dot indicates the number of peaks, and the color indicates the number of genes in these experiments.\\n\\nAdditionally, the model&#x27;s architecture includes a modality prediction network fine-tuned on a pretrained model. This network uses a multilayer perceptron with 1,000 hidden layer units and an output layer with G units, corresponding to the number of genes. The input to this network is the embedding from the ATAC modality, and the output is the predicted gene counts.\\n\\nHyperparameter tuning was conducted to optimize the learning rate, which varied from 1\\u00d710^-4 to 1\\u00d710^-6 using the Adam optimizer. The model was trained for different epochs depending on the task: 20 epochs for contrastive pretraining, 5 epochs for the transformer, and 40 epochs for the modality prediction network. The temperature coefficient in the NT-Xent loss was set to 0.1, the mini-batch size to 32, and the dimension of the embedding to 128.\\n\\nThese parameters were carefully selected and tuned to ensure the model&#x27;s robustness and efficiency in integrating multimodal data and predicting gene expression levels.&quot;,\n",
       "  &quot;optimization/features&quot;: &quot;In our study, the input features for the modality prediction network are derived from the ZATAC student embedding, which has a dimension of 128. This embedding is used as the input to a multilayer perceptron with 1,000 hidden layer units. The number of units in the last layer corresponds to the number of genes, denoted as G.\\n\\nFeature selection was performed to optimize the integration process. Specifically, we evaluated the integration performance under different numbers of gene and peak settings. The size of the dots in the relevant figures indicates the number of peaks, while the color represents the number of genes. This approach ensures that the most relevant features are used, enhancing the model&#x27;s performance.\\n\\nThe feature selection process was conducted using the training set only, adhering to best practices to prevent data leakage and ensure the robustness of our results. This meticulous approach to feature selection and optimization contributes to the reliability and effectiveness of our modality prediction network.&quot;,\n",
       "  &quot;optimization/fitting&quot;: &quot;In our study, we employed a multilayer perceptron (MLP) for the modality prediction task, which is a relatively simple regression model. The input to this network is the embedding from the ATAC modality, and it consists of 1,000 hidden layer units. The output layer has a number of units equal to the number of genes, predicting gene counts. Given the complexity of the biological data and the number of genes involved, the number of parameters in our model is indeed large compared to the number of training points.\\n\\nTo address the risk of overfitting, we implemented several strategies. Firstly, we utilized a pretrained model, Attune, which provides a robust starting point for our MLP. This pretraining helps in capturing relevant features from the data, reducing the likelihood of overfitting to the training data. Secondly, we performed hyperparameter tuning, adjusting the learning rate and other parameters to find an optimal balance. The learning rate varied from 1 \\u00d7 10\\u22124 to 1 \\u00d7 10\\u22126 using the Adam optimizer, and we trained the model for 40 epochs. Additionally, we used a mini-batch size of 32 and an embedding dimension of 128, which helps in generalizing the model to unseen data.\\n\\nTo further mitigate overfitting, we conducted comparative experiments and ablation studies. These studies helped us understand the impact of different components of our model and ensured that the improvements in performance were not due to overfitting. We also evaluated our model using various metrics, including mean average precision (MAP), cell-type ASW, neighbor consistency (NC), Seurat alignment score (SAS), Batch ASW, graph connectivity (GC), and FOSCTTM. These metrics provided a comprehensive assessment of our model&#x27;s performance and helped in identifying any signs of overfitting.\\n\\nRegarding underfitting, we ensured that our model had sufficient capacity to learn the underlying patterns in the data. The use of a pretrained model and a sufficiently complex MLP architecture helped in capturing the intricate relationships between the modalities. Moreover, the extensive training process, with 40 epochs and careful hyperparameter tuning, allowed the model to learn effectively from the data. The comparative experiments and ablation studies also provided insights into the model&#x27;s capacity and helped in ruling out underfitting.&quot;,\n",
       "  &quot;optimization/regularization&quot;: &quot;In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach was the use of cross-modal contrastive learning, which helps the model to learn more generalized features by contrasting different modalities. This method encourages the model to focus on the most relevant features that are consistent across modalities, thereby reducing the risk of overfitting to noise or modality-specific artifacts.\\n\\nAdditionally, we utilized dropout layers within our multilayer perceptron (MLP) for the modality prediction network. Dropout is a regularization technique that randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not rely too heavily on any single neuron.\\n\\nWe also conducted hyperparameter tuning to optimize the learning rate and other parameters. For instance, the learning rate in contrastive pretraining varied from 1 \\u00d7 10\\u22124 to 1 \\u00d7 10\\u22126 using the Adam optimizer. This range was chosen to find an optimal balance between convergence speed and the prevention of overfitting.\\n\\nFurthermore, we performed ablation studies to evaluate the impact of different components of our model. These studies helped us understand which parts of the model were crucial for performance and which could be simplified or removed to prevent overfitting. For example, we compared different configurations of the cross-modal contrastive learning module and found that the design involving asymmetric teacher-student networks yielded the best results.\\n\\nOverall, these regularization techniques and careful tuning of hyperparameters contributed to the robustness and generalizability of our models, ensuring that they performed well on both seen and unseen data.&quot;,\n",
       "  &quot;optimization/config&quot;: &quot;The hyper-parameter configurations and optimization schedule are reported in the publication. The learning rate for contrastive pretraining ranges from 1 \\u00d7 10\\u22124 to 1 \\u00d7 10\\u22126 using the Adam optimizer, with training durations specified for different components: 20 epochs for contrastive pretraining, 5 epochs for the transformer, and 40 epochs for the modality prediction network. The temperature coefficient in NT-Xent loss is set to 0.1, the mini-batch size is 32, and the dimension of the embedding is 128.\\n\\nThe model files and optimization parameters are not explicitly detailed in the main text, but supplementary materials provide additional comparative experiments and ablation studies. These details can be found in Supplementary Tables S5, S6, S10, S11, and S13, as well as in figures such as Figure 2 and Figure 3. These supplementary materials offer insights into the performance and configuration of different models and settings.\\n\\nRegarding availability and licensing, the datasets used in this study are publicly accessible. The scRNA-seq and scATAC-seq data for PBMC are available from 10x Genomics. The SHARE-seq dataset, which includes data from mouse skin in the late anagen stage, can be accessed via NCBI GEO (accession GSE140203). The NeurIPS dataset, comprising data from human bone marrow, is publicly available on GEO (accession GSE194122). The Greenleaf 2021 dataset, derived from human brain cortex, is provided through GEO (accession GSE162170). However, specific details about the licensing of the model files and optimization parameters are not provided in the text.&quot;,\n",
       "  &quot;model/interpretability&quot;: &quot;The model, Attune, leverages a transformer-based architecture, which inherently provides some level of interpretability due to its attention mechanisms. Unlike black-box models, transformers allow for the examination of attention weights, which indicate the importance of different input features when making predictions. This transparency is particularly useful in understanding regulatory interactions between genes and peaks.\\n\\nFor instance, the model utilizes attention weights to quantify associations between genes and peaks. By focusing on gene\\u2013peak pairs with the highest attention weights, it is possible to identify key regulatory events. In our study, we selected gene\\u2013peak pairs with the top 10% of attention weights, resulting in a subset of 8,744 gene\\u2013peak pairs. This approach helps in pinpointing dense peak-associated genes (DPAGs), which are genes linked to at least 10 peaks. These DPAGs and their associated peaks are likely involved in crucial regulatory processes.\\n\\nFurthermore, the model&#x27;s ability to capture promoter-interacting regions effectively is demonstrated through the use of a Promoter capture Hi-C (PCHi-C) dataset. The attention mechanism in the transformer model, when fine-tuned with Attune\\u2019s pretrained model, outperforms other methods in regulatory prediction. This indicates that the model can reveal regulatory interactions by highlighting the relevant regions in the genome.\\n\\nThe interpretability of the model is also enhanced by the use of global cross-attention weights. These weights contain information from two global CLS tokens\\u2019 attention weights, which can be used to define a global attention weight matrix. This matrix provides a comprehensive view of the interactions between different modalities, further aiding in the interpretation of the model&#x27;s predictions.\\n\\nIn summary, Attune is not a black-box model. Its transformer-based architecture and the use of attention mechanisms provide a transparent way to interpret the model&#x27;s decisions. By examining attention weights, it is possible to gain insights into regulatory interactions and identify key genes and peaks involved in these processes.&quot;,\n",
       "  &quot;model/output&quot;: &quot;The model encompasses both classification and regression tasks. For classification, the model identifies whether a given pair of gene and peak is positive or negative. This is achieved by feeding gene and peak hidden representations into a transformer, which then uses a classifier on top of the CLS token&#x27;s embedding to predict a binary label. The probability of prediction is denoted as pmatch.\\n\\nFor regression, the model predicts RNA expression levels. This is done using a modality prediction network, which is fine-tuned on the pretrained model. The network employs a multilayer perceptron with a single hidden layer containing 1,000 units. The input to this network is the student embedding from the ATAC modality, and the output is the predicted gene counts. The loss function for this regression task is defined as the mean squared error between the true gene counts and the predicted gene counts.\\n\\nThe model&#x27;s training involves different epochs for various components. The transformer trains for 5 epochs, while the modality prediction network trains for 40 epochs. The learning rate for contrastive pretraining varies from 1 \\u00d7 10\\u22124 to 1 \\u00d7 10\\u22126 using the Adam optimizer. The temperature coefficient in the NT-Xent loss is set to 0.1, and the mini-batch size is 32, with an embedding dimension of 128.&quot;,\n",
       "  &quot;model/duration&quot;: &quot;The execution time for the model varied depending on the specific task and dataset. For contrastive pretraining, the model was trained for 20 epochs using a learning rate ranging from 1 \\u00d7 10\\u22124 to 1 \\u00d7 10\\u22126 with the Adam optimizer. The transformer component was trained for 5 epochs, while the modality prediction network underwent training for 40 epochs. These training durations were chosen to balance performance and computational efficiency. The mini-batch size was set to 32, and the dimension of the embedding was 128, which also influenced the overall execution time. Comparative experiments and detailed performance metrics are provided in Supplementary Tables S5 and S6. The model&#x27;s architecture, including the use of teacher-student networks and cross-modal contrastive learning, was designed to handle large-scale datasets efficiently, ensuring that the execution time remained manageable even for complex tasks.&quot;,\n",
       "  &quot;model/availability&quot;: &quot;The source code for the project, named Attune, is publicly available. It can be accessed via the project&#x27;s homepage on GitHub. The project is platform-independent and is written in Python, requiring Python 3.6 or higher and TensorFlow 2.5.0 to run. The software is licensed under the GPL-3.0 License, which permits free use, modification, and distribution under certain conditions. Additionally, an archival copy of the code is available via Software Heritage. The project also has a bio.tools ID, attune, for easier identification and access.&quot;,\n",
       "  &quot;evaluation/method&quot;: &quot;The evaluation of our method, Attune, involved a comprehensive assessment using various metrics and datasets to ensure its robustness and effectiveness in integrating multimodal data and performing cross-modal predictions.\\n\\nWe conducted comparative experiments to demonstrate the benefits of fine-tuning Attune\\u2019s pretrained model using a multilayer perceptron (MLP). This included comparing an MLP network with pretraining, an MLP network without pretraining (de novo training), and classical regression methods such as LASSO. Additionally, we performed an ablation study to understand the impact of Attune\\u2019s structure on cross-modal prediction performance.\\n\\nSeveral metrics were utilized to evaluate the integration performance, including mean average precision (MAP), cell-type average silhouette width (ASW), neighbor consistency (NC), Seurat alignment score (SAS), batch ASW, graph connectivity (GC), biology conservation, omics mixing, overall integration score, and fraction of samples closer than the true match (FOSCTTM). These metrics provided a thorough assessment of the congruity between cell types, the preservation of intercellular neighbors, the alignment of modalities, and the overall integration quality.\\n\\nThe evaluation also involved benchmarking Attune against other methods using datasets such as the 10x Multiome dataset and the SHARE-seq dataset. We assessed the biology conservation score versus omics integration score, overall integration score, and FOSCTTM. The results were visualized using UMAP plots to show the cell embeddings aligned with different integration methods.\\n\\nFurthermore, we conducted an ablation study on the cross-modal contrastive learning module to establish the intrinsic soundness of our module design. This involved comparing different designs of teacher-student networks and their impact on multimodal integration performance.\\n\\nIn cross-modal prediction, our objective was to predict all feature values for each cell in scRNA-seq using scATAC-seq. We compared Attune\\u2019s performance against state-of-the-art methods using metrics such as gene-wise Pearson correlation coefficient, gene-wise Spearman correlation coefficient, and root mean square error (RMSE). The results highlighted Attune\\u2019s superior ability to capture regulatory interactions and achieve accurate predictions.\\n\\nOverall, the evaluation of Attune demonstrated its effectiveness in integrating multimodal data and performing cross-modal predictions, showcasing its potential for downstream biological applications.&quot;,\n",
       "  &quot;evaluation/measure&quot;: &quot;In the evaluation of our proposed solution, we employ a comprehensive array of metrics to assess both integration and modality prediction performance. For integration, we utilize metrics such as mean average precision (MAP), cell-type average silhouette width (ASW), neighbor consistency (NC), Seurat alignment score (SAS), batch ASW, graph connectivity (GC), biology conservation, omics mixing, overall integration score, and FOSCTTM. These metrics collectively provide a robust evaluation of the integration quality across different modalities.\\n\\nMAP measures the congruity between cell types in neighboring cells, quantifying the accuracy of clustering outcomes with respect to cell-type assignments. Cell-type ASW assesses the silhouette of cell-type labels, scaled between 0 and 1, while batch ASW evaluates the integration among multimodalities by computing cell modality labels, also scaled between 0 and 1. NC measures the degree of intercellular neighbor retention after integrating multimodal data, ranging from 0 to 1, where higher values indicate better preservation. SAS calculates the alignment score to assess how well two or more modalities have been aligned, with values ranging from 0 to 1, where higher values indicate better integration. GC evaluates the proximity of cells with the same identity across different modalities in the embedding, ranging from 0 to 1, with higher values indicating better integration. FOSCTTM measures the accuracy of modal alignment at the single-cell level in paired cells, with a range from 0 to 1, where lower values indicate higher accuracy.\\n\\nBiology conservation is evaluated through MAP, cell-type ASW, and NC, which collectively assess the biological conservation of integration. These metrics are min-max scaled, and their average is calculated as a single metric for biological conservation. Omics mixing is evaluated using SAS, batch ASW, and GC, which collectively assess the mixing performance of multimodalities. These metrics are also min-max scaled, and their average is calculated as a single metric for omics mixing. The overall integration score is computed as an overall weighted average of omics mixing and biology conservation scores.\\n\\nFor modality prediction, we report metrics such as the root mean square error (RMSE), gene-wise Pearson correlation coefficient, and gene-wise Spearman correlation coefficient. The RMSE appraises the precision of RNA expression prediction across individual cells, while the gene-wise Pearson or Spearman correlation coefficients gauge the average per-gene correlation.\\n\\nThese metrics are representative of the current literature and provide a thorough evaluation of both integration and modality prediction performance. The use of multiple metrics ensures that different aspects of the integration and prediction processes are assessed, providing a comprehensive understanding of the method&#x27;s effectiveness.&quot;,\n",
       "  &quot;evaluation/comparison&quot;: &quot;In the evaluation of our method, Attune, we conducted comprehensive benchmarks against several publicly available multimodal integration methods. These comparisons were performed on well-established benchmark datasets, specifically the 10x Multiome dataset and the SHARE-seq dataset. The 10x Multiome dataset consists of 11,909 cells, while the SHARE-seq dataset includes 32,231 cells. These datasets are widely recognized in the field for evaluating the performance of single-cell multi-omics integration methods.\\n\\nWe benchmarked Attune against methods such as GLUE, LIGER, Seurat, Cobolt, MinNet, scJoint, and MultiVI. The evaluation metrics used included biology conservation score, omics integration score, overall integration score, and the fraction of samples closer than the true match (FOSCTTM). These metrics provide a robust assessment of the integration performance, ensuring that the methods are evaluated on both biological fidelity and the mixing of different omics data.\\n\\nThe results, as depicted in Figure 2, show that Attune consistently outperforms other methods in terms of balancing omics mixing and biological conservation. For instance, on the 10x Multiome dataset, Attune achieves the highest overall integration score, indicating its superior performance in integrating multi-omics data while preserving biological meaning. Similarly, on the SHARE-seq dataset, Attune demonstrates the lowest FOSCTTM scores, suggesting effective modality matching at the single-cell level.\\n\\nIn addition to these comparisons, we also performed ablation studies to evaluate the impact of different settings and hyperparameters on Attune&#x27;s performance. These studies included varying the number of genes and peaks, as well as different dimensional settings for feature embeddings. The results of these ablation studies further validate the robustness and effectiveness of Attune&#x27;s integration approach.\\n\\nOverall, the comprehensive benchmarks and comparisons against publicly available methods, along with the ablation studies, provide strong evidence of Attune&#x27;s superior performance in integrating single-cell RNA sequencing and single-cell ATAC sequencing data.&quot;,\n",
       "  &quot;evaluation/confidence&quot;: &quot;In our evaluation, we have taken steps to ensure the reliability and statistical significance of our results. For various performance metrics, we have included confidence intervals to provide a range within which the true value is likely to fall. Specifically, error bars representing the 95% confidence interval are shown in figures comparing different methods and datasets. This approach allows for a clearer understanding of the variability and precision of our measurements.\\n\\nTo assess the statistical significance of our findings, we have conducted multiple replicates of our experiments. For instance, the integration and cross-modal prediction performance benchmarks were repeated five times with different random seeds. This repetition helps to ensure that our results are robust and not dependent on a particular random initialization.\\n\\nAdditionally, we have employed statistical tests such as the Wilcoxon rank-sum test to examine differences in performance metrics. These tests help to determine whether observed differences between methods are statistically significant, thereby strengthening our claims about the superiority of our approach over baselines and other methods.\\n\\nOverall, our evaluation includes confidence intervals and statistical tests to provide a comprehensive and statistically sound assessment of our method&#x27;s performance.&quot;,\n",
       "  &quot;evaluation/availability&quot;: &quot;The raw evaluation files are not explicitly mentioned as being available. However, the datasets utilized in this study are publicly accessible. The scRNA-seq and scATAC-seq data for PBMC can be obtained from 10x Genomics. The SHARE-seq dataset, which includes data from mouse skin in the late anagen stage, is available via NCBI GEO under accession GSE140203. The NeurIPS dataset, comprising data from human bone marrow, is publicly available on GEO under accession GSE194122. The Greenleaf 2021 dataset, derived from human brain cortex, is provided through GEO under accession GSE162170. Additionally, data utilized in this study are available via Figshare, including the Attune 10x multiome dataset and the SHARE-seq TAC dataset. A complete list of all datasets and additional information can be found in Supplementary Table S1. Additional supporting data are available via the GigaScience database, GigaDB. DOME-ML annotations are available via the DOME registry under accession q1loj0zi07. The datasets are released under the terms of the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.&quot;\n",
       "}</pre>\n",
       "                </details>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Run cell again [Ctrl+Enter] to flip between Matches and Mismatches)\n"
     ]
    }
   ],
   "source": [
    "# Block 13.0: Manual Visual Inspection Interface for Remediated Registry JSONs\n",
    "# Comparison:\n",
    "# - JSONs: Copilot_v0_Processed_2025-12-04_Updated_Metadata (Updated via API)\n",
    "# - TSV: DOME_Registry_Remediation/registry_metadata_remediation.tsv (Source of truth from API)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import difflib\n",
    "import random\n",
    "import html\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Global toggle for alternation (persists across cell runs)\n",
    "if 'remediation_inspection_mode_high' not in globals():\n",
    "    remediation_inspection_mode_high = True # True=High, False=Low\n",
    "\n",
    "source_json_folder = 'Copilot_v0_Processed_2025-12-04_Updated_Metadata'\n",
    "tsv_path = 'DOME_Registry_Remediation/registry_metadata_remediation.tsv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(tsv_path) and os.path.exists(source_json_folder):\n",
    "        # Load Data\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        \n",
    "        # Prepare TSV data \n",
    "        # Remediation TSV uses 'PMCID' as key\n",
    "        df['PMCID_clean'] = df['PMCID'].apply(lambda x: str(x).strip() if pd.notna(x) else None)\n",
    "        df_mapped = df.drop_duplicates(subset=['PMCID_clean'], keep='first')\n",
    "        \n",
    "        # Helper to clean numeric strings\n",
    "        def clean_val(v):\n",
    "            if pd.isna(v) or v == '': return \"\"\n",
    "            try: return str(int(float(v)))\n",
    "            except: return str(v).strip()\n",
    "\n",
    "        high_sim = [] \n",
    "        low_sim = []\n",
    "        \n",
    "        json_files = [f for f in os.listdir(source_json_folder) if f.endswith('.json')]\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            pmcid = json_file.replace('.json', '')\n",
    "            \n",
    "            # Find matching TSV row\n",
    "            row = df_mapped[df_mapped['PMCID_clean'] == pmcid]\n",
    "            if len(row) == 0: continue\n",
    "            row = row.iloc[0]\n",
    "            \n",
    "            with open(os.path.join(source_json_folder, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            j_title = str(data.get('publication/title', \"\")).strip()\n",
    "            # TSV column is 'Title' in the remediation file\n",
    "            t_title = str(row['Title']).strip() if pd.notna(row['Title']) else \"\"\n",
    "            \n",
    "            # We expect them to be identical if update worked, but let's check\n",
    "            if j_title != t_title:\n",
    "                ratio = difflib.SequenceMatcher(None, j_title, t_title).ratio()\n",
    "            else:\n",
    "                ratio = 1.0\n",
    "                \n",
    "            entry = {\n",
    "                'pmcid': pmcid,\n",
    "                'json': data,\n",
    "                'tsv': row,\n",
    "                'ratio': ratio\n",
    "            }\n",
    "            \n",
    "            if ratio >= 0.8: high_sim.append(entry)\n",
    "            elif ratio <= 0.8: low_sim.append(entry)\n",
    "\n",
    "        # Toggle Selection Logic\n",
    "        target_pool = []\n",
    "        mode_str = \"\"\n",
    "        \n",
    "        if remediation_inspection_mode_high:\n",
    "            if high_sim: \n",
    "                target_pool = high_sim\n",
    "                mode_str = \"Exact Match / High Similarity (Sampling verified entries)\"\n",
    "            elif low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Mismatches (Sampling despite preference)\"\n",
    "        else:\n",
    "            if low_sim:\n",
    "                target_pool = low_sim\n",
    "                mode_str = \"Mismatches / Differences\"\n",
    "            elif high_sim:\n",
    "                target_pool = high_sim\n",
    "                mode_str = \"Exact Matches (No mismatches found)\"\n",
    "        \n",
    "        remediation_inspection_mode_high = not remediation_inspection_mode_high\n",
    "        \n",
    "        if not target_pool:\n",
    "            print(\"No entries found.\")\n",
    "        else:\n",
    "            item = random.choice(target_pool)\n",
    "            \n",
    "            # Prepare IDs for Link\n",
    "            curr_pmcid = item['pmcid']\n",
    "            curr_pmid = '' # Not in this TSV usually, unless we want to fetch it? The remediation TSV in Block 11 didn't save PMID.\n",
    "            \n",
    "            # --- DISPLAY SECTION ---\n",
    "            \n",
    "            display(Markdown(f\"### {mode_str} | Similarity: {item['ratio']:.2f}\"))\n",
    "            \n",
    "            # HTML Links\n",
    "            url_pmcid = f\"https://europepmc.org/search?query={curr_pmcid}\"\n",
    "            \n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"background-color: #e8e8e8; padding: 12px; border-radius: 4px; border-left: 5px solid #6f42c1; font-family: sans-serif;\">\n",
    "                <span style=\"font-weight: bold; margin-right: 10px;\">ID:</span>\n",
    "                <a href=\"{url_pmcid}\" target=\"_blank\" style=\"text-decoration: none; font-weight: bold; color: #6610f2; margin-right: 20px; font-size: 1.1em;\">{curr_pmcid} ↗</a>\n",
    "            </div>\n",
    "            <br>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Comparison Loop for Remediation Fields ('Title', 'Authors', 'Journal', 'Year', 'DOI')\n",
    "            fields = [\n",
    "                ('Title', 'publication/title', 'Title'),\n",
    "                ('Authors', 'publication/authors', 'Authors'),\n",
    "                ('Journal', 'publication/journal', 'Journal'),\n",
    "                ('Year', 'publication/year', 'Year'),\n",
    "                ('DOI', 'publication/doi', 'DOI')\n",
    "            ]\n",
    "            \n",
    "            for label, k_json, k_tsv in fields:\n",
    "                v_json = str(item['json'].get(k_json, \"\")).strip()\n",
    "                v_tsv = item['tsv'].get(k_tsv, \"\")\n",
    "                \n",
    "                if k_tsv == 'Year': v_tsv = clean_val(v_tsv)\n",
    "                else: v_tsv = str(v_tsv).strip() if pd.notna(v_tsv) else \"\"\n",
    "                \n",
    "                match = v_json == v_tsv\n",
    "                symbol = \"✅\" if match else \"❌\"\n",
    "                \n",
    "                if label == 'DOI':\n",
    "                    def make_doi_link(v):\n",
    "                        if not v: return \"<em>(empty)</em>\"\n",
    "                        return f'<a href=\"https://doi.org/{v}\" target=\"_blank\" style=\"text-decoration: underline; color: #0066cc;\">{v} ↗</a>'\n",
    "                    \n",
    "                    display(HTML(f\"<strong>{symbol} [{label}]</strong>\"))\n",
    "                    if not match:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;JSON: {make_doi_link(v_json)}\"))\n",
    "                        display(HTML(f\"&nbsp;&nbsp;TSV : {make_doi_link(v_tsv)}\"))\n",
    "                    else:\n",
    "                        display(HTML(f\"&nbsp;&nbsp;{make_doi_link(v_json)}\"))\n",
    "                    print(\"-\" * 60)\n",
    "                else:\n",
    "                    print(f\"{symbol} [{label}]\")\n",
    "                    if not match:\n",
    "                        print(f\"  JSON: {v_json}\")\n",
    "                        print(f\"  TSV : {v_tsv}\")\n",
    "                    else:\n",
    "                        if len(v_json) > 100:\n",
    "                            print(f\"  {v_json[:100]}...\")\n",
    "                        else:\n",
    "                            print(f\"  {v_json}\")\n",
    "                    print(\"-\" * 60)\n",
    "            \n",
    "            # --- EXPANDABLE REST OF DATA ---\n",
    "            shown_keys = [f[1] for f in fields]\n",
    "            remaining_data = {k: v for k, v in item['json'].items() if k not in shown_keys}\n",
    "            \n",
    "            if remaining_data:\n",
    "                json_str = json.dumps(remaining_data, indent=2)\n",
    "                safe_json_str = html.escape(json_str)\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                <br>\n",
    "                <details style=\"border: 1px solid #ddd; border-radius: 4px; padding: 10px; background-color: #fafafa;\">\n",
    "                    <summary style=\"cursor: pointer; color: #555; font-weight: bold; padding: 5px;\">\n",
    "                        ▶ Show Remaining JSON Data ({len(remaining_data)} fields)\n",
    "                    </summary>\n",
    "                    <pre style=\"margin-top: 10px; background-color: #fff; padding: 10px; border: 1px solid #eee; border-radius: 4px; overflow-x: auto;\">{safe_json_str}</pre>\n",
    "                </details>\n",
    "                \"\"\"))\n",
    "            \n",
    "            print(\"\\n(Run cell again [Ctrl+Enter] to flip between Matches and Mismatches)\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Files not found.\")\n",
    "        print(f\"TSV Exists: {os.path.exists(tsv_path)}\")\n",
    "        print(f\"JSON Folder Exists: {os.path.exists(source_json_folder)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
