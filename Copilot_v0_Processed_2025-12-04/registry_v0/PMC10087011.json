{
  "publication/title": "Delineating Regions-of-interest for Mass Spectrometry Imaging by Multimodally Corroborated Spatial Segmentation",
  "publication/authors": "The authors contributing to this article are Ang Guo, Zhiyu Chen, Fang Li, and Qian Luo. Ang Guo, Zhiyu Chen, and Fang Li are listed as having contributed equally to the work, with their affiliations being the Institute of Biomedicine and Biotechnology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen. Zhiyu Chen and Qian Luo are also affiliated with the University of Chinese Academy of Sciences, Beijing. Qian Luo is noted as the corresponding author, with an email address provided for further correspondence.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Mass Spectrometry Imaging\n- Tissue Segmentation\n- Multimodal Fusion\n- Histology Microscopy\n- Deep Learning\n- Clustering Algorithms\n- Biomedical Imaging\n- Data Integration\n- Spatial Segmentation\n- ROI Validation",
  "dataset/provenance": "The dataset utilized in this study originates from mass spectrometry imaging (MSI) experiments conducted on biological tissue samples. Specifically, the data includes MSI measurements from both kidney and tumor specimens. The MSI data was preprocessed using the R Cardinal 2 package, following standard protocols that include total ion count normalization, spectral smoothing, baseline reduction, peak picking, alignment, binning, and filtering. The pixel resolution for the tumor specimen was set to 50×50 μm², adapted to the smaller tissue area, while the kidney experiments used a resolution of 200×200 pixels, ensuring each image tile matched the physical size of an MSI pixel.\n\nIn addition to MSI data, hematoxylin and eosin (H&E) stained images were also used. These images underwent preprocessing steps such as tissue detection and color normalization using the HistomicsTK 1.2.3 package. The H&E images were split into tiles, with the size of these tiles adjusted to match the MSI pixel size for accurate spatial registration. For the kidney experiments, tiles containing at least 90% tissue area were classified as foreground, while for the tumor specimen, only the tumor area and its surroundings were used to reduce computational cost.\n\nThe data processing and analysis were implemented using Python 3.7, with image processing handled by OpenCV 4.2.0.32 and the pretrained deep convolutional neural network (DCNN) downloaded from Keras 2.4.3. The specific DCNN architecture used was DenseNet 201, which was chosen for its high performance in encoding the histology appearance of H&E image tiles into informative high-frequency (HF) spectra.\n\nThe dataset has been used in previous studies within the MSI community to understand biomolecular mechanisms underlying cancers and to investigate therapeutic agents. The experimental procedures and data processing methods have been standardized and are routinely applied in similar research. The use of orthotopic models, which mimic the organ-specific tumor environment in humans, has been particularly valuable in these studies. The data has also been utilized to develop and validate multimodal imaging techniques, integrating MSI and H&E data for enhanced spatial segmentation and region-of-interest delineation.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work is a gradient descent algorithm, which is a well-established class of machine-learning algorithms. This algorithm is not new; it has been widely used and studied in the field of machine learning and optimization.\n\nThe specific parameters used for the gradient descent algorithm include a learning rate of 1e-3 and a total of 200 iterations. This algorithm was chosen for its efficiency and effectiveness in optimizing the spatial registration process.\n\nThe decision to use gradient descent was driven by its proven track record in similar applications and its ability to handle the complexity of the data involved. The algorithm was implemented within a multi-resolution framework to accelerate convergence, with shrink factors and smoothing sigmas carefully selected to ensure optimal performance.\n\nThe choice of gradient descent over other optimization algorithms was based on empirical evidence and its suitability for the tasks at hand. While gradient descent is a fundamental algorithm in machine learning, its application in our specific context required careful tuning and integration with other techniques to achieve the desired results.\n\nThe implementation of the spatial registration algorithms was carried out using the SimpleITK library, which provided the necessary tools and functionalities to execute the gradient descent algorithm effectively. This library is widely recognized in the field of medical imaging and was chosen for its robustness and reliability.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Therefore, questions about the machine-learning methods that constitute the whole or the independence of training data are not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the multimodal imaging data for analysis. For mass spectrometry imaging (MSI) data, we followed standard protocols using the R Cardinal 2 package. This involved several steps: total ion count normalization to account for variations in ion detection, spectral smoothing with a Gaussian kernel to reduce noise, baseline reduction to subtract background signals, peak picking to identify meaningful m/z peaks, peak alignment to correct for minor m/z shifts, peak binning to represent peak intensities, and peak filtering to retain only the most relevant peaks.\n\nFor hematoxylin and eosin (H&E) stained histological images, we used the HistomicsTK package for preprocessing. This included tissue detection to create a binary mask delineating the tissue area and color normalization using the Reinhard method to standardize staining variations. The H&E images were then split into tiles, with each tile's size matching the physical size of an MSI pixel. Tiles containing at least 90% tissue area were classified as foreground and used for further analysis.\n\nTo encode the H&E image tiles into histomorphological features (HFs), we employed a deep convolutional neural network (DCNN) architecture, specifically DenseNet 201, pretrained on the ImageNet database. Each H&E tile was resized to 224x224 pixels and processed through the DCNN's densely connected convolutional layers. The output from the conv5_block32 layer was globally average-pooled, Min-Max scaled, and reshaped into an HF spectrum of 1,920 variables. These HF spectra were then formatted into a hyperspectral data cube, where each tile became a pixel with an associated HF spectrum. This encoding allowed us to capture the stain color and morphology of the H&E images in a format suitable for multimodal analysis with MSI data.",
  "optimization/parameters": "In our study, the number of parameters, often denoted as p, is not explicitly stated as a single value because our model involves multiple stages and components, each with its own set of parameters. However, we can provide details on how parameters were selected and optimized at different stages.\n\nFor the gradient descent algorithm used in one of our stages, we set the learning rate to 1e-3 and performed 200 iterations. We employed a multi-resolution framework with three levels, where the shrink factors per level were [4, 2, 1] and the smoothing sigmas per level were [2, 1, 0]. These parameters were chosen to accelerate convergence.\n\nIn the non-linear local registration stage, we used the BSpline-based Free Field Deformation algorithm with a transform domain mesh size of [1, 1] and an order of 3. This stage also utilized a multi-resolution framework with four levels, where the shrink factors per level were [8, 4, 2, 1] and the smoothing sigmas per level were [4, 2, 1, 0]. The similarity metrics were set using Mattes Mutual Information with 50 histogram bins. The optimizer used was LBFGS2, with a solution accuracy of 1e-4, 2000 iterations, and a delta convergence tolerance of 1e-3.\n\nThe selection of these parameters was based on empirical testing and existing literature to ensure optimal performance and convergence of our algorithms. The spatial registration algorithms were implemented using the SimpleITK library.\n\nAdditionally, the number of clusters (#Clusters) is a crucial parameter for our spectral clustering algorithm, as it determines the granularity of regions of interest (ROIs) used in downstream statistical analysis. To select the most probable number of clusters for our kidney specimen, we compared the segmentation results using different numbers of clusters based on internal validation measures. For instance, we found that the optimal number of clusters was 4, which aligned with the results described in other figures. This selection process ensures that the chosen number of clusters provides a meaningful and accurate segmentation of the tissue.",
  "optimization/features": "The input features for our analysis are derived from histomorphological features (HFs) extracted from hematoxylin and eosin (H&E) stained histology images. These features are obtained using a deep convolutional neural network (DCNN) architecture, specifically the DenseNet 201, which is pretrained on the ImageNet database. The DCNN processes each 224x224 pixel tile of the H&E image, generating a set of 1,920 features per tile. These features are then globally average-pooled, Min-Max scaled, and reshaped into an HF spectrum. This spectrum serves as the input for subsequent clustering and segmentation analyses.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the choice of features is inherently determined by the architecture and layers of the DCNN used. The conv5_block32 layer of DenseNet 201 was empirically found to provide the most informative features for our histomorphological context. This layer's output is used directly as the feature set, ensuring that the features are both rich in information and relevant to the task at hand. The use of a pretrained DCNN on a large and diverse dataset like ImageNet helps in extracting robust and generalizable features, reducing the need for additional feature selection steps.",
  "optimization/fitting": "The fitting method employed in our study involved a multi-resolution framework to accelerate convergence, which helped in managing the complexity of the data. The number of parameters in our model was indeed large, given the high-dimensional nature of the data cubes used in mass spectrometry imaging (MSI) and histology (HF) data. To address the potential issue of over-fitting, we utilized cross-validation techniques and external validation criteria. Specifically, we employed Cohen’s kappa score (CKS) to quantify the agreement between MSI and histology segmentation, ensuring that the model's performance was consistent across different modalities. This external validation criterion integrated bioinformation from both molecular profiles and histomorphological appearances, making it robust against noise and artifacts.\n\nTo prevent under-fitting, we carefully selected the number of clusters (#Clusters) using multiple validation measures. Internal validation measures such as the Pearson correlation (PC) and the Davies–Bouldin index (DBI) were initially used to evaluate the segmentation results. However, these measures sometimes provided contradictory results due to their reliance on internal information from the MSI data alone. To overcome this, we introduced an external validation criterion based on CKS, which considered the consistency between MSI and histology segmentation. This approach ensured that the model captured the underlying biological structures accurately, avoiding both over-fitting and under-fitting.\n\nAdditionally, the use of nonlinear dimension reduction techniques like UMAP helped in visualizing the high-dimensional data, making it easier to interpret and validate the segmentation results. The integration of these methods ensured that our model was neither too complex nor too simplistic, providing a balanced and reliable fitting method for the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was the application of densely connected convolutional layers in our deep convolutional neural network (DCNN). These layers help in mitigating overfitting by encouraging feature reuse and reducing the number of parameters, thereby enhancing the model's generalization capability.\n\nAdditionally, we utilized a pretrained DCNN, which was downloaded from Keras. Pretrained models are trained on large datasets and have already learned to recognize a wide range of features. By fine-tuning these models on our specific dataset, we leveraged the knowledge they had gained, which helped in preventing overfitting to our smaller, task-specific dataset.\n\nWe also implemented spatial registration techniques to align multimodal imaging datasets. This involved using nonnegative matrix factorization (NMF) to reduce the dimensions of the data cubes and manually selecting score maps that best matched each other. This process ensured that the features extracted were spatially consistent, further aiding in the prevention of overfitting.\n\nMoreover, we employed a multi-resolution framework for accelerating convergence during the registration process. This approach involved using different shrink factors and smoothing sigmas at various levels, which helped in capturing both global and local features effectively. The use of similarity metrics like Mattes Mutual Information and optimizers such as LBFGS further ensured that the registration was accurate and robust.\n\nIn summary, our regularization methods included the use of densely connected convolutional layers, pretrained models, spatial registration techniques, and a multi-resolution framework. These techniques collectively helped in preventing overfitting and ensuring that our models generalized well to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are detailed within the publication. Specifically, for the linear global registration stage, we employed a gradient descent algorithm with a learning rate of 1e-3 and 200 iterations. This process was accelerated using a multi-resolution framework with three levels, where the shrink factors per level were [4, 2, 1] and the smoothing sigmas per level were [2, 1, 0]. In the non-linear local registration stage, we used a BSpline-based Free Field Deformation algorithm within a four-level multi-resolution framework. The shrink factors per level were [8, 4, 2, 1], and the smoothing sigmas per level were [4, 2, 1, 0]. The optimizer for this stage was LBFGS2, with a solution accuracy of 1e-4, 2000 iterations, and a delta convergence tolerance of 1e-3.\n\nThe spatial registration algorithms were implemented using the SimpleITK library. The specific model files and optimization parameters, such as the number of histogram bins and the similarity metrics used, are also reported. For instance, the Mattes Mutual Information metric with 50 histogram bins was utilized. The pretrained deep convolutional neural network (DCNN) used for feature extraction was downloaded from Keras 2.4.3, and image processing was implemented using OpenCV 4.2.0.32.\n\nAll the data processing and analysis programs were implemented using Python 3.7. The raw data preprocessing steps, including total ion count normalization, spectral smoothing, baseline reduction, peak picking, peak alignment, peak binning, and peak filtering for MSI data, are detailed. Similarly, the preprocessing of H&E images, involving tissue detection and color normalization, is also described. The specific configurations and parameters used in these preprocessing steps are provided to ensure reproducibility.\n\nThe license information for the software and libraries used, such as SimpleITK, OpenCV, and Keras, is available through their respective official websites and repositories. These tools are open-source and freely available for use, subject to their respective licensing agreements.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as it integrates multiple modalities to enhance interpretability. The strategy used combines molecular profiles from Mass Spectrometry Imaging (MSI) and histomorphological appearance from Hematoxylin and Eosin (H&E) stained images. This integration allows for a more objective and reliable approach to clustering/segmentation validation, making the model more interpretable than those relying on a single modality.\n\nOne key aspect of interpretability is the use of Deep Convolutional Neural Networks (DCNNs) as feature extractors. Specifically, the conv5-block32-concat layer of DenseNet 201 was chosen for encoding the stain color and morphology of H&E image tiles into a set of informative high-dimensional features (HFs). This layer was selected based on its empirical performance in various histology and cytology object classification tasks, providing a transparent and well-justified choice for feature extraction.\n\nThe integration of MSI and H&E data results in segmentation maps that can be visually inspected. For instance, pixels that are labeled consistently by both modalities are shown in solid colors, indicating high confidence in the labeling. Conversely, pixels with inconsistent labels between MSI and H&E data are displayed with reduced transparency, highlighting areas where the model's predictions may be less reliable. This visual representation allows researchers to easily identify regions of interest (ROIs) and understand the spatial heterogeneity of chemical composition and histological morphology.\n\nFurthermore, the model's performance can be evaluated against known anatomical structures. For example, in the case of the kidney specimen, the model's segmentation results were compared to the ground truth of renal anatomy, showing good accordance with the known regions such as the inner cortex, outer cortex, medulla, and pelvis. This comparison provides a clear example of how the model's outputs can be interpreted in the context of established biological knowledge.\n\nIn summary, the model's transparency is enhanced by the integration of multiple data modalities, the use of well-justified feature extractors, and the visual representation of segmentation results. These aspects allow for a more interpretable and reliable approach to tissue segmentation and clustering validation.",
  "model/output": "The model's output is not a direct classification or regression result. Instead, it generates a hyperspectral data cube where each tile of the input H&E image is transformed into a pixel with an associated hyperspectral (HF) spectrum. This spectrum consists of 1,920 variables, which are derived from the features extracted by the conv5_block32 layer of DenseNet201. The process involves forward propagating the image tiles through densely connected convolutional layers, followed by 2D global average pooling, Min-Max scaling, and reshaping. This approach allows for the encoding of stain color and morphology from H&E images into a format similar to mass spectrometry imaging (MSI) data, facilitating multimodal imaging analysis.\n\nThe output is designed to align with the MSI data, enabling spatial registration and subsequent analysis. The HF spectra are used to represent the histology appearance of each image tile, making it possible to integrate and compare data from different imaging modalities. This integration is crucial for understanding the biomolecular mechanisms underlying cancers and investigating therapeutic agents. The model's output is thus a spectral representation of the input images, which can be used for further analysis and interpretation in the context of multimodal imaging studies.",
  "model/duration": "The computational workload for processing the data involved approximately 8.6 Giga FLOPs of calculations. This was carried out using an AMAX workstation equipped with two Intel Xeon Silver 4110 CPUs (2.10 GHz), 192 GB of RAM, and two NVIDIA GeForce RTX 2080Ti GPUs, each with 11 GB of memory. The specific execution time for the model to run was not explicitly measured or reported, but the hardware specifications and computational intensity provide an indication of the resources required for the task.",
  "model/availability": "The source code from this work is freely accessible. It can be found on GitHub at the repository \"ROIforMSI\" under the GPL-3 license. Additionally, the software tool is registered in the bio.tools database as \"roiformsi\" and in the SciCrunch database with the identifier RRID:SCR_023275. The computational workflow is also available on workflowhub.eu.",
  "evaluation/method": "The evaluation method employed in this study involved a multimodal fusion strategy that integrated data from mass spectrometry imaging (MSI) and histology microscopy. This approach aimed to provide an objective and reliable way to determine the optimal number of clusters (#Clusters) for segmentation.\n\nThe evaluation process began by segmenting the kidney specimen into different regions using both MSI and hematoxylin and eosin (H&E)-stained histology images. The MSI data were formatted into hyperspectral data cubes, and the histology images were divided into small tiles from which quantitative histomorphological features (HFs) were extracted using a deep convolutional neural network (DCNN)-based feature extractor. This generated another hyperspectral data cube similar to the MSI data, but with the depth corresponding to HFs rather than mass-to-charge ratios (m/z).\n\nTo evaluate the segmentation results, several clustering validation criteria were used. Internal validation measures, such as the Pearson correlation (PC) and the Davies–Bouldin index (DBI), were initially employed. These criteria evaluated the \"goodness\" of clustering based on the intrinsic structure of the MSI data alone. However, these internal criteria can be susceptible to noise or artifacts and may yield inconsistent outcomes.\n\nTo address these limitations, an external validation criterion was introduced. This involved calculating the Cohen’s kappa score (CKS) between each pair of MSI and histology segmentation results. The CKS provided a measure of agreement between the segmentation maps generated by the two distinct bioimaging modalities. By comparing the CKS values for different numbers of clusters, the optimal #Clusters that resulted in the best multimodal consistency was determined.\n\nThe evaluation also included a comparison of the segmentation results with the ground truth of renal anatomy. The optimal #Clusters was found to correspond well with the known anatomical regions of the kidney, further validating the reliability of the multimodal fusion strategy.\n\nIn summary, the method was evaluated using a combination of internal and external validation criteria, with a focus on achieving consistent and biologically relevant segmentation results. The integration of MSI and histology data provided a more objective and reliable approach to clustering/segmentation validation, making it less susceptible to noise and artifacts.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the quality and validity of our segmentation and registration results. One of the primary metrics used was Cohen’s Kappa Score (CKS), which quantifies the agreement between two independent raters in categorizing items into mutually exclusive classes. CKS is particularly useful because it accounts for the possibility of agreement occurring by chance, making it a more robust measure than simple percentage agreement. A CKS of 1 indicates complete agreement, while a CKS of 0 suggests agreement purely by chance.\n\nWe also utilized internal validation criteria such as the Pearson Correlation (PC) and the Davies–Bouldin Index (DBI). PC measures the resemblance between the UMAP image and the segmentation maps by applying a Canny edge detector to both and computing their edge correlation. A higher PC value indicates a higher resemblance. DBI, on the other hand, measures the ratio of within-cluster distances to between-cluster distances, with a smaller DBI indicating better-defined clusters.\n\nIn addition to these metrics, we evaluated the registration quality by visually assessing the overlap between the hyperspectral fluorescence (HF)-derived and mass spectrometry imaging (MSI)-derived tissue masks, as well as the alignment of distinct anatomical features observed in both HF and MSI images. This visual assessment, combined with the quantitative metrics, provided a comprehensive evaluation of our registration and segmentation methods.\n\nOur choice of metrics is representative of the literature in the field of image registration and segmentation. CKS, in particular, is widely used for evaluating the agreement between different segmentation methods or modalities. The use of PC and DBI is also common in the context of clustering and segmentation validation, ensuring that our evaluation is both thorough and aligned with established practices.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our method with publicly available techniques to assess its performance and robustness. Specifically, we compared our approach with two widely used internal validation criteria: the Pearson Correlation (PC) method and the Davies-Bouldin Index (DBI).\n\nThe PC method evaluates the resemblance between the UMAP image and the segmentation maps by applying a Canny edge detector and computing their edge correlation. This method suggested that the optimal number of clusters was three, but it exhibited relatively large error bars due to the variability introduced by random color assignments in each run.\n\nThe DBI method, on the other hand, measures the ratio of within-cluster distances to between-cluster distances, aiming to identify clusters that are well-separated and compact. This method indicated that the optimal number of clusters was two. However, both PC and DBI rely solely on internal information from the MSI data, making them susceptible to instrumental noise and experimental artifacts.\n\nIn contrast, our method employs an external validation criterion by integrating information from both molecular profiles and histomorphological appearances. We used Cohen’s Kappa Score (CKS) to quantify the agreement between MSI and histology segmentation, which provided a more objective and reliable approach. Our strategy demonstrated better multimodal consistency, selecting four clusters as the optimal number. This result aligned well with the ground truth of renal anatomy, corresponding to distinct regions such as the inner cortex, outer cortex, medulla, and pelvis.\n\nAdditionally, we performed comparisons using different deep convolutional neural network (DCNN) extractors, such as Inception ResNet V2 and DenseNet. The results showed negligible influence of the specific choice of DCNN extractor on the outcome, reinforcing the robustness of our method.\n\nIn summary, our evaluation involved a comprehensive comparison with established methods and simpler baselines, demonstrating the superiority of our multimodal fusion-based strategy in providing more reliable and biologically valid segmentation results.",
  "evaluation/confidence": "In our evaluation, we employed several metrics to assess the performance of our clustering and segmentation strategies, and we indeed considered the confidence and statistical significance of our results.\n\nFor the Pearson Correlation (PC) method, we calculated standard deviations from five runs with random color labeling to account for variability. This approach helped us understand the robustness of the PC values and provided a measure of confidence. However, we noted that the specific choices of false color for each region could considerably affect the Canny edges detected from the segmentation maps, leading to relatively large error bars.\n\nThe Davies–Bouldin Index (DBI) was used to find the optimal segmentation, and it measures the ratio of within-cluster distances to between-cluster distances. A smaller DBI indicates better-defined clusters. While DBI provides a clear metric for internal validation, it is subject to the limitations of using internal information alone, which can be vulnerable to instrumental noise or experimental artifacts.\n\nOur primary metric, Cohen’s Kappa Score (CKS), quantifies the agreement between two independent raters—in this case, the segmentation results from mass spectrometry imaging (MSI) and histology. CKS accounts for the agreement expected by chance, providing a more reliable measure of multimodal consistency. The CKS values were calculated for a range of cluster numbers, and the maximum CKS was obtained with four clusters. This result suggests that setting the number of clusters to four gives rise to better consistency between the segmentation produced by the two distinct bioimaging modalities, indicating better multimodally corroborated biological validity.\n\nTo further support the statistical significance of our findings, we integrated the bioinformation from both molecular profiles and the histomorphological appearance of the tissue specimen. This external validation criterion makes our strategy immune to noise and artifacts that are unlikely to exist in both MSI and histology data, providing a more objective and reliable approach to clustering/segmentation validation.\n\nIn summary, our evaluation includes confidence intervals and statistical significance checks, ensuring that our claims of superiority over other methods and baselines are well-founded. The use of CKS, along with internal validation metrics like PC and DBI, provides a comprehensive assessment of our method's performance and reliability.",
  "evaluation/availability": "Not enough information is available."
}