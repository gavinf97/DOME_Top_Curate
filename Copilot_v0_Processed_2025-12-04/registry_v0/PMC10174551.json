{
  "publication/title": "ResMiCo: increasing the quality of metagenome-assembled genomes with deep learning",
  "publication/authors": "The authors who contributed to the article are:\n\n- OM\n- DD\n- GR\n- NDY\n- REL\n- BS\n\nOM, DD, and GR were supported by Eidgeno¨ssische Technische Hochschule Zu¨rich core funding and received a salary from the same institution. They contributed significantly to the development and evaluation of the ResMiCo model.\n\nNDY, REL, and BS were supported by the Max Planck Institute and received a salary from the same institution. They also played crucial roles in the research, particularly in the design and implementation of the deep learning model.\n\nAdditionally, OM was supported by the Max Planck ETH Center for Learning Systems. The authors collectively ensured the perpetual public accessibility of the data used in the study. All authors were committed to the study's design, data collection and analysis, decision to publish, and preparation of the manuscript. They declared no competing interests.",
  "publication/journal": "PLOS Computational Biology",
  "publication/year": "2024",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Metagenomics\n- Deep Learning\n- Genome Assembly\n- Bioinformatics\n- Computational Biology\n- Metagenome-Assembled Genomes\n- Machine Learning\n- Data Simulation\n- Microbiome Analysis\n- TensorFlow",
  "dataset/provenance": "The datasets used in our study include both synthetic and real-world metagenomes. For synthetic datasets, we generated the n9k-train and n9k-novel datasets using our own pipeline. These datasets are quite large, with n9k-train containing 52.5 million contigs and 159 billion bases, and n9k-novel containing 6.8 million contigs and 19.7 billion bases. The other synthetic datasets—gut, skin, oral, marine, and plant—were created from CAMI reads and are smaller in size, ranging from approximately 0.32 million to 1.0 million contigs.\n\nThe real-world metagenomes evaluated include several published datasets: UHGG, TwinsUK, Animal-gut, Pinnell2019, Mantri2021, and MarineMetagenomeDB. These datasets represent a variety of environments and have been used in previous studies, providing a robust benchmark for our tool.\n\nAdditionally, we evaluated our method on mock communities, specifically BMock12 and MBARC-26. These communities, while not as complex as real-world biomes, offer the advantage of having fully known ground truth, which is crucial for validating the performance of our model.\n\nAll the datasets, including the large n9k-train and n9k-novel datasets, have been made publicly accessible. The n9k datasets are available for download from the MPI for Biology FTP server using tools like wget or curl. This ensures that other researchers can replicate our findings and build upon our work.",
  "dataset/splits": "The dataset used for training and testing the model consists of several splits. Initially, a pool of 18,000 reference genomes was selected from Release 202 of the Genome Taxonomy Database (GTDB). This pool was split at the family taxonomic level, ensuring that all genomes in the test dataset belonged to families not present in the training dataset. The resulting split was even, with 9,000 genomes used for both training and testing.\n\nThe training dataset, referred to as n9k-train, was generated using various parameter combinations and initially comprised 4,560 metagenomes and 80 million contigs. Due to storage limitations, this dataset was randomly subset to 3,000 metagenomes, comprising 52 million contigs.\n\nThe test dataset, known as n9k-novel, was created using a subset of parameters and one simulation replicate to save computational time. This dataset includes genomes novel at the family taxonomic level.\n\nAdditionally, another test dataset named n2k-novel-intra-species was created to include higher intra-species diversity. This dataset used 2,000 reference genomes and was generated with consistent simulation parameters, except for specific adjustments in community richness, insert size distribution, and sequencing depths.\n\nFurthermore, the Critical Assessment of Metagenome Interpretation (CAMI) datasets were used for benchmarking. These datasets include paired-end reads from various assembly challenges, such as human skin, human oral, human gut, plant-associated, and marine environments. The number of misassembled contigs in the CAMI datasets is approximately 50% lower, while coverage is approximately 50% higher relative to the n9k-train dataset.\n\nPublished, real-world metagenome datasets were also evaluated, including UHGG, TwinsUK, Animal-gut, Pinnell2019, Mantri2021, MarineMetagenomeDB, and others. These datasets were used to assess the model's performance in real-world scenarios.",
  "dataset/redundancy": "The datasets were split at the family taxonomic level to ensure independence between the training and test sets. This means that all genomes in the test dataset belonged to families not present in the training dataset. The resulting split was even, with 9000 genomes used for both training and testing. To reduce bias toward particular species, at most 50 genomes per species were included in the reference genome pool, with genomes selected at random. The pool was also filtered by CheckM-estimated completeness and contamination, as well as other criteria such as MIMAG quality and the exclusion of single-cell genome assemblies.\n\nThe training dataset, referred to as n9k-train, consisted of 4560 metagenomes and 80 million contigs. Due to resource limitations, this dataset was randomly subset to 3000 metagenomes, comprising 52 million contigs. The test set, known as n9k-novel, was generated using a subset of parameters and one simulation replicate to save computational time. Additionally, a test dataset with higher intra-species diversity, named n2k-novel-intra-species, was created using 2000 reference genomes to include families with high intra-species genome diversity, while still ensuring novelty relative to the training dataset.\n\nThe distribution of the datasets was designed to cover a wide range of metagenome simulation parameters, including community richness, genome abundance, read length, insert size, error profile, and sequencing depth. This approach aimed to create a diverse and representative training dataset, although it was noted that the parameter grid was not uniformly sampled due to resource limitations. The test datasets included both synthetic and real-world metagenomes, with the latter sourced from various environments such as human gut, skin, oral, marine, and plant-associated biomes. This ensured that the model was evaluated on a variety of data types and complexities, enhancing its robustness and generalizability.",
  "dataset/availability": "The data used in our study, including the data splits, are publicly available. The datasets, specifically the n9k-train and n9k-novel datasets, are quite large and have been deposited on the MPI for Biology FTP server. These datasets can be downloaded using tools such as wget or curl through the provided link. The link for downloading the datasets is http://ftp.tue.mpg.de/ebio/projects/ResMiCo/.\n\nWe are committed to ensuring the perpetual public accessibility of these data. The datasets are available under a license that allows for public access and use, although the specific terms of the license are not detailed here. The authors have taken steps to ensure that the data remains accessible to the public, which includes depositing the data in a reliable and publicly accessible repository.\n\nThe datasets were split at the family taxonomic level to ensure that the genomes in the test dataset belonged to families not present in the training dataset. This split was even, with 9000 genomes used for both training and testing. To reduce bias toward particular species, at most 50 genomes per species were included in the reference genome pool, with genomes selected at random. Additionally, the pool was filtered by CheckM-estimated completeness and contamination, as well as other criteria such as MIMAG quality and the exclusion of single-cell genome assemblies.",
  "optimization/algorithm": "The optimization algorithm employed in our work is the Adam optimizer, which is a widely used method for stochastic optimization. Adam, which stands for Adaptive Moment Estimation, is not a new algorithm; it was introduced by Diederik P. Kingma and Jimmy Ba in 2014. This optimizer is particularly well-suited for training deep learning models due to its efficiency and adaptability.\n\nThe choice of Adam optimizer is driven by its ability to handle sparse gradients on noisy problems, making it highly effective for training neural networks. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp. Specifically, Adam computes adaptive learning rates for each parameter, which helps in accelerating convergence and improving the stability of the training process.\n\nGiven that Adam is a well-established and extensively studied algorithm, it was not necessary to publish it in a machine-learning journal. Instead, our focus was on applying this robust optimization technique to enhance the performance of our deep learning model, ResMiCo, which is designed to improve the quality of metagenome-assembled genomes. The use of Adam optimizer allowed us to efficiently train our model on large and complex datasets, ensuring that it could generalize well to new, unseen data.",
  "optimization/meta": "The model described, ResMiCo, is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a standalone deep convolutional neural network designed for reference-free identification of misassembled contigs in metagenome assemblies. ResMiCo utilizes a novel high-throughput pipeline to generate complex and realistic training data, covering a wide range of parameters such as data richness, sequencing depth, sequencing error rate, community diversities, and metagenome assembly methods.\n\nResMiCo's architecture includes skip connections between non-adjacent layers, which have proven successful in various fields when trained on large datasets. The model was trained on a diverse set of features, with the top 14 features selected for input based on their importance, as determined by Shapley values approximated using the Deep SHAP algorithm. This approach ensures that the model is robust to metagenome data heterogeneity, including taxonomic novelty and metagenome assembly parameters.\n\nThe training data for ResMiCo is independent and carefully simulated to cover a broad parameter space. This includes varying community richness, abundance distribution, reference genomes, read length, insert size distribution, sequencer error profile, sequencing depth, and metagenome assembler. The model's performance has been extensively evaluated, showing that it outperforms existing state-of-the-art methods and is robust to various sources of dataset novelty.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involved several key steps to ensure that the features were appropriately formatted and normalized. Various features were generated and aggregated over multiple reads aligned to a position in a contig. These features included metrics such as coverage, reference base, number of query nucleotides, number of SNPs, and alignment scores, among others. The type of aggregation varied depending on the feature, with options including counting, taking the minimum, mean, maximum, or standard deviation.\n\nPreprocessing techniques were applied to these features to standardize and normalize the data. Standardization involved scaling the features to have a mean of zero and a standard deviation of one, while normalization adjusted the features to a common scale, typically between 0 and 1. Additionally, one-hot encoding was used for categorical features to convert them into a format suitable for machine learning algorithms.\n\nThe features were then selected based on their importance, with the top 14 features being chosen as input to the model. This selection included at least one feature of each kind, such as mapping quality and alignment score, to ensure a comprehensive representation of the data. Limiting the number of features significantly reduced the training time without compromising the model's performance.\n\nFor contigs shorter than 20,000 base-pairs, the entire contig was selected and zero-padded to the maximum batch length. For misassembled contigs longer than 20,000 base-pairs, a random 20,000 base-pair interval around each breakpoint was selected. For long contigs with no misassemblies, a random 20,000 base-pair interval was chosen. This approach ensured that the model could handle contigs of variable lengths effectively.\n\nDuring training, the contigs in a batch were padded to the longest length, and the effects of padding were neutralized by creating a mask that was fed to the global average pooling layer. This allowed the model to handle contigs of different lengths uniformly. The resulting features from the global average pooling were then fed into the final layers of the model, which included a fully connected layer with 50 neurons and a one-neuron output layer with a sigmoid activation function.",
  "optimization/parameters": "The ResMiCo model utilizes a specific set of input parameters that were carefully selected through a rigorous process. The model's architecture is based on a residual convolutional neural network, which was chosen for its ability to handle deeper networks and capture more complex patterns. This architecture was selected after evaluating several types of neural network architectures, including deep convolutional networks, bidirectional recurrent networks with LSTM and GRU units, and transformer encoders. All these models were trained on a subset of the n9k-train dataset and evaluated on a validation set. The residual convolutional neural network outperformed the other architectures for all contig lengths, leading to its selection for further optimization.\n\nThe number of residual blocks within a group, the number of groups, and other model design choices were determined through iterative testing on the validation set. The final architecture includes a global average pooling layer that summarizes features along the spatial axis, resulting in an output shape that depends only on the number of filters in the last convolutional layer. This design allows the model to handle contigs of variable length effectively.\n\nThe input to the neural network consists of 14 selected features, which were chosen based on their importance as determined by Shapley values using the Deep SHAP algorithm. These features include mapping quality, alignment score, base composition, and other relevant metrics. The selection of these features was crucial in reducing the training time significantly while maintaining the model's performance.\n\nThe model was trained on the n9k-train dataset for 50 epochs, with a batch size of 200 and an initial learning rate of 0.0001. The training process involved minimizing the binary cross-entropy loss using the Adam optimizer, with gradients clipped to a norm of 1 and a value of 0.5. The model's performance was evaluated using the Area Under the Precision-Recall Curve (AUPRC) on the validation set, and the best-performing model was saved based on this metric.\n\nIn summary, the ResMiCo model uses a residual convolutional neural network architecture with 14 carefully selected input features. The architecture and features were chosen through extensive testing and evaluation on a validation set, ensuring that the model is optimized for performance and efficiency.",
  "optimization/features": "The input features for the model were selected based on their importance, as determined by SHAP values. Feature selection was indeed performed to identify the most relevant features for the model. This process involved evaluating a larger set of features and then narrowing down to the top 14 features that contributed most significantly to the model's predictions. These selected features include a variety of types, such as mapping quality, alignment score, and others, ensuring a diverse representation of relevant information.\n\nThe feature selection process was conducted using a subset of data that was separate from the training set, specifically from the n9k-novel dataset. This approach helps to ensure that the feature selection is not biased by the training data, thereby enhancing the model's generalizability. The selected features were then used as input to the final model, which significantly reduced the training time without compromising the model's performance.",
  "optimization/fitting": "The ResMiCo model was trained on the n9k-train dataset for 50 epochs, with each epoch representing one complete pass through the entire training dataset. The model's architecture includes a residual convolutional neural network, which was selected after evaluating various architectures, including deep convolutional networks, bidirectional recurrent networks with LSTM and GRU units, and a transformer encoder. All evaluated models had approximately 0.5 million trainable weights, ensuring that the number of parameters was not excessively large compared to the number of training points.\n\nTo address potential overfitting, several strategies were employed. Firstly, a validation set comprising 10% of the n9k-train dataset was used to monitor the model's performance during training. The Area Under the Precision-Recall Curve (AUPRC) on the validation set was computed every second epoch, and the model with the highest AUPRC was saved. This approach helped in selecting the best-performing model and prevented overfitting to the training data. Additionally, techniques such as gradient clipping and learning rate decay were used to stabilize training and prevent the model from becoming too complex.\n\nTo mitigate underfitting, the training dataset was balanced by artificially increasing the positive sample rate to 24%. This was achieved by using all misassembled contigs as positive examples and randomly selecting a 10% subset of correctly assembled contigs at each training epoch. Furthermore, the model's architecture was carefully designed with multiple residual blocks and convolutional layers, allowing it to capture complex patterns in the data. The use of global average pooling and fully connected layers also helped in extracting meaningful features from the input data, ensuring that the model could generalize well to unseen data.\n\nIn summary, the model's architecture and training procedures were designed to balance the risk of overfitting and underfitting. The use of a validation set, gradient clipping, learning rate decay, and dataset balancing techniques contributed to the development of a robust and generalizable model.",
  "optimization/regularization": "In our work, several regularization methods were employed to prevent overfitting and improve the generalization of our model. One key technique used was gradient clipping, which involved setting a maximum norm for the gradients during training. This helped to stabilize the training process and prevent the model from diverging.\n\nAdditionally, we utilized an exponential decay strategy for the learning rate. This approach gradually reduces the learning rate when the model's performance on the validation set plateaus, allowing for finer adjustments to the model weights and helping to avoid overfitting.\n\nAnother important regularization technique we implemented was the use of a mask during global average pooling. This mask neutralized the effects of padding, ensuring that the model did not learn spurious patterns from the padded regions of the contigs.\n\nFurthermore, we balanced the dataset by artificially increasing the positive sample rate. This was achieved by randomly selecting a subset of correctly assembled contigs at each training epoch, which helped to mitigate the class imbalance and reduce the computational load during training.\n\nWe also employed early stopping based on the Area Under the Precision-Recall Curve (AUPRC) on the validation set. This involved saving the model at epochs where the AUPRC improved, ensuring that we captured the model's best performance without overfitting to the training data.\n\nThese regularization techniques collectively contributed to the robustness and generalization of our model, enabling it to perform well on both training and validation datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters for the ResMiCo architecture are detailed in the supplementary materials. Specifically, the final choices for the ResMiCo architecture are marked in a table, providing clear insights into the selected hyper-parameters. These include the number of residual blocks and groups, the number of filters for the first convolutional layer, kernel sizes, aggregation methods, the number of hidden units in fully connected layers, the number of fully connected layers, initial learning rates, and maximum input lengths.\n\nThe model was trained using TensorFlow, and the datasets used for training and validation are available for public access. The n9k-train and n9k-novel datasets, which are crucial for the training and evaluation of the model, have been deposited on the MPI for Biology FTP server. These datasets can be downloaded using tools like wget or curl through a provided link, ensuring perpetual public accessibility.\n\nThe code and additional resources, including the ResMiCo Python package, are available on GitHub. This repository contains the necessary scripts and tools to replicate the experiments and further develop the model. The licensing details for these resources are typically provided in the repository's documentation, ensuring that users can access and utilize the materials under specified terms.\n\nThe optimization schedule and model files are not explicitly detailed in the provided information, but the training process involved 50 epochs with specific adjustments to handle imbalanced datasets and gradient clipping. The model selection was based on the Area Under the Precision-Recall Curve (AUPRC) on a validation set, with the best-performing model saved at epoch 46.\n\nIn summary, while the hyper-parameter configurations and optimization parameters are well-documented, specific details about the optimization schedule and model files are not fully covered. However, the necessary datasets and code are publicly available, facilitating further research and development.",
  "model/interpretability": "The ResMiCo model is not a blackbox model. To ensure interpretability, we employed the SHAP (SHapley Additive exPlanations) method, which is a game theory-based approach to explain the output of machine learning models. This method helps in understanding the contribution of each feature to the model's predictions.\n\nWe assessed the usefulness of SHAP by progressively applying it to a varying number of contigs. The overall rankings of feature importance remained stable when applying SHAP to 200, 100, 50, 20, or 10 contigs, indicating that SHAP was consistent at a broad scale. While the absolute importance values varied across these runs, the top features selected as input to the ResMiCo model were consistent. This consistency suggests that the model's decisions are driven by a set of key features, making it more interpretable.\n\nThe top 14 features selected for the ResMiCo model include a variety of metrics such as mapping quality, alignment score, and base composition. These features were chosen to represent different aspects of the data, ensuring that the model's predictions are based on a diverse set of information. By limiting the number of features, we significantly reduced the training time without compromising the model's performance.\n\nIn addition to SHAP, we also used UMAP (Uniform Manifold Approximation and Projection) for dimensionality reduction and visualization of the data. This allowed us to examine how the data were represented by ResMiCo and to gain insights into the types of misassemblies in the data. However, UMAP only allows for visual inspection and does not provide a quantitative measure of feature importance.\n\nTo further test the interpretability of the model, we clustered contigs based on their representations in the ResMiCo model and assessed whether these clusters matched the grouping of contigs by misassembly type. The results showed that the model embeddings do not strongly correspond with misassembly types, suggesting that the model's decisions are not solely based on the type of misassembly.\n\nIn summary, the ResMiCo model is not a blackbox model. We have taken steps to ensure its interpretability by using SHAP for feature importance analysis and UMAP for data visualization. The model's decisions are driven by a set of key features, making it more transparent and easier to understand.",
  "model/output": "The model is a classification model designed to identify misassembled contigs. It uses a binary output, with a sigmoid activation function in the final layer, indicating whether a contig is correctly or incorrectly assembled.\n\nThe model's output is a probability score, which can be interpreted as the likelihood of a contig being misassembled. This score is generated after processing the contig through several layers, including convolutional layers, global average pooling, and fully connected layers.\n\nDuring training, the model minimizes binary cross-entropy loss to improve its classification accuracy. The final output is a single neuron with a sigmoid activation function, producing a value between 0 and 1. A threshold is typically applied to this output to make a binary classification decision.\n\nThe model's performance is evaluated using metrics such as the Area Under the Precision-Recall Curve (AUPRC), which provides insight into the model's ability to distinguish between correctly and incorrectly assembled contigs.\n\nThe output of the model can be used to identify misassembled regions within contigs, although further evaluation is needed to assess its robustness in this specific application. The model's output is not calibrated by default, so techniques like isotonic regression can be used to transform the scores into probabilities.\n\nThe model's architecture and training process are designed to handle contigs of variable lengths, making it versatile for different datasets. The final output shape depends on the number of filters in the last convolutional layer, allowing the model to process contigs efficiently.",
  "model/duration": "The execution time for the model varied significantly due to the computational challenges posed by the expanded training dataset. The initial model training involved 50 epochs, with each epoch representing one complete pass through the entire training dataset. Given the size of the n9k-train dataset and the complexity of the model, this process was computationally intensive.\n\nTo address the reviewers' feedback, a new model was trained on a substantially larger dataset, which required an extension of time to complete. This extended training period was necessary to ensure that the model could handle the increased data volume and to address the computational challenges that arose. The exact duration of the training process is not specified, but it is clear that it involved a considerable amount of time to achieve the desired improvements in model robustness and performance.\n\nAdditionally, the feature selection process using SHAP (SHapley Additive exPlanations) was computationally burdensome. To mitigate this, the analysis was conducted on a subset of 200 contigs, which was found to be sufficient for stable and consistent results. This approach helped in managing the computational load while ensuring the reliability of the feature importance rankings.\n\nIn summary, while the exact execution time for the model is not detailed, it is evident that the training process was extensive and required significant computational resources and time, especially for the updated model trained on the larger dataset.",
  "model/availability": "The source code for the ResMiCo tool is publicly available. The ResMiCo Python package can be accessed via GitHub at https://github.com/nick-youngblut/MGSIM. The package includes continuous integration testing and is designed to be robust and easy to use.\n\nThe tool was initially available via both PyPI and Bioconda, but there were some installation issues reported, particularly with the C++ code used for converting BAM files to features. These issues have been addressed in the updated package, and efforts are ongoing to further improve the installation process. Users are encouraged to report any issues they encounter via the GitHub repository, where the development team responds promptly to ensure a smooth user experience.\n\nFor those who prefer not to install the software locally, the datasets used in the study are available for download from the MPI for Biology FTP server. These datasets can be accessed using tools like wget or curl through the link http://ftp.tue.mpg.de/ebio/projects/ResMiCo/. The authors are committed to ensuring the perpetual public accessibility of these data.",
  "evaluation/method": "The evaluation of ResMiCo involved a comprehensive assessment using various datasets and benchmarks to ensure its robustness and generalizability. Initially, the method was tested against the n9k-novel dataset, which included family-level taxonomically novel genomes not present in the training dataset. This evaluation demonstrated ResMiCo's ability to handle taxonomic novelty, showing consistent performance across different datasets.\n\nTo address reviewer concerns, additional benchmarks were included. Specifically, ResMiCo was evaluated on mock communities, such as BMock12 and MBARC-26, which provided real-world datasets with known ground truths. Despite the limitations of mock communities in terms of richness and inter-genome similarity, ResMiCo performed well, with an average AUPRC of 0.475 and an AUROC of 0.951 across the two datasets.\n\nFurthermore, ResMiCo was assessed using the CAMI gut, oral, and skin metagenome datasets, which are commonly used for evaluating metagenomics analysis tools. These datasets differed significantly from the training datasets in terms of coverage and class imbalance, but ResMiCo's performance remained robust and outperformed all competitors. The evaluation also included non-human biomes, such as CAMI-marine and CAMI-plant-associated datasets, where ResMiCo showed comparable performance, indicating its ability to generalize across diverse biomes.\n\nIn addition to these datasets, ResMiCo was benchmarked against other methods, including metaMIC, DeepMAsED, and ALE. The performance was measured using metrics such as AUPRC and AUROC, which are particularly relevant given the class imbalance in the datasets. The results showed that ResMiCo consistently outperformed existing models, demonstrating its superiority in identifying misassembled contigs.\n\nThe evaluation also considered computational efficiency, comparing the performance of ResMiCo on CPUs versus GPUs. The results indicated that while GPUs significantly speed up the processing, CPUs are still feasible for handling large datasets, although multiple GPUs are recommended for training on extensive datasets.\n\nOverall, the evaluation of ResMiCo involved a rigorous and diverse set of benchmarks, ensuring that the method is robust, generalizable, and superior to existing tools in the field.",
  "evaluation/measure": "In our evaluation of ResMiCo, we primarily report two key performance metrics: the Area Under the Precision-Recall Curve (AUPRC) and the Area Under the Receiver Operating Characteristic Curve (AUROC). These metrics are widely used in the literature for evaluating the performance of machine learning models, particularly in the context of imbalanced datasets, which is common in metagenomic studies.\n\nThe AUPRC is particularly useful for our purposes because it takes into account the class imbalance present in our datasets, where misassembled contigs are less frequent than correctly assembled ones. This metric provides a more accurate reflection of a model's performance in real-world scenarios where positive samples (misassemblies) are rare.\n\nThe AUROC, on the other hand, is used to compare the model's performance across different datasets with varying percentages of positive samples. While AUPRC is sensitive to the prevalence of positive samples, AUROC is not, making it a valuable complementary metric.\n\nWe also benchmarked ResMiCo's resource requirements, comparing its performance when using a CPU versus a GPU. This provides additional context on the practicality and efficiency of using ResMiCo in different computational environments.\n\nThese metrics are representative of the standards in the field, ensuring that our evaluation is both rigorous and comparable to other studies in metagenomic analysis.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, ResMiCo was evaluated against metaMIC, DeepMAsED, and ALE. For metaMIC, default parameters were used with a reduced minimum contig length to match the data used by other methods. DeepMAsED was applied following the feature generation scheme and trained model provided by its authors. ALE's positional sub-scores were aggregated using thresholds defined in previous work. Additionally, a random baseline was included, where a random misassembly probability was assigned to each contig.\n\nThe performance was measured using the area under the precision-recall curve (AUPRC) due to class imbalance in the datasets. The area under the receiver operating characteristic curve (AUROC) was also used to compare performance across datasets with varying percentages of positive samples. ResMiCo outperformed all competitors across various datasets, including n9k-novel, CAMI gut, oral, and skin datasets, demonstrating its robustness and superior performance.",
  "evaluation/confidence": "The evaluation of ResMiCo includes several performance metrics, such as the area under the precision-recall curve (AUPRC) and the area under the receiver operating characteristic curve (AUROC). These metrics are used to compare ResMiCo's performance against other models like DeepMAsED, ALE, and metaMIC.\n\nPerformance metrics are presented with specific values, indicating the model's effectiveness across different datasets. For instance, ResMiCo achieved an AUPRC of 0.76 on the n9k-novel dataset, significantly outperforming DeepMAsED, which had an AUPRC of 0.25. Similarly, on the CAMI gut dataset, ResMiCo was more than twice as fast when using a GPU compared to a CPU, processing 108 ± 0.7 contigs per second versus 38.7 ± 10.3 contigs per second. These values include standard deviations, which provide a measure of variability and confidence in the performance metrics.\n\nStatistical significance is implied through the consistent outperforming of ResMiCo across various datasets and conditions. The model's robustness to taxonomic novelty is demonstrated by its similar AUPRC scores on both training validation and novel datasets. Additionally, the model's performance remains largely unaffected by differences in sequencing depth and class imbalance, as shown by consistent AUROC scores across different datasets.\n\nThe evaluation also includes comparisons with baseline methods, such as assigning random misassembly probabilities, which results in a horizontal line on precision-recall curves. This highlights the superiority of ResMiCo's performance metrics over random chance.\n\nIn summary, the performance metrics for ResMiCo are presented with confidence intervals, and the results are statistically significant, demonstrating the model's superiority over other methods and baselines. The consistent outperforming across various datasets and conditions provides strong evidence of ResMiCo's effectiveness and reliability.",
  "evaluation/availability": "The raw evaluation files for the datasets used in our study are not directly available in the main publication. However, due to the size of the n9k-train and n9k-novel datasets, these data have been deposited on the MPI for Biology FTP server. These datasets can be downloaded using tools such as wget or curl through the following link: http://ftp.tue.mpg.de/ebio/projects/ResMiCo/. The authors are committed to ensuring the perpetual public accessibility of these data.\n\nAdditionally, the deep learning model was built using TensorFlow, and the source code for MGSIM is available at https://github.com/nick-youngblut/MGSIM. This ensures that the tools and models used in our evaluations are accessible to the public for further research and validation."
}