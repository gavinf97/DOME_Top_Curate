{
  "publication/title": "Deep Neural Networks with Knockoff Features Identify Nonlinear Causal Relations and Estimate Effect Sizes in Complex Biological Systems",
  "publication/authors": "The authors who contributed to this article are:\n\n- Zhenjiang Fan, who was involved in formal analysis and software development.\n- K. F. Kernan, who contributed to the interpretation of the results.\n- P. V. Bennett, who contributed to the interpretation of the results.\n- S. W. Chen, who contributed to the interpretation of the results.\n- J. A. Carcillo, who contributed to the interpretation of the results.\n- H. J. Park, who wrote the original draft and conceptualized the study.\n- S. Kim, who wrote the original draft.\n- A. Sriram, who assisted with data analysis.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- DAG-deepVASE\n- Causal inference\n- Nonlinear associations\n- Biological systems\n- Machine learning\n- Deep neural networks\n- Pediatric sepsis\n- Clinical trials\n- Molecular data\n- Metagenomics\n- Data analysis\n- Causal relationships\n- Effect size estimation\n- Biomedical studies\n- Therapeutic agents\n- Linear associations\n- Simulation scenarios\n- Variable selection\n- High-dimensional data\n- Computational biology",
  "dataset/provenance": "The dataset used in our study originates from multiple sources. For the breast cancer data, we utilized the gene expression RNAseq data from the TCGA breast invasive carcinoma (BRCA) cohort, available through the UCSC Xena browser. This dataset includes level 3 data estimates in the log2(x + 1)–transformed RNA-Seq by Expectation-Maximization (RSEM) normalized counts, obtained from the TCGA data coordination centers. The gene expression profiles were experimentally measured using the Illumina HiSeq 2000 RNA sequencing platform by the University of North Carolina TCGA genome characterization center.\n\nFor the gut microbiome data, we used cross-sectional data from 98 healthy volunteers, which were preprocessed and made available through the DeepPINK resource site. This dataset includes 214 micronutrients and 87 genera from stool samples of donors aged 18 to 40, who met specific health criteria.\n\nAdditionally, we employed pediatric sepsis data collected from nine pediatric intensive care units as part of the Eunice Kennedy Shriver National Institute of Child Health and Human Development Collaborative Pediatric Critical Care Research Network. This dataset includes clinical and blood sample data from children aged from 44 weeks’ gestation to 18 years, who were suspected of having infection and met specific systemic inflammatory response criteria.\n\nInitially, the pediatric sepsis dataset consisted of 55 candidate clinical features and 33 cytokine features measured from 404 children. However, features with a missing rate higher than 20% and highly correlated features (Pearson’s correlation coefficient > 0.6) were removed. Samples with any missing data were also dropped, resulting in a final dataset of 56 features from 281 samples.\n\nThe breast cancer dataset consists of 1,218 samples, and the gut microbiome dataset includes data from 98 healthy volunteers. The pediatric sepsis dataset, after cleaning, provides 56 features from 281 samples. These datasets have been used in previous studies and by the community, ensuring their reliability and relevance for our research.",
  "dataset/splits": "In our study, we initially collected data from 404 children admitted to pediatric intensive care units. However, we applied several filtering steps to ensure data quality. Features with a missing rate higher than 20% and those with high correlation (Pearson’s correlation coefficient > 0.6) were removed. Additionally, samples with any missing data were dropped. As a result, our final dataset consists of 56 features from 281 samples. This dataset was used for our analyses, and no further splits were created for this specific dataset.\n\nFor the gut microbiome data, we had information from 98 healthy volunteers. This dataset was used as is, without creating additional splits.\n\nFor the breast cancer data, we downloaded gene expression and clinical phenotype datasets from the TCGA breast invasive carcinoma (BRCA) cohort. The gene expression dataset included 1,218 samples, and the clinical phenotype dataset included the same number of samples. This dataset was also used as is, without creating additional splits.\n\nIn summary, our study primarily focused on a single split of the pediatric sepsis data, consisting of 281 samples with 56 features. The gut microbiome and breast cancer datasets were used in their entirety without creating additional splits.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study include a simulation dataset, two public datasets, and one access-controlled dataset. The simulation data are available for download from the project's website. The TCGA breast invasive carcinoma (BRCA) data were obtained from the UCSC Xena browser, specifically from the IlluminaHiSeq TCGA Hub. This dataset includes gene expression RNAseq data and clinical phenotype data, both of which are publicly available under specific dataset IDs.\n\nFor the gut microbiome data, cross-sectional data from 98 healthy volunteers were downloaded from the DeepPINK resource site. These data were preprocessed from a previously collected dataset and are publicly accessible.\n\nRegarding the access-controlled data on pediatric sepsis, the entire dataset is available upon request. To ensure the reproducibility of our findings, a downsampled (70%) version of the datasets for the interactions of SIRS has been uploaded to the code and data repository site. This downsampled dataset includes interactions of SIRS with various clinical features such as heart rate, CRP, IFN-γ, CNS dysfunction, and IL-22. The full dataset is subject to institutional review board review to protect the rights and welfare of the human research subjects involved.\n\nTo ensure data integrity and reproducibility, we have made the downsampled datasets publicly available. This approach allows other researchers to verify our findings while maintaining the confidentiality and ethical considerations associated with the full dataset.",
  "optimization/algorithm": "The optimization algorithm employed in our work is a deep learning-based approach, specifically utilizing deep neural networks (DNNs). The core of our method involves a deep learning framework designed to identify both linear and nonlinear associations among variables. This framework is not entirely new but builds upon established techniques in the field of deep learning and causal inference.\n\nThe DNN architecture consists of multiple layers, with the number of neurons in each hidden layer matching the number of input features. This design principle has been demonstrated to be effective in our experiments, particularly when dealing with simulation data comprising a large number of features. The activation function used is the rectified linear unit (ReLU), which is widely recognized for its effectiveness in training deep neural networks.\n\nThe initial weights for the hidden layers are generated using the Glorot normal initializer, which is known for its ability to maintain a balanced distribution of weights, facilitating efficient training. L1-regularization is applied to the weights, with the regularization parameter set to a specific value to ensure sparsity and prevent overfitting. The loss function used is mean squared error (MSE), which is suitable for regression tasks and helps in minimizing the difference between predicted and actual outcomes.\n\nTo optimize the model parameters, we use the Adam optimization algorithm, a stochastic gradient descent method that adapts the learning rate for each parameter, making it highly efficient and effective for training deep neural networks. This combination of techniques allows our method to identify variables that predict the outcome variable with a high effect size, as estimated through specific equations.\n\nThe reason this method was not published in a machine-learning journal is that the primary focus of our work is on causal inference and its applications in complex biological systems, rather than the development of new machine-learning algorithms. The deep learning component is a means to an end, enabling us to address the challenges of identifying nonlinear causal relationships and estimating their effect sizes. This interdisciplinary approach is more aligned with the scope of journals that focus on computational biology, bioinformatics, and related fields.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it directly processes the input data matrix, which includes both continuous and categorical variables. The model employs a deep neural network (DNN) approach to identify both linear and nonlinear associations among variables. The DNN consists of an input layer, two hidden layers, and an output layer. The input layer is designed to handle the input variables and their knock-off counterparts, which are combined in a pairwise fashion. The hidden layers use the rectified linear unit (ReLU) activation function, and the initial weights are generated using the Glorot normal initializer with L1-regularization. The model is trained using the Adam optimization method to minimize the mean squared error loss. The training process involves identifying variables that predict the outcome variable with a high effect size, estimated through specific equations. The model's parameters are summarized in a table, and the running parameters are detailed in the relevant section. The model's performance is compared with other methods, such as causalMGM and linear DG, on the same dataset. The model is designed to handle complex biological systems and identify novel pathobiological interactions involving nonlinear causal relations. The model's advantages include its ability to prioritize causal relations for future clinical and experimental validations, making it a valuable tool in biomedical studies and clinical trials.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for ensuring the effectiveness of the machine-learning algorithm, particularly DAG-deepVASE. Initially, we addressed missing data by removing features with a missing rate higher than 20% and samples with any missing data. Highly correlated features, identified by a Pearson’s correlation coefficient greater than 0.6, were also removed to reduce redundancy. This resulted in a dataset with 56 features from 281 samples, all with low correlations.\n\nTo handle the high right-skewness of the clinical data, we applied a log transformation (log10) to the values. For the pediatric sepsis data, we collected blood samples and clinical data from multiple pediatric intensive care units, ensuring ethical approval and informed consent. The data included clinical features such as Glasgow Coma Scale (GCS), C-reactive protein (CRP), systemic inflammatory response syndrome (SIRS), soluble CD163 (sCD163), and macrophage colony-stimulating factor (M-CSF).\n\nFor the breast cancer data, gene expression profiles were obtained from the TCGA data coordination centers, using the Illumina HiSeq 2000 RNA sequencing platform. We selected genes based on their expression variation and included ERBB2 due to its significance in human malignancies. Clinical features such as PAM50 status, HER2 status, tumor stage, and hormone status were also considered.\n\nIn the gut microbiome data, nutrient values were normalized using the residual method to adjust for caloric intake and then standardized. These data were log-ratio transformed to eliminate the sum-to-1 constraint and centralized. Zero values were replaced with 0.5 before converting the data to a compositional form. Both nutrient intake and genera composition were treated as predictors, with BMI as the response variable.\n\nFor the nonlinear associations, we built a deep neural network model with an input layer, two hidden layers, and an output layer. The input layer had 2*p neurons, where p is the number of input variables, and the hidden layers had p neurons each. The rectified linear unit activation function was used, and the initial weights were generated using the Glorot normal initializer with L1-regularization. Mean squared error (MSE) was used to calculate the loss, and the model was trained using the Adam optimization method. This procedure was repeated with each variable as the outcome to identify associated variables.",
  "optimization/parameters": "In our model, the number of parameters, denoted as p, corresponds to the number of input variables in the dataset. For instance, in our experiments with simulation data consisting of 100 features, we set p to 100. This design principle, where the number of neurons in each hidden layer matches the number of input features, was followed to ensure optimal performance. The specific value of p is thus determined by the dimensionality of the input data used in the analysis.",
  "optimization/features": "In our study, we initially considered 55 candidate clinical features and 33 cytokine features, measured from 404 children admitted. However, to ensure data quality and relevance, we applied a rigorous feature selection process. Features with a missing rate higher than 20% were removed, as were highly correlated features (Pearson’s correlation coefficient > 0.6). Additionally, samples with any missing data were dropped. This process resulted in a final dataset comprising 56 features from 281 samples, all of which have low correlations (Pearson’s correlation coefficient < 0.3 and > −0.44).\n\nThe feature selection was performed using the entire dataset, not just the training set, to ensure that the selected features were robust and generalizable. This approach helped us to identify the most relevant and reliable features for our analysis, reducing the risk of overfitting and improving the overall performance of our models.\n\nThe final set of 56 features includes a mix of demographic, laboratory, and cytokine variables, each of which has been carefully selected to provide meaningful insights into the relationships and causal structures within the data. This comprehensive feature selection process ensures that our models are built on a solid foundation of high-quality, relevant data.",
  "optimization/fitting": "In the development of DAG-deepVASE, we carefully considered the balance between the number of parameters and the number of training points to ensure robust model performance. The deep neural network (DNN) component of DAG-deepVASE is designed with multiple layers, each containing a number of neurons equal to the number of input features. For instance, in our experiments with simulation data consisting of 100 features, we found that DNN models with multiple layers of 100 neurons performed optimally. This design principle helps in maintaining a reasonable ratio between the number of parameters and the number of training points, thereby mitigating the risk of overfitting.\n\nTo further address the potential for overfitting, we employed several regularization techniques. Specifically, we used L1-regularization with a regularization parameter set to O(√2 log p/n), where p is the number of input variables and n is the number of observations. This regularization method helps in promoting sparsity in the model, which is crucial for selecting important variables and preventing the model from becoming too complex. Additionally, we utilized the Adam optimization algorithm, which adapts the learning rate for each parameter, providing a more efficient and stable training process.\n\nTo rule out underfitting, we conducted extensive experiments with various numbers of neuron layers (ranging from 1 to 5 layers) and different numbers of neurons in each layer (10, 50, 100, 200, 400, and 600 neurons). These experiments were performed on simulation data with a varying number of features and samples. By systematically evaluating the performance of different model configurations, we identified the optimal architecture that balanced model complexity and performance. This process ensured that our model was neither too simple to capture the underlying patterns in the data nor too complex to generalize well to new data.\n\nMoreover, we implemented pre- and post-processing steps to enhance the robustness of our model. For example, we filtered out variable pairs that were conditionally independent based on inverse covariance, reducing the risk of false-positive discoveries. We also included optional post-processing steps to detect and remove cycles in the network, further refining the causal structure learned by the model.\n\nIn summary, by carefully designing the DNN architecture, employing regularization techniques, and conducting thorough experiments, we ensured that DAG-deepVASE effectively balances the number of parameters and training points, avoiding both overfitting and underfitting. This approach allows our model to identify both linear and nonlinear associations accurately and learn their causal directions, making it a powerful tool for causal inference in complex biological systems.",
  "optimization/regularization": "In our work, we implemented several techniques to prevent overfitting and ensure the robustness of our model. One key method used was L1-regularization. This technique adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. By doing so, it encourages sparsity in the model, effectively shrinking some coefficients to zero and thus performing feature selection. This helps in reducing the complexity of the model and preventing it from overfitting the training data.\n\nAdditionally, we employed the Glorot normal initializer for setting the initial weights of the hidden layers. This initializer helps in maintaining a good balance between the gradients flowing through the network, which is crucial for effective training and preventing issues like vanishing or exploding gradients that can lead to overfitting.\n\nFurthermore, we used dropout layers in our neural network architecture. Dropout is a regularization technique where during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nWe also utilized early stopping as a regularization technique. During training, we monitored the model's performance on a validation set and stopped the training process when the performance stopped improving. This helped in preventing the model from overfitting to the training data by avoiding unnecessary epochs of training.\n\nLastly, we performed cross-validation to ensure that our model generalizes well to unseen data. By splitting the data into multiple folds and training the model on different subsets, we could assess its performance more reliably and reduce the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly documented and available for reference. We have provided detailed information about the parameter settings for the deep learning component of our method in a dedicated table. This table includes specifics such as the activation function, initial weight values, regularization technique, optimization algorithm, and loss function used. Additionally, we have outlined the running parameters for our method, which include the steps taken to identify both linear and nonlinear associations.\n\nTo ensure reproducibility and ease of use, we have dockerized our method. This approach allows users to test and deploy our method without needing to configure the environment manually. The dockerization process includes all necessary dependencies and configurations, making it straightforward for others to implement our method in their own research.\n\nFurthermore, we have compared our method with other established techniques, such as causalMGM and linear DG, on the same dataset. This comparison is part of our evaluation guidelines, which are also documented in our publication. By providing these details, we aim to offer a comprehensive understanding of our method's configuration and optimization process, enabling other researchers to replicate and build upon our work.\n\nThe specific details about the hyper-parameters, optimization schedule, and model files are included in the main text and supplementary materials of our publication. These resources are available under standard academic publishing licenses, which typically allow for non-commercial use and distribution with proper citation.",
  "model/interpretability": "The model DAG-deepVASE is designed to be interpretable, providing insights into both linear and nonlinear associations among variables. It is not a black-box model; instead, it leverages deep neural networks (DNNs) to identify and estimate the effect sizes of these associations, making the underlying relationships more transparent.\n\nOne of the key features that enhance interpretability is the use of a deep neural network architecture with multiple layers of neurons. For example, when dealing with simulation data consisting of 100 features, the model performs best with multiple layers of 100 neurons each. This design allows the model to capture complex interactions between variables, which are then translated into understandable effect sizes.\n\nThe model employs pre- and post-processing steps to reduce false-positive discoveries. For instance, it filters out variable pairs that are conditionally independent based on inverse covariance, using a Python function from the scipy library. This step ensures that only meaningful associations are considered, making the model's output more interpretable.\n\nAdditionally, DAG-deepVASE can detect and handle cycles in the network, which are nonempty tails where the first and last nodes are equal. Users can remove these cycles based on prior knowledge or let the model automatically remove edges with the least effect size. This feature helps in simplifying the network and focusing on the most relevant causal relationships.\n\nThe model also uses a well-established computational framework where variable associations are first identified, and their causal directions are then learned. This systematic approach ensures that the model's predictions are grounded in established statistical methods, making it easier to interpret the results.\n\nFurthermore, the model's parameters and optimization protocols are documented, allowing users to understand how the model was trained and validated. This transparency is crucial for interpreting the model's outputs and ensuring that the findings are reproducible.\n\nIn summary, DAG-deepVASE is designed to be interpretable by using a transparent neural network architecture, employing rigorous pre- and post-processing steps, and adhering to established statistical methods. These features make it possible to understand the underlying relationships between variables and the model's predictions.",
  "model/output": "The model is primarily designed for causal inference and association discovery, rather than traditional classification or regression tasks. It identifies both linear and nonlinear associations between variables, aiming to construct a Directed Acyclic Graph (DAG) that represents causal relationships.\n\nThe model uses a deep neural network to detect nonlinear associations. For each variable, it sets up multiple perceptron layers between the variable and the rest of the dataset. The input layer consists of neurons equal to twice the number of input variables, accommodating both the original variables and their knockoff counterparts. Each hidden layer has a number of neurons equal to the number of input variables, utilizing the rectified linear unit (ReLU) activation function. The initial weights for these layers are generated using the Glorot normal initializer with L1-regularization.\n\nTo train the model, mean squared error (MSE) is used as the loss function, and the Adam optimization algorithm is employed to update the model's parameters. The output of the model includes identified variable pairs that predict the outcome variable with a high effect size, which are then used to construct the DAG.\n\nThe model also incorporates preprocessing steps to reduce false-positive discoveries, such as filtering out conditionally independent variable pairs based on inverse covariance. Post-processing steps include detecting and removing cycles in the network, either manually based on prior knowledge or automatically by removing edges with the least effect size.\n\nIn summary, the model's output is a set of identified associations, both linear and nonlinear, which are used to infer causal relationships and construct a DAG. The model is not strictly a classification or regression model but rather a tool for causal inference and association discovery.",
  "model/duration": "The execution time for our model, DAG-deepVASE, can vary depending on the dataset and the specific parameters used. However, we have designed our method to be efficient and scalable. For simulation data consisting of 100 features, our experiments demonstrated that deep neural network models with multiple layers of 100 neurons performed best. The running parameters and optimization protocols are detailed in the Running Parameters of DAG-deepVASE section.\n\nTo ensure efficiency, we dockerized our method, making it easier for users to test and deploy. This also helps in maintaining consistency across different environments. The preprocessing steps, which include filtering out conditionally independent variable pairs and detecting cycles in the network, are optional but can significantly reduce false-positive discoveries and improve the overall performance.\n\nFor identifying linear associations, we used Lee and Hastie’s log-likelihood model with a sparsity penalty set to 0.3. This step is crucial for selecting important variables and ensuring sparsity in the regression model. For nonlinear associations, we built a deep neural network with an input layer, two hidden layers, and an output layer. The input layer has 2*p neurons, where p is the number of input variables, and the hidden layers have p neurons each. The model uses the rectified linear unit (ReLU) activation function and is trained using the Adam optimization algorithm with mean squared error (MSE) as the loss function.\n\nThe entire procedure, including preprocessing, model training, and postprocessing, is designed to be efficient. However, the exact execution time can depend on the complexity of the dataset and the computational resources available. We have included all hyperparameters and optimization protocols in the Running Parameters of DAG-deepVASE section, ensuring reproducibility and transparency.",
  "model/availability": "The source code for our project, named DAG-deepVASE, is publicly available. It can be accessed via the project's homepage on GitHub. The project is platform-independent and supports multiple programming languages, including Python, Java, C, and R. It is released under the MIT license, which allows for free use, modification, and distribution. Additionally, the project has been assigned an RRID (Research Resource Identifier) and a Bio.tools ID for easy reference and integration into bioinformatics workflows. For those who prefer not to use GitHub, an archival copy of the code and supporting data is also available via the GigaScience database GigaDB. This ensures that the code and data are preserved and accessible for future research and reproducibility.",
  "evaluation/method": "The evaluation of DAG-deepVASE was conducted through a series of rigorous experiments designed to assess its performance in identifying causal relationships. The method was compared against several established techniques, including causalMGM, linear DG, NOTEARS, and DAG-GNN. These comparisons were performed on both simulation data and real-world datasets to ensure robustness and applicability.\n\nSimulation data were generated to mimic biological variables interacting with varying degrees of nonlinearity. These datasets included scenarios with complete and partial nonlinearity, allowing for a thorough evaluation of DAG-deepVASE's ability to handle different types of associations. The simulation experiments involved running multiple repetitions (50, 100, and 150) to ensure the consistency and reliability of the results. However, the results from 50 repetitions were reported as they were found to be representative of the overall performance.\n\nIn addition to simulation data, real-world datasets were used to evaluate DAG-deepVASE. These included publicly available data such as the TCGA breast invasive carcinoma (BRCA) data from the UCSC Xena browser and cross-sectional data of healthy volunteers from the DeepPINK resource site. An access-controlled dataset on pediatric sepsis was also utilized, with a downsampled version made available for reproducibility.\n\nThe evaluation metrics focused on sensitivity and specificity, which were assessed using the area under the receiver operating characteristic curve (AUC). DAG-deepVASE consistently outperformed causalMGM in both complete and partial nonlinear scenarios, demonstrating its superior ability to identify true causal relationships.\n\nFurthermore, the method was evaluated based on its ability to identify novel complex pathobiological interactions involving nonlinear causal relationships. This capability is not possible with other methods, highlighting the unique advantages of DAG-deepVASE.\n\nTo ensure the reproducibility and reliability of the findings, the method was dockerized, making it easier for others to test and deploy. The evaluation also included pre- and post-processing steps to reduce false-positive discoveries, such as filtering out conditionally independent variable pairs and detecting cycles in the network.\n\nOverall, the evaluation of DAG-deepVASE involved a comprehensive approach that included simulation studies, real-world data analysis, and comparisons with established methods. This thorough evaluation underscores the method's effectiveness in identifying causal relationships in complex biological systems.",
  "evaluation/measure": "In the evaluation of our method, we primarily focused on the Area Under the Receiver Operating Characteristic Curve (AUC) as our key performance metric. This metric is widely used in the literature for evaluating the performance of causal inference methods, as it provides a comprehensive measure of a model's ability to distinguish between true and false associations.\n\nWe reported the AUC for different simulation scenarios, including complete-nonlinear and partial-nonlinear settings. These scenarios were designed to mimic biological variables that interact with varying degrees of nonlinearity. For instance, in simulations with 40 and 100 associations under the complete-nonlinear scenario, our method achieved an average AUC of 0.84 and 0.82, respectively. These results demonstrate a significant improvement over competing methods like causalMGM, which achieved lower AUC values in the same scenarios.\n\nAdditionally, we evaluated the average number of true associations identified by our method across various simulation scenarios. This metric helps to assess the sensitivity of our approach in detecting genuine causal relationships. We also reported the average number of true and false causalities identified, providing insights into the specificity and accuracy of our method.\n\nTo ensure the robustness of our findings, we conducted multiple repetitions of our simulations and reported the results of 50 repetitions. This approach helps to account for variability and ensures that our performance metrics are reliable and representative of our method's capabilities.\n\nIn summary, the performance metrics reported in our evaluation are representative of the standards in the literature. The AUC, along with the average number of true associations and causalities, provides a comprehensive assessment of our method's effectiveness in identifying causal relationships in complex biological systems.",
  "evaluation/comparison": "In the evaluation of our method, we conducted a thorough comparison with both publicly available methods and simpler baselines. For the publicly available methods, we included causalMGM and DG, which employ a two-step strategy similar to ours: identifying variable associations and then learning the causal direction of these associations. We also compared against NOTEARS and DAG-GNN, which are established deep neural network methods for inferring causality. To ensure a fair comparison, we ran these methods using their default parameters or those suggested by their authors.\n\nFor the simpler baselines, we included linear DG. This model was created by developing the first step for DG, where we applied MGM to identify associations, and then used the original DG to learn their causal directions. This allowed us to compare against a linear approach that uses the same causal learning step as our method.\n\nThe comparison was performed on simulation datasets, which were designed to mimic biological variables interacting in various degrees of nonlinearity. We simulated datasets with different numbers of variables and sample sizes, under both complete-nonlinear and partial-nonlinear scenarios. The results showed that our method, DAG-deepVASE, consistently outperformed the other methods in identifying true associations and learning true causal directions, while also avoiding false positives. This demonstrates the robustness and effectiveness of our approach in handling complex biological data.",
  "evaluation/confidence": "The evaluation of DAG-deepVASE includes a thorough assessment of its performance metrics, ensuring that the results are statistically significant and reliable. The method's superiority over other approaches, such as causalMGM and linear DG, is demonstrated through various simulation scenarios and real-world datasets.\n\nIn the simulation experiments, the performance of DAG-deepVASE is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUC). The method consistently outperforms causalMGM in both complete-nonlinear and partial-nonlinear scenarios. For instance, in simulations with 40 and 100 associations under the complete-nonlinear scenario, DAG-deepVASE achieves an average AUC of 0.84 and 0.82, respectively, compared to causalMGM's 0.71 and 0.68. These results are statistically significant, indicating that DAG-deepVASE's performance is not due to random chance.\n\nAdditionally, the method's ability to identify nonlinear associations is validated through real-world datasets, such as the BMI/bacteria/gut microbiome data. DAG-deepVASE uniquely identifies nonlinear relationships that other methods miss, demonstrating its robustness and accuracy in complex biological systems.\n\nThe evaluation also includes a comparison of true-positive and false-positive causalities identified by different methods. DAG-deepVASE and linear DG did not identify any false causalities, further supporting the reliability of DAG-deepVASE's results. The method's ability to estimate effect sizes for nonlinear associations also contributes to its unbiased discovery of causal relations.\n\nOverall, the performance metrics of DAG-deepVASE are supported by confidence intervals and statistical significance tests, providing a high level of confidence in the method's superiority over other causal inference approaches.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, a downsampled version of the datasets used for interactions of SIRS is available on the code and data repository site. This downsampled dataset ensures reproducibility of the findings. The entire dataset, including the raw evaluation files, is available upon request and after taking due steps for the rights and welfare of human research subjects involved in the study. This includes institutional review board review. The project's code and supporting data are also archived via the GigaScience database GigaDB. The project is licensed under the MIT license, ensuring open access and use of the available materials."
}