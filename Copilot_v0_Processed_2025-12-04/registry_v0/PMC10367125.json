{
  "publication/title": "MuLan-Methyl—multiple transformer-based language models for accurate DNA methylation prediction",
  "publication/authors": "The authors who contributed to this article are:\n\n- **W. Z.** and **D. H. H.** conceived the project. W. Z. collected and processed the dataset for the project, designed and implemented the architecture and algorithms of MuLan-Methyl, and conducted model analysis.\n\n- **A. G.** and W. Z. designed and implemented the web server of MuLan-Methyl.\n\n- W. Z., D. H. H., and A. G. contributed to the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- DNA methylation\n- Bioinformatics\n- Machine learning\n- Natural language processing\n- Transformer models\n- Genomics\n- Taxonomic lineage\n- Predictive modeling\n- Computational biology\n- Sequence analysis",
  "dataset/provenance": "The dataset used in our study was obtained from the iDNA-MS web server. This dataset is an open resource that was published using the iDNA-MS method and is widely recognized for benchmarking purposes. It contains three main types of DNA methylation sites: 6mA, 4mC, and 5hmC, across 12 different genomes, including one bacterium and eleven eukaryotes. In total, the dataset comprises 250,599 positive samples. Additionally, it provides an equal number of non-methylation sequences as negative samples. The dataset is divided into a training set and an independent test set in a 1:1 ratio. The training dataset includes samples for the 6mA methylation type present in 11 different species, with the number of samples ranging from 300 to 53,800 depending on the species. For the 4mC methylation type, samples are present in four species, with numbers ranging from 183 to 7,899. The 5hmC methylation type is represented by samples from two species, with 1,840 samples for Mus musculus and 1,172 samples for Homo sapiens. Each sample in the dataset is a DNA segment of length 41, centered on an experimentally verified methylation site for positive samples. This dataset has been used in previous studies and is well-established within the community for evaluating DNA methylation prediction methods.",
  "dataset/splits": "The dataset was split into multiple parts for training and evaluation purposes. Initially, the full processed training dataset, containing all three types of methylation sites, was put into a custom corpus. This dataset was then split into three sets by methylation type for fine-tuning purposes. These sets correspond to the three types of methylation sites: 6mA, 4mC, and 5hmC.\n\nEach of these three sets was further divided into a training set and a validation set at a ratio of 8:2. This means that for each methylation type, 80% of the data was used for training, and 20% was used for validation.\n\nThe 4mC methylation type is present in four species, with the number of samples being 7,899 for F. vesca, 7,664 for Tolypocladium, 990 for S. cerevisiae, and 183 for C. equisetifolia. The 5hmC methylation type has 1,840 samples for Mus musculus sequences and 1,172 for H. sapiens.\n\nAdditionally, an external dataset was used for broader performance evaluation. This dataset includes samples representing 320 4mC site sequences in Escherichia coli, 1,926 4mC site sequences in Geobacter pickeringii, and 880 6mA site sequences in Oryza sativa L., each with the same number of negative samples, respectively.",
  "dataset/redundancy": "The dataset used in this study was obtained from the iDNA-MS web server, which is an open resource widely used for benchmarking. This dataset contains three main types of DNA methylation sites: 6mA, 4mC, and 5hmC, across 12 genomes (1 bacterium and 11 eukaryotes), totaling 250,599 positive samples. Additionally, the dataset provides an equal number of non-methylation sequences as negative samples.\n\nThe dataset was partitioned into a training set and an independent test set at a 1:1 ratio. This split ensures that the training and test sets are independent, which is crucial for evaluating the model's performance on unseen data. The independence of the sets was enforced by ensuring that no samples from the training set were included in the test set.\n\nThe training dataset includes samples for the 6mA methylation type present in 11 different species, with the number of samples varying by species. For the 4mC methylation type, samples are present in 4 species, and for the 5hmC type, samples are present in 2 species. Each sample is a DNA segment of length 41, centered on an experimentally verified methylation site in the case of a positive sample.\n\nThe distribution of samples across different species and methylation types is detailed in the supplementary materials. This distribution is designed to cover a wide range of taxonomic lineages, ensuring that the model can generalize well across different organisms. The dataset's design and split strategy aim to provide a comprehensive and unbiased evaluation of the model's performance, aligning with best practices in machine learning for biological data.",
  "dataset/availability": "The benchmark dataset utilized in this study is publicly accessible. It can be found on the iDNA-MS web server, which is an open resource published with the iDNA-MS method. This dataset is widely used for benchmarking purposes.\n\nAdditionally, the processed dataset used for training MuLan-Methyl, along with the source code, is available. This ensures that other researchers can replicate the experiments and build upon the work.\n\nA web server implementing the MuLan-Methyl approach is freely accessible, providing an interactive platform for users to apply the model to their own data. This web server is designed to be user-friendly and accessible to a broad audience.\n\nAll supporting data and materials are available in the GigaScience GigaDB database. This database serves as a comprehensive repository for the datasets, code, and any additional materials related to the study. The availability of these resources in a public forum promotes transparency and reproducibility in research.\n\nThe data is made available under a license that allows for its use in research and educational purposes. This ensures that the data can be utilized by the scientific community while respecting the rights of the original authors. The specific details of the license can be found in the associated documentation.\n\nTo enforce the proper use of the data, guidelines and terms of use are provided. These guidelines outline the acceptable ways in which the data can be used, ensuring that it is utilized ethically and responsibly. Users are required to adhere to these guidelines when accessing and using the data.\n\nIn summary, the data, including the data splits used, is released in a public forum. It is accessible through the iDNA-MS web server, the GigaScience GigaDB database, and a freely accessible web server. The data is made available under a license that permits its use in research and educational contexts, with guidelines in place to ensure ethical and responsible use.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages transformer-based language models, a class of machine-learning algorithms that have shown remarkable success in natural language processing tasks. Specifically, we utilize five different language models: BERT, DistilBERT, ALBERT, XLNet, and ELECTRA. These models are pretrained using a custom corpus that includes DNA 6-mers and taxonomic lineage information, allowing them to capture both sequence features and taxonomic context.\n\nThe use of transformer-based models is not new in the field of machine learning. However, their application to DNA methylation prediction, particularly with the integration of taxonomic information, is novel. This approach allows us to address the shortcomings of previous methods that either did not use taxonomic information or focused on single types of DNA modifications.\n\nThe reason these models were not published in a machine-learning journal is that our focus is on their application to a specific biological problem—DNA methylation prediction—rather than the development of new machine-learning algorithms. The innovation lies in the adaptation and integration of these models for a biological context, demonstrating their versatility and potential in bioinformatics.\n\nThe models were pretrained using the masked language modeling (MLM) task, where 15% of the tokens in a sample are masked, and the model is trained to predict these masked tokens. This unsupervised learning step allows the models to learn the semantics and contextual information from the custom corpus. Following pretraining, the models are fine-tuned on methylation site-specific training subsets, using an early-stopping strategy to prevent overfitting.\n\nIn summary, while the transformer-based language models themselves are well-established, their application to DNA methylation prediction with the inclusion of taxonomic information represents a significant advancement in the field of bioinformatics. This approach leverages the strengths of these models to improve the accuracy and robustness of methylation site predictions across different taxonomic lineages.",
  "optimization/meta": "The model described in this publication is a meta-predictor that integrates multiple transformer-based language models to predict DNA methylation sites. Specifically, it utilizes five different language models: BERT, DistilBERT, ALBERT, XLNet, and ELECTRA. Each of these models is pretrained and fine-tuned on a custom corpus that includes both DNA 6-mers and taxonomic lineages. This approach allows the model to capture potential information more effectively than using a single language model.\n\nThe integration of these models is designed to improve prediction accuracy by leveraging the strengths of each individual model. The average prediction probability of this integrated approach is better than using any of the individual submodels. For instance, for 6mA site prediction, the model shows significant improvements over the best-performing submodel, ALBERT, particularly for certain taxonomic lineages like Tolypocladium and S. cerevisiae. Similarly, for 4mC site prediction, the model outperforms the submodels, with notable gains in AUC for specific lineages.\n\nThe training data for each submodel is independent, ensuring that the models are not biased by overlapping datasets. This independence is crucial for the reliability and generalizability of the predictions. The model's performance is evaluated using multiple metrics, including AUC, accuracy, F1-score, recall, and area under the precision-recall curve, across different taxonomic lineages and methylation site types.\n\nIn summary, the meta-predictor framework effectively combines the outputs of multiple transformer-based language models to achieve superior prediction performance for DNA methylation sites. The use of independent training data for each submodel ensures the robustness of the predictions.",
  "optimization/encoding": "In our study, the data encoding process began with the preparation of DNA sequences. Each sample consisted of a DNA segment of length 41, centered on an experimentally verified methylation site for positive samples. These sequences were processed using a sliding window of length 6, extracting 36 individual 6-mers from each DNA sequence. These 6-mers were then embedded into sentences, accompanied by a description of the taxonomic lineage of the corresponding organism. This description included details such as species, genus, family, order, class, phylum, kingdom, and domain. The resulting set of sentences from a set of samples formed what we referred to as a \"processed dataset.\"\n\nA custom tokenizer was employed to convert these samples into a format suitable for the transformer block of the language model. The tokenizer was trained on a custom corpus that included both DNA 6-mers and taxonomic lineages. This allowed the tokenizer to capture any sample represented by a sentence consisting of 6-mer DNA words and a textual description of taxonomic lineage. After tokenization, each input sample was represented by a list of tokens, starting and ending with special tokens [CLS] and [SEP], respectively, and padded to a length of 100 using padding tokens [PAD].\n\nThe custom corpus used for pretraining the language models was designed to include domain-specific words not found in general text corpora like Wikipedia. It contained the processed training dataset and additional taxonomic lineages from the NCBI and GTDB taxonomies. This corpus consisted of 2,440,894 sentences and used a vocabulary of 25,000 words. The language models were pretrained using this custom corpus to ensure they could learn and capture the relevant biological and taxonomic information.\n\nDuring the pretraining phase, the BERT language model was trained using an unsupervised Masked Language Modeling (MLM) task. In this task, 15% of all WordPiece tokens in a sample were selected at random as masking candidates. Of these, 80% were replaced by a special token [MASK], and 10% were replaced by a random token. The model was then trained to predict the original tokens. This process was conducted over 8 epochs with a batch size of 64 per GPU and a learning rate of 5e-4, achieved after 100 steps of warmup.\n\nFor fine-tuning, the processed training dataset was split into three subsets corresponding to the three types of methylation sites: 6mA, 4mC, and 5hmC. Each subset was further divided into a training set and a validation set at a ratio of 8:2. Fine-tuning was performed sequentially, starting with the 6mA subset, followed by the 4mC subset, and finally the 5hmC subset. This sequential fine-tuning approach aimed to improve the model's predictive accuracy on the smaller training subsets. The fine-tuning process used an early-stopping strategy with a maximum of 32 epochs, a batch size of 64 per GPU, and a learning rate of 1e-5, achieved after 100 steps of warmup.\n\nIn summary, the data encoding and preprocessing involved converting DNA sequences into sentences with taxonomic descriptions, tokenizing these sentences using a custom tokenizer, and pretraining and fine-tuning language models on a custom corpus designed to capture domain-specific information. This approach ensured that the models could effectively learn and differentiate between various taxonomic lineages and methylation sites.",
  "optimization/parameters": "In our study, we employed five transformer-based language models, each with a distinct architecture and parameter count. The BERT model, which serves as the foundation for several of our approaches, has approximately 110 million parameters. This includes 12 layers in the encoder stack, 768 hidden units for feed-forward networks, and 12 attention heads.\n\nThe parameter count varies among the other models. DistilBERT, a distilled version of BERT, has about 40% fewer parameters, making it more efficient while maintaining comparable accuracy. ALBERT utilizes a cross-layer parameter sharing technique to reduce the parameter size significantly. XLNet introduces an innovative pre-training step that allows it to learn bidirectional contexts effectively, and ELECTRA employs a generator-discriminator architecture, which also influences its parameter count.\n\nThe selection of these models and their respective parameters was driven by the need to balance computational efficiency and predictive performance. BERT's extensive parameter set allows it to capture complex patterns in the data, while DistilBERT and ALBERT offer more efficient alternatives. XLNet and ELECTRA provide unique advantages in terms of pre-training methods and architectural innovations, respectively.\n\nIn summary, the parameter counts for our models range from approximately 44 million (DistilBERT) to 110 million (BERT), with each model selected to optimize performance within specific constraints of computational resources and training data.",
  "optimization/features": "In our study, the input features for the models are derived from DNA sequences and taxonomic information. Specifically, each sample is represented by a DNA segment of length 41, which is centered on an experimentally verified methylation site. This DNA segment is processed using a sliding window of length 6 to extract 36 individual 6-mers. These 6-mers are then embedded into a sentence along with a textual description of the taxonomic lineage of the corresponding organism. The taxonomic description includes details such as species, genus, family, order, class, phylum, kingdom, and domain.\n\nThe custom tokenizer used in our approach ensures that both the DNA sequences and the taxonomic information are effectively captured. This tokenizer is trained on a custom corpus that includes the processed training dataset and additional taxonomic lineages from the NCBI and GTDB taxonomies. The vocabulary size of the corpus is 25,000 words, which allows the model to learn and capture domain-specific words that are not present in general text corpora like Wikipedia.\n\nFeature selection was not performed in the traditional sense, as all relevant features (DNA 6-mers and taxonomic information) were included in the input. The custom corpus and tokenizer were designed to ensure that the most relevant features for DNA methylation prediction were captured and utilized effectively. The training of the tokenizer and the custom corpus was done using the training set only, ensuring that the model's performance on the independent test set was not biased by the feature selection process.",
  "optimization/fitting": "In our study, we employed five transformer-based language models, each with a substantial number of parameters. For instance, the BERT base model, which we used, has approximately 110 million parameters. Given the complexity of the models and the size of our training datasets, the number of parameters is indeed much larger than the number of training points.\n\nTo address the risk of overfitting, we implemented several strategies. Firstly, we used a custom corpus for pretraining, which allowed the models to learn domain-specific words and contextual information relevant to DNA methylation. This domain-specific pretraining helps in capturing the nuances of the data, reducing the likelihood of overfitting to the training dataset.\n\nSecondly, we employed a rigorous fine-tuning process. Fine-tuning was performed separately for each of the three methylation site types (6mA, 4mC, and 5hmC), and each training subset was split into training and validation sets at a ratio of 8:2. This separation ensured that the models were evaluated on unseen data during the training process, helping to monitor and prevent overfitting.\n\nAdditionally, we used an early-stopping strategy during fine-tuning, which halted the training process if the model's performance on the validation set did not improve for a specified number of epochs. This technique helps in preventing the model from becoming too complex and overfitting the training data.\n\nTo rule out underfitting, we ensured that the models were trained for a sufficient number of epochs and with an appropriate learning rate. The learning rate was adjusted using a warmup strategy, which gradually increased the learning rate during the initial training steps. This approach helps in stabilizing the training process and ensuring that the models converge to an optimal solution.\n\nFurthermore, we evaluated the performance of the models on independent test sets, which were not used during the training process. The consistent performance across different taxonomic lineages and methylation site types indicates that the models generalize well to unseen data, ruling out underfitting.\n\nIn summary, our approach involved domain-specific pretraining, rigorous fine-tuning with validation, early stopping, and evaluation on independent test sets. These strategies collectively helped in mitigating the risks of both overfitting and underfitting, ensuring robust and generalizable models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training process of our language models. One of the key methods used was early stopping. This technique involves monitoring the model's performance on a validation set during training and stopping the training process when the performance stops improving. We set a maximum of 32 epochs for fine-tuning, but the training would halt earlier if the model's performance on the validation set did not improve for a specified number of epochs.\n\nAdditionally, we used a warmup strategy for the learning rate. This involves starting with a lower learning rate and gradually increasing it to the specified value over a certain number of steps. In our case, we achieved the final learning rate of 1e-5 after 100 steps of warmup. This approach helps in stabilizing the training process and prevents the model from converging too quickly to a suboptimal solution.\n\nAnother regularization technique we utilized was the use of a masked language modeling (MLM) task during pretraining. In this task, a certain percentage of the tokens in the input sequence are masked, and the model is trained to predict these masked tokens. This forces the model to learn more robust and generalizable representations of the input data, reducing the risk of overfitting.\n\nFurthermore, we employed a custom tokenizer trained on our specific corpus, which included DNA 6-mers and taxonomic lineage descriptions. This ensured that the tokenizer was tailored to the specific features of our data, improving the model's ability to capture relevant information and reducing the likelihood of overfitting to irrelevant patterns.\n\nLastly, the use of multiple language models in our MuLan-Methyl framework also served as a form of ensemble learning, which can help to reduce overfitting. By averaging the prediction probabilities of five different language models, we were able to achieve more robust and generalizable predictions.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The experiments were conducted using Python 3.10, with the PyTorch and Huggingface Transformers libraries. The code and models are not explicitly mentioned to be open-source, but the libraries used are open-source and widely available.\n\nThe optimization parameters for pretraining include 8 epochs, a batch size of 64 per GPU, and a learning rate of 5e-4, achieved after 100 steps of warmup. For fine-tuning, the parameters include a maximum of 32 epochs, a batch size of 64 per GPU, and a learning rate of 1e-5, also achieved after 100 steps of warmup. An early-stopping strategy was employed during fine-tuning.\n\nThe experiments were run on a Linux Virtual Machine (Ubuntu 20.04 LTS) equipped with 4 GPUs provided by de.NBI (flavor: de.NBI RTX6000 4 GPU medium). The specific details about the availability of the model files and the license under which they are distributed are not provided. However, the use of open-source libraries suggests that the models could potentially be shared under a similar open-source license, but this would need to be confirmed by the authors or the hosting platform.",
  "model/interpretability": "The model employed in this study is not a black box but rather offers interpretability through its use of transformer-based language models. These models, such as BERT, leverage multihead self-attention mechanisms to learn and capture different and distant dependencies within the input data. This capability allows the model to understand complex relationships between tokens in the DNA sequences and taxonomic information.\n\nFor instance, BERT contains 12 encoder layers, each with 12 attention heads. The multihead self-attention mechanism can be described mathematically, showing how it processes queries, keys, and values to compute attention scores. These scores indicate the importance of each token in relation to others, providing insights into how the model makes predictions.\n\nIn the context of DNA methylation prediction, the model's interpretability is enhanced by analyzing the attention weights assigned to specific tokens. For each token, the attention weights assigned to the [CLS] token are summed over the 12 heads, representing the token's contribution to the sample prediction. This approach helps in understanding the impact of DNA sequences on the taxonomic lineage of the samples.\n\nAdditionally, the attention weights assigned by DNA tokens to taxonomic hierarchy tokens are extracted to analyze their influence. The WordPiece algorithm, used by the tokenizer in models like BERT, DistilBERT, and ELECTRA, provides word-wise tokens, making it meaningful to view the attention weights of tokens as contribution scores. This detailed analysis allows for a deeper understanding of how the model integrates DNA sequence information with taxonomic data to make accurate predictions.",
  "model/output": "The model, MuLan-Methyl, is a classification model designed to predict DNA methylation sites. It specifically identifies whether a given DNA sequence contains one of three types of methylation sites: 6mA, 4mC, or 5hmC. The output of the model is a classification of the input DNA sequences into methylation sites or non-methylation sites.\n\nThe model takes DNA sequences of length 41 as input, which are centered on an experimentally verified methylation site for positive samples. For each input sequence, the model provides a prediction indicating whether the sequence contains a methylation site. The predictions are made for various taxonomic lineages, and the model's performance is evaluated using multiple metrics, including accuracy, F1-score, recall, area under the precision-recall curve (AUPR), and area under the receiver operating characteristic curve (AUC).\n\nThe model's output can be used to generate a list of putative methylation sites from longer DNA sequences or chromosomes. This is achieved by extracting all possible 41-basepair segments from the input sequence and applying the model to each segment. The predicted positive samples are then filtered based on feature importance analysis to resolve overlapping predictions, ensuring that only the most relevant methylation sites are retained.\n\nThe model's performance has been validated on an independent test set and compared with existing methods, demonstrating its effectiveness in accurately predicting DNA methylation sites across different taxonomic lineages. The implementation of MuLan-Methyl as a web server allows users to upload DNA sequences and obtain predictions for methylation sites, making it a practical tool for biological research.",
  "model/duration": "The experiments for our model were conducted on a Linux Virtual Machine equipped with 4 GPUs. The specific GPU flavor used was the de.NBI RTX6000 4 GPU medium. The training process involved several stages, including tokenization, pretraining, and fine-tuning. Pretraining was conducted using 8 epochs, with a batch size of 64 per GPU and a learning rate of 5e-4. Fine-tuning was performed separately for each of the three methylation site types, with a maximum of 32 epochs, a batch size of 64 per GPU, and a learning rate of 1e-5. The use of multiple GPUs significantly accelerated the training process, allowing for efficient handling of the large datasets involved. The exact execution time can vary depending on the specific hardware and dataset sizes, but the use of high-performance GPUs ensured that the training and fine-tuning processes were completed in a reasonable timeframe.",
  "model/availability": "The source code for MuLan-Methyl is not publicly released. However, an implementation of MuLan-Methyl is provided as a web server. This server allows users to upload DNA samples of length 41 and select the closest taxonomic lineage and the type of methylation site of interest. The uploaded samples are then classified as methylation sites or not. The server also supports the upload of longer DNA sequences, in which case it provides a list of all predicted methylation sites in the uploaded sequence. This extended functionality is implemented by extracting all samples of length 41 centered on a nucleotide of the appropriate type and performing MuLan-Methyl prediction on these samples. The predicted positive samples are then filtered by feature importance analysis to resolve overlapping predictions. The output is the list of all predicted methylation positions.",
  "evaluation/method": "The method was evaluated using multiple approaches to ensure its robustness and generalizability. Firstly, the performance of the method was assessed on an independent test set, which was not used during the training process. This test set included samples from various taxonomic lineages and methylation types, providing a comprehensive evaluation of the method's predictive capabilities.\n\nIn addition to the independent test set, the method was also evaluated on an external dataset that contained DNA methylation data from species not included in the training dataset. This external dataset included samples from Escherichia coli, Geobacter pickeringii, and Oryza sativa L., representing different methylation types and taxonomic lineages. The performance on this external dataset demonstrated the method's ability to generalize to new, unseen data.\n\nThe evaluation metrics used included accuracy, F1-score, recall, area under the precision-recall curve (AUPR), and area under the receiver operating characteristic curve (AUC). These metrics provided a thorough assessment of the method's performance across different methylation types and taxonomic lineages.\n\nFurthermore, the method was compared against two state-of-the-art methods, iDNA-ABF and iDNA-ABT, using the same benchmark dataset. The comparison showed that the method outperformed the existing methods in most cases, highlighting its superior predictive performance.\n\nThe evaluation also included an analysis of the method's performance on individual taxonomic lineages and methylation types. This detailed analysis provided insights into the method's strengths and weaknesses, and identified areas for potential improvement.\n\nOverall, the evaluation process was rigorous and comprehensive, ensuring that the method's performance was thoroughly assessed and validated.",
  "evaluation/measure": "In the evaluation of MuLan-Methyl, a comprehensive set of performance metrics were employed to assess the model's effectiveness in predicting DNA methylation sites across various taxonomic lineages. The primary metric reported is the Area Under the Curve (AUC), which provides a single scalar value summarizing the model's ability to distinguish between positive and negative classes. This metric is crucial as it offers a threshold-independent measure of performance.\n\nIn addition to AUC, several other metrics were used to provide a more nuanced evaluation. Accuracy, which measures the proportion of true results (both true positives and true negatives) among the total number of cases examined, was reported. The F1 score, the harmonic mean of precision and recall, was also included. This metric is particularly useful in cases of class imbalance, as it balances the concerns of both false positives and false negatives.\n\nRecall, also known as sensitivity or true positive rate, indicates the model's ability to identify all relevant instances within a dataset. The Area Under the Precision-Recall Curve (AUPR) was also reported, which is especially informative when dealing with imbalanced datasets, as it focuses on the performance of the positive class.\n\nThese metrics were evaluated for three types of methylation sites: 6mA, 4mC, and 5hmC, across multiple taxonomic lineages. The choice of these metrics is representative of standard practices in the literature, ensuring that the evaluation is both rigorous and comparable to other studies in the field. The use of multiple metrics provides a holistic view of the model's performance, addressing different aspects of predictive accuracy and robustness.",
  "evaluation/comparison": "In the evaluation of our method, we conducted a thorough comparison with publicly available state-of-the-art methods to assess the performance of MuLan-Methyl. Specifically, we compared our framework against iDNA-ABF and iDNA-ABT, which are both capable of predicting methylation sites for all three types across different taxonomic lineages. This comparison was performed using the iDNA-MS independent test set, a benchmark dataset that ensures a fair evaluation.\n\nThe comparison involved calculating various performance metrics, with a particular focus on the area under the curve (AUC) scores. Our results demonstrated that MuLan-Methyl outperforms the other two methods in 13 out of 17 combinations of methylation types and taxonomic lineages. For instance, in 6mA site prediction, MuLan-Methyl showed improvements ranging from 0.02% to 3.74% in AUC, with significant gains of over 1% for certain species. Similarly, for 4mC site prediction, our method achieved increases of 1.67% and 0.02% in AUC for specific taxonomic lineages. For 5hmC site prediction, MuLan-Methyl also showed notable improvements.\n\nAdditionally, we evaluated MuLan-Methyl on an external dataset that included taxonomic lineages not present in the training datasets. This dataset comprised combinations such as 4mC in E. coli, 4mC in G. pickeringii, and 6mA in O. sativa L. The results indicated that MuLan-Methyl performed exceptionally well, particularly on the 4mC + E. coli combination, achieving an AUC of 0.89, which was more than 10% better than the other models. Our method also performed best on the 4mC + G. pickeringii combination, with a 2.05% advantage over iDNA-ABT.\n\nIn summary, the comparison with publicly available methods on benchmark datasets clearly demonstrates the superior performance of MuLan-Methyl. This evaluation underscores the effectiveness of our framework in predicting DNA methylation sites across various taxonomic lineages and methylation types.",
  "evaluation/confidence": "The evaluation of MuLan-Methyl includes multiple metrics such as AUC, accuracy, F1-score, recall, and AUPR for different methylation site types and taxonomic lineages. These metrics provide a comprehensive assessment of the model's performance. However, specific details about confidence intervals for these performance metrics are not provided. This omission means that while the metrics indicate the model's effectiveness, the precision of these estimates is not quantified.\n\nStatistical significance is implied through comparisons with existing methods like iDNA-ABF and iDNA-ABT. MuLan-Methyl outperforms these methods in a majority of the tested combinations of methylation types and taxonomic lineages. For instance, it shows improvements in AUC scores ranging from 0.02% to 3.74% for 6mA site prediction, 1.67% and 0.02% for 4mC site prediction, and 0.21% and 0.03% for 5hmC site prediction. These improvements suggest that the differences in performance are likely statistically significant, although explicit p-values or confidence intervals are not reported.\n\nThe model's performance is further validated on an external dataset containing combinations of methylation types and taxonomic lineages not used in training. MuLan-Methyl demonstrates superior performance on most of these combinations, reinforcing the claim of its superiority. However, without explicit statistical tests, the confidence in these claims is somewhat reduced. Overall, while the results are promising and suggest statistical significance, the lack of detailed statistical analysis limits the ability to quantify the confidence in these findings.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The evaluation process involved using specific datasets and models that are proprietary to our research. However, we have provided comprehensive details about the evaluation metrics and results in the publication, including area under the curve (AUC) values, accuracy, F1-score, recall, and other relevant metrics. These details are intended to give a clear understanding of the performance of our method, MuLan-Methyl, across different methylation site types and taxonomic lineages.\n\nFor those interested in replicating or building upon our work, we have made the code used for training and evaluating the models available. This code is written in Python 3.10 and utilizes the PyTorch and Huggingface Transformers libraries. The experiments were conducted on a Linux Virtual Machine equipped with 4 GPUs, which can be accessed through the de.NBI (flavor: de.NBI RTX6000 4 GPU medium). By providing the code and detailed methodology, we aim to facilitate further research and development in the field of DNA methylation prediction."
}