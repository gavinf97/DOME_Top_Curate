{
  "publication/title": "Protein intrinsically disordered region prediction by combining neural architecture search and multi-objective genetic algorithm",
  "publication/authors": "The authors who contributed to the article are Yi-Jun Tang, Ke Yan, Xingyi Zhang, Ye Tian, and Bin Liu. All authors read and approved the final manuscript. Bin Liu is the corresponding author.",
  "publication/journal": "BMC Biology",
  "publication/year": "2023",
  "publication/doi": "10.1186/s12915-023-01672-5",
  "publication/tags": "- Intrinsically disordered regions (IDRs)\n- Neural architecture search (NAS)\n- Length-dependent models\n- Protein structure prediction\n- Machine learning in bioinformatics\n- Genetic algorithms\n- Protein sequence analysis\n- Disordered proteins\n- Bioinformatics tools\n- Predictive modeling",
  "dataset/provenance": "The dataset used in this study is derived from two primary sources: STrainall and SValidationall. STrainall is constructed based on data from DisProt and MobiDB, which are well-known databases for intrinsically disordered proteins. The training dataset includes 614 long disordered regions (LDR) proteins, 3024 short disordered regions (SDR) proteins, and 616 fully ordered proteins. To avoid overestimating the performance of the predictor, protein sequences in STrainall that share more than 25% similarity with any protein in the seven independent test datasets were removed.\n\nFive validation datasets were constructed with varying ratios between SDR proteins and LDR proteins by randomly selecting protein sequences from SValidationall. The statistical information for these validation datasets is provided in supplementary tables.\n\nThe independent test datasets used in this study include MXD494, SL329, DISORDER723, CASP, and Disprot504. These datasets are commonly used in the community for evaluating the performance of methods predicting intrinsically disordered proteins. Additionally, the MSDCD dataset, constructed by combining the five independent test datasets, was used to further test the generalization of various methods. The statistical information for these datasets is also available in supplementary tables.\n\nThe datasets used in this study are publicly available and can be accessed at the provided URL. This ensures that the data utilized in this research is transparent and can be verified or reused by other researchers in the field.",
  "dataset/splits": "In our study, we utilized multiple datasets to ensure comprehensive evaluation and validation of our methods. The training dataset comprised 614 long disordered region (LDR) proteins, 3024 short disordered region (SDR) proteins, and 616 fully ordered proteins. This dataset was constructed based on STrainall, derived from DisProt and MobiDB, with sequences sharing more than 25% similarity with any protein in the seven independent test datasets removed to avoid overestimation.\n\nWe also constructed five validation datasets with varying ratios between SDR and LDR proteins by randomly selecting sequences from SValidationall. The statistical information for these validation datasets is available in supplementary materials.\n\nFor independent testing, we used five commonly used datasets with different ratios between SDRs and LDRs: MXD494, SL329, DISORDER723, CASP, and Disprot504. Additionally, we constructed the MSDCD dataset by combining these five datasets to further test the generalization of various methods. We also evaluated our methods on the CAID1 dataset. The statistical information for these datasets is provided in supplementary materials.\n\nFurthermore, we created eight test datasets simulating real-world application scenarios, with their statistical information listed in supplementary materials. These datasets allowed us to assess the performance and stability of our methods across diverse conditions.",
  "dataset/redundancy": "The datasets used in this study were carefully constructed to ensure independence between training and test sets. The training dataset included 614 long disordered regions (LDR) proteins, 3024 short disordered regions (SDR) proteins, and 616 fully ordered proteins. This dataset was built based on STrainall, derived from DisProt and MobiDB. To avoid overestimating the performance of the predictor, protein sequences in STrainall that shared more than 25% similarity with any protein in the seven independent test datasets were removed.\n\nFive validation datasets with varying ratios between SDR and LDR proteins were also constructed by randomly selecting protein sequences from SValidationall. The statistical information for these validation datasets is provided in supplementary tables.\n\nThe independent test datasets included five commonly used datasets with different ratios between SDRs and LDRs: MXD494, SL329, DISORDER723, CASP, and Disprot504. Additionally, the MSDCD independent test dataset was constructed by combining these five datasets. The IDP-Fusion predictor was also evaluated on the CAID1 dataset. The statistical information for these datasets is available in supplementary tables.\n\nThe distribution of these datasets is designed to reflect real-world application scenarios, ensuring that the predictor's performance is evaluated under diverse conditions. This approach helps in assessing the generalization capability of the IDP-Fusion predictor across different datasets with varying ratios of SDRs and LDRs.",
  "dataset/availability": "The data utilized in this study is publicly available. The IDP-Fusion webserver, which includes all the data used in this study, can be accessed at http://bliulab.net/IDP-Fusion/benchmark/. Additionally, the code and datasets used in this study are available in the Zenodo repository with the accession number 10.5281/zenodo.8190096. All data generated or analyzed during this study are included in this published article, its supplementary information files, and the publicly available repositories. The datasets include training, validation, and test datasets, with detailed statistical information provided in the supplementary tables. The training dataset included 614 LDR proteins, 3024 SDR proteins, and 616 fully ordered proteins. The independent test datasets used for evaluation include MXD494, SL329, DISORDER723, CASP, and Disprot504. The MSDCD dataset, constructed by combining these five datasets, was also used for further evaluation. The datasets were constructed to avoid overestimating the performance of the predictor by removing protein sequences with more than 25% similarity to any protein in the test datasets. The data is available for public use, ensuring transparency and reproducibility of the results.",
  "optimization/algorithm": "The optimization algorithm employed in our study is a multi-objective genetic algorithm (MOGA). This approach falls under the class of evolutionary algorithms, which are a subset of machine learning techniques inspired by natural evolution. The MOGA is used to optimize the weights of six complementary base methods, ensuring that the final prediction is robust and stable across different datasets with varying ratios of long disordered regions (LDRs) and short disordered regions (SDRs).\n\nThe MOGA is not a novel algorithm in the field of machine learning. It is a well-established technique that has been widely used for optimization problems involving multiple objectives. The reason it was not published in a machine-learning journal is that our primary focus is on the application of this algorithm to the specific problem of predicting intrinsically disordered regions (IDRs) in proteins. The innovation lies in how we integrate and apply the MOGA within the context of our IDP-Fusion predictor, rather than in the development of a new optimization algorithm.\n\nThe MOGA is employed to maximize the sum of the area under the curve (AUC) scores of the six base predictors across multiple validation datasets. This approach ensures that the weights assigned to each base method are optimized for stability and performance, regardless of the specific ratio of LDRs to SDRs in the dataset. By doing so, we achieve a more reliable and accurate prediction of IDRs, which is crucial for understanding protein functions and designing effective treatments for related diseases.",
  "optimization/meta": "The IDP-Fusion predictor is indeed a meta-predictor, leveraging the outputs of multiple base models to enhance its predictive performance. It integrates six distinct base models, each contributing unique features and strengths. These models include CAN, HAN, IDP-Seq2Seq, CNN-LSTM, LSTM-CNN, and DARTS. Each of these models was trained on different datasets, with some using specific types of proteins like SDR or LDR proteins, while others utilized a mixture of protein types.\n\nThe training datasets for these base models were carefully constructed to ensure independence from the test datasets. For instance, the training dataset included LDR proteins, SDR proteins, and fully ordered proteins, but sequences with more than 25% similarity to any protein in the test datasets were removed. This step was crucial to prevent overestimation of the predictor's performance and to ensure that the training data was independent of the test data.\n\nThe fusion of these base models is achieved through a multi-objective genetic ensemble algorithm. This algorithm optimizes the weights of the six base methods, considering the influence of different ratios between SDR and LDR proteins. By doing so, it ensures that the final predictor is stable and performs well across various test datasets with differing ratios of SDR and LDR proteins.\n\nThe use of a neural architecture search (NAS) model, specifically DARTS, is particularly noteworthy. DARTS automatically optimizes the neural network architectures, allowing it to capture hidden information that other models might miss. This capability further enhances the overall performance and stability of the IDP-Fusion predictor.\n\nIn summary, IDP-Fusion is a sophisticated meta-predictor that combines the strengths of multiple machine-learning models, ensuring independent training data and optimizing performance through advanced algorithms.",
  "optimization/encoding": "In our study, we employed a comprehensive approach to encode and preprocess the data for our machine-learning algorithm. We combined three types of features to represent the residues in proteins: residue-profile features, evolutionary features, and structural features.\n\nResidue-profile features included seven commonly used amino acid physicochemical properties. These properties provide a basic characterization of the amino acids in the protein sequences.\n\nFor evolutionary features, we utilized position-specific frequency matrices (PSFM) and position-specific scoring matrices (PSSM). These matrices were obtained using PSI-BLAST by searching against the nrdb90 database with an E-value of 0.001. Additionally, we included hidden Markov model (HMM) profiles generated by searching against the uniprot20_2016_02 database using HHblits software. These evolutionary-level features are 20-dimensional.\n\nStructural-level features encompassed several dimensions. We predicted secondary structure (SS) using the SPIDER2 software tool, resulting in an 8-dimensional feature. Contact numbers (CN) were predicted in a 2-dimensional format. Hidden-to-state encoding (HSE) was also predicted using SPIDER2, yielding a 4-dimensional feature. Residue-residue contacts (CCMs) were predicted using the CCMpred software tool, resulting in a 21-dimensional feature. Solvent accessibility (SA) was predicted using the Sable Version 2 software tool, providing a 1-dimensional feature.\n\nTo optimize the input features for different base methods, we conducted ablation experiments. The detailed features used in the six base methods are listed in a supplementary table. Notably, the Differentiable Architecture Search (DARTS) model required a three-dimensional feature matrix due to its two-dimensional convolution operation. Therefore, protein sequences were represented in a format that accommodated this requirement.\n\nThis multi-faceted encoding approach ensured that our machine-learning algorithm could capture a wide range of information, leading to improved prediction performance.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varies depending on the specific base method and the features employed. We utilized a combination of residue-profile features, evolutionary features, and structural features to represent the input data. These features include amino acid physicochemical properties, position-specific frequency matrix (PSFM), position-specific scoring matrix (PSSM), hidden Markov model (HMM) profiles, secondary structure (SS), contact numbers (CN), half-sphere exposure (HSE), residue-residue contacts (CCMs), and solvent accessibility (SA).\n\nThe selection of these features was optimized through ablation experiments, which helped in determining the most effective combination for each base method. For instance, the Differentiable Architecture Search (DARTS) model, which employs a two-dimensional convolution operation, requires a three-dimensional feature matrix as input. This ensures that the model can capture complex patterns and hidden information that other models might miss.\n\nThe multi-objective genetic ensemble algorithm was used to optimize the weights of the six base methods, ensuring that the final predictor, IDP-Fusion, is robust and performs well across different datasets with varying ratios of long disordered regions (LDRs) and short disordered regions (SDRs). This approach allows the model to achieve stable and superior performance on independent test datasets.",
  "optimization/features": "In the optimization process of our model, we utilized a combination of three types of features as input: residue-profile features, evolutionary features, and structural features.\n\nThe residue-profile features encompass seven commonly used amino acid physicochemical properties.\n\nFor evolutionary-level features, we incorporated the position-specific frequency matrix (PSFM) and position-specific scoring matrix (PSSM), which were obtained using PSI-BLAST by searching against the nrdb90 database. Additionally, we included the hidden Markov model (HMM) profile generated by searching against the uniprot20_2016_02 database using HHblits software. These evolutionary-level features are 20-dimensional.\n\nStructural-level features included 8-dimensional secondary structure (SS), 2-dimensional CN, and 4-dimensional HSE predicted using the SPIDER2 software tool. We also included 21-dimensional predicted residue-residue contacts (CCMs) predicted using the CCMpred software tool, and 1-dimensional solvent accessibility (SA) predicted using the Sable Version 2 software tool.\n\nThe total number of features (f) used as input is the sum of the dimensions of these features. Specifically, we have 7 (residue-profile) + 20 (PSFM) + 20 (PSSM) + 20 (HMM) + 8 (SS) + 2 (CN) + 4 (HSE) + 21 (CCMs) + 1 (SA) = 103 features.\n\nFeature selection was performed through ablation experiments to optimize the input features of different base methods. This process ensured that the most relevant features were included in the final model. The feature selection was conducted using the training set only, ensuring that the evaluation on independent test datasets remained unbiased.",
  "optimization/fitting": "In our study, we employed a multi-objective genetic ensemble algorithm to fuse the prediction probabilities generated by six complementary base methods. This approach was designed to optimize the weights of the base methods, ensuring that the final predictor, IDP-Fusion, is insensitive to different ratios between long disordered regions (LDRs) and short disordered regions (SDRs).\n\nTo address the potential issue of overfitting, we utilized a comprehensive validation strategy. Five validation datasets with varying ratios between SDRs and LDRs were constructed. The weights of the base methods were optimized by maximizing the sum of the AUC scores across these validation datasets, rather than focusing on a single dataset. This multi-dataset approach helps to generalize the model's performance and reduces the risk of overfitting to any specific dataset.\n\nAdditionally, the use of a neural architecture search (NAS) model called DARTS allowed for the automatic optimization of neural network architectures. This process ensures that the model can capture hidden information that other base models might miss, further enhancing its generalization capabilities.\n\nTo mitigate underfitting, we incorporated a diverse set of features into our models. These features included residue-profile features, evolutionary features, and structural features. The residue-profile features encompassed seven commonly used amino acid physicochemical properties. Evolutionary-level features included position-specific frequency matrix (PSFM), position-specific scoring matrix (PSSM), and hidden Markov model (HMM) profiles. Structural-level features included secondary structure, contact number, hidden-to-exposed state, residue-residue contacts, and solvent accessibility.\n\nBy combining these diverse features and using a multi-objective genetic ensemble algorithm, we ensured that our model could effectively learn from the data without underfitting. The resulting IDP-Fusion predictor demonstrated stable and superior performance across various independent test datasets, indicating that it successfully balances the trade-off between overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our IDP-Fusion predictor. One key strategy involved the use of a diverse and comprehensive training dataset. This dataset included 614 long disordered region (LDR) proteins, 3024 short disordered region (SDR) proteins, and 616 fully ordered proteins. To avoid overestimating performance, we removed protein sequences from the training set that shared more than 25% similarity with any protein in the seven independent test datasets.\n\nAdditionally, we utilized a multi-objective genetic ensemble algorithm to optimize the weights of the six base methods. This approach considered the influence of different ratios between SDRs and LDRs on the final performance, thereby enhancing the stability and generalization of our predictor. The algorithm maximized the sum of the AUC scores of the six base predictors across five validation datasets with varying ratios between SDRs and LDRs, rather than focusing on a single dataset.\n\nFurthermore, we incorporated fully ordered proteins into the training dataset to accurately predict both intrinsically disordered proteins (IDPs) and fully ordered proteins. This inclusion helped in capturing a broader range of features, reducing the risk of overfitting to specific types of protein sequences.\n\nThe use of neural architecture search (NAS) with the DARTS model also played a crucial role in preventing overfitting. DARTS automatically optimized the neural network architectures, allowing it to capture hidden information that other models might miss. This continuous and automatic iteration during the learning process ensured that the model selected was optimal and less prone to overfitting.\n\nOverall, these strategies collectively contributed to the development of a robust and stable predictor that performs well across different datasets with varying ratios of LDR and SDR proteins.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the optimization parameters and the overall approach used in the study are described.\n\nThe IDP-Fusion predictor employs a multi-objective genetic ensemble algorithm to optimize the weights of six base methods. These base methods include various linguistic models derived from natural language processing (NLP) and a neural architecture search (NAS) model called DARTS. The optimization process involves constructing validation datasets with different ratios between short disordered regions (SDRs) and long disordered regions (LDRs) to ensure stability and performance across varied datasets.\n\nThe specific details of the hyper-parameter configurations and optimization schedule are not provided in the main text but are likely included in the supplementary materials. The supplementary information, which includes additional tables and datasets, can be accessed at the provided URL. This supplementary material is crucial for replicating the experiments and understanding the exact configurations used.\n\nFor those interested in the implementation details, the IDP-Fusion webserver and all data utilized in this study are available at http://bliulab.net/IDP-Fusion/benchmark/. This resource should provide the necessary information for researchers to replicate the study and explore the optimization parameters in depth.",
  "model/interpretability": "The IDP-Fusion predictor leverages a combination of neural architecture search (NAS) and multi-objective genetic algorithms to enhance the prediction of intrinsically disordered regions (IDRs) in proteins. While the model incorporates sophisticated techniques for feature extraction and optimization, it is not entirely transparent. The use of NAS, specifically the DARTS model, allows for the automatic construction of optimal neural network architectures. This process involves learning weights for various operations within the network, which can capture hidden features in protein sequences that other models might miss. However, the exact nature of these hidden features and the specific operations selected by DARTS are not explicitly interpretable. The model's decision-making process is thus somewhat of a black box, particularly in how it integrates and weights the outputs of its six base predictors.\n\nThe multi-objective genetic algorithm (MOGA) used for fusing the predictions of the base models adds another layer of complexity. MOGA optimizes the weights of these base models to achieve stable performance across different datasets with varying ratios of short disordered regions (SDRs) and long disordered regions (LDRs). While the genetic algorithm provides a systematic way to optimize these weights, the specific reasons behind the chosen weights for each base model are not straightforward to interpret. This further contributes to the model's black-box nature.\n\nIn summary, while IDP-Fusion benefits from advanced techniques that improve its predictive performance, the model's internal workings, particularly those involving NAS and MOGA, are not easily interpretable. The model's transparency is limited by the complexity of these optimization processes, making it challenging to provide clear examples of how specific predictions are made.",
  "model/output": "The model IDP-Fusion is designed for classification, specifically to predict intrinsically disordered regions (IDRs) in proteins. These regions are classified into short disordered regions (SDRs) and long disordered regions (LDRs), as well as fully ordered proteins. The model combines multiple base predictors, each trained on different datasets, to capture a wide range of features. These base predictors include CAN, HAN, IDP-Seq2Seq, CNN-LSTM, LSTM-CNN, and DARTS. The fusion of these models is achieved using a multi-objective genetic ensemble algorithm, which optimizes the weights of the base predictors to ensure stable performance across different datasets with varying ratios of SDRs and LDRs.\n\nThe output of IDP-Fusion is a prediction probability for each residue in a protein sequence, indicating whether it belongs to an SDR, LDR, or is fully ordered. This probability is derived from the weighted sum of the prediction probabilities generated by the six base methods. The model's performance is evaluated using metrics such as AUC (Area Under the Curve) and MCC (Matthews Correlation Coefficient), demonstrating its superiority over other competing methods on various independent test datasets. The stability and generalization of IDP-Fusion are further verified through extensive testing on datasets with different ratios of SDRs and LDRs, ensuring its robustness in real-world applications.",
  "model/duration": "The execution time of the IDP-Fusion model is significantly faster compared to other models like SPOT-Disorder2. Specifically, IDP-Fusion is approximately 10 times faster than SPOT-Disorder2. This efficiency is attributed to the design of IDP-Fusion, which avoids the computational overhead associated with the complex features used in SPOT-Disorder2. The streamlined architecture and optimized feature extraction methods in IDP-Fusion contribute to its reduced running time, making it more suitable for real-world applications where computational efficiency is crucial.",
  "model/availability": "The source code and datasets used in this study are publicly available in online repositories. Specifically, the data reported in this paper can be accessed via the Zenodo repository under the accession number 10.5281/zenodo.8190096. All generated or analyzed data during this study are included in the published article, its supplementary information files, and the publicly available repositories.\n\nAdditionally, the IDP-Fusion webserver is accessible at http://bliulab.net/IDP-Fusion/benchmark/, providing a user-friendly interface for running the algorithm. This webserver includes all the data utilized in the study, ensuring reproducibility and accessibility for further research.",
  "evaluation/method": "The evaluation of IDP-Fusion was conducted using several independent test datasets to ensure its robustness and generalization. These datasets included MXD494, SL329, DISORDER723, CASP, and Disprot504, each with varying ratios of long disordered regions (LDRs) to short disordered regions (SDRs). Additionally, a combined dataset called MSDCD was created by merging these five datasets to further test the method's performance.\n\nThe evaluation metrics used included sensitivity (Sn), specificity (Sp), balanced accuracy (BACC), Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic curve (AUC). These metrics provided a comprehensive assessment of the method's performance in predicting intrinsically disordered regions (IDRs).\n\nTo ensure the stability and insensitivity of IDP-Fusion to different dataset ratios, a multi-objective genetic algorithm (MOGA) was employed. This algorithm optimized the weights of the six base predictors, which included models like CAN, HAN, IDP-Seq2Seq, CNN-LSTM, LSTM-CNN, and a neural architecture search (NAS) model called DARTS. The optimization was performed on five validation datasets with different SDR-to-LDR ratios, aiming to maximize the sum of AUC scores across these datasets.\n\nThe performance of IDP-Fusion was compared with other competing methods, and it was found to outperform them on all the independent test datasets. This superior performance was attributed to the complementary features of the base predictors and the ability of DARTS to capture hidden information in the protein sequences. The results demonstrated that IDP-Fusion achieves stable and high performance across different datasets, making it a reliable tool for predicting IDRs.",
  "evaluation/measure": "In our evaluation of IDP-Fusion, we employed several key performance metrics to comprehensively assess the predictor's effectiveness. These metrics include Sensitivity (Sn), Specificity (Sp), Balanced Accuracy (BACC), Matthews Correlation Coefficient (MCC), and the Area Under the Receiver Operating Characteristic Curve (AUC).\n\nSensitivity measures the proportion of true positive predictions among all actual positives, indicating how well the model identifies intrinsically disordered regions (IDRs). Specificity, on the other hand, assesses the proportion of true negative predictions among all actual negatives, reflecting the model's ability to correctly identify ordered regions.\n\nBalanced Accuracy is the average of Sensitivity and Specificity, providing a single metric that balances the performance on both positive and negative classes. This is particularly useful when dealing with imbalanced datasets, ensuring that the model's performance is not skewed by the majority class.\n\nThe Matthews Correlation Coefficient offers a more nuanced view by considering all four quadrants of the confusion matrix, providing a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation.\n\nFinally, the Area Under the ROC Curve (AUC) provides a single scalar value that summarizes the model's performance across all classification thresholds. An AUC of 1 indicates perfect classification, while an AUC of 0.5 indicates performance no better than random guessing.\n\nThese metrics are widely used in the literature for evaluating IDR predictors, ensuring that our results are comparable with other studies in the field. By reporting these metrics, we aim to provide a clear and comprehensive understanding of IDP-Fusion's performance, highlighting its strengths and areas for potential improvement.",
  "evaluation/comparison": "In the evaluation of our IDP-Fusion predictor, a comprehensive comparison was conducted with various publicly available methods on several benchmark datasets. These datasets included MXD494, SL329, DISORDER723, CASP, and Disprot504, each with different ratios between short disordered regions (SDRs) and long disordered regions (LDRs). The performance metrics used for this comparison included sensitivity (Sn), specificity (Sp), balanced accuracy (BACC), Matthews correlation coefficient (MCC), and the area under the ROC curve (AUC).\n\nThe results, as shown in the supplementary tables, demonstrated that IDP-Fusion outperformed all other competing methods across these datasets. This superior performance can be attributed to several factors. Firstly, IDP-Fusion integrates six base predictors, each capturing different features of the protein sequences. This diversity ensures that the model can leverage complementary information from various sources. Secondly, the use of a neural architecture search (NAS) model, specifically DARTS, allows IDP-Fusion to automatically optimize its neural network architectures, thereby capturing hidden information that might be missed by other models.\n\nAdditionally, the multi-objective genetic ensemble algorithm employed in IDP-Fusion considers the influence of different ratios between SDRs and LDRs during the training process. This consideration enhances the model's stability and robustness across different datasets. Furthermore, the inclusion of fully ordered proteins in the training dataset enables IDP-Fusion to accurately predict both intrinsically disordered proteins (IDPs) and fully ordered proteins.\n\nTo further validate the generalization of IDP-Fusion, it was also evaluated on the MSDCD dataset, which is a combination of the five independent test datasets. The results confirmed that IDP-Fusion achieved the best performance in terms of both AUC and MCC, reinforcing its superiority over other methods.\n\nIn summary, the comparison with publicly available methods on benchmark datasets clearly shows that IDP-Fusion provides a more stable and accurate prediction of IDRs, making it a reliable tool for real-world applications.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in this study are publicly available. The IDP-Fusion webserver, along with all the data utilized in this study, can be accessed at a specific online repository. The name of the repository and the accession number for the data reported in this paper is zenodo, with the identifier 10.5281/zenodo.8190096. This repository includes all data generated or analyzed during this study, ensuring transparency and reproducibility. Additionally, all relevant data are included in the published article and its supplementary information files. The datasets and code used in this study are also available in online repositories, further facilitating access and verification by other researchers."
}