{
  "publication/title": "Confound-leakage: Confound Removal in Machine Learning Leads to Leakage",
  "publication/authors": "The authors who contributed to this article are:\n\n- Sami Hamdan, who was involved in the study concept and design, data analysis and interpretation, drafting of the manuscript, and critical revision of the manuscript for important intellectual content and final approval.\n- Bradley C. Love, who contributed to the study concept and design, supervision, and critical revision of the manuscript for important intellectual content and final approval.\n- Georg G. von Polier, who was involved in the study concept and design, data collection and processing for the ADHD data, data analysis and interpretation, and critical revision of the manuscript for important intellectual content and final approval.\n- Susanne Weis, who contributed to the study concept and design, data analysis and interpretation, and critical revision of the manuscript for important intellectual content and final approval.\n- Holger Schwender, who was involved in the study concept and design, data analysis and interpretation, and critical revision of the manuscript for important intellectual content and final approval.\n- Simon B. Eickhoff, who contributed to the study concept and design, supervision, and critical revision of the manuscript for important intellectual content and final approval.\n- Kaustubh R. Patil, who was involved in the study concept and design, supervision, data analysis and interpretation, and critical revision of the manuscript for important intellectual content and final approval.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "10.5524/102420",
  "publication/tags": "- Machine Learning\n- Confound Removal\n- Data Leakage\n- UCI Benchmark Datasets\n- Clinical Data Analysis\n- Target as a Confound\n- Simulation Studies\n- Ethical Approval\n- Confound Regression\n- Cross-Validation\n- Logistic Regression\n- Decision Trees\n- Random Forests\n- Support Vector Machines\n- Multilayer Perceptrons\n- Confound-leakage\n- ADHD Data\n- Permutation Testing\n- Feature Shuffling\n- Prediction Performance",
  "dataset/provenance": "We utilized a diverse range of datasets to ensure the generalizability of our findings. The majority of these datasets are publicly accessible through the UCI machine learning repository. This repository is a well-known resource in the machine learning community, providing a wide array of datasets that have been extensively used in various studies. The datasets cover a broad spectrum of features and sample sizes, which allows for robust and comprehensive analysis.\n\nIn total, we included ten datasets from the UCI repository, encompassing both classification and regression tasks. These datasets vary significantly in terms of the number of features and the size of the samples, providing a rich dataset for our experiments. For instance, some datasets have a large number of features, while others have a more modest feature set. Similarly, the sample sizes range from a few hundred to tens of thousands, ensuring that our models are tested under different conditions.\n\nAdditionally, we analyzed one real-world clinical dataset, specifically a balanced subsample of the ADHD speech dataset. This dataset includes 126 individuals and features 6,016 speech-related features. The target variable in this dataset is binary, indicating ADHD status (ADHD or control). The dataset also includes four confounds: gender, education level, age, and depression score measured using the Beck’s Depression Inventory (BDI). This clinical dataset is not publicly accessible and is available from PeakProfiling GmbH under certain restrictions.\n\nThe use of both publicly available and restricted datasets allows us to draw conclusions that are applicable to a wide range of scenarios, from general machine learning tasks to specific clinical applications. The UCI datasets have been widely used in the community, ensuring that our results are comparable and reproducible. The clinical dataset, while more specialized, provides valuable insights into real-world applications of machine learning in healthcare.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in our study are primarily sourced from the UCI machine learning repository, which provides open access to most of them. Specifically, ten benchmark datasets from UCI are freely available, ensuring reproducibility of our key findings. These datasets cover a wide range of features and sample sizes, suitable for both classification and regression tasks.\n\nAdditionally, we utilized simulated data, which is also publicly accessible. This simulated data, along with the UCI datasets, forms the minimal datasets required to reproduce our results.\n\nHowever, one dataset, the ADHD speech dataset, is not publicly accessible due to its sensitive nature. This dataset is available from PeakProfiling GmbH under certain restrictions. Access to this dataset requires contacting Jörg Langner, the cofounder and CTO of PeakProfiling GmbH, and adhering to the specified restrictions.\n\nTo facilitate reproducibility and transparency, an archival copy of the code and supporting data is available via the GigaScience database, GigaDB. This ensures that researchers can access the necessary resources to replicate our study.",
  "optimization/algorithm": "The machine-learning algorithms employed in our study span a variety of classes, both linear and nonlinear, to comprehensively evaluate the effects of confound removal. Specifically, we utilized linear logistic regression (LR), linear kernel support vector machines (SVM), radial basis function (RBF) kernel SVM, decision trees (DT), random forests (RF), and multilayer perceptrons (MLP) with a single hidden layer using the ReLU activation function. These algorithms were chosen to represent a broad spectrum of modeling techniques, from simple linear models to complex nonlinear ones.\n\nThe algorithms used are well-established and widely recognized in the machine-learning community. They are not new but are fundamental tools in the field. The choice to use these established algorithms was deliberate, as our focus was on understanding the impact of confound removal rather than introducing new machine-learning techniques. The study's primary contribution lies in exposing the confound-leakage pitfall and providing guidelines for dealing with it, rather than developing novel algorithms.\n\nGiven the nature of our research, which centers on the interaction between confound removal and machine-learning performance, it was appropriate to publish in a journal that emphasizes the application of machine-learning techniques in specific domains, such as biomedical data analysis. This allowed us to reach an audience that would benefit most from our findings, particularly those involved in clinical and epidemiological research.",
  "optimization/meta": "The model discussed in the publication does not explicitly use a meta-predictor approach. However, it does involve the use of multiple machine learning algorithms in its analysis. Specifically, the study employs various algorithms such as linear regression (LR), decision trees (DT), random forests (RF), multi-layer perceptrons (MLP), linear support vector machines (Lin SVM), and radial basis function support vector machines (RBF SVM) to evaluate the impact of confound removal on prediction performance.\n\nThese algorithms are used to demonstrate how confound removal can affect the performance of different types of models. The study shows that nonlinear models, in particular, can exhibit increased performance when confound removal is applied, which can be attributed to confound-leakage. This indicates that the information from the confound might be leaking into the features, thereby inflating the model's performance.\n\nThe datasets used in the study include both simulated data and real-world clinical data. The simulated data is designed to control for specific variables, ensuring that the training data is independent. For the real-world clinical dataset, the study acknowledges the sensitivity of the data and provides information on how to access it, ensuring that the data handling processes are transparent and ethical.\n\nIn summary, while the model does not explicitly use a meta-predictor, it leverages multiple machine learning methods to analyze the effects of confound removal. The training data for the simulated datasets is designed to be independent, and the study provides clear guidelines on accessing and handling the real-world clinical data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms performed optimally. For continuous features and confounds, we applied normalization to achieve a mean of zero and unit variance. This process was done in a cross-validation (CV)-consistent manner to prevent data leakage. Categorical features were one-hot encoded following standard practices. This encoding method converts categorical variables into a binary matrix representation, which is essential for algorithms that require numerical input.\n\nAdditionally, we balanced the class labels for all classification problems to exclude biases due to class imbalance. This step is vital for ensuring that the models learn from a representative sample of each class, leading to more robust and generalizable results.\n\nFor the ADHD speech dataset, which included speech-related features, a binary target describing ADHD status, and four confounds (gender, education level, age, and depression score measured using the Beck’s Depression Inventory), we ensured that the data was preprocessed similarly. This consistency in preprocessing across different datasets helped maintain the integrity of our comparisons and analyses.\n\nIn summary, our data encoding and preprocessing steps involved normalization for continuous features, one-hot encoding for categorical features, and balancing class labels. These steps were performed in a CV-consistent manner to ensure the reliability and validity of our machine-learning models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the dataset and the specific machine learning algorithm employed. We utilized a range of machine learning algorithms, including Logistic Regression (LR), Decision Trees (DT), Random Forests (RF), Multilayer Perceptron (MLP), Linear Support Vector Machine (Lin SVM), and Radial Basis Function Support Vector Machine (RBF SVM). Each of these algorithms has its own set of hyperparameters that were tuned during the optimization process.\n\nThe selection of the number of features (p) was guided by the nature of the datasets used. We analyzed multiple datasets, including 10 UCI benchmark datasets and one real-world clinical dataset. The UCI datasets are freely accessible and cover a wide range of features and sample sizes. For the clinical dataset, which is sensitive and available under certain restrictions, we ensured that the features were appropriately handled to maintain data integrity and confidentiality.\n\nIn our experiments, we performed feature shuffling to assess the impact of confound removal. This process involved shuffling the features while keeping the confounds and target intact, which helped us understand the feature-target and feature-confound relationships. We also conducted permutation testing with 1,000 iterations to further validate our findings. These methods allowed us to detect confound leakage and ensure that our models were robust and reliable.\n\nAdditionally, we used Bayesian Region of Practical Equivalence (ROPE) to compare the performance of machine learning pipelines with and without confound removal. This approach provided probabilistic statements regarding the differences in model performance, helping us to determine whether the results were meaningfully higher, lower, or not meaningfully different.\n\nOverall, the selection of parameters was carefully considered to ensure that our models were optimized for performance and that any potential confound leakage was identified and addressed.",
  "optimization/features": "In our study, we utilized a diverse range of datasets, each with varying numbers of features. The datasets cover a wide spectrum of problem types, including classification and regression tasks. For instance, the \"Income (Adult)\" dataset contains 14 features, while the \"Residential Building\" dataset has 107 features. The \"Speech ADHD\" dataset is an outlier with 6016 features, but it is not publicly accessible.\n\nFeature selection was not explicitly performed as a separate step in our analysis. Instead, we focused on evaluating the impact of confounds and the effectiveness of confound regression (CR) techniques on the original feature sets. Our experiments involved using both raw features and features processed through CR to assess performance differences. We also conducted permutation testing with 1000 iterations to shuffle the features and observe the changes in model performance. This approach helped us understand the potential leakage of target-related confounding information into the features.\n\nAll datasets, except for the \"Speech ADHD\" dataset, are freely accessible through the UCI machine learning repository. This accessibility ensures reproducibility and allows other researchers to validate our findings. The \"Speech ADHD\" dataset, due to its sensitive nature, is available from PeakProfiling GmbH under certain restrictions.",
  "optimization/fitting": "In our study, we primarily focused on decision tree-based models, specifically decision trees (DT) and random forests (RF), due to their popularity and susceptibility to the issues we were investigating. We used DT when there was only one feature and RF when dealing with multiple features. This approach helped us decrease the complexity of our results and focus on the key issues at hand.\n\nRegarding the number of parameters, in the case of RF, the number of parameters can indeed be large, especially with a high number of trees and features. However, RF is generally robust to overfitting due to its ensemble nature and the randomness introduced in the tree-growing process. To further ensure that overfitting was not a concern, we employed a 10 times repeated 5-fold nested cross-validation strategy. This rigorous validation approach helped us to assess the model's performance on unseen data and provided a reliable estimate of its generalization capability.\n\nUnderfitting was not a significant concern in our study. The models we used, particularly RF, are known for their ability to capture complex patterns in the data. Moreover, the simulations and real-world datasets we used were designed to have a sufficient number of features and samples to allow the models to learn the underlying relationships. The performance improvements we observed after confound removal further indicated that the models were not underfitting the data.\n\nIn summary, we mitigated the risks of overfitting and underfitting through careful model selection, rigorous cross-validation, and thoughtful experimental design.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key methods used was nested cross-validation. This approach involves an outer loop for model selection and an inner loop for hyperparameter tuning, which helps in providing an unbiased estimate of model performance and reduces the risk of overfitting.\n\nAdditionally, we utilized feature shuffling as a diagnostic tool. By shuffling the features while keeping the confounds and target intact, we could assess whether any performance improvements after confound removal were due to genuine signal or leakage of confounding information. This method helped us identify cases where confound removal might inadvertently introduce bias, thereby aiding in the creation of more reliable models.\n\nWe also compared the performance of machine learning pipelines with and without confound regression (CR) using out-of-sample metrics such as AUC-ROC for classification and predictive R² for regression problems. This comparison was conducted within the framework of 10 times repeated 5-fold nested cross-validation, ensuring that our results were robust and generalizable.\n\nFurthermore, we adopted the Bayesian Region of Practical Equivalence (ROPE) approach to qualitatively determine differences between the models. This Bayesian framework allowed us to make probabilistic statements about whether one model performed meaningfully better than another, providing a more nuanced understanding of model performance beyond traditional significance testing.\n\nIn summary, our optimization process included nested cross-validation, feature shuffling, and Bayesian ROPE analysis to prevent overfitting and ensure the reliability of our machine learning models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are available. The supporting code and data for our study can be accessed via the GigaScience database, GigaDB. Additionally, the code is available on GitHub under the GNU Affero General Public License v3.0. This license permits the use, modification, and distribution of the code, provided that the original work is properly cited. The project homepage on GitHub provides further details on the operating system, programming language, and other requirements needed to run the code. The specific versions of the software dependencies are also listed, ensuring reproducibility of the results.",
  "model/interpretability": "The model discussed in our publication primarily focuses on the pitfalls of confound-leakage in machine learning, particularly when using nonlinear models. The core of our work revolves around the unintended consequences of linear confound regression (CR) in combination with nonlinear machine learning approaches.\n\nIn terms of interpretability, the models we examine can be considered somewhat of a black box, especially when nonlinear methods are involved. Nonlinear models, such as random forests and neural networks, are known for their complexity and ability to capture intricate relationships within data. This complexity, while powerful, often comes at the cost of transparency. These models do not provide straightforward, interpretable rules like linear models might.\n\nHowever, our work does offer some insights into the interpretability of feature importance. For instance, in our analysis of a real-world ADHD speech dataset, we identified the most important features used by a random forest model both before and after confound removal. We found that the feature rankings changed significantly, indicating that confound removal can alter the perceived importance of features. This highlights the need for careful interpretation of feature importance in the context of confound removal.\n\nAdditionally, we used decision boundaries and density plots to visualize how confound-leakage can affect model predictions. These visualizations help to illustrate how skewed feature distributions and limited precision can lead to information leakage, making the model's behavior more interpretable in specific cases.\n\nOverall, while the models themselves may not be fully transparent, our research provides tools and guidelines for understanding and mitigating the effects of confound-leakage, thereby enhancing the interpretability of machine learning models in clinical and other applications.",
  "model/output": "The output of our models can be either classification or regression, depending on the specific dataset and problem at hand. We have utilized various datasets that cover a wide range of problem types. For instance, datasets like Income (Adult), Bank Marketing, Heart, Blood Transfusion, and Breast Cancer are used for classification tasks. These datasets involve predicting categorical outcomes, such as whether an individual has a certain condition or not.\n\nOn the other hand, datasets like Student Performance, Abalone, Concrete Compressive Strength, Residential Building, and Real Estate are used for regression tasks. These datasets involve predicting continuous outcomes, such as the performance score of a student or the compressive strength of concrete.\n\nIt is important to note that all datasets, except for the speech ADHD dataset, are freely accessible through the UCI machine learning repository. The speech ADHD dataset, however, is not publicly accessible.\n\nOur models have been designed to handle both types of outputs effectively, ensuring that they can provide accurate predictions regardless of whether the task is classification or regression. This versatility allows us to address a broad spectrum of real-world problems, from medical diagnostics to property valuation.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the project is publicly available. The project, named \"Confound-leakage,\" can be accessed on GitHub at the URL https://github.com/juaml/ConfoundLeakage. The code is released under the GNU Affero General Public License v3.0, which permits free use, modification, and distribution of the software, provided that the original work is properly cited.\n\nThe project is designed to run on GNU/Linux operating systems and requires Python 3.10.8. Additionally, several other requirements need to be met, including specific versions of libraries such as scikit-learn, baycomp, matplotlib, seaborn, dtreeviz, numpy, and pandas. These dependencies are essential for the proper functioning of the algorithms and analyses presented in the study.\n\nThe availability of the source code and the detailed requirements ensure that other researchers can reproduce the findings and build upon the methodology described in the publication. This transparency is crucial for validating the results and fostering further advancements in the field.",
  "evaluation/method": "To evaluate the performance of machine learning pipelines with and without confound regression, we employed a rigorous methodology. We computed the out-of-sample AUCROC for classification problems and predictive R² for regression problems using a 10 times repeated 5-fold nested cross-validation approach. This method ensures a robust assessment of model performance by providing multiple train-test splits and reducing the risk of overfitting.\n\nTo determine whether the results for a given dataset and algorithm with and without confound regression were meaningfully different, we utilized the Bayesian Region of Practical Equivalence (ROPE) approach. This Bayesian framework allows us to compute probabilities of the metric falling into a defined region of practical equivalence or of one machine learning pipeline scoring higher than the other. By defining a region of equivalence (set at 0.05), we can make probabilistic statements regarding whether and, if so, which of the machine learning pipelines score higher. The differences are summarized using symbols: \"=\" for practically equivalent pipelines, \"<\" for the right pipeline scoring higher, and \">\" for the left pipeline scoring higher.\n\nAdditionally, we adopted a computationally feasible methodology involving feature shuffling. By shuffling the features while keeping the confounds and target intact, we destroy the feature–target and feature–confound relationships while preserving the confound–target relationship. Any performance above the chance level after confound regression on shuffled features indicates confound leakage. This approach, combined with the Bayesian ROPE, helps detect confound leakage effectively.\n\nWe also performed permutation testing with 1,000 iterations to further validate our findings. After shuffling the features, a significantly lower performance was observed compared to the original features. No significant difference between raw and shuffled features was observed when using the confound-regressed features, supporting the leakage hypothesis. This indicates that higher accuracy after shuffling and confound regression suggests leaking target-related confounding information into the features.\n\nIn summary, our evaluation method combines nested cross-validation, Bayesian ROPE, feature shuffling, and permutation testing to thoroughly assess the performance and detect confound leakage in machine learning pipelines.",
  "evaluation/measure": "In our evaluation, we focused on key performance metrics to assess the effectiveness of our machine learning pipelines with and without confound removal (CR). For classification problems, we computed the out-of-sample Area Under the Receiver Operating Characteristic Curve (AUCROC). This metric is widely used in the literature due to its robustness in evaluating the performance of binary classifiers, especially when dealing with imbalanced datasets. For regression problems, we used the predictive R² score from scikit-learn. This metric provides a measure of how well the model's predictions approximate the real data points, with values closer to 1 indicating better performance.\n\nTo ensure the reliability of our results, we employed a 10 times repeated 5-fold nested cross-validation (CV) approach. This method helps in obtaining a more accurate estimate of model performance by reducing the variance associated with a single train-test split. Additionally, we used the Bayesian Region of Practical Equivalence (ROPE) approach to determine whether the differences in performance between models with and without CR were meaningful. This Bayesian framework allows us to make probabilistic statements about the equivalence or superiority of one model over another, providing a more nuanced understanding of our results compared to traditional hypothesis testing methods.\n\nThe choice of these metrics is representative of current practices in the field, ensuring that our evaluation is both rigorous and comparable to other studies. The AUCROC and predictive R² are standard metrics for classification and regression tasks, respectively, and their use in conjunction with nested CV and Bayesian ROPE provides a comprehensive assessment of model performance.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of machine learning pipelines with and without confound removal (CR). We utilized a variety of algorithms, including both linear and nonlinear models, such as linear regression (LR), decision trees (DT), random forests (RF), multi-layer perceptrons (MLP), and support vector machines (SVM) with linear and RBF kernels. This approach allowed us to assess the impact of CR across different types of models.\n\nFor benchmark datasets, we primarily used the UCI machine learning repository, which provides openly accessible datasets. These datasets cover a wide range of domains and include both classification and regression tasks. By using these benchmark datasets, we ensured that our findings are generalizable and reproducible.\n\nIn addition to comparing different machine learning algorithms, we also evaluated the performance of dummy models to establish a chance-level baseline. This comparison helped us understand whether the improvements observed with CR were meaningful or merely due to random chance.\n\nTo quantify the differences in performance, we computed out-of-sample metrics such as AUCROC for classification tasks and predictive R² for regression tasks. These metrics were calculated using a 10 times repeated 5-fold nested cross-validation (CV) approach, which provides a robust estimate of model performance.\n\nFurthermore, we employed the Bayesian Region of Practical Equivalence (ROPE) approach to determine whether the results with and without CR were meaningfully different. This Bayesian framework allowed us to make probabilistic statements about the performance differences, providing a more nuanced understanding than traditional hypothesis testing.\n\nOverall, our evaluation methodology was designed to be comprehensive and rigorous, ensuring that our conclusions about the effects of confound removal are well-supported and reliable.",
  "evaluation/confidence": "In our evaluation, we employed a robust methodology to ensure the confidence and statistical significance of our results. We computed out-of-sample performance metrics, specifically the AUCROC for classification and predictive R² for regression problems, using a 10 times repeated 5-fold nested cross-validation (CV) approach. This method provides a comprehensive assessment of model performance and helps in understanding the variability and reliability of the results.\n\nTo determine the statistical significance and meaningfulness of the differences between models with and without confound removal (CR), we utilized the Bayesian Region of Practical Equivalence (ROPE) approach. This Bayesian framework allows us to compute probabilities of the performance metric falling into a defined region of practical equivalence or of one model scoring higher than the other. By defining a region of equivalence (set at 0.05), we can make probabilistic statements regarding whether the differences in performance are meaningful. We summarize these differences using symbols to indicate whether the pipelines are practically equivalent, or if one pipeline scores higher than the other.\n\nAdditionally, we performed feature shuffling to detect confound leakage. By shuffling the features while keeping the confounds and target intact, we destroy the feature-target and feature-confound relationships, preserving only the confound-target relationship. Any performance above the chance level after CR on shuffled features indicates confound leakage. We further validated this approach using permutation testing, which showed consistent results with the Bayesian ROPE, confirming the presence of confound leakage in the clinical dataset.\n\nIn summary, our evaluation methodology includes confidence intervals through nested CV and statistical significance testing using the Bayesian ROPE approach. These techniques ensure that our claims of superiority over other methods and baselines are well-founded and reliable.",
  "evaluation/availability": "All 10 UCI benchmark datasets used in our study are freely accessible at the UCI machine learning repository. These datasets, along with our simulated data, serve as the minimal datasets required to reproduce our key findings. Additionally, we analyzed one real-world clinical dataset, which is sensitive and available from PeakProfiling GmbH under certain restrictions. Access to this dataset requires contacting Jörg Langner, the cofounder and CTO of PeakProfiling GmbH. An archival copy of the code and supporting data is also available via the GigaScience database, GigaDB."
}