{
  "publication/title": "Machine learning–based feature selection to search stable microbial biomarkers: application to inflammatory bowel disease",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Metagenomics\n- Biomarker Discovery\n- Feature Selection\n- Recursive Feature Elimination\n- Microbial Data Analysis\n- Inflammatory Bowel Disease\n- Data Integration\n- Classification Algorithms\n- Stability Metrics",
  "dataset/provenance": "The datasets used in this study were obtained from Qiita, an open-source microbial study management platform. Four datasets were downloaded, each identified by a unique Qiita study ID for reproducibility. These datasets consist of fecal samples from patients with inflammatory bowel disease (IBD) and healthy controls. The samples were sequenced for the V4 hypervariable region of the 16S rRNA gene. The datasets originate from different geographic locations, including the USA, Sweden, the UK, and Australia. The total number of samples varies across datasets, with some labeled as IBD and others as non-IBD. Specifically, Crohn's disease (CD) and ulcerative colitis (UC) are the types of IBD considered.\n\nThe datasets were preprocessed using standard bioinformatics steps, including trimming and picking closed-reference operational taxonomic units (OTUs). The abundance matrices were generated for each dataset, and taxa with the same taxonomy classification were aggregated. A core set of features was selected, consisting of 283 taxa at the species level and 220 at the genus level, which were common across all datasets. This approach ensures that the datasets are comparable and that the features selected are robust and generalizable. The datasets were then split into two ensemble datasets, ED1 and ED2, for training and testing purposes. Additionally, an external dataset was used to impose additional constraints during feature mapping, enhancing the stability and relevance of the selected features for classification tasks.",
  "dataset/splits": "The study involved creating two main ensemble datasets from the combined data of three initial datasets. These ensemble datasets were named Ensemble Dataset 1 (ED1) and Ensemble Dataset 2 (ED2). Each ensemble dataset was split into training and test sets, with 80% of the data used for training and 20% for testing. This resulted in four primary splits:\n\n1. **Ensemble Dataset 1—Training Dataset**: This split contained 627 samples, with 281 labeled as IBD and 346 as non-IBD.\n2. **Ensemble Dataset 1—Test Dataset**: This split contained 157 samples, with 70 labeled as IBD and 87 as non-IBD.\n3. **Ensemble Dataset 2—Training Dataset**: This split also contained 628 samples, with 281 labeled as IBD and 347 as non-IBD.\n4. **Ensemble Dataset 2—Test Dataset**: This split contained 157 samples, with 70 labeled as IBD and 87 as non-IBD.\n\nAdditionally, models developed using ED1 were tested on the entire ED2, and vice versa. This cross-testing approach was used to ensure the robustness and generalizability of the models. The distribution of samples from different datasets was similar in ED1 and ED2, although the abundance of each taxon was expected to vary due to the high number of features and the inherent variability and sparsity of metagenomics datasets.",
  "dataset/redundancy": "The datasets were split into two ensemble datasets, ED1 and ED2, by mixing samples from the original studies. This process ensured that the percentage of samples from different datasets was similar in both ED1 and ED2. However, due to the high number of features and the variability and sparsity of metagenomics datasets, the abundance of each taxon was expected to differ between the two datasets. This characteristic was intentionally included to assess how it might affect the performance and generalizability of the methods used.\n\nThe training and test sets were designed to be independent. To enforce this independence, each ensemble dataset was used for training and testing in a symmetric manner. Specifically, ED1 was used for training and testing with ED2, and vice versa. This approach helped to evaluate the robustness and generalizability of the feature selection and classification methods.\n\nThe distribution of samples in these datasets is notable for its high variability and sparsity, which is typical of metagenomics studies. This contrasts with many previously published machine learning datasets that often have more uniform and dense feature distributions. The high variability and sparsity in our datasets posed unique challenges, particularly in feature selection and stability assessment. To address these challenges, recursive feature elimination (RFE) within a bootstrap embedding was employed. This method helped identify and select robust features that could generalize well across different datasets. Additionally, a mapping strategy using a similarity matrix was used to impose additional constraints during feature mapping, further enhancing the stability and relevance of the selected features.",
  "dataset/availability": "The data used in this study is publicly available to ensure reproducibility and accessibility. Four datasets were downloaded from Qiita, an open-source microbial study management platform. The specific Qiita study IDs for each dataset are provided in Table 1, allowing for easy reproducibility. Additionally, all code and supplementary data supporting this work, including DOME-ML annotations, are openly available in the GigaScience repository GigaDB. This repository serves as a comprehensive resource for accessing the datasets and the code used in the analysis. The data is licensed under the GNU GPL, ensuring that it can be freely used, modified, and distributed by others. To enforce the consistency and reproducibility of the data splits, the datasets were processed using standardized bioinformatics steps, including trimming and picking closed-reference OTUs. This ensures that the data preprocessing steps are consistent across all datasets, maintaining the integrity of the analysis. The datasets were then integrated by selecting only the common features across all datasets, resulting in a core set of features at both the species and genus levels. This approach ensures that the data splits used in the study are transparent and reproducible.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are well-established and widely recognized in the field. These include logistic regression, support vector machines (SVM), random forests, extreme gradient boosting (XGBoost), perceptron, and multilayer perceptron (MLP) with varying numbers of hidden layers. These algorithms are part of conventional machine learning techniques and are not new.\n\nThe choice of these algorithms was driven by their proven effectiveness in handling complex datasets, particularly in the context of microbial data analysis. For instance, random forests and XGBoost are tree-based ensemble methods that have shown superior performance in recent studies involving microbial data. These algorithms utilize feature splitting based on thresholds, which is particularly advantageous for microbial data.\n\nLogistic regression and SVM are linear models that provide a straightforward approach to classification tasks. Perceptron and MLP, on the other hand, are neural network-based methods that can capture nonlinear relationships among variables, making them suitable for more complex data structures.\n\nGiven that these algorithms are standard in the machine learning community, they were not published in a machine-learning journal. Instead, they were applied and validated within the context of our specific study, focusing on the classification of IBD-affected patients and healthy controls. The performance of these algorithms was assessed using various metrics, including the Matthews correlation coefficient (MCC), area under the ROC curve (AUC), accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV).\n\nThe use of these established algorithms ensures the robustness and reliability of our findings, as they have been extensively tested and validated in numerous studies. This approach allows us to focus on the application and interpretation of these algorithms in the context of our research, rather than developing new methods.",
  "optimization/meta": "The meta-predictor approach used in this study does not rely on data from other machine-learning algorithms as input. Instead, it focuses on feature selection and classification using various machine-learning methods independently.\n\nThe classification algorithms employed include logistic regression, support vector machines (SVM), random forests, extreme gradient boosting (XGBoost), perceptron, and multilayer perceptron (MLP) with different numbers of hidden layers. These methods were used to classify samples in inflammatory bowel disease (IBD) versus healthy controls using features selected within the recursive feature elimination (RFE) phase.\n\nThe stability of the feature selection process was ensured through the use of bootstrapping within the RFE procedure. Additionally, a mapping strategy was employed to integrate prior knowledge into the learning process, aiming to achieve higher stability of the biomarker list. This strategy involved a kernel-based data transformation that projects data into a new space where similar features are mapped closer together.\n\nThe training data for each classification model was split into training and test sets, ensuring that the data used for training was independent of the test data. This independence is crucial for evaluating the generalizability and performance of the models. The models were trained using 5-fold cross-validation within the training set, with hyperparameters tuned using grid search.\n\nIn summary, the meta-predictor approach leverages multiple classification algorithms and a robust feature selection process to identify stable biomarkers for classifying IBD-affected patients and healthy controls. The independence of the training data is maintained throughout the process, ensuring reliable and generalizable results.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the datasets for machine learning algorithms. Initially, four datasets were obtained from Qiita, an open-source microbial study management platform. These datasets included metadata, abundance matrices of processed 16S rDNA sequences, and associated taxonomy.\n\nEach dataset underwent independent preprocessing, which involved aggregating taxa with the same taxonomy classification and summing their respective counts. This step was performed for both species and genus levels, resulting in two abundance matrices per dataset. Taxa with more than 99% of abundances equal to zero were filtered out to maintain the integrity of the abundance profiles.\n\nThe abundance profiles were then divided by the geometric mean, and the log (base 2) of the ratios was taken, leveraging the concept of centered log-ratio (clr) transformation. A pseudocount value equal to the minimum observed data was used to handle zero values. This transformation helped in managing the highly skewed distribution of microbial abundances.\n\nAfter preprocessing, the three datasets were integrated by retaining only the features common across all datasets. This integration resulted in a core set of features, with 283 taxa at the species level and 220 at the genus level.\n\nFor feature selection, recursive feature elimination (RFE) was employed using a linear support vector machine (SVM) as the prediction model. RFE was performed 100 times with bootstrapping, where each bootstrap involved splitting the training dataset into an internal training set and an internal test set. The regularization parameter was tuned using grid search and 5-fold cross-validation, with the Matthew correlation coefficient (MCC) as the performance index.\n\nFeature importance was measured based on feature weights, and the least important feature was iteratively eliminated until only one feature remained. The global feature rank was calculated by averaging the ranked lists across the 100 bootstraps. The number of features corresponding to the maximum MCC was considered optimal.\n\nAdditionally, the data was mapped using either Pearson correlation or Bray–Curtis similarity to create a similarity matrix. This matrix encoded the likeness between features and was used as input for machine learning methods, such as convolutional neural networks, to perform classification tasks.\n\nThe preprocessing and encoding steps ensured that the data was in a suitable format for machine learning algorithms, enabling effective classification and feature selection. The use of log transformation and similarity matrices helped in handling the complexities of microbial data, leading to robust and generalizable results.",
  "optimization/parameters": "In our study, we employed several machine learning models, each with its own set of input parameters. For the random forest model, key parameters included the minimum number of samples required to split an internal node and to be at a leaf node, both set to 2. The number of features considered for the best split was set to the square root of the total number of features. The forest consisted of 100 trees.\n\nFor the extreme gradient boosting (XGBoost) model, we used 200 gradient boosted trees with a maximum tree depth of 20. The subsample ratio of the training instances was 0.2, and the subsample ratio of columns when constructing each tree was 0.5. The minimum loss reduction for a partition on a leaf node was set to 1. The learning rate and regularization factor (alpha) were tuned using a grid search.\n\nThe multilayer perceptron (MLP) regressor was built with a single output layer and used the LBFGS optimizer to minimize the squared error function. The ReLU activation function was utilized. The learning rate and L2 regularization term were tuned similarly to XGBoost. We constructed MLPs with 1, 2, and 3 hidden layers, with the size of each hidden layer corresponding to the number of features.\n\nFor logistic regression, we used an L2 penalty (ridge regression) and the LBFGS solver with a tolerance parameter of 1e-4. The regularization parameter was tuned using a grid search.\n\nIn the linear support vector machine (SVM), we applied a squared L2 penalty and used the \"rbf\" kernel with \"gamma\" set to \"scale\". The regularization parameter was also tuned using a grid search.\n\nThe selection of these parameters was guided by a combination of domain knowledge and empirical performance. Grid search with 5-fold cross-validation was employed to tune the hyperparameters, ensuring that the models were optimized for the specific datasets used in our experiments. This approach helped in identifying the most effective parameter settings for each model, leading to improved predictive performance.",
  "optimization/features": "In our study, we initially considered a core set of features consisting of 283 taxa at the species level and 220 at the genus level. These features were selected by integrating three datasets, keeping only the features common across all datasets.\n\nFeature selection was performed using Recursive Feature Elimination (RFE) with bootstrapping. This process was conducted 100 times, where in each bootstrap, the training dataset was split into an internal training set and an internal test set. A prediction model was trained within the internal training set using a linear support vector machine (SVM). The feature selection was done using the training set only, ensuring that the test set remained unseen during this process.\n\nThe optimal number of features was determined by calculating the average Matthew's Correlation Coefficient (MCC) across the 100 bootstrap internal test sets for different numbers of features. The number of features corresponding to the maximum MCC was taken as optimal. For most experiments, the optimal number of features was found to be high, around 267 at the species level.\n\nHowever, to balance between optimal performance and generalizability, we also considered a constant number of top features. We chose to consider the top 14 features as a trade-off, based on the point where the differential of average MCC started to converge to zero. This selection was done to ensure that the model could generalize well to unseen data.\n\nIn summary, feature selection was a crucial step in our methodology, and it was performed using the training set only, with the optimal number of features determined through a rigorous bootstrapping and RFE process.",
  "optimization/fitting": "In our study, we employed several machine learning algorithms, each with its own set of hyperparameters, to classify samples in IBD versus healthy using selected features. The number of parameters in our models varied significantly, with some models having a much larger number of parameters than the number of training points.\n\nTo address the risk of overfitting, especially in models with a high number of parameters, we implemented several strategies. For tree-based models like random forest and XGBoost, we used techniques such as setting a minimum number of samples required to split an internal node and to be at a leaf node, limiting the maximum depth of the trees, and using subsampling of the training instances and features. For the multi-layer perceptron (MLP), we applied L2 regularization and early stopping based on the loss score. Additionally, we used grid search with 5-fold cross-validation to tune the hyperparameters, which helped in selecting models that generalize well to unseen data.\n\nTo mitigate underfitting, we ensured that our models had sufficient complexity to capture the underlying patterns in the data. For instance, in the random forest, we allowed the trees to grow until the leaves were pure or contained a minimum number of samples. In the MLP, we experimented with different architectures, including models with 1, 2, and 3 hidden layers, and used the ReLU activation function to introduce non-linearity. Furthermore, we performed recursive feature elimination (RFE) to select the optimal number of features, which helped in improving the model performance by focusing on the most relevant features.\n\nIn summary, we carefully balanced the complexity of our models to avoid both overfitting and underfitting. By using regularization techniques, cross-validation, and feature selection methods, we aimed to build robust models that generalize well to new data.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and enhance the robustness of our models. For the logistic regression model, we utilized L2 regularization, also known as ridge regression, which adds a penalty equal to the square of the magnitude of coefficients to the loss function. This helps to shrink the coefficients and prevent the model from fitting the noise in the data.\n\nFor the linear Support Vector Machine (SVM), we applied the squared L2 penalty to the loss function. This technique also helps to control the complexity of the model and avoid overfitting by penalizing large coefficients.\n\nIn the context of the random forest model, we implemented a form of regularization by setting constraints on the minimum number of samples required to split an internal node and to be at a leaf node. Both parameters were set to 2, which helps to control the depth of the trees and prevent them from becoming too complex.\n\nThe extreme gradient boosting (XGBoost) model incorporated several regularization techniques. We used a subsample ratio of the training instances set to 0.2 and a subsample ratio of columns when constructing each tree set to 0.5. These techniques help to introduce randomness and prevent the model from overfitting. Additionally, we set a minimum loss reduction for the partition on a leaf node to 1, which ensures that only meaningful splits are made. We also included an L1 regularization term, alpha, which was tuned using a grid search.\n\nFor the multi-layer perceptron (MLP) model, we employed L2 regularization to prevent overfitting. The regularization term was tuned using the same procedure as for XGBoost. Furthermore, we used early stopping based on the loss score, which halted the training process when the loss score was under 1e-4, ensuring that the model did not overfit to the training data.\n\nOverall, these regularization techniques were crucial in ensuring that our models generalized well to unseen data and did not overfit to the training datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are available and have been detailed in the publication. The specific configurations for various models, such as random forest, extreme gradient boosting (XGBoost), and multilayer perceptron (MLP), including parameters like the number of trees, maximum depth, learning rate, and regularization terms, are explicitly mentioned. These details ensure reproducibility of the experiments conducted.\n\nThe source code and additional data supporting this work are openly available in the GigaScience repository GigaDB. This repository includes the code used for preprocessing, feature selection, model training, and evaluation. The project, named MLonMicrobiome, is hosted on GitLab, and the code is written in Python and R. The repository also includes requirements for running the code, such as specific versions of libraries like scikit-learn and phyloseq.\n\nThe license under which the code and data are available is the GNU General Public License (GPL), which allows for free use, modification, and distribution of the software, provided that the same license is applied to any derivative works. This ensures that the community can build upon the work presented in the publication while maintaining the openness and accessibility of the research.\n\nAdditionally, the datasets used in the study were downloaded from Qiita, an open-source microbial study management platform. The reference Qiita study IDs are provided in the publication, enabling others to reproduce the data preprocessing steps and validate the findings. The supplementary data, including DOME-ML annotations, are also available in the GigaDB repository, further supporting the transparency and reproducibility of the research.",
  "model/interpretability": "The model employed in our study is not entirely a black box, as we have utilized techniques to enhance its interpretability. One of the key methods used for this purpose is the calculation of Shapley values. Shapley values provide a way to understand the contribution of each feature to the model's predictions. By calculating these values on the training, test, and external datasets, we can determine the extent to which each feature influences the output of the prediction model. This approach is model-agnostic, meaning it can be applied to any machine learning model, and it offers local explanations for the predictions made.\n\nIn our specific case, we focused on the top 14 ranked features selected through recursive feature elimination (RFE). The Shapley values for these features were calculated using a random forest model. The results showed a high similarity in the rank of these features across different datasets, indicating the consistency and reliability of our feature selection process. Moreover, the directions in which these features increase or decrease the possibility of inflammatory bowel disease (IBD) were identical, further validating our model's interpretability.\n\nFor instance, features such as Lachnospiraceae (family) Butyrivibrio (genus) and Bacteroides (genus) fragilis (species) were identified as indicators of IBD state, while Corynebacteriaceae (family) Corynebacterium (genus) and Tissierellaceae (family) WAL_1855D (genus) were indicators of the absence of IBD. This level of detail allows for a clearer understanding of which microbial features are most influential in predicting IBD, making the model more transparent and interpretable.\n\nAdditionally, the use of Shapley values ensures that the model's decisions are not merely based on statistical significance but also on the practical impact of each feature. This dual approach enhances the robustness of our model and provides a more comprehensive understanding of the underlying biological processes.",
  "model/output": "The model is a classification model. It is designed to predict the status of Inflammatory Bowel Disease (IBD) in samples, distinguishing between IBD-affected patients and healthy controls. Various classification algorithms were employed, including logistic regression, linear Support Vector Machines (SVM), random forest, Extreme Gradient Boosting (XGBoost), perceptron, and multilayer perceptron (MLP) with different numbers of hidden layers. These algorithms were used to classify samples based on selected features, with the goal of achieving high performance in terms of metrics such as Matthews Correlation Coefficient (MCC), Area Under the Curve (AUC), accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). The models were trained and evaluated using cross-validation techniques to ensure robustness and generalizability. The final predictive model, based on random forest with a selected set of top features, was used to identify key indicators of IBD status.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the project named MLonMicrobiome is publicly available. It can be accessed through the project's homepage, which is hosted on GitLab. The project is platform-independent and is developed using Python and R programming languages. Specific versions of R (4.1.3) and Python (3.85) are required, along with several other dependencies such as phyloseq (1.27.6), openxlsx (4.2.4), and scikit-learn (1.0.2). The software is released under the GNU General Public License (GPL), ensuring that users have the freedom to use, modify, and distribute the code. Additionally, the code and supplementary data, including DOME-ML annotations, are available in the GigaScience repository GigaDB, further supporting the reproducibility of the work.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to assess the stability and performance of the feature selection process. The primary metric used for evaluating the recursive feature elimination (RFE) process was the Matthew correlation coefficient (MCC), which considers all four categories of the confusion matrix. This metric was chosen for its appropriateness in evaluating binary classification performance.\n\nIn addition to MCC, several other performance metrics were measured to compare the models' effectiveness. These included the area under the receiver operating characteristic curve (AUC), accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). The ROC curve was plotted with the false-positive rate on the x-axis and the true-positive rate on the y-axis.\n\nThe study utilized a symmetric analysis approach, where ensemble dataset 1 (ED1) was used for training and ensemble dataset 2 (ED2) for testing, and vice versa. This method ensured that the models were evaluated on independent datasets, providing a robust assessment of their generalizability.\n\nTo further enhance the stability of the biomarker list, a mapping strategy was integrated into the learning process. This strategy involved using an external dataset (dataset 4) to impose additional constraints during feature mapping. The mapping transformation procedure projected data into a new space where similar features were mapped closer together, taking into account the correlation among different taxa.\n\nThe stability of the feature selection process was assessed using various metrics, including Spearman's rank correlation coefficient (SRCC), Hamming distance, Pearson correlation, Bray–Curtis dissimilarity, and Euclidean distance. These metrics were computed for each pair of bootstrap samples used in the RFE process and averaged across all possible pairs.\n\nAdditionally, a noise-filtering experiment was conducted to evaluate the ability of the feature selection method to distinguish between important and unimportant features. Randomly generated noisy features were added to the original dataset, and the average rank of these noisy features was compared to the average rank of true features.\n\nThe evaluation process also involved tuning hyperparameters within each model using 5-fold cross-validation and grid search. This ensured that the models were optimized for the specific datasets used in the study.\n\nOverall, the evaluation method combined multiple metrics and techniques to provide a thorough assessment of the feature selection process and the performance of the classification models. This approach aimed to ensure the robustness and generalizability of the findings.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our classification models. These metrics include the Matthews Correlation Coefficient (MCC), Area Under the Curve (AUC), accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). MCC was particularly emphasized due to its ability to consider all four categories of the confusion matrix, providing a balanced measure of classification performance. AUC, derived from the Receiver Operating Characteristic (ROC) curve, was used to assess the model's ability to distinguish between classes across different threshold levels. Accuracy, specificity, and sensitivity offered insights into the overall correctness, true negative rate, and true positive rate of the models, respectively. PPV and NPV further detailed the precision of positive and negative predictions. This diverse set of metrics ensures a thorough evaluation, aligning with established practices in the literature for binary classification tasks.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, the AggMapNet approach was used for comparison. This method employs a unique data transformation technique, converting original data into multi-channel 2-dimensional spatial-correlated images through pairwise correlation distances. This is achieved using the Uniform Manifold Approximation and Projection (UMAP) method. The feature maps generated are then fed into machine learning models, such as convolutional neural networks, for effective classification tasks.\n\nHowever, traditional machine learning methods typically require 1-dimensional vectors as input. Our approach can map unordered features as 1D vectors for conventional machine learning models, making it complementary to the AggMapNet approach. The comparison demonstrated the strength of our pipeline, particularly in scenarios with small datasets and a high number of features, which is common in omics studies.\n\nAdditionally, simpler baselines were implicitly considered through the use of different mapping strategies and feature selection methods. For instance, recursive feature elimination (RFE) was performed with and without transformations, including Pearson correlation and Bray–Curtis similarity. The Bray–Curtis similarity matrix provided the best stability without sacrificing classification performance, highlighting its effectiveness compared to simpler baselines.",
  "evaluation/confidence": "The evaluation of our models involved several performance metrics, including the Matthews Correlation Coefficient (MCC), AUC, accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV). These metrics were used to compare the performance of different models and to assess the stability and robustness of our feature selection method.\n\nTo ensure the reliability of our results, we employed statistical tests such as the Wilcoxon rank-sum test to calculate False Discovery Rate (FDR)-adjusted P values. This allowed us to determine the statistical significance of the differences observed between the IBD and non-IBD groups. For instance, 13 out of the 14 top biomarkers selected had a q-value below 0.001, indicating strong statistical significance.\n\nAdditionally, we performed a noise-filtering experiment to evaluate the ability of our method to distinguish between important and unimportant features. By adding random noisy features and comparing their ranks to those of the true features, we demonstrated that our method effectively filters out noise, as the noisy features consistently received higher ranks.\n\nThe stability of our feature selection method was further assessed using various stability indexes, including Spearman's rank correlation coefficient (SRCC), Hamming distance, Pearson correlation, and Bray-Curtis dissimilarity. These indexes were computed for each pair of bootstrap samples used in the recursive feature elimination (RFE) process, providing a comprehensive evaluation of the method's consistency across different datasets and conditions.\n\nMoreover, we conducted cross-validation within the training set using grid search to tune the hyperparameters of each model. This approach helped to optimize the models' performance and to ensure that the results were not due to overfitting.\n\nIn summary, our evaluation process included statistical tests, noise-filtering experiments, stability indexes, and cross-validation to provide confidence in the performance and robustness of our method. The results demonstrated that our approach is statistically significant and superior to other methods and baselines in the context of the datasets and conditions tested.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the code and other data supporting this work, including annotations, are openly available in the GigaScience repository GigaDB. This repository can be accessed for further details and to reproduce the results. The project is licensed under the GNU GPL, ensuring that users can freely access, modify, and distribute the code and data. The datasets used in the study were downloaded from Qiita, an open-source microbial study management platform, and their reference IDs are provided for reproducibility. Additionally, the supplementary data and code are publicly available, promoting transparency and facilitating verification of the findings."
}