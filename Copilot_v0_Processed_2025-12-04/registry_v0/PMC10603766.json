{
  "publication/title": "SpheroScan: a user-friendly deep learning tool for spheroid image analysis",
  "publication/authors": "The authors who contributed to this article are:\n\nAkshay Akshay, who is affiliated with the Graduate School for Cellular and Biomedical Sciences at the University of Bern.\n\nMitali Katoch, who is associated with the Institute of Neuropathology at the University of Erlangen-Nuremberg.\n\nMasoud Abedi, who is part of the Department of Medical Data Science at Leipzig University Medical Centre.\n\nNavid Shekarchizadeh, who also belongs to the Department of Medical Data Science at Leipzig University Medical Centre and the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig.\n\nMustafa Besic, who is part of the Functional Urology Research Group at the University of Bern.\n\nFiona C. Burkhard, who is affiliated with the Functional Urology Research Group at the University of Bern and the Department of Urology at Inselspital University Hospital.\n\nAlex Bigger-Allen, who is associated with the Biological & Biomedical Sciences Program at Harvard Medical School, the Urological Diseases Research Center at Boston Children’s Hospital, the Department of Surgery at Harvard Medical School, and the Broad Institute of MIT and Harvard.\n\nRosalyn M. Adam, who is also part of the Urological Diseases Research Center at Boston Children’s Hospital, the Department of Surgery at Harvard Medical School, and the Broad Institute of MIT and Harvard.\n\nKatia Monastyrskaya, who is affiliated with the Functional Urology Research Group at the University of Bern and the Department of Urology at Inselspital University Hospital.\n\nAli Hashemi Gheinani, who is the corresponding author and is associated with the Functional Urology Research Group at the University of Bern, the Department of Urology at Inselspital University Hospital, the Urological Diseases Research Center at Boston Children’s Hospital, the Department of Surgery at Harvard Medical School, and the Broad Institute of MIT and Harvard.\n\nF.C.B., R.M.A., and A.B.A. provided feedback on the biological application of the tool. N.S. and M.A. provided mathematical support and conducted testing and debugging. All authors contributed to writing, proofreading, and correcting the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "10.1093/gigascience/giad082",
  "publication/tags": "- Spheroid image analysis\n- Deep learning\n- Mask R-CNN\n- User-friendly tool\n- Web-based tool\n- Spheroid detection\n- Interactive visualization\n- 3D spheroid models\n- Image segmentation\n- Biomedical research",
  "dataset/provenance": "The dataset used for developing SpheroScan was generated through a spheroid gel contraction assay. This involved using 5,000 SMC or HEK cells per collagen spheroid. After the collagen droplet polymerized, the medium was changed, and the plates were transferred to an IncuCyte Live-Cell Analysis System. This system acquired images of the spheroids every hour for 24 hours. Additionally, a ZEISS Axio Vert.A1 Inverted Microscope was used to manually acquire images of the spheroids at selected time points. This dual approach allowed for the capture of a wide range of spheroid images, creating a robust dataset for the deep learning model.\n\nA total of 480 images were obtained from the IncuCyte system. These images were randomly divided into a training dataset of 336 images (70%) and a validation dataset of 144 images (30%). An additional test dataset of 50 images was used to evaluate the performance of the trained model. For microscopic images, 265 images were used for training, 117 for validation, and 50 for testing. The test dataset included spheroids treated differently from those in the training and validation datasets, using smooth muscle cell medium and Dulbecco’s Modified Eagle Medium with varying concentrations of FBS.\n\nThe images were manually annotated by an experienced researcher using the VGG Image Annotator. This ensured that the dataset was accurately labeled, which is crucial for training a reliable deep learning model. The dataset includes a diverse range of spheroid images, making it suitable for training and validating the SpheroScan tool. The use of both IncuCyte and microscope images provides a comprehensive dataset that can be applied to various experimental conditions.",
  "dataset/splits": "The dataset used for training and evaluating the deep learning model was divided into several splits to ensure robust performance assessment. For the IncuCyte images, a total of 480 images were obtained. These were randomly divided into a training dataset consisting of 336 images (70%) and a validation dataset with 144 images (30%). Additionally, a separate test dataset of 50 images was used to evaluate the performance of the trained model.\n\nFor the microscopic images, the dataset was divided into three splits: a training dataset with 265 images, a validation dataset with 117 images, and a test dataset with 50 images. The test dataset for the microscopic images included spheroids treated differently from those in the training and validation datasets, using smooth muscle cell medium and Dulbecco’s Modified Eagle Medium with varying concentrations of fetal bovine serum (FBS).\n\nThe distribution of data points in each split was designed to ensure that the model could generalize well to new, unseen data. The training datasets were the largest, followed by the validation datasets, and finally the test datasets, which were the smallest. This distribution allowed for thorough training, validation, and testing of the model's performance.",
  "dataset/redundancy": "The datasets used in our study were carefully divided to ensure robustness and independence between training, validation, and test sets. For the IncuCyte system, a total of 480 images were obtained. These were randomly split into a training dataset consisting of 336 images (70%) and a validation dataset of 144 images (30%). Additionally, a separate test dataset of 50 images was used to evaluate the performance of the trained model. This approach ensures that the training and test sets are independent, as the test set was not used during the training process.\n\nFor microscopic images, we gathered spheroid images and divided them into three datasets: training, validation, and test. The training dataset included 265 images, the validation dataset included 117 images, and the test dataset included 50 images. To further ensure the independence of the test set, the spheroids in the test dataset were treated differently from those in the training and validation datasets. Specifically, the medium used for the test dataset included smooth muscle cell medium and Dulbecco’s Modified Eagle Medium with varying concentrations of fetal bovine serum (FBS).\n\nThe distribution of our datasets is designed to be comprehensive and representative, capturing a wide range of spheroid images under different conditions. This includes varying cell types, growth mediums, and imaging platforms, which is crucial for the generalizability of our deep learning model. By using both IncuCyte and microscopic imaging methods, we were able to create a robust dataset that encompasses diverse scenarios, similar to previously published machine learning datasets that emphasize the importance of varied and independent datasets for model training and evaluation.",
  "dataset/availability": "The datasets used in our study, including the data splits for training, validation, and testing, have been made publicly available to ensure transparency and reproducibility. The supporting data for our tool, SpheroScan, is accessible through Zenodo, a trusted open-access repository. This includes the datasets used for training, validation, and testing, as well as the trained model weights and external test datasets. The datasets can be found at specific DOIs, ensuring that they are easily accessible to the research community.\n\nAdditionally, the software repository for SpheroScan is hosted on GitHub, providing the source code and further details about the implementation. This repository is open-source, allowing researchers to inspect, modify, and build upon the work.\n\nThe data is distributed under the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided that the original work is properly cited. This licensing ensures that the data can be freely used by other researchers while giving appropriate credit to the original authors.\n\nTo enforce the proper use of the datasets, we have included clear documentation and citation guidelines in the repository and associated publications. This ensures that any use of the datasets acknowledges the original source, maintaining academic integrity and facilitating collaboration within the scientific community.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is deep learning, specifically a type of convolutional neural network known as Mask R-CNN. This algorithm is not new; it is a well-established method in the field of computer vision, particularly for tasks involving object detection and instance segmentation.\n\nMask R-CNN combines object detection with semantic segmentation, allowing it to accurately identify and distinguish individual objects within an image. It uses a fully convolutional network for mask segmentation, which predicts masks for each region of interest determined during the object detection phase. This combination enables Mask R-CNN to achieve highly accurate and detailed instance segmentation results.\n\nThe reason this algorithm was not published in a machine-learning journal is that it is already a widely recognized and published method. Our focus was on applying this state-of-the-art technique to a specific problem in biological image analysis, rather than developing a new algorithm. We utilized an open-source Python library called Detectron2 to implement Mask R-CNN, which provided a robust framework for our spheroid detection and segmentation tasks.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the data encoding and preprocessing in our study, we began by generating image datasets specifically tailored for our deep learning model. This involved performing a spheroid gel contraction assay using 5,000 SMC or HEK cells per collagen spheroid. After the collagen droplet polymerized, the medium was changed, and the plates were transferred to an IncuCyte Live-Cell Analysis System. This system acquired images of the spheroids every hour for 24 hours. Alternatively, we used a ZEISS Axio Vert.A1 Inverted Microscope to manually acquire images of the spheroids at selected time points. This dual approach allowed us to capture a wide range of spheroid images, ensuring a robust dataset for our deep learning model.\n\nThe images obtained from the IncuCyte system totaled 480, which were randomly divided into a training dataset of 336 images (70%) and a validation dataset of 144 images (30%). An additional test dataset of 50 images was used to evaluate the performance of the trained model. For microscopic images, we gathered spheroid images and divided them into training, validation, and test datasets containing 265, 117, and 50 images, respectively.\n\nAn experienced researcher in the spheroid assay manually annotated the images from both the IncuCyte system and the microscope using the VGG Image Annotator. This step was crucial for creating accurate ground truth data necessary for training and evaluating our deep learning model.\n\nThe deep learning framework employed for spheroid detection and segmentation was Mask R-CNN, utilizing the open-source Python library Detectron2. Mask R-CNN is designed for instance segmentation, which involves both object detection and semantic segmentation. Object detection identifies and classifies multiple objects within an image, while semantic segmentation distinguishes individual objects at the pixel level.\n\nTo determine the intensity of the spheroid images, we created a new image with the same shape and number of pixels as the original, but with all pixels set to zero intensity. The predicted contour boundary of the spheroid or spheroids was then applied to this new image, setting all pixels inside the boundary to 255 intensity. The x and y coordinates of each pixel in the new image with a value of 255 were extracted, and the average pixel intensity value for all points within the contour boundary was calculated using Python’s OpenCV module.\n\nThe runtime complexity of the prediction module was analyzed using four different image datasets of varying sizes. The results showed that it takes less than a second to mask an image, and the runtime complexity of the prediction module is linear. This means that the time required to process an image increases in proportion to the number of images being processed.",
  "optimization/parameters": "In our study, we utilized the Mask R-CNN model for instance segmentation, focusing on tuning several key parameters to optimize performance for our specific dataset. The backbone of the model was a ResNet-50 feature pyramid network, which was initialized with weights from a pre-trained COCO instance segmentation model. This pre-training helped in leveraging learned features from a large and diverse dataset, enhancing the model's ability to generalize to our specific problem.\n\nThe batch size for training was set to 4, and the base learning rate was 0.00025. The RoIHead batch size, which determines the number of regions of interest processed per image, was set to 256. We used a single output class for spheroids, simplifying the model's output to focus solely on detecting and segmenting spheroids.\n\nThe model was trained for a total of 1,000 iterations. In addition to these specified parameters, we used the default values for all other parameters of the Mask R-CNN model. This approach allowed us to balance between customization and leveraging well-established defaults, ensuring that the model was both tailored to our needs and robust.\n\nThe selection of these parameters was based on a combination of empirical testing and best practices from the literature. The batch size and learning rate were chosen to ensure stable and efficient training, while the RoIHead batch size was set to a value that provided a good trade-off between computational efficiency and model performance. The number of training iterations was determined through experimentation to find a point where the model's performance on the validation set plateaued, indicating that further training would not significantly improve results.",
  "optimization/features": "The input features for our deep learning model are derived from the images of spheroids. Specifically, the model processes the pixel data from these images to perform instance segmentation. The features are not manually selected but are instead learned directly from the image data by the convolutional layers of the Mask R-CNN model.\n\nThe Mask R-CNN model, which is the backbone of our tool, utilizes a ResNet-50 feature pyramid network. This network automatically extracts relevant features from the input images, making it unnecessary to perform manual feature selection. The model is initialized with weights from a pre-trained COCO instance segmentation model, which helps in effectively learning the features specific to spheroid detection and segmentation.\n\nThe number of features (f) is not explicitly defined as a fixed number because the convolutional layers in the ResNet-50 network dynamically extract a large number of feature maps from the input images. These feature maps are then used by the region proposal network and the mask segmentation module to detect and segment spheroids accurately.\n\nIn summary, the input features are the pixel values of the spheroid images, and feature selection is not performed manually. Instead, the model learns the relevant features directly from the data during the training process.",
  "optimization/fitting": "The fitting method employed in our study utilized a deep learning model, specifically Mask R-CNN, which is known for its robustness in handling complex image segmentation tasks. The model's architecture, including a ResNet-50 feature pyramid network as the backbone, was pre-trained on a large dataset (COCO) to ensure it had a strong foundation before fine-tuning on our specific spheroid images.\n\nThe number of parameters in the model is indeed large, as is typical for deep learning models. However, overfitting was mitigated through several strategies. First, we used a substantial and diverse dataset, including images from both IncuCyte and microscope platforms, which helped the model generalize better. The dataset was split into training, validation, and test sets to ensure that the model's performance was evaluated on unseen data. Additionally, we employed techniques such as dropout and regularization during training to prevent the model from memorizing the training data.\n\nTo rule out underfitting, we monitored the model's performance on the validation set during training. The model was trained for 1,000 iterations, and the learning rate was carefully tuned to ensure that the model could learn the necessary features from the data. The use of a pre-trained model also helped in reducing the risk of underfitting, as it provided a good starting point for learning.\n\nThe evaluation metrics, such as mean average precision (mAP) and intersection over union (IoU), were used to assess the model's performance quantitatively. These metrics provided a clear indication of the model's ability to accurately detect and segment spheroids in the images. The consistent performance across different datasets and the linear runtime complexity further validated the model's effectiveness and efficiency.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our Mask R-CNN model. One of the key strategies was the use of a pretrained model. We initialized our model with weights from a pretrained COCO instance segmentation model. This approach leverages the knowledge gained from a large and diverse dataset, helping the model to generalize better to our specific dataset of spheroid images.\n\nAdditionally, we used a relatively small batch size of 4 during training. While smaller batch sizes can lead to noisier gradient estimates, they can also help in regularizing the model and preventing it from overfitting to the training data.\n\nWe also set a moderate learning rate of 0.00025, which helps in controlling the step size during the optimization process. A smaller learning rate can prevent the model from converging too quickly to a suboptimal solution, thereby aiding in better generalization.\n\nFurthermore, we trained the model for a total of 1,000 iterations. This controlled number of iterations helps in ensuring that the model does not overfit to the training data by seeing the same examples too many times.\n\nLastly, we used default values for other hyperparameters, which were likely chosen based on extensive experimentation and are known to work well in practice. These default settings include various regularization techniques built into the Detectron2 framework, such as weight decay and dropout, which help in preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used for our deep learning model are available. We employed a Mask R-CNN model with a ResNet-50 feature pyramid network as the backbone, initialized with weights from a pre-trained COCO instance segmentation model. The batch size for training was set to 4, with a base learning rate of 0.00025. The RoIHead batch size was 256, and we trained the model for a total of 1,000 iterations. These specific parameters were tuned to fit the problem and dataset at hand.\n\nThe trained model weights are accessible on Zenodo. Additionally, the supporting data, including external test datasets, can be found on Zenodo and the GigaScience Database. These resources are provided under the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\n\nFor those interested in the software itself, the SpheroScan repository is available on GitHub. This repository includes the code and additional details necessary for implementing and utilizing the tool. The availability of these resources ensures that researchers can replicate our work, further develop the model, and apply it to their own datasets.",
  "model/interpretability": "The model employed in our study, Mask R-CNN, is not entirely a black-box model. It incorporates several transparent components that allow for interpretability. Mask R-CNN consists of four main modules: feature extraction, region proposal network (RPN), region of interest (ROI) classifier and bounding box regressor, and mask segmentation. Each of these modules contributes to the model's ability to detect and segment spheroids in images.\n\nThe feature extraction module processes input images to produce feature maps, which are then used by the RPN to identify regions of interest (ROIs) that likely contain objects. The ROI classifier and bounding box regressor further refine these regions, determining the class labels and precise locations of the objects. Finally, the mask segmentation module uses a fully convolutional network (FCN) to predict masks for each ROI, effectively segmenting individual spheroids from the background.\n\nOne clear example of the model's transparency is the use of intersection over union (IoU) as an evaluation metric. IoU measures the overlap between predicted and ground-truth regions, providing a quantitative assessment of the model's accuracy. This metric is crucial for understanding how well the model performs in segmenting spheroids, as it considers both the shape and size of the predicted regions.\n\nAdditionally, the model's architecture and training parameters are well-documented. For instance, the backbone of the model is a ResNet-50 feature pyramid network, and the model was initialized with weights from a pre-trained COCO instance segmentation model. These details allow researchers to understand the model's design and replicate the results if necessary.\n\nThe visualization module of our tool, SpheroScan, further enhances interpretability by providing various types of plots and statistical analyses. Users can generate bar plots, bubble plots, line plots, treemaps, and scatterplots to visualize the data, making it easier to interpret the results. These visualizations help researchers gain a deeper understanding of their spheroid data and the model's performance.\n\nIn summary, while Mask R-CNN is a complex deep learning model, it is not a black-box. Its modular architecture, use of interpretable metrics like IoU, and the availability of detailed training parameters and visualization tools make it a transparent and interpretable model for spheroid detection and segmentation.",
  "model/output": "The model is primarily designed for instance segmentation, which is a type of classification task at the pixel level. It identifies and distinguishes individual spheroids within an image, classifying each pixel as part of a spheroid or background. The output of the model includes masked images where spheroids are accurately segmented from the background. Additionally, a CSV file is generated containing the area and intensity data of the identified spheroids. This data can be used for further analysis and visualization. The model does not perform regression in the traditional sense but provides quantitative measurements (area and intensity) that can be used for statistical analysis and visualization.",
  "model/duration": "The execution time of the prediction module in our model is highly efficient. We analyzed the runtime complexity using four different image datasets of varying sizes. The results showed that it takes less than a second to mask an image. This efficiency is due to the linear runtime complexity of the prediction module, which means that the time required to process an image increases in proportion to the number of images being processed. This scalability ensures that the model can handle large datasets with ease. The runtime performance was evaluated on a Red Hat server equipped with 16 central processing unit cores and 64 GB of RAM, demonstrating the model's capability to manage extensive data efficiently.",
  "model/availability": "The source code for SpheroScan is publicly available and can be accessed via GitHub. This open-source web tool is designed to facilitate the analysis of spheroid images, offering both prediction and visualization modules. The prediction module utilizes pre-trained deep learning models to detect spheroids in input images, generating a CSV file with details such as the area, circularity, and intensity of each detected spheroid. The visualization module allows users to analyze these results through various plots and statistical analyses, which are ready for publication and can be saved as high-quality images in PNG format.\n\nThe tool is user-friendly and accessible to researchers regardless of their computational skills, aiming to simplify the process of analyzing spheroid images. It is efficient and scalable, capable of handling large datasets with ease. The runtime complexity of the prediction module is linear, ensuring that it scales proportionally to the size of the input data.\n\nFor those interested in utilizing SpheroScan, the source code and a detailed tutorial are available at the GitHub repository. This resource provides all the necessary information to run the algorithm effectively. Additionally, the tool is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.",
  "evaluation/method": "The evaluation of the deep learning model involved several steps to ensure its robustness and effectiveness. Two distinct models were trained using datasets from IncuCyte and microscope platforms. These models were then evaluated on separate validation and test datasets to assess their performance.\n\nTo quantify the model's accuracy in detecting and segmenting spheroids, the average precision (AP) metric was calculated for both bounding boxes and segmentation masks. This metric was evaluated over an intersection over union (IoU) range of 0.5 to 0.95, denoted as APbbox @ [0.5:0.95] for bounding boxes and APmmask @ [0.5:0.95] for segmentation masks. The results showed strong performance, with APbbox @ [0.5:0.95] and APmmask @ [0.5:0.95] values of 0.937 and 0.972, respectively, for the validation data of IncuCyte images, and 0.927 and 0.97 for the test data. Similarly, for microscopic images, the scores were 0.89 and 0.944 for the validation data and 0.899 and 0.977 for the test data.\n\nAdditionally, the applicability of the model was assessed using external datasets from previous studies. These datasets included images generated by different imaging platforms, various cell types, growth mediums, and lighting conditions. The model successfully detected spheroids in all tested datasets, demonstrating its adaptability and effectiveness in real-world scenarios.\n\nThe evaluation also included a runtime analysis, which showed that the prediction module could mask an image in less than a second. The runtime complexity was found to be linear, meaning the processing time increases proportionally with the number of images.\n\nOverall, the evaluation methods included a combination of standard metrics, external dataset testing, and runtime analysis to thoroughly assess the model's performance and practical applicability.",
  "evaluation/measure": "In our evaluation of the SpheroScan tool, we focused on several key performance metrics to ensure a comprehensive assessment of its effectiveness in detecting and segmenting spheroids in images. The primary metric we used is the Average Precision (AP), which is a widely recognized metric in the field of object detection and instance segmentation. AP is calculated as the area under the precision-recall curve, providing a single value that balances both precision and recall. This metric is particularly useful because it gives a holistic view of the model's performance across different thresholds.\n\nWe reported AP for both bounding boxes and segmentation masks over a range of Intersection over Union (IoU) thresholds from 0.5 to 0.95. Specifically, APbbox @ [0.5:0.95] represents the AP for bounding boxes, while APmmask @ [0.5:0.95] represents the AP for segmentation masks. These metrics were chosen because they are standard in the literature and provide a clear indication of the model's accuracy in detecting and segmenting spheroids.\n\nFor IncuCyte images, the APbbox @ [0.5:0.95] and APmmask @ [0.5:0.95] values were 0.937 and 0.972, respectively, for the validation data, and 0.927 and 0.97 for the test data. Similarly, for microscopic images, the scores were 0.89 and 0.944 for the validation data, and 0.899 and 0.977 for the test data. These high values indicate that our model performs well in both detection and segmentation tasks, demonstrating its robustness and effectiveness.\n\nIn addition to AP, we also considered other metrics such as precision and recall, which are fundamental to understanding the model's performance. Precision measures the proportion of true-positive detections among all positive detections, while recall measures the proportion of true-positive detections among all ground-truth objects. These metrics, along with IoU, provide a detailed evaluation of the model's accuracy and reliability.\n\nThe set of metrics we reported is representative of the current standards in the literature. Many state-of-the-art object detection algorithms, such as Faster R-CNN, Mask R-CNN, MobileNet SSD, and YOLO, as well as benchmark challenges like PASCAL VOC, use AP to evaluate their models. This ensures that our evaluation is comparable to other works in the field, providing a clear benchmark for the performance of SpheroScan.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "The evaluation of our method, SpheroScan, focuses on the average precision (AP) metric, which is a standard measure for assessing the performance of object detection and instance segmentation models. The AP is calculated as the area under the precision-recall curve, providing a comprehensive evaluation of a model's ability to achieve both high precision and high recall.\n\nIn our study, we report AP values for bounding boxes (APbbox) and segmentation masks (APmask) across a range of Intersection over Union (IoU) thresholds from 0.5 to 0.95. For instance, on IncuCyte images, the APbbox @ [0.5:0.95] was 0.937 for the validation dataset and 0.927 for the test dataset, while the APmask @ [0.5:0.95] was 0.972 for validation and 0.97 for testing. Similarly, for microscopic images, the APbbox @ [0.5:0.95] was 0.89 for validation and 0.899 for testing, and the APmask @ [0.5:0.95] was 0.944 for validation and 0.977 for testing.\n\nWhile we provide specific AP values, confidence intervals for these metrics are not explicitly stated in our publication. However, the consistency of our results across different datasets and the high AP values indicate strong performance and reliability. The statistical significance of our method's superiority over others and baselines is implied by these high AP values, but detailed statistical tests and confidence intervals would provide additional rigor. Future work could include a more thorough statistical analysis to quantify the confidence in these performance metrics.",
  "evaluation/availability": "The raw evaluation files for our study are publicly available. These include supporting data, trained model weights, and external test datasets. The supporting data can be accessed through Zenodo and the GigaScience Database. The trained model weights and external test datasets are also available on Zenodo. All these resources are distributed under the Creative Commons Attribution License, which allows for unrestricted reuse, distribution, and reproduction, provided the original work is properly cited. This ensures that other researchers can access and utilize these materials for their own studies, promoting transparency and reproducibility in scientific research."
}