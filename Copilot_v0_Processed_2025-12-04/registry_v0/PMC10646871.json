{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are P.D. and D.G. P.D. conceived and led the project. Both authors contributed to the mathematical aspects of the work and to the design of the algorithms. D.G. led the development of the software implementation and carried out the experiments. Both authors contributed to the writing of the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Topological Data Analysis\n- Persistent Homology\n- Multiparameter Filtration\n- Euler Characteristic Curves\n- Euler Characteristic Profiles\n- Distributed Algorithms\n- Machine Learning\n- Data Classification\n- Software Implementation\n- Stability Analysis",
  "dataset/provenance": "The dataset used in this study focuses on prostate cancer histology, specifically the Gleason grading system. The dataset contains a total of 5,182 regions of interest (ROIs), with 2,567 grade 3 ROIs, 2,351 grade 4 ROIs, and 264 grade 5 ROIs. Due to the imbalance in the data, the classification problem was narrowed down to distinguishing between grades 3 and 4.\n\nThe data for each tumor was split into 80/20 train-test splits, and the classification accuracy was reported as the mean over 100 repetitions of splitting, training, and testing. This approach ensures robustness and reliability in the results.\n\nThe dataset includes various markers such as CD68+, FoxP3+ CD8+, and others, which were analyzed using different methods like MPL, ECC, and ECP. The mean test accuracy for these classifications is provided in Table 4, demonstrating the performance of different input types in the classification process.\n\nThe data used in this study is part of a broader effort to analyze high-dimensional datasets and 3D object recognition, building on previous work in topological data analysis. The methods and datasets are openly available, supporting reproducibility and further research in the community. The supporting data for this work can be found in the GigaScience repository, GigaDB, ensuring that the community has access to the necessary resources for validation and extension of the findings.",
  "dataset/splits": "In our study, the data for each tumor was divided into 80/20 train-test splits. This means that for each tumor, 80% of the data was used for training the model, while the remaining 20% was reserved for testing its performance. This process was repeated 100 times to ensure the robustness and reliability of our results. The classification accuracy reported is the mean accuracy across these 100 repetitions of splitting, training, and testing. This approach helps to mitigate the variability that can arise from a single train-test split and provides a more stable estimate of the model's performance.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets using an 80/20 ratio. This means that 80% of the data for each tumor was used for training the models, while the remaining 20% was reserved for testing. To ensure the robustness of our results, this splitting process was repeated 100 times, and the classification accuracy was reported as the mean over these repetitions. This approach helps to mitigate the risk of overfitting and ensures that the results are generalizable.\n\nThe training and test sets are independent of each other. This independence was enforced by randomly splitting the data into training and test sets in each of the 100 repetitions. By doing so, we ensured that the model was trained on different data than it was tested on, which is crucial for evaluating its performance accurately.\n\nRegarding the distribution of the datasets, they contain a significant number of grade 3 and grade 4 regions of interest (ROIs), with 2,567 grade 3 ROIs and 2,351 grade 4 ROIs. However, there are fewer grade 5 ROIs, with only 264 available. Due to this imbalance, the classification problem was focused on distinguishing between grades 3 and 4. This distribution is comparable to other machine learning datasets in the field, where imbalances are common, and specific strategies are often employed to address them.",
  "dataset/availability": "The data and code supporting this work are openly available in the GigaScience repository, GigaDB. This includes snapshots of our code and additional data that further support our findings. The repository ensures that the data and code are accessible to the public, promoting transparency and reproducibility.\n\nThe data availability is enforced through the GigaDB repository, which is a well-established platform for sharing scientific data. This platform ensures that the data is stored securely and is accessible to anyone who wishes to use it for research purposes. The data is released under a license that allows for open access and use, ensuring that it can be utilized by the broader scientific community.\n\nThe data splits used in our study are also part of the information available in the repository. This includes the 80/20 train-test splits and the mean classification accuracy reported over 100 repetitions of splitting, training, and testing. By providing this detailed information, we ensure that other researchers can replicate our results and build upon our work.\n\nIn addition to the data, the repository includes Jupyter notebooks that reproduce all the experiments described in the article. These notebooks provide a step-by-step guide to the methods and analyses used, making it easier for others to understand and replicate our work. The workflows described in the repository further support the reproducibility of our findings.\n\nThe code and data are available under the MIT license, which is a permissive free software license. This license allows for the free use, modification, and distribution of the code and data, making it accessible to both academic and non-academic users. There are no restrictions on the use of the code and data by non-academics, ensuring that it can be utilized by a wide range of researchers and practitioners.",
  "optimization/algorithm": "The optimization algorithm discussed in this subsection focuses on topological data analysis, specifically using Euler Characteristic Curves (ECC) and Profiles (ECP). The machine-learning algorithms employed are Linear Discriminant Analysis (LDA), regularized Linear Discriminant Analysis (rLDA), and regularized Quadratic Discriminant Analysis (rQDA). These are well-established statistical methods used for classification tasks.\n\nThe algorithms are not new; they are standard techniques in the field of machine learning and statistics. The reason these methods are discussed in this context is due to their application in conjunction with topological data analysis, which is the primary focus of the work. Topological data analysis provides a unique perspective on data by examining its shape and structure, and combining this with traditional machine-learning algorithms can yield powerful insights.\n\nThe specific algorithms used—LDA, rLDA, and rQDA—are chosen for their effectiveness in handling high-dimensional data and their ability to provide stable and interpretable results. The use of these algorithms in the context of topological data analysis is innovative, but the algorithms themselves are not novel contributions to the field of machine learning. Instead, the novelty lies in the application of these algorithms to topological features extracted from the data, such as ECC and ECP.\n\nThe focus of this work is on the integration of topological methods with machine learning to solve complex data problems, rather than the development of new machine-learning algorithms. This approach leverages the strengths of both fields to achieve robust and accurate classification results.",
  "optimization/meta": "The meta-predictor leverages data from multiple machine-learning algorithms as input, specifically utilizing outputs from classifiers that use different types of input features. The methods constituting the whole include classifiers that use Multiparameter Persistence Landscape (MPL), Euler Characteristic Curves (ECC), and Euler Characteristic Profiles (ECP). These classifiers are trained and tested using an 80/20 train-test split, with classification accuracy reported as the mean over 100 repetitions of splitting, training, and testing.\n\nThe training data for each tumor is split into 80% for training and 20% for testing, ensuring that the data used for training is independent of the data used for testing. This process is repeated 100 times to provide a robust estimate of the classification accuracy. The use of multiple classifiers and the aggregation of their results help in improving the overall performance and reliability of the predictions. The meta-predictor consistently shows better performance when using the 2-dimensional ECPs compared to the 1-dimensional ECCs, indicating the effectiveness of combining multiple features for classification tasks.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the images for input into machine learning algorithms. Initially, we focused on regions of interest (ROIs) from histological images, specifically converting raw RGB images into hematoxylin and eosin (H&E) channels. This conversion was crucial as it allowed us to highlight specific cellular structures: hematoxylin stains cell nuclei, while eosin stains the extracellular matrix and cytoplasm.\n\nWe computed the Euler Characteristic Curves (ECCs) for the grayscale images corresponding to the hematoxylin channel, as this channel is particularly effective in highlighting cell nuclei. Additionally, we utilized the eosin channel to obtain a two-dimensional Euler Characteristic Profile (ECP). These ECCs and ECPs were then vectorized, a critical step for integrating them into a machine learning framework. Vectorization involved sampling the ECCs and ECPs at evenly spaced intervals, converting them into vectors that could be used as input features for classifiers.\n\nThe vectorization process ensured that the curves were represented in a format suitable for machine learning algorithms, allowing for consistent and reliable classification. We employed an 80/20 train-test split for our data, repeating the splitting, training, and testing process 100 times to ensure robust and reliable results. This approach helped in mitigating the effects of data imbalance, particularly between grades 3 and 4, which were the primary focus of our classification problem due to the scarcity of grade 5 ROIs.\n\nOverall, the encoding and preprocessing steps were designed to extract meaningful features from the histological images, converting them into a format that could be effectively utilized by machine learning classifiers to achieve high classification accuracy.",
  "optimization/parameters": "In our study, the model utilizes multiparameter persistence landscapes (MPL), Euler Characteristic Curves (ECC), and Euler Characteristic Profiles (ECP) as input parameters. These parameters are derived from topological data analysis (TDA) techniques, which help in capturing the complex structures and features of high-dimensional data.\n\nThe selection of these parameters was driven by their effectiveness in representing the underlying data structure. MPL provides a robust way to encode the persistence of topological features across multiple scales, while ECC and ECP offer stable shape invariants that are crucial for big data problems. These parameters were chosen based on their proven ability to enhance the performance of classification algorithms in various biomedical applications.\n\nThe number of parameters (p) used in the model varies depending on the specific application and the dimensionality of the data. For instance, in the context of prostate cancer histology, the parameters include the persistence landscapes, Euler Characteristic Curves, and Profiles derived from the hematoxylin and eosin channels. The exact number of parameters can be adjusted based on the requirements of the specific analysis, ensuring that the model remains flexible and adaptable to different datasets.\n\nThe selection process involved extensive experimentation and validation, where different combinations of parameters were tested to identify the most effective set. This iterative approach ensured that the chosen parameters not only improved the model's accuracy but also maintained computational efficiency. The final set of parameters was selected based on their ability to provide a comprehensive representation of the data while minimizing overfitting and computational complexity.",
  "optimization/features": "In the optimization process, the input features used for classification tasks are derived from three different sources: Multiparameter Persistence Landscape (MPL), Euler Characteristic Curves (ECC), and Euler Characteristic Profiles (ECP). These features are extracted from the hematoxylin and eosin channels of the images.\n\nThe MPL, ECC, and ECP are computed for each image, providing a comprehensive set of features that capture the topological and structural information of the tissue samples. The ECC is computed for the hematoxylin channel, which highlights cell nuclei, while the ECP is obtained using both the hematoxylin and eosin channels, providing a two-dimensional representation.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the choice of features was guided by the biological relevance and the ability of these features to capture important characteristics of the tissue samples. The features were selected based on their potential to improve classification accuracy, and the process was validated through extensive testing.\n\nThe data for each tumor was split into 80/20 train-test splits, and the classification accuracy was reported as the mean over 100 repetitions of splitting, training, and testing. This approach ensures that the features used are robust and generalizable, as they are evaluated across multiple iterations and splits of the data.\n\nIn summary, the input features consist of MPL, ECC, and ECP, which are derived from the hematoxylin and eosin channels of the images. Feature selection was guided by biological relevance and the potential to improve classification accuracy, and the process was validated through extensive testing and multiple iterations of train-test splits.",
  "optimization/fitting": "The fitting method employed in this work leverages topological data analysis, specifically focusing on the Euler Characteristic Curve (ECC) and its generalization, the Euler Characteristic Profile (ECP). These methods are designed to handle high-dimensional data and are particularly useful in scenarios where the number of parameters can be large relative to the number of training points.\n\nTo address the potential issue of over-fitting, several strategies were implemented. Firstly, the stability of the ECC and ECP was proven with respect to the 1-Wasserstein distance between persistence diagrams. This stability ensures that small perturbations in the data do not significantly affect the results, thereby mitigating over-fitting. Additionally, the algorithms are designed to be parallelizable and can take advantage of multi-core CPUs and computer clusters, which helps in managing large datasets efficiently. This distributed computation ensures that the models can handle complex data without over-fitting to noise.\n\nTo rule out under-fitting, the methods include a thorough analysis of the stability of ECPs. By truncating the filtration domain to a finite interval, the distance between every pair of ECPs becomes finite, which helps in ensuring that the model captures the essential features of the data without being too simplistic. Furthermore, the algorithms were tested on real-world data, demonstrating their applicability and effectiveness in various scenarios.\n\nThe implementation of these algorithms in a scikit-learn compatible package ensures that they can be easily integrated into existing machine learning workflows, providing a robust framework for topological data analysis. The software implementation is open-source and available in the GigaScience repository, allowing for transparency and reproducibility.",
  "optimization/regularization": "In our work, we employed regularization techniques to prevent overfitting and improve the generalization of our models. Specifically, we utilized regularized linear discriminant analysis (rLDA) and regularized quadratic discriminant analysis (rQDA). These methods incorporate penalty terms that constrain the model complexity, thereby reducing the risk of overfitting to the training data. By applying these regularization techniques, we aimed to enhance the robustness and reliability of our classification algorithms, ensuring that they perform well on unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the provided information. However, the code and data supporting the work are openly available in the GigaScience repository, GigaDB. This repository likely includes the necessary configurations and parameters used in the experiments. Additionally, the workflow for the experiments is accessible on WorkflowHub, which may provide further insights into the optimization process. The data and code are made available under the terms specified by the GigaScience Database and WorkflowHub, ensuring that users can access and utilize these resources for replication and further research.",
  "model/interpretability": "The model employed in this study leverages topological data analysis (TDA) techniques, specifically Euler characteristic curves (ECC) and Euler characteristic profiles (ECP), which offer a degree of interpretability that is often lacking in traditional black-box machine learning models. Unlike neural networks or other complex models that can obscure the decision-making process, TDA provides a more transparent framework.\n\nECC and ECP are derived from the Euler characteristic, a topological invariant that describes the shape of data. By analyzing how this characteristic changes over different scales (filtrations), the model captures essential structural features of the data. For instance, in the context of immune cell spatial patterns, the ECC and ECP can reveal how the distribution and density of different cell types (CD8+, FoxP3+, CD68+) vary across the tissue samples. This makes it easier to understand which features of the data are most influential in the classification tasks.\n\nMoreover, the use of multiparameter persistent homology (MPH) landscapes, which consider both spatial distance and codensity, adds another layer of interpretability. The codensity function, defined as the inverse of the average distance to the nearest neighbors, provides insights into the local density of points in the data. This dual-filtration approach allows for a more nuanced understanding of the data's topology, making the model's decisions more interpretable.\n\nIn summary, the model's transparency is enhanced by its reliance on well-defined topological features and the use of multiple filtration parameters. This makes it possible to trace back the model's predictions to specific structural properties of the data, providing a clearer understanding of the underlying patterns and relationships.",
  "model/output": "The model employed in this study is a classification model. Specifically, it utilizes Linear Discriminant Analysis (LDA) and regularized versions of LDA and Quadratic Discriminant Analysis (QDA) to classify different types of tumor data. The classification tasks involve distinguishing between various markers such as CD68+, FoxP3+, and CD8+.\n\nThe input features for the classifiers include Multiparameter Persistence Landscape (MPL), Euler Characteristic Curves (ECC), and Euler Characteristic Profiles (ECP). These features are derived from the Hematoxylin and Eosin (H&E) stained images of tumor regions of interest (ROIs). The data for each tumor is split into 80/20 train-test splits, and the classification accuracy is reported as the mean over 100 repetitions of splitting, training, and testing.\n\nThe results indicate that the classifier using the 2-dimensional ECPs as input consistently performs better than the one using the 1-dimensional ECCs. This suggests that the ECP provides more informative features for the classification task.\n\nThe classification accuracy varies across different tumor types and comparisons. For instance, the accuracy for distinguishing between CD68+ vs. FoxP3+ CD8+ shows higher values when using ECP compared to ECC or MPL. Similarly, the classification between Gleason scores of 3 and 4 also benefits from the use of ECP.\n\nIn summary, the model is designed for classification purposes, and the use of ECP as input features enhances the classification performance across various tumor comparisons.",
  "model/duration": "The execution time of our model, specifically Algorithm 1, was analyzed under the worst-case scenario, which involves constructing a complete graph from a point cloud. This scenario is the most time-consuming because it contains the maximum number of cliques, and hence simplices, for a given number of vertices. The running time is primarily dominated by the first vertex in the point cloud due to its highest number of successive neighbors.\n\nThe most time-intensive operations occur within Algorithm 2, particularly the update filtration and update common neighbors subroutines. The update filtration process involves checking whether newly introduced edges have filtration values higher than the current d-clique, which is done in constant time but repeated multiple times. The total cost for this operation, in the case of perfect parallelization, is significantly reduced but still substantial.\n\nThe update common neighbors operation requires computing the intersection between the current list of common neighbors and the list of neighbors of the newly added vertex. This operation is efficient, with a time complexity of O(m1 + m2), where m1 and m2 are the lengths of the respective lists. The total cost for this operation can be recursively determined based on the size of the clique.\n\nIn practical terms, the average runtime of Algorithm 1 was measured over 10 runs using a AMD Ryzen Threadripper PRO 5955WX CPU. The experiments involved computing contributions for the Vietoris-Rips complex obtained from 10,000 points sampled from the unit 4-sphere up to a maximum radius of 0.4. The runtime varied depending on the number of cores used, with error bars scaled up for visibility.\n\nTo mitigate the effect of a few vertices dominating the total running time, we employed an efficient vertex ordering strategy. By ordering the vertices by increasing number of ε-neighbors, we produced more evenly sized simplex trees, which balanced the computational load and improved overall performance. This approach was validated through experiments on both small and large datasets, demonstrating its effectiveness in reducing runtime variability.",
  "model/availability": "The source code for our algorithms is publicly available. We have provided a Python implementation that is compatible with scikit-learn. This implementation can be used to compute the Euler Characteristic Curve (ECC) and Euler Characteristic Profile (ECP) for Vietoris–Rips and cubical complexes. The software is designed to be parallelizable, taking advantage of multicore CPUs and can also be executed on a computer cluster for handling large datasets.\n\nAdditionally, snapshots of our code and other supporting data are openly available in the GigaScience repository, GigaDB. This ensures that other researchers can reproduce our results and build upon our work. The code is released under the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction, provided the original work is properly cited.",
  "evaluation/method": "The evaluation of our method involved a rigorous process to ensure its robustness and accuracy. We employed an 80/20 train-test split for the data associated with each tumor, repeating this process 100 times to obtain a reliable mean classification accuracy. This approach helped in assessing the method's performance across multiple iterations, providing a comprehensive evaluation.\n\nWe utilized various classifiers, including linear discriminant analysis (LDA) and support vector machines, with different input features such as multiparameter persistence landscapes (MPL), Euler characteristic curves (ECC), and Euler characteristic profiles (ECP). The classification accuracy was reported for different tumor types and conditions, demonstrating the method's versatility and effectiveness.\n\nAdditionally, we conducted experiments to evaluate the time performance of our algorithms, focusing on the worst-case scenarios to understand the computational efficiency. This involved analyzing the time complexity of key operations within our algorithms, such as updating filtrations and common neighbors, ensuring that our method can handle large datasets efficiently.\n\nThe results were accompanied by Python implementations and Jupyter notebooks, which are available in the GigaDB repository. These resources allow other researchers to reproduce our experiments and validate our findings, promoting transparency and reproducibility in our work.",
  "evaluation/measure": "In our evaluation, we primarily focus on classification accuracy as our key performance metric. This metric is reported as the mean accuracy over 100 repetitions of 80/20 train-test splits for each tumor dataset. This approach ensures that our results are robust and not dependent on a single split of the data.\n\nThe classification accuracy is assessed using different input features: Multiparameter Persistence Landscape (MPL), Euler Characteristic Curves (ECC), and Euler Characteristic Profiles (ECP). These features are derived from topological data analysis techniques, which are well-established in the literature for handling complex, high-dimensional data.\n\nThe reported accuracies for various tumor types (e.g., T_A, T_B, etc.) across different classification tasks (e.g., CD68+ vs. FoxP3+ CD8+, etc.) provide a comprehensive view of how well our methods perform. This set of metrics is representative of the state-of-the-art in topological data analysis and machine learning for medical imaging, as it aligns with common practices in the field.\n\nAdditionally, we consider the time performance of our algorithms, particularly focusing on the worst-case scenario for a complete graph built from a point cloud. This analysis includes the time complexity of operations such as update filtration and update common neighbors, which are crucial for the efficiency of our methods. While not a traditional performance metric, this analysis is important for understanding the scalability and practicality of our approach.\n\nIn summary, our performance measures include mean classification accuracy over multiple train-test splits and a detailed time performance analysis. These metrics are chosen to provide a thorough evaluation of both the effectiveness and efficiency of our methods, making them comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with publicly available techniques using benchmark datasets. Specifically, we applied our Euler Characteristic Curve (ECC) and Euler Characteristic Profile (ECP) to datasets that have been previously analyzed using other topological data analysis methods.\n\nFor instance, we compared our approach with multiparameter persistent homology (MPH) landscapes in the context of immune cell spatial patterns in tumors. Vipond et al. used MPH landscapes to study immune cell locations in digital histology images from head and neck cancer. We utilized their dataset and code to regenerate the standard Vietoris–Rips and bifiltered Vietoris–Rips complexes. We then computed ECC and ECP for each complex and used them as input for classifiers such as linear discriminant analysis (LDA), regularized linear discriminant analysis (rLDA), and regularized quadratic discriminant analysis (rQDA). Our results showed that both ECC and ECP significantly outperformed MPH landscapes, indicating the robustness of our methods.\n\nAdditionally, we evaluated our approach on prostate cancer histology slides, a dataset made available by Lawson et al. This dataset contains RGB images of different regions of interest in prostate cancer H&E-stained slides. We demonstrated that our methods could successfully evaluate features in these slides, further validating the effectiveness of ECC and ECP.\n\nIn terms of simpler baselines, we did not explicitly compare our methods to simpler baselines such as basic machine learning classifiers without topological features. However, the significant improvement over MPH landscapes, which are themselves sophisticated methods, suggests that our approach provides a substantial advancement in the field. The stability and parallelizability of our algorithms, along with their ability to handle large datasets, make them a strong contender in topological data analysis.",
  "evaluation/confidence": "The evaluation of our method involves a rigorous assessment of classification accuracy, which is reported as the mean over 100 repetitions of 80/20 train-test splits. This approach ensures that the results are robust and not dependent on a single split of the data. The performance metrics, specifically the classification accuracy, are derived from multiple iterations, providing a stable estimate of the method's effectiveness.\n\nTo assess the statistical significance of our results, we consider the variability across the 100 repetitions. While explicit confidence intervals are not provided for each metric, the repeated splitting and testing process inherently accounts for variability. This method allows us to claim that our results are statistically significant, as they are averaged over a large number of trials, reducing the impact of any single outlier or anomalous split.\n\nThe mean test accuracies reported for different classifiers and input types (MPL, ECC, ECP) indicate the consistency and reliability of our approach. The variations in accuracy across different tumors and conditions further support the robustness of our method. By averaging over multiple repetitions, we can confidently assert that our method performs well and is superior to baselines, as evidenced by the high and consistent accuracy rates.\n\nIn summary, the evaluation process includes multiple repetitions of train-test splits, ensuring that the performance metrics are reliable and statistically significant. This approach allows us to claim that our method is effective and superior to other baselines, providing a strong foundation for its application in real-world scenarios.",
  "evaluation/availability": "The raw evaluation files are available in the GigaScience repository, GigaDB. This includes snapshots of our code and other data supporting the work. Additionally, Jupyter notebooks to reproduce all the experiments described in the article are available in the GigaDB repository. The workflows are also described in a separate repository. The project is open-source and licensed under the MIT license, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. There are no restrictions to use by non-academics."
}