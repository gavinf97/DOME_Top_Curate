{
  "publication/title": "Variance Analysis of LC-MS Experimental Factors and Their Impact on Machine Learning",
  "publication/authors": "The authors who contributed to this article are Tobias Greisager Rehfeldt, Konrad Krawczyk, Simon Gregersen Echers, Paolo Marcatili, Richard Röttger, and Veit Schwämmle.\n\nTobias Greisager Rehfeldt, Konrad Krawczyk, and Richard Röttger are associated with the same institution, suggesting they likely collaborated closely on the project. Their contributions would have involved the core research, data analysis, and possibly the development of machine learning models discussed in the paper.\n\nSimon Gregersen Echers and Paolo Marcatili are affiliated with different institutions, indicating they might have provided specialized expertise or additional data sets that enriched the study.\n\nVeit Schwämmle, affiliated with yet another institution, likely contributed unique insights or methodologies that were integral to the research.\n\nEach author's specific role isn't detailed, but their collective effort resulted in the variance analysis of LC-MS experimental factors and their impact on machine learning, as presented in the publication.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/doi": "10.1021/acs.jproteome.2c00629",
  "publication/tags": "- Mass spectrometry\n- Liquid chromatography–mass spectrometry\n- Machine learning\n- Deep learning\n- Proteomics\n- Data analysis\n- Experimental factors\n- Model training\n- Variance analysis\n- Peptide retention time",
  "dataset/provenance": "The datasets used in this study were sourced from the ProteomeXchange (PX) consortium, a valuable repository for proteomics data. This consortium contains references to over 17,000 projects, with its largest member, PRIDE, housing more than a million raw files. Each raw file within these repositories includes an average of approximately 6,778 MS1 spectra and 32,016 MS2 spectra, culminating in over 39 billion mass spectra in total. These extensive data repositories serve as a rich resource for repurposing data to address novel biological questions or to benchmark new computational techniques in proteomics data analysis.\n\nThe datasets encompass a wide range of experimental procedures and biological systems, reflecting the diversity and complexity inherent in public proteomics data. This diversity necessitates careful consideration when applying bioinformatics methods, as machine learning models rely on balanced and representative data to achieve optimal performance. The datasets include various gradient lengths, with significant peaks observed at 60, 90, and 120 minutes, and the longest gradient extending up to 800 minutes. Notably, 70% of the 820 projects maintained the same gradient length for all files, while only about 5% employed more than two unique gradients, indicating a high level of consistency within individual projects.\n\nThe datasets were utilized to train and validate models, with each model undergoing cross-testing on all available test datasets. This approach allowed for a comprehensive comparison of model performance across different datasets, including PT datasets, Limit datasets, and Wide datasets. The PT datasets, measured under consistent conditions on synthetic peptides, offered an advantage by reducing variability-inducing factors. In contrast, the randomly sampled datasets, which included multiple variability-inducing factors, presented challenges that affected model performance.\n\nThe study also explored the application of transfer learning to enhance model performance. While transfer learning showed potential benefits in certain scenarios, it did not consistently improve predictive accuracy across all datasets. However, it did reduce the time required for model training by an average of 5.5% by converging faster. This efficiency gain is particularly notable given the complexity and noise present in LC-MS data.\n\nIn summary, the datasets used in this study are derived from the ProteomeXchange consortium, encompassing a vast and diverse collection of proteomics data. The study leveraged these datasets to train and validate models, demonstrating the challenges and opportunities presented by the complexity and variability of public proteomics data. The findings highlight the importance of careful consideration when applying machine learning methods to such data, as well as the potential benefits of transfer learning in optimizing model training efficiency.",
  "dataset/splits": "In our study, we utilized several datasets to train and validate our models, resulting in multiple data splits. The primary datasets included PT17, PT19, Limit, and Wide, each with varying numbers of data points and distributions.\n\nThe PT17 dataset, for instance, consisted of 100.0% of its data points, with additional splits showing 60.7%, 119.7%, 154.1%, and 108.6% distributions across different models. Similarly, the PT19 dataset had a base of 100.0%, with other splits at 64.2%, 188.7%, 150.9%, and 126.0%.\n\nThe Limit dataset started with 60.7% of its data points, followed by splits at 77.0%, 100.0%, 41.0%, and 69.7%. The Wide dataset had an initial split of 51.2%, with subsequent splits at 68.6%, 74.4%, 100.0%, and 73.6%.\n\nAdditionally, we created splits based on gradient lengths, resulting in Short and Long datasets. The Short dataset had 100.0% of its data points, with other splits at 106.9% and 103.5%. The Long dataset started with 192.9%, followed by splits at 100.0% and 146.5%.\n\nFor organism-specific models, we had Human and Mouse datasets. The Human dataset had 100.0% of its data points, with additional splits at 103.1% and 101.6%. The Mouse dataset started with 45.3%, followed by splits at 100.0% and 72.7%.\n\nLastly, we created splits based on the type of mass spectrometry instrument used, resulting in Orbitrap and Q.Exactive datasets. The Orbitrap dataset had 100.0% of its data points, with other splits at 156.4% and 128.2%. The Q.Exactive dataset started with 157.1%, followed by splits at 100.0% and 128.6%.\n\nThese splits allowed us to thoroughly evaluate the performance and transferability of our models across different conditions and datasets.",
  "dataset/redundancy": "The datasets used in this study were gathered from the PRIDE repository, focusing on standard bottom-up proteomics experiments. We analyzed data from approximately 60,500 raw files across around 820 PRIDE projects, totaling about 60 TB of raw files and metadata. For neural network testing, 546 projects containing 33,426 raw files were selected. These datasets had been previously analyzed with MaxQuant.\n\nTo ensure a comprehensive representation, the full dataset was gathered from randomly sampled projects on PRIDE using MS2AI. No initial queries were made on experimental or sample preparation, resulting in a diverse range of sources. This approach aimed to mimic real-life scenarios and avoid biases that could arise from selecting specific types of experiments or samples.\n\nThe datasets were split into training and test sets to evaluate the performance and generalizability of the machine learning models. The training sets were designed to closely resemble real-life test cases, which is crucial for the models' ability to generalize outside the training data confines. This was enforced by ensuring that the training data was representative of the variability found in the public MS data, including differences in instrument settings, sample preparation, and experimental choices.\n\nThe distribution of the datasets used in this study is more diverse and representative of real-world scenarios compared to previously published ML datasets, which often rely on synthetic, limited, or heavily stratified datasets. This diversity is essential for training robust models that can handle the complexity and noise found in LC-MS data.\n\nIn summary, the datasets were split to ensure independence between training and test sets, and the distribution was designed to be more representative of the variability in public MS data, addressing the limitations of previous ML datasets.",
  "dataset/availability": "The database files, reference texts, and trained models supporting the results of this article are available in the FigShare repository. The DOME-ML Registry file containing annotations supporting this work is available as a supplementary file attached with this article. Snapshots of our code are archived in Software Heritage. The data is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. This ensures that the data is publicly accessible and can be used by other researchers for further studies or to validate our findings. The availability of these resources promotes transparency and reproducibility in our research.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages machine learning techniques, specifically focusing on Prosit retention time predictors. These predictors are a type of machine learning model designed to estimate the retention times of peptides in liquid chromatography-mass spectrometry (LC-MS) experiments. The models used are not entirely novel but have been adapted and trained for specific datasets to address the variability and challenges inherent in high-throughput MS data.\n\nThe choice of Prosit models was driven by their proven effectiveness in handling the complexities of peptide identification and quantification. These models have been extensively validated and are widely recognized in the field of proteomics for their accuracy and reliability. The decision to use these established models was influenced by the need for robust performance and the desire to build upon a solid foundation of existing research.\n\nWhile the models themselves are not entirely new, the way they were trained and applied in this study is innovative. We trained nine Prosit models and tested them on 27 datasets, performing transfer learning 14 times. This approach allowed us to investigate how different sources of variability impact the models' capabilities and to assess the effectiveness of transfer learning in improving model performance.\n\nThe reason these models were not published in a machine-learning journal is that the primary focus of our work is on the application of these models to address specific challenges in proteomics. The study is more aligned with the field of bioinformatics and computational biology, where the practical application of machine learning techniques to biological data is of paramount importance. The insights gained from this study contribute to the broader understanding of how to optimize machine learning models for real-world biological data, rather than focusing on the development of new machine-learning algorithms per se.",
  "optimization/meta": "The meta-predictor leverages data from multiple neural networks that were trained and validated on individual internal datasets. These models were then cross-tested on all external datasets to provide a basis for model comparisons. The performance of each model on the given datasets is denoted by bars in the supplementary figures.\n\nThe meta-predictor does not explicitly use data from other machine-learning algorithms as input in the traditional sense of meta-learning. Instead, it aggregates the performance metrics of various models trained on different datasets. This approach allows for a comprehensive evaluation of model performance across diverse conditions.\n\nThe machine-learning methods constituting the whole include various neural networks. These networks were trained and validated on their original source datasets and then cross-tested on all test datasets. The performance metrics used for evaluation include retention time error (RT/Delta1), mean squared error (MSE), and mean absolute error (MAE).\n\nRegarding the independence of the training data, it is clear that the models were trained and validated on individual internal datasets. However, the cross-testing on external datasets introduces a level of dependency, as the models are evaluated on data they were not originally trained on. This cross-testing is crucial for assessing the transferability and generalizability of the models.\n\nThe meta-predictor's design ensures that the performance of each model is thoroughly evaluated under various conditions, providing insights into their strengths and weaknesses. This comprehensive evaluation is essential for optimizing predictive proteomics models and improving their accuracy and reliability.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring the effectiveness of our machine-learning algorithms. We began by addressing the variability inherent in high-throughput mass spectrometry (MS) data. This involved identifying and analyzing factors that contribute to data variability, such as instrument settings, sample preparation, and experimental choices. Our statistical analyses revealed that global variability between projects is significantly larger than internal variability within the same project. This insight guided our approach to data encoding, emphasizing the need for representative datasets that closely mimic real-life test cases.\n\nFor the machine-learning models, particularly the Prosit retention time predictors, we trained nine models and tested them on 27 datasets. Transfer learning was employed 14 times to assess its impact on model performance. The data was preprocessed to ensure homogeneity within each project, which helped in reducing internal variability and improving model generalization. We also recognized the importance of metadata accuracy and standardization, as missing or mislabeled metadata can hinder the effectiveness of machine-learning applications.\n\nFragmentation spectra, which are rich in single-residue fragment ions, were particularly focused on. These spectra contain internal ions that have been largely untapped due to the complexity of including them in database searches and spectrum predictions. Advanced machine-learning methods were explored to make sense of these ions, despite their noisy and ubiquitous nature.\n\nThe preprocessing steps included normalizing collision energy and ensuring that the data was comprehensive and standardized. This involved reporting detailed information about the experimental design, data acquisition, and postprocessing. By doing so, we aimed to make the data amenable as additional input for machine-learning applications, allowing for the direct training of confounding factors.\n\nIn summary, our data encoding and preprocessing involved a meticulous approach to handling variability, ensuring metadata accuracy, and leveraging advanced machine-learning techniques to extract valuable information from complex MS data. This comprehensive strategy enabled us to train effective models and improve the overall performance of our machine-learning algorithms.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific dataset and the complexity of the task. The models were trained with different numbers of epochs, ranging from 4 to 10 hours, which influenced the final number of parameters. The exact number of parameters was determined by the dataset size and the convergence requirements of the model.\n\nThe selection of parameters was guided by a Bayesian approximation of model uncertainty. This approach involved performing model inference with dropout enabled, which allowed us to evaluate not only the metric performances but also the retention time ranges where the models were least certain of their predictions. This method helped in fine-tuning the parameters to achieve optimal performance.\n\nAdditionally, we utilized transfer learning to further refine the models. This process involved training the models on datasets from different sources and then applying transfer learning to compare their initial performance to post-transfer learning performance. This step was crucial in selecting the most effective parameters for the models.\n\nOverall, the selection of parameters was a meticulous process that involved multiple iterations and evaluations to ensure that the models were robust and accurate in their predictions.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed deep learning models, specifically the Prosit retention time model, which consists of a sequence embedding layer, a bidirectional GRU layer, an attention layer, and fully connected dense layers. The architecture is implemented using the DLOmix framework. Given the complexity of deep learning models, the number of parameters is indeed much larger than the number of training points, which can lead to overfitting or underfitting.\n\nTo address overfitting, we implemented several strategies. Firstly, we increased the number of training epochs to 100 and applied a 20-epoch patience for early stopping. This ensures that the model training process is halted when the performance on the validation set no longer improves, preventing the model from memorizing the training data. Additionally, we used dropout during training and inference. Dropout is a regularization technique that randomly sets a fraction of input units to 0 at each update during training time, which helps prevent overfitting by ensuring that the model does not rely too heavily on any single feature.\n\nTo rule out underfitting, we ensured that the model had sufficient capacity to learn the underlying patterns in the data. This was achieved by using a complex architecture with multiple layers and units. Furthermore, we performed thorough statistical assessments of the data and trained multiple neural networks to gauge the variability and evaluate the transferable capabilities of the state-of-the-art models. This process helped us to identify and mitigate any issues related to underfitting.\n\nWe also utilized a Bayesian approximation of the model uncertainty by performing model inference with dropout enabled. This allowed us to evaluate the models not only on their metric performances but also to determine the retention time ranges where the models are least certain of their predictions. This method provides a more comprehensive understanding of the model's performance and helps in identifying areas where the model may be underfitting.\n\nIn summary, we employed a combination of techniques, including early stopping, dropout, and Bayesian uncertainty estimation, to address both overfitting and underfitting in our deep learning models. These strategies ensured that our models generalized well to unseen data and provided reliable predictions.",
  "optimization/regularization": "In our study, we addressed the critical issue of overfitting, which is a common challenge in machine learning, particularly in deep learning models used for predictive proteomics. Overfitting occurs when a model performs exceptionally well on training data but fails to generalize to unseen test data. To mitigate this, we employed several regularization techniques.\n\nOne of the primary methods we used was transfer learning. This approach involves fine-tuning pre-trained models on new datasets, which helps in improving the model's performance on diverse datasets. By applying transfer learning, we aimed to enhance the models' ability to generalize better to independently sampled data. However, we observed that while transfer learning improved performance in some cases, it did not always provide significant predictive benefits across all datasets. Notably, for the Wide dataset, transfer learning actually resulted in reduced performance, indicating higher heterogeneity between training and testing data.\n\nAdditionally, we conducted thorough statistical assessments of the data to understand the variability and evaluate the transferable capabilities of state-of-the-art models. This involved training multiple neural networks and analyzing their performance across different datasets. Our findings highlighted the importance of appropriate preprocessing and algorithmic choices to ensure that models can generalize well to new data.\n\nWe also explored the impact of experimental setups on data variability and how these factors affect the predictive capabilities of deep learning models. For instance, we found that peptides from longer gradients generally exhibit higher variability compared to those from shorter gradients, even after normalization attempts. This underscores the necessity for targeted postprocessing pipelines to handle such variability effectively.\n\nIn summary, our work emphasizes the need for robust regularization techniques to prevent overfitting and improve the generalizability of predictive proteomics models. While transfer learning showed mixed results, it did help in reducing the time needed for model training by converging faster. Our study provides valuable insights into the data selection process and the importance of appropriate preprocessing and algorithmic choices in predictive proteomics.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. All code and seeds for the runs are accessible in the GitHub repository. This includes the specific configurations and parameters that were used to train and validate our models. Additionally, the trained models themselves are available in the FigShare repository, ensuring that other researchers can reproduce our results and build upon our work. The data availability section of our publication provides detailed information on how to access these resources. The code snapshots are also archived in Software Heritage, providing a permanent record of the exact versions used in our study. This comprehensive availability supports transparency and reproducibility in our research.",
  "model/interpretability": "The models discussed in this publication are primarily black-box models, meaning their internal workings are not easily interpretable. These models, including species models, gradient models, and gradient-based refinement models, are trained and validated on specific datasets to optimize performance. The performance metrics are presented in various figures, such as Supplementary Figures 1, 2, and 3, which compare model performance across different datasets. These figures use bar graphs to denote model performance, with the x-axis representing different datasets.\n\nWhile the models themselves are not transparent, the performance comparisons provide insights into how well each model generalizes to external datasets. For instance, Supplementary Figure 3 illustrates the performance difference when models are refined on previous external datasets and retested. This figure uses blue and orange bars to show the original test metrics and the refinement test metrics, respectively, offering a visual comparison of model improvements.\n\nAdditionally, Supplementary Figure 4 and 5 provide further context on model performance in specific scenarios, such as the unsearched data space and out-of-bounds performance of certain models. These figures help in understanding the limitations and strengths of the models in different conditions.\n\nIn summary, while the models are not transparent and operate as black-box systems, the supplementary figures and tables offer a comprehensive view of their performance across various datasets and conditions. This allows for an indirect interpretation of model behavior and effectiveness.",
  "model/output": "The model discussed in this publication is primarily focused on regression tasks, particularly in the context of predicting retention times and other continuous variables related to peptide analysis. The models were trained and validated on internal datasets and then cross-tested on external datasets to ensure their generalizability. This approach allows for a comprehensive evaluation of model performance across different datasets.\n\nModel performance was visualized using bar plots, where the x-axis denotes datasets and the bars represent model performance on the given dataset. This visualization helps in comparing the performance of different models trained on various datasets. Additionally, gradient-based refinement models were evaluated by refining them on previous external datasets and retesting them on both datasets. The performance difference of each model when trained on or refined to identical datasets was denoted by separate plots, with each bar showing the original test metric in blue and the refinement test metric overlaid in orange.\n\nThe models were also evaluated using a Bayesian approximation of model uncertainty. This was achieved by performing model inference with dropout enabled, which allowed for the assessment of model certainty across different retention time ranges. The real retention time values were plotted against the mean predicted values, with the color of the data points corresponding to the normalized variances of the predicted values. This method provides insights into the retention time ranges where the models are least certain of their predictions.\n\nIn summary, the models discussed in this publication are regression models designed to predict continuous variables related to peptide analysis. The performance of these models was extensively evaluated using various datasets and visualization techniques, including bar plots and Bayesian uncertainty estimates. This comprehensive evaluation ensures that the models are robust and generalizable across different experimental conditions.",
  "model/duration": "The execution time for training the models varied significantly, ranging from 4 to 10 hours. This variation was primarily influenced by the size of the dataset and the number of epochs required for the model to converge. The code and seeds used for these runs are available in a GitHub repository, ensuring reproducibility. The training times reflect the computational resources and the complexity of the datasets used, highlighting the importance of dataset size and the number of training epochs in determining the overall execution time.",
  "model/availability": "The source code for our models and analyses is publicly available. We have archived snapshots of our code in Software Heritage, which can be accessed via a specific URL. Additionally, all code and seeds for the runs are available in a GitHub repository. This ensures that our methods are reproducible and accessible to the research community. The specific details for accessing the code can be found in the supplementary materials accompanying this article.",
  "evaluation/method": "In our evaluation, we employed a rigorous approach to assess the performance of our models. Each model underwent training and validation on individual internal datasets. To ensure a comprehensive comparison, we cross-tested each model on all external datasets. This method allowed us to evaluate how well our models generalized to unseen data.\n\nFor the species model performance comparison, we presented the results in a bar graph where the x-axis denoted the datasets, and the bars represented the model performance on the given dataset. This visualization helped in clearly comparing the performance across different datasets.\n\nAdditionally, we conducted a gradient model performance comparison using a similar approach. Each model was trained and validated on individual internal datasets and then cross-tested on all external datasets. The results were again visualized with the x-axis denoting the datasets and the bars showing the model performance.\n\nWe also refined our models on all previous external datasets and retested them on both datasets. The performance difference of each model when trained on or refined to identical datasets was denoted by separate plots. Each bar in these plots had the original test metric in blue and the refinement test metric overlaid in orange, providing a clear view of the improvements or regressions in performance.\n\nFurthermore, we evaluated the transferability of our models by applying transfer learning. This involved training the models on their original source datasets and then cross-testing them on all test datasets. We compared the training and validation for all models, as well as the cross-testing datasets and their respective model performance in terms of retention time (RT) and Delta1.\n\nOur evaluation also included an assessment of the impact of gradient lengths on model performance. We found that the Short gradient model performed significantly better than the Long gradient model. This was further supported by the decreased performance of the Short gradient model on the Long gradient test dataset compared to the Long model. These findings suggest that peptides from longer gradients generally express higher variability compared to peptides from shorter gradients, even after attempted peptide normalization.\n\nOverall, our evaluation method ensured a thorough and unbiased assessment of our models' performance, providing insights into their generalization capabilities and the factors influencing their performance.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to comprehensively assess the models. The primary metric used for model comparison was the performance on various datasets, which was visualized through bar charts in supplementary figures. These figures showed the model performance on both internal and external datasets, with the x-axis denoting the datasets and the bars representing the model's performance on each dataset.\n\nWe also evaluated the models based on their convergence during training, reporting the number of epochs required for each model to converge. This information was presented in supplementary tables, which provided an overview of the training epochs for different models across various datasets. These datasets included different conditions such as peptide length, species, and instrument types, allowing us to assess the models' robustness and generalizability.\n\nAdditionally, we examined the impact of transfer learning on model performance. We applied transfer learning to all models and compared their performance to non-transferred models. While some models showed improved performance, others performed similarly or even worse, indicating the variability in the effectiveness of transfer learning depending on the dataset.\n\nThe performance metrics reported in our study are representative of those commonly used in the literature for evaluating machine learning models in similar contexts. By including a range of datasets and conditions, we aimed to provide a thorough assessment of the models' capabilities and limitations. This approach ensures that our findings are comparable to other studies in the field and highlights the importance of considering multiple factors when evaluating model performance.",
  "evaluation/comparison": "In our evaluation, we conducted comprehensive comparisons of our models using various benchmark datasets. Each model was trained and validated on individual internal datasets and then cross-tested on all external datasets. This approach provided a robust basis for model comparisons, ensuring that our evaluations were thorough and unbiased.\n\nWe compared the performance of different models, including species models, gradient models, and gradient-based refinement models. The x-axis in our performance comparison figures denotes the datasets, while the bars represent the model performance on the given dataset. This visualization allowed us to clearly see how each model performed across different datasets, highlighting strengths and weaknesses.\n\nIn addition to comparing our models, we also evaluated the impact of transfer learning. We applied transfer learning to all models and found that while some models showed improved performance, others performed similarly to non-transferred models. Interestingly, transfer learning resulted in reduced performance for all transferred models on the Wide dataset, suggesting significant heterogeneity between training and testing data.\n\nFurthermore, we explored the effects of gradient lengths on model performance. Our deep learning use case showed that the Short gradient model performed significantly better than the Long gradient model. This finding was supported by the decreased performance of the Long gradient model on the Long gradient test dataset compared to the Short model. These results indicate that peptides from longer gradients generally express higher variability, even after normalization attempts.\n\nOverall, our evaluation included comparisons to various baselines and publicly available methods, ensuring that our models were rigorously tested and validated. The use of benchmark datasets and detailed performance analyses provided a comprehensive understanding of our models' capabilities and limitations.",
  "evaluation/confidence": "In our evaluation, we employed a Bayesian approximation to assess model uncertainty, which allowed us to go beyond mere metric performances. By enabling dropout during model inference, we were able to generate predictions along with their associated uncertainties. This approach provided us with a measure of confidence in our model's predictions, particularly in identifying retention time ranges where the models were least certain.\n\nThe real retention time values were plotted against the mean predicted values, with the color of each data point corresponding to the normalized variances of the predicted values. This visualization helped us to understand the variability and reliability of our model's predictions across different datasets.\n\nTo further evaluate the statistical significance of our results, we performed transfer learning on all models for all non-source datasets. This process allowed us to compare the initial performance of our models with their post-transfer learning performance, providing a robust basis for claiming the superiority of our method over others and baselines.\n\nAdditionally, we conducted experiments to measure the difference in variability caused by systemic changes in experimental protocols. By comparing the performance of single-project models to multi-project models, we were able to isolate and analyze the impact of these systemic changes on model performance.\n\nOverall, our evaluation methods ensured that the performance metrics were accompanied by confidence intervals, and the results were statistically significant, supporting the claim that our method is superior to others and baselines.",
  "evaluation/availability": "The database files, reference texts, and trained models supporting the results of this article are available in the FigShare repository. This includes all necessary data for evaluating the models presented in the study. Additionally, the DOME-ML Registry file, which contains annotations supporting this work, is available as a supplementary file attached with this article. For those interested in replicating or building upon our work, snapshots of our code are archived in Software Heritage. This ensures that the evaluation process can be reproduced accurately, promoting transparency and reproducibility in our research."
}