{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Heidi J. Imker: Project conceptualization and planning, manual data curation, development of preliminary code for data analysis, project oversight and administration, data validation, and writing of the manuscript.\n- Kenneth E. Schackart, III: Manual data curation, implementation of machine learning methodology, design and implementation of code for processing and augmenting predicted resources, automation of pipelines, unit testing and static code checks, data analysis, creation of data visualizations and figures, data validation, and writing of the manuscript.\n- Ana-Maria Istrate: Design of the machine learning methodology, implementation of code for training, prediction, and evaluation of the NLP models used to classify articles and extract individual resources, and writing of the manuscript.\n- Charles E. Cook: Project conceptualization and planning, manual data curation, funding acquisition, project oversight, data validation, and writing of the manuscript.",
  "publication/journal": "PLOS ONE",
  "publication/year": "2023",
  "publication/doi": "10.1371/journal.pone.0294812",
  "publication/tags": "- Data repositories\n- Bioinformatics\n- Machine learning\n- Natural language processing\n- Data inventory\n- Article classification\n- Named entity recognition\n- Data curation\n- Open science\n- Reproducibility\n- Data analysis\n- Scientific literature\n- Data mining\n- Information retrieval\n- Data management",
  "dataset/provenance": "The dataset for this project was sourced from open public data accessible via APIs, ensuring that all project data are freely distributable and that the inventory can be updated programmatically in the future. The primary source was the Europe PMC API, which was accessed using the Python requests library to gather English-language articles that potentially describe a biodata resource. The query for Europe PMC was iteratively developed to enrich the corpus with relevant articles. Initially, various biodata resource-related terms and degrees of complexity were tested, and the number and quality of results were manually inspected. Some terms, like \"repository,\" were ruled out due to a high number of false positives. Limiting the results to Open Access articles was considered but deemed overly restrictive. The final query string was designed to locate abstracts containing a URL and focused keywords that suggested the articles described a data resource, while excluding retracted articles and those with general URLs.\n\nAdditionally, data from the re3data.org and FAIRsharing APIs were used to benchmark the resulting inventory. These APIs provided licensed data for reuse. The Wayback Machine URLs and geo coordinates were retrieved via the Internet Archive Wayback Availability, ipinfo, and ip-api APIs.\n\nThe training dataset for the machine learning models was created in two phases. The first phase contained 638 records, and the second phase added 996 records, resulting in a total of 1634 records. Article titles and abstracts were independently reviewed by two curators in each phase. The final dataset included articles that were manually reviewed and classified as describing biodata resources.\n\nThe dataset has not been used in previous papers by the community, as this is the first publication detailing this specific inventory of biodata resources. The inventory aims to provide a comprehensive and updatable resource for the scientific community, leveraging open data and advanced natural language processing techniques.",
  "dataset/splits": "For the article classification task, the dataset was divided into three splits: training, validation, and testing. The training set consisted of 1110 articles, the validation set had 238 articles, and the test set contained 239 articles. Within these splits, the distribution of positive and negative labels varied. The training set had 337 positive labels and 773 negative labels. The validation set included 61 positive labels and 177 negative labels. The test set comprised 80 positive labels and 159 negative labels.\n\nFor the Named Entity Recognition (NER) task, the dataset was also split into three parts: training, validation, and testing. The training set included 306 articles with 1192 biodata resource mentions. The validation set had 66 articles with 269 biodata resource mentions, and the test set contained 66 articles with 293 biodata resource mentions.\n\nThese splits were used to fine-tune and evaluate the models, ensuring that the performance could be assessed on both seen and unseen data. The training sets were used to fine-tune the models, the validation sets were used to compare and select the best-performing models, and the test sets were used to evaluate the models' performance on unseen data.",
  "dataset/redundancy": "The datasets were split into training, validation, and test sets to ensure independence and to evaluate the model's performance on unseen data. The training set was used to train the machine learning models, the validation set was used to tune hyperparameters and select the best-performing model, and the test set was used to report the final performance metrics.\n\nTo enforce independence, we ensured that the test set was not used during any stage of model training or selection. This was crucial to prevent data leakage and to provide an unbiased evaluation of the model's performance. The test set was set aside before any model training began, and it was only used at the very end to assess the final model.\n\nThe distribution of the datasets was designed to be representative of the overall data, similar to previously published machine learning datasets. However, due to the nature of the data collection process, there were some articles in the test set that had minimal impact on the model training and selection process. These articles were openly disclosed to maintain transparency.\n\nThe decision to use precision rather than F1-score for article classification model selection was made based on the validation set performance. This decision was confidently made due to the very low portion of test set articles in the evaluation, ensuring that the test set remained a representative sample for model evaluation.\n\nIn summary, the datasets were carefully split and managed to ensure independence and representativeness, aligning with best practices in machine learning model evaluation.",
  "dataset/availability": "The data used in this study is publicly available and has been released in multiple forums to ensure accessibility and reproducibility. The primary location for the data is on GitHub, where a living version of the dataset is maintained. This allows for continuous updates and improvements. Additionally, archived versions of the data are available on Zenodo, providing a stable and citable snapshot of the dataset at specific points in time.\n\nThe data includes various types of information, such as the results of queries to Europe PMC, manually reviewed inventory, and training data for machine learning models. Each dataset is accompanied by a license that permits reuse and redistribution, adhering to open science principles. The specific licenses and terms of use can be found in the repository documentation on GitHub and Zenodo.\n\nTo enforce the public availability and proper use of the data, we have implemented several measures. First, all data files are version-controlled on GitHub, ensuring that any changes are tracked and documented. Second, the use of Zenodo for archiving provides a permanent and citable link to the dataset, which is essential for academic referencing and reproducibility. Third, the data is accompanied by detailed documentation, including README files and protocols, which guide users on how to access, use, and cite the data.\n\nFurthermore, the data is structured in a way that allows for easy integration with other tools and workflows. For example, the pipeline developed for this project can accommodate user-provided queries, making it flexible for different use cases. This ensures that the data can be used not only for the specific purposes outlined in the study but also for a broader range of applications in the scientific community.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages machine learning techniques to enhance the classification and named entity recognition (NER) tasks. Specifically, we utilized a model that incorporates a linear classification layer, which is a common approach in many machine learning pipelines. This choice was driven by its effectiveness in handling the complexities of our dataset and its compatibility with our overall methodology.\n\nThe algorithm itself is not entirely novel but represents an adaptation and optimization of existing techniques tailored to our specific use case. The decision to use this particular approach was based on its proven track record in similar applications and its ability to achieve high precision, which was a critical factor in our model selection process.\n\nRegarding the publication venue, our focus was on demonstrating the practical application of machine learning methods in the context of biodata resource inventory. While the algorithmic innovations might not be groundbreaking in the field of machine learning, their application to this specific problem domain is novel and contributes significantly to the field of bioinformatics. Therefore, publishing in a bioinformatics journal allowed us to highlight the real-world impact and utility of our work, rather than focusing solely on the algorithmic details.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it relies on individual machine learning models for specific tasks. For article classification, the Huggingface’s AutoModelForSequenceClassification module was used to fine-tune BERT models. For the Named Entity Recognition (NER) task, the AutoModelForTokenClassification module was employed. These models were trained independently, with the best-performing model on the validation set, as determined by the F1 score, being selected for downstream tasks. The training data for these models was kept independent to ensure robust evaluation and to prevent data leakage, which was assessed and reported. The models were fine-tuned using the Adam optimizer and evaluated using partial-match entity-level metrics computed with the seqeval module. The training process involved a maximum of 10 epochs, and the models were implemented using Python and various third-party packages.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps to ensure optimal performance. Initially, the title and abstract of each article were concatenated into a single string, with a space character separating the two fields. This concatenation allowed the model to consider both the title and abstract for classification.\n\nXML tags were removed from the concatenated string using regular expressions. To maintain proper formatting, whitespace was added after punctuation if it was not already present following the removal of tags. This step ensured that the text was clean and ready for tokenization.\n\nThe cleaned input string was then tokenized using a pre-trained tokenizer associated with the specific pre-trained model used for classification. This tokenization process converted the text into a format that the model could understand and process effectively.\n\nThe tokenized input was subsequently passed through the pre-trained model module to obtain context embeddings. These embeddings captured the semantic meaning of the text and were then fed into a linear classification layer. This layer performed binary classification, determining whether an article described a biodata resource or not.\n\nThe entire process was designed to leverage the strengths of pre-trained models and ensure that the input data was properly encoded and preprocessed for accurate classification. This approach allowed for efficient and effective classification of articles based on their titles and abstracts.",
  "optimization/parameters": "The models utilized in this study were fine-tuned using the Huggingface framework, specifically the BERT model architectures. The exact number of parameters (p) in the models can vary depending on the specific BERT variant used. For instance, BERT-base typically has around 110 million parameters, while BERT-large has approximately 340 million parameters. The choice of model architecture and thus the number of parameters was influenced by the need to balance computational efficiency and model performance.\n\nThe selection of the model architecture and consequently the number of parameters was guided by the task requirements and the available computational resources. The models were fine-tuned on the article classification and named entity recognition (NER) tasks, with the goal of achieving high precision and minimizing false positives. The final selection of the model with the highest precision on the validation set ensured that the chosen architecture was optimal for the specific tasks at hand. Additionally, the models were trained for a maximum of 10 epochs, and the checkpoint with the highest precision was saved for further use. This approach ensured that the models were well-tuned to the data and capable of producing reliable results.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study utilized machine learning models, specifically BERT architectures, which are known for their high capacity and large number of parameters. Given the complexity of the models and the nature of the tasks (article classification and named entity recognition), the number of parameters in our models is indeed much larger than the number of training points.\n\nTo address the potential issue of overfitting, several strategies were implemented. Firstly, we used a validation set to monitor the performance of the models during training. The model checkpoint with the highest F1 score on the validation set was saved and used for downstream tasks, ensuring that the model generalized well to unseen data. Additionally, we employed early stopping, limiting the training to a maximum of 10 epochs, which helped prevent the model from overfitting to the training data.\n\nTo rule out underfitting, we conducted a mid-project manual evaluation. This evaluation involved assessing the predictions on a 10% random sample of the articles to determine the correctness of the article classifications and NER extracted terms. The results of this evaluation confirmed that the machine learning models were performing well in practice, indicating that the models were not underfitting.\n\nFurthermore, the use of precision as the primary metric for model selection in the article classification task ensured that the models were not overly simplistic, as precision focuses on the correctness of positive predictions. This approach helped to balance the trade-off between bias and variance, ensuring that the models were neither overfitting nor underfitting.\n\nIn summary, the fitting method involved careful monitoring of model performance on a validation set, early stopping to prevent overfitting, and a mid-project manual evaluation to confirm the practical performance of the models. These strategies collectively ensured that the models were neither overfitting nor underfitting the data.",
  "optimization/regularization": "The optimization process for our models included several techniques to prevent overfitting. We trained our models for a maximum of 10 epochs, which helped in avoiding overfitting by limiting the number of times the model saw the training data. Additionally, we utilized early stopping based on the validation set performance, saving the model checkpoint with the highest F1 score. This ensured that the model was selected based on its performance on unseen data, rather than just memorizing the training data.\n\nWe also employed dropout layers within our model architectures, which randomly set a fraction of input units to zero at each update during training time. This technique helps prevent overfitting by ensuring that the model does not rely too heavily on any single neuron.\n\nFurthermore, we used a validation set to tune hyperparameters and select the best model, which is a standard practice to ensure that the model generalizes well to new data. The validation set was used to monitor the model's performance and make decisions about when to stop training to prevent overfitting.\n\nIn summary, our regularization methods included limiting the number of training epochs, using early stopping, incorporating dropout layers, and leveraging a validation set for model selection and hyperparameter tuning. These techniques collectively helped in mitigating overfitting and ensuring that our models performed well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used for model fine-tuning are available and have been reported. These configurations include details such as batch sizes, learning rates, and weight decay values for various models like BERT, BioBERT, BioELECTRA, and others. The specific hyper-parameters used for each model can be found in a table that outlines these settings.\n\nThe model files and optimization parameters are also accessible. The models fine-tuned for tasks such as article classification and named entity recognition (NER) are available on the Hugging Face Hub. This platform provides access to the trained model checkpoints, which can be used for further experimentation or deployment.\n\nAdditionally, the configurations and code used for training and predicting are available on GitHub. This includes Python scripts, Snakemake workflows, and IPython notebooks that detail the steps involved in training the models and generating predictions. These resources are part of an open science initiative, ensuring that the methods and results are reproducible.\n\nThe data used for training, including the article classification training data and NER training data, is also available on GitHub. This data is crucial for understanding how the models were trained and for replicating the experiments.\n\nAll these resources are licensed under open science principles, making them freely available for use and further development. This includes the data, code, and model checkpoints, which are hosted on platforms like GitHub, Zenodo, and Hugging Face. The open access to these resources ensures transparency and reproducibility in the research process.",
  "model/interpretability": "The model employed in this publication is not a black box. We have taken steps to ensure transparency and interpretability throughout our work. For instance, we explicitly state the use of a linear classification layer in our figures, which helps in understanding the decision-making process of the model. Additionally, we have provided detailed documentation and code, including configurations for model fine-tuning and training predictions, which are publicly available. This allows other researchers to replicate our results and understand the inner workings of our model. Furthermore, we have discussed the interplay between the two tasks—article classification and Named Entity Recognition (NER)—in the discussion section, highlighting how errors might propagate between tasks. This discussion provides insights into the model's behavior and potential limitations. We have also included examples and basic analyses to illustrate the differences in selection criteria, further enhancing the interpretability of our model.",
  "model/output": "The model discussed in the publication is primarily focused on classification tasks. Specifically, it involves article classification and named entity recognition (NER). For article classification, the model is designed to categorize articles based on their titles and abstracts. The classification task uses a sequence classification layer to predict the appropriate labels for the articles. The model architectures evaluated include various BERT-based models, such as BioMed-RoBERTa, SciBERT, BioBERT, and others, which were fine-tuned for this specific task.\n\nIn addition to article classification, the model also performs named entity recognition (NER) to extract resource names from the articles. For NER, the model uses a token classification layer to identify and classify tokens corresponding to common and full names. The input data for NER is tagged using a BIO scheme, which helps in distinguishing between the beginning, inside, and outside of named entities.\n\nThe performance of the models was evaluated using metrics such as precision, recall, and F1-score on both validation and test sets. The model with the highest precision on the validation set was selected for further use, as precision was deemed crucial for reducing false positives and ensuring the overall quality of the inventory.\n\nDuring the mid-project evaluation, it was determined that preferencing precision would improve the inventory's quality, allowing for the finalization of the inventory without manually validating every resource included. This approach was particularly important for the article classification task, where high precision was essential for minimizing false positives.\n\nThe models were trained for a maximum of 10 epochs, and the checkpoint with the highest precision was saved for each model. The Huggingface framework was utilized for loading and fine-tuning the BERT model architectures, with specific modules used for sequence classification and token classification tasks. The training process involved using the Adam optimizer and the seqeval module for computing partial match entity-level metrics.\n\nIn summary, the model is a classification model designed for article classification and named entity recognition tasks. The focus on precision in the classification task ensures high-quality outputs, making it suitable for creating an inventory of biodata resources from scientific literature.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our project is publicly available and has been released to facilitate reproducibility and further development. All code and data are archived in Zenodo, ensuring long-term accessibility and version control. The specific archive can be found at https://zenodo.org/doi/10.5281/zenodo.10105161.\n\nIn addition to Zenodo, the code is hosted on GitHub, providing a living repository where updates and improvements are continuously integrated. The GitHub repository can be accessed at https://github.com/globalbiodata/inventory_2022. This repository includes various components such as Python scripts and modules, Snakemake workflows, and Jupyter notebooks for reproducing the original results and updating the inventory.\n\nFor those who prefer a more interactive environment, we have also provided a Google Colab protocol, which can be found at https://www.protocols.io/view/set-up-biodata-resource-inventory-in-google-colab-5jyl89o36v2w/v1. This protocol allows users to set up and run the inventory in a cloud-based Jupyter notebook environment, making it accessible even without local computational resources.\n\nThe code is released under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided that the original authors and source are credited. This license ensures that the community can build upon our work while giving appropriate credit.\n\nFurthermore, we have provided detailed documentation to guide users through the setup and execution of the code. This includes a README file in the GitHub repository, an open science implementation plan, a curation guide for selective review, and use case articles. These resources are designed to make the software as accessible and user-friendly as possible.",
  "evaluation/method": "The evaluation method for this work involved a combination of automated metrics and manual evaluations to ensure the robustness and accuracy of the models used.\n\nInitially, the models were evaluated using standard metrics such as Precision, Recall, and F1 scores. The training datasets were divided into 70/15/15 splits for training, validation, and testing, respectively. Models were fine-tuned and evaluated using these same splits. The best-performing models on the validation set were selected for final implementation. For the article classification task, precision was prioritized, while for the Named Entity Recognition (NER) task, the F1 score was used as the primary metric.\n\nIn addition to these automated evaluations, manual evaluations were conducted at two key points in the project. The first evaluation occurred midway through the project, where a 10% random sample of preliminary results was manually reviewed. This evaluation assessed the viability of the machine learning approach and identified areas for improvement. The second evaluation was performed prior to finalizing the inventory, focusing on resources with low probabilities and suspected duplicates.\n\nDuring the mid-project evaluation, 468 articles were manually reviewed, with a precision of 0.938 for correct classifications. The evaluation also examined the NER extraction results, finding that common names were correctly predicted in 93.0% of cases, while full names were correctly predicted in 61.8% of cases. These results indicated that the method was viable for both classifying articles and predicting resource names.\n\nThe final inventory contained 3112 resources, and the evaluation process highlighted the strengths and limitations of the approach. The interplay between the article classification and NER tasks was also discussed, noting that high precision in article classification minimized the risk of error propagation. The evaluation method ensured that the inventory was accurate and reliable, providing a valuable resource for further analysis and use.",
  "evaluation/measure": "In our evaluation, we report several key performance metrics to assess the effectiveness of our models. For both the article classification and named entity recognition (NER) tasks, we present the F1-score, precision, and recall for both the validation and test sets. These metrics are widely used in the literature and provide a comprehensive view of model performance.\n\nThe F1-score is the harmonic mean of precision and recall, offering a balance between the two. Precision measures the accuracy of the positive predictions made by the model, while recall indicates the model's ability to identify all relevant instances. By reporting these metrics, we ensure that our evaluation is thorough and representative of standard practices in the field.\n\nFor the article classification task, we prioritize precision due to the critical nature of false positives in this context. This choice is motivated by the need to minimize errors in identifying relevant articles, even if it means potentially missing some true positives. The reported metrics reflect this focus, with precision often being the highest among the three metrics.\n\nIn the NER task, we also report the same set of metrics, ensuring consistency across our evaluations. The F1-score is particularly important here as it provides a single value that balances precision and recall, which is crucial for tasks where both false positives and false negatives are costly.\n\nOur selection of these metrics aligns with common practices in machine learning and natural language processing evaluations. This approach allows for a clear and comparable assessment of our models' performance, both within our study and against other works in the literature.",
  "evaluation/comparison": "A comparison to publicly available methods was not explicitly performed on benchmark datasets. Instead, the evaluation focused on comparing the developed methods to other data inventory organizations, such as re3data and FAIRsharing. This comparison aimed to understand the overlap and differences between the inventories, acknowledging that these inventories do not include all repositories. The methods section now includes details about the different possibilities with this comparison, such as what a large or small overlap might mean.\n\nRegarding simpler baselines, there is no direct mention of comparing the developed methods to simpler baselines. The evaluation primarily centered on the performance of various models, such as BioMed-RoBERTa-RCT1, BioMed-RoBERTa, and others, for tasks like article classification and named entity recognition (NER). The model selection was based on the validation set to avoid biasing the results with the test set. The precision was chosen over F1 score for article classification due to the specific needs of the task, with more details added to the introduction and relevant sections to explain this choice. The interplay between the two tasks, article classification and NER, was also discussed, noting that high precision in article classification helps minimize error propagation.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are available through several platforms. For living, up-to-date versions, they can be accessed on GitHub. Specifically, the data and code used for evaluations are hosted in the globalbiodata/inventory_2022 repository. This includes various datasets such as the ePMC query output, manually reviewed inventory, and final inventory, among others. Additionally, the Python scripts and modules, Snakemake workflows, and iPython notebooks for reproducing results and updating the inventory are also available on GitHub.\n\nFor archived versions, the files are available on Zenodo. Each dataset and code module has a corresponding DOI on Zenodo, ensuring that the versions used for specific evaluations are permanently accessible. The datasets and code are released under licenses that allow for open access and reuse, facilitating transparency and reproducibility in the evaluation process."
}