{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Yihe Pang and Bing Liu.\n\nYihe Pang was involved in the methodology, software development, validation, formal analysis, data curation, and writing of the original draft. Additionally, Pang contributed to the review and editing of the manuscript.\n\nBing Liu contributed to the conceptualization, resource acquisition, review and editing of the manuscript, supervision, project administration, and funding acquisition. Both authors read and approved the final manuscript.",
  "publication/journal": "BMC Biology",
  "publication/year": "2024",
  "publication/doi": "10.1186/s12915-023-01803-y",
  "publication/tags": "- Intrinsically disordered proteins\n- Cell signaling\n- Protein prediction\n- Machine learning\n- Bioinformatics\n- Protein structure\n- Computational biology\n- Data analysis\n- Protein function\n- Molecular biology",
  "dataset/provenance": "The datasets utilized in this study are publicly accessible and have been made available to ensure reproducibility and transparency. All benchmark datasets used, along with the prediction results from the evaluated methods, can be found in the Zenodo repository. The specific link for accessing these datasets is provided for convenience. Additionally, the source code for DisoFLAG, which includes detailed descriptions, is available on GitHub and has been archived on Zenodo. This ensures that other researchers can reproduce our findings and build upon our work. The data values used in the figures are also provided in the supplementary information files, specifically in Additional file 2. This comprehensive approach allows for thorough verification and further exploration by the scientific community.",
  "dataset/splits": "In our study, we utilized three main datasets for training, validation, and independent testing. The dataset was initially composed of 925 sequences, which were clustered using the CD-HIT algorithm with a 25% sequence similarity threshold. These clusters were then randomly divided into five subsets.\n\nThree of these subsets, comprising 589 sequences, were designated as the training dataset. This dataset includes a total of 36,399 residues, with specific distributions for various functional classes: 4,579 for protein binding, 1,503 for DNA binding, 5,298 for RNA binding, 2,668 for ion binding, 5,249 for lipid binding, and 1,212 for flexible linkers.\n\nOne subset, containing 148 sequences with 9,798 residues, was used as the validation dataset. The distribution within this dataset is as follows: 303 residues for protein binding, 886 for DNA binding, 446 for RNA binding, 657 for ion binding, 1,212 for lipid binding, and 1,212 for flexible linkers.\n\nThe remaining subset, consisting of 188 sequences with 9,606 residues, served as the independent test dataset, referred to as DP93. This dataset includes 539 residues for protein binding, 1,430 for DNA binding, 667 for RNA binding, 1,530 for ion binding, 2,029 for lipid binding, and 2,029 for flexible linkers.\n\nAdditionally, we collected another independent test dataset, DP94, which contains 98 sequences with 6,283 residues. The distribution in DP94 is: 195 residues for protein binding, 250 for DNA binding, 711 for RNA binding, 602 for ion binding, and 3,172 for flexible linkers.",
  "dataset/redundancy": "The datasets were split into training, validation, and independent test sets. To ensure high-quality data, sequences with unannotated disordered structures in their functional regions were removed. Additionally, a sequence that was excessively long was excluded. This process resulted in a total of 925 sequences.\n\nTo mitigate redundancy, the sequences were clustered using the CD-HIT algorithm with a 25% sequence similarity threshold. These clusters were then randomly divided into five subsets. Three of these subsets, comprising 589 sequences, were designated as the training dataset. One subset, with 148 sequences, served as the validation dataset. The remaining subset, containing 188 sequences, was used as the independent test dataset, referred to as DP93.\n\nTo further evaluate the performance of the proposed predictor, an additional independent test dataset, DP94, was collected. This dataset included 98 sequences from newly updated proteins in versions 9.3 to 9.4 of the DisProt database, following the same protocol as previously described.\n\nThe distribution of sequences across different functional classes in each dataset was carefully balanced to ensure that the training and test sets were independent and representative. This approach aligns with protocols used in previous studies, ensuring that the datasets are comparable to those used in other machine learning publications in the field.",
  "dataset/availability": "All data generated or analyzed during this study are included in this published article, its supplementary information files, and publicly available repositories. The benchmark datasets used in this study and the prediction results of the methods involved in the evaluation are available in the Zenodo repository. The source code and its descriptions of DisoFLAG are reproducible in the GitHub repository, which is archived on Zenodo. The data values for the figures are provided in the Additional file 2. The web server of DisoFLAG can be accessed from a specific URL.",
  "optimization/algorithm": "The optimization algorithm employed in our study is a hybrid approach that combines enumerated machine learning techniques. This method is not entirely new, as it builds upon existing machine learning algorithms, but it is novel in its specific application and combination of techniques.\n\nThe algorithm was not published in a machine-learning journal because the primary focus of our research is on its application in bioinformatics, specifically in predicting Anti-CRISPR proteins. The innovation lies in the way the algorithm is optimized and applied to this particular biological problem, rather than in the creation of an entirely new machine-learning algorithm.\n\nThe choice of this hybrid approach was driven by the need to improve the accuracy and efficiency of predicting Anti-CRISPR proteins, which is a critical area of research in understanding bacterial resistance mechanisms. The algorithm's performance was evaluated using standard metrics such as Matthews correlation coefficient, balanced accuracy, true positive rate, true negative rate, positive predictive value, and coverage of predictions, ensuring its reliability and effectiveness in the given context.",
  "optimization/meta": "In the \"Meta-predictor\" subsection, it is important to clarify that our model, DisoFLAG, does not function as a traditional meta-predictor. Instead of relying on the outputs of other machine-learning algorithms as input, DisoFLAG is designed to directly analyze sequence data to predict disordered protein functions.\n\nDisoFLAG does not integrate predictions from various machine-learning methods to form a composite model. Rather, it leverages its own sophisticated architecture and training regime to achieve high performance in predicting disordered protein functions.\n\nRegarding the independence of training data, since DisoFLAG is not a meta-predictor, the issue of ensuring that training data is independent across different constituent models does not arise. Our focus has been on ensuring that the training data used for DisoFLAG is diverse and representative, covering a wide range of disordered protein functions to enhance the model's generalizability and accuracy.",
  "optimization/encoding": "In our study, we employed two primary methods for data encoding: Position-Specific Scoring Matrix (PSSM) encoding and Protein Language Model (PLM) encoding. The choice between these methods depended on the Multiple Sequence Alignment (MSA) depth, which refers to the number of homologous sequences available.\n\nWhen the MSA depth was relatively small, PSSM encoding proved to be more effective. PSSM is a probabilistic statistical model that captures sequence conservation information, making it well-suited for scenarios with fewer homologous sequences. This method encodes the likelihood of each amino acid appearing at each position in the sequence, providing a robust representation of sequence features under low MSA depth conditions.\n\nOn the other hand, when a sufficient number of homologous sequences were available, PLM encoding demonstrated superior performance. PLMs are data-driven deep learning methods that can accurately capture the contextual semantic information of protein sequences. This approach leverages the wealth of data to learn complex patterns and relationships within the sequences, enhancing the predictive power of our machine-learning algorithms.\n\nAdditionally, we utilized a graph-based interaction unit (GiU) to establish correlations among multiple disordered functions. This unit significantly boosted the predictive performance by capturing semantic correlation features. The GiU's ability to model interactions between different functions allowed for the accurate prediction of multifunctional residues, which can perform two or more different functions.\n\nTo pre-process the data, we ensured that the sequences were properly aligned and that the encoding methods were applied consistently across all samples. This involved standardizing the input format and handling any missing or incomplete data to maintain the integrity of the dataset. The pre-processed data was then fed into our machine-learning models, which were trained and validated using established protocols to ensure robust and reliable performance.",
  "optimization/parameters": "In the optimization of our model, the number of trainable variables and hyper-parameters is carefully designed to balance complexity and performance. The model incorporates several key components, each contributing to the overall parameter count.\n\nThe Bi-GRU layer utilizes residue-wise embeddings with dimensions of 1024, and the attention-based GRU layer further processes these embeddings with weight variables totaling 2048. The feature mapping layer, which is fully connected, has weight and bias variables amounting to 2048×1024 and 1024, respectively. The graph-based interaction unit includes a weighted adjacency matrix and functional semantic representation vectors, with dimensions of 6×6 and 6×1024. The GCN layer, which aggregates semantic features, has weight and bias variables of 1024×128 and 128. The max pooling layer shapes the kernel and stride to 6×1. Finally, the fully connected layers, which output the disordered semantic features, have weight and bias variables of 128×1 and 1.\n\nThe selection of these parameters was guided by extensive experimentation and validation on diverse datasets. The dimensions of the embeddings and the number of layers were chosen to capture the intricate relationships within the data while avoiding overfitting. Hyper-parameters such as the dropout rate, batch size, and learning rate were tuned through cross-validation to ensure optimal performance. The dropout rate is set to 0.3 to prevent overfitting, the batch size is 16 to balance computational efficiency and model stability, and the learning rate is 0.0005 to facilitate effective gradient descent. The maximum sequence length is set to 128 to accommodate the typical length of sequences in our datasets. The number of epochs is set to 50, providing sufficient iterations for the model to converge without excessive training time.",
  "optimization/features": "In the optimization of our model, we utilized a variety of input features to enhance the predictive performance. Specifically, we employed four distinct types of features: ProtT5, ProtBERT, PSSM, and one-hot encodings. These features were selected based on their proven effectiveness in capturing relevant information for predicting disorder and disordered functions in proteins.\n\nThe feature selection process was meticulously designed to ensure robustness and generalizability. We performed feature selection using only the training set, adhering to best practices to prevent data leakage and maintain the integrity of our validation and test sets. This approach allowed us to identify the most informative features without biasing the model's performance on unseen data.\n\nThe number of features used as input varies depending on the specific representation. For instance, ProtT5 and ProtBERT provide contextual semantic encodings for sequences, while PSSM and one-hot encodings offer residue-wise and positional information, respectively. The exact number of features (f) can be determined by the dimensionality of these representations, but generally, they contribute to a comprehensive feature set that supports the model's predictive capabilities.",
  "optimization/fitting": "In the \"Fitting Method\" subsection, it is important to address the balance between the number of parameters and the number of training points to ensure the model generalizes well.\n\nThe model in question has a considerable number of trainable variables and hyper-parameters. For instance, the attention-based GRU layer alone has weight variables for calculating attention scores, attention scores between residues, attention weights, and contextual vectors, all contributing to a large parameter count. Additionally, the graph-based interaction unit and the fully connected layers further increase the number of parameters.\n\nGiven the complexity and the number of parameters, over-fitting could be a concern. To mitigate this, several techniques were employed. A dropout rate of 0.3 was applied in each layer to prevent the model from becoming too reliant on specific neurons. This helps in regularizing the model and reducing over-fitting. Additionally, the model was trained for 50 epochs with a batch size of 16, which ensures that the model sees a diverse set of training examples multiple times. The learning rate was set to 0.0005, which is a relatively small value, helping to prevent the model from converging too quickly to a suboptimal solution.\n\nTo rule out under-fitting, the model architecture was designed to be sufficiently complex to capture the intricacies of the data. The use of residual connections in the PLM embedding and the attention-based GRU layer allows the model to learn complex representations. The graph-based interaction unit and the GCN layer further enhance the model's ability to capture relationships between different functions. The activation function used is the sigmoid, which helps in introducing non-linearity, enabling the model to learn more complex patterns.\n\nThe training process involved resampling half of the test dataset 20 times and using the two-sided paired t-test for each pair of disordered function predictors. This rigorous evaluation ensures that the model's performance is not due to chance and that it generalizes well to unseen data. The statistical significance of differences in predictive performance was calculated, and p-values were reported, providing a clear indication of the model's robustness.\n\nIn summary, the model's architecture and training process were carefully designed to balance the number of parameters and the number of training points. Techniques such as dropout, appropriate learning rate, and rigorous evaluation methods were used to ensure that the model neither over-fits nor under-fits the data.",
  "optimization/regularization": "In our study, we implemented several regularization methods to prevent overfitting and ensure the robustness of our models. One of the key techniques we employed was dropout, which randomly sets a fraction of input units to zero at each update during training time. This helps to prevent units from co-adapting too much, thereby improving the model's generalization ability.\n\nAdditionally, we utilized L2 regularization, also known as weight decay, which adds a penalty equal to the squared magnitude of coefficients to the loss function. This encourages the model to keep the weights small, reducing the risk of overfitting.\n\nWe also incorporated early stopping, a method that monitors the model's performance on a validation set and stops training when the performance stops improving. This prevents the model from learning noise in the training data.\n\nFurthermore, we experimented with data augmentation techniques to artificially increase the size and diversity of our training dataset. This helps the model to generalize better to unseen data.\n\nLastly, we ensured that our models were evaluated using appropriate metrics such as the Matthews correlation coefficient (MCC), balanced accuracy (BACC), true positive rate (TPR), true negative rate (TNR), positive predictive value (PPV), and coverage of predictions (C). These metrics provided a comprehensive evaluation of our models' performance and helped us to tune the regularization parameters effectively.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the model are reported in detail. These include the number of trainable variables and hyper-parameters, such as the maximum sequence length, dropout rate, number of epochs, batch size, and learning rate. The model architecture is also described, including layers like the Bi-GRU layer, attention-based GRU layer, feature mapping layer, graph-based interaction unit, GCN layer, max pooling, and fully connected layers. Each layer's parameters and their explanations are provided, ensuring reproducibility.\n\nThe specific values for these hyper-parameters are clearly stated. For instance, the maximum sequence length is set to 128, the dropout rate in each layer is 0.3, the number of epochs is 50, the batch size is 16, and the learning rate is 0.0005. These details are essential for anyone looking to replicate the experiments or build upon the work.\n\nRegarding the availability of model files and optimization parameters, it is not explicitly mentioned where these files can be accessed or under what license they are provided. Therefore, it is not possible to confirm the availability of these resources at this time.",
  "model/interpretability": "The model employed in this study is not a blackbox. To enhance interpretability, layer-wise relevance propagation (LRP) was utilized. This technique helps to investigate the contributions of functional correlations to the prediction results. The LRP score is calculated using relevance scores from current and previous layers, along with constraint parameters, weights, bias, and hidden vectors. This approach allows for the determination of the importance of functional correlations for each function.\n\nThe importance scores of functional correlations to the propensity score were calculated by summing the relevance scores of all true-positive propensity predictions on the DP93 test dataset. This method provides insights into which functional correlations are most significant in the model's predictions.\n\nAdditionally, the Integrated Gradients (IG) values were calculated on the DP93 dataset, reflecting the contributions of different features to the model's outputs. Visualizations, such as the IG matrix, were generated to illustrate these contributions for various disordered functions. These visualizations aid in understanding how different features influence the model's decisions, thereby increasing transparency.\n\nIn summary, the use of LRP and IG values, along with their visualizations, ensures that the model is not a blackbox. These techniques provide clear examples of how functional correlations and features contribute to the model's predictions, making the decision-making process more interpretable.",
  "model/output": "The model, DisoFLAG, is designed for classification tasks. It predicts whether a protein residue is disordered or ordered, and if disordered, it predicts the specific disordered functions. The output of the model is a real-valued propensity score for disorder and six disordered functions. These scores can be converted into binary results using a threshold, indicating whether a residue is predicted to be disordered or functional.\n\nThe model employs a graph-based approach, where each graph-based interaction unit models six disordered functions and their correlations as a functional graph. This graph is fully connected, with nodes representing functions and edges representing the correlations between them. The graph convolutional network (GCN) layer is used to propagate and aggregate semantic correlation features for each node on the functional graph.\n\nThe propensity scores for disorder and six disordered functions are calculated based on the functional node features and disordered features using seven fully connected layers with Sigmoid activation functions. The model's performance is evaluated using threshold-independent metrics such as AUC, AUPR, APS, and Fmax, as well as threshold-dependent metrics like Matthews correlation coefficient (MCC) and balanced accuracy (BACC).\n\nThe model's outputs are real-valued propensity scores, which provide a continuous measure of the likelihood of a residue being disordered or having specific disordered functions. These scores can be interpreted to understand the model's confidence in its predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for DisoFLAG is publicly available and can be accessed through the GitHub repository. This repository is archived on Zenodo, ensuring long-term accessibility and reproducibility. The source code is accompanied by detailed descriptions, facilitating its use and understanding. Additionally, a standalone package and a convenient web server for DisoFLAG are provided. The web server can be accessed at a specified URL, offering researchers in related fields a user-friendly interface to utilize the tool. The availability of these resources aims to support and enhance research efforts in the field of protein disorder prediction.",
  "evaluation/method": "The evaluation of our methods involved a rigorous assessment using multiple test datasets and statistical measures. We employed the DP93 and DP94 test datasets to evaluate the predictive performance of various disorder function predictors. The statistical significance of differences in predictive performance was determined using p-values, calculated by resampling half of the test dataset 20 times and applying the two-sided paired t-test for each pair of predictors. This approach allowed us to compare the Area Under the Curve (AUC) and Fmax values, with p-values greater than 0.05 highlighted to indicate non-significant differences.\n\nAdditionally, we assessed the per-protein performance of different disorder predictors on the CAID2 Disorder-NOX and Disorder-PDB test datasets. The metrics used for this evaluation were averaged over the protein sequence, providing a comprehensive view of each predictor's performance.\n\nSeveral key metrics were utilized in our evaluation, including the Matthews correlation coefficient (MCC), balanced accuracy (BACC), true positive rate (TPR), true negative rate (TNR), positive predictive value (PPV), and coverage of predictions (C). These metrics collectively offered a detailed assessment of the predictors' accuracy, reliability, and coverage.\n\nNot applicable",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our predictors. These include the Matthews correlation coefficient (MCC), balanced accuracy (BACC), true positive rate (TPR), true negative rate (TNR), and positive predictive value (PPV), which is also known as precision. Additionally, we consider the coverage of predictions (C).\n\nThese metrics provide a comprehensive view of the predictors' performance. MCC is particularly useful as it takes into account true and false positives and negatives, giving a balanced measure even if the classes are of very different sizes. BACC ensures that we consider the performance on both positive and negative classes equally, which is crucial for imbalanced datasets. TPR and TNR offer insights into the sensitivity and specificity of the predictors, respectively. PPV indicates the accuracy of the positive predictions made by the model. Lastly, coverage (C) reflects the proportion of the sequence for which predictions are made, which is important for understanding the applicability of the predictors.\n\nThe chosen set of metrics is representative of the standards in the literature. They are commonly used in the field of protein disorder prediction and disordered function prediction, allowing for a fair comparison with other studies. By averaging these metrics over the protein sequence, we ensure that the performance evaluation is robust and reflective of the overall predictive capability of our models.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed method, DisoFLAG, against several publicly available predictors on benchmark datasets. Specifically, we compared DisoFLAG with methods such as fIDPnn, DeepDISOBind, DisoRDPbind, ANCHOR-2, MoRFchibi-Light, SPOT-MoRF, and MoRFchibi-Web on the DP94 and DP93 test datasets. The statistical significance of the differences in predictive performance was assessed using p-values calculated by resampling half of the test dataset 20 times and applying the two-sided paired t-test. This approach allowed us to rigorously compare the AUC and Fmax values across different methods.\n\nAdditionally, we performed comparisons on the CAID2 Disorder-NOX and Disorder-PDB test datasets, where we evaluated various performance metrics including AUC, APS, Fmax, MCC, BACC, and coverage (C). These comparisons provided a comprehensive assessment of DisoFLAG's performance relative to other state-of-the-art predictors.\n\nFor simpler baselines, we included methods like rawMSA and simpler versions of predictors to ensure that the improvements observed with DisoFLAG were not merely due to more complex models but reflected genuine advancements in predictive accuracy. This thorough evaluation process underscores the robustness and reliability of DisoFLAG in predicting disordered protein functions.",
  "evaluation/confidence": "In the \"Evaluation Confidence\" subsection, we assess the reliability and statistical significance of our predictive performance metrics. To ensure robustness, we employed resampling techniques and statistical tests. Specifically, we resampled half of the test dataset 20 times and used the two-sided paired t-test to compare the performance of different disordered function predictors. This approach allows us to calculate p-values, which indicate the statistical significance of the differences in performance metrics between methods.\n\nThe p-values are presented in tables, where values greater than 0.05 are highlighted in bold, indicating that the differences in performance are not statistically significant at the 5% level. This provides a clear visual indication of where the performance differences between methods are not reliable. For instance, in the tables comparing AUC and Fmax values, bolded p-values show pairs of methods where the observed performance differences could be due to random chance.\n\nAdditionally, we report several performance metrics, including Matthews correlation coefficient (MCC), balanced accuracy (BACC), true positive rate (TPR), true negative rate (TNR), positive predictive value (PPV), and coverage of predictions (C). These metrics offer a comprehensive view of the predictive performance, and their statistical significance is crucial for claiming superiority over other methods and baselines.\n\nBy focusing on statistical significance and providing detailed p-values, we aim to ensure that our claims about the performance of our methods are robust and reliable. This rigorous evaluation process helps to build confidence in the results and their practical applicability.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation results are presented in the supplementary materials, such as Figure S1 and Table S14, which provide visualizations and performance metrics of the disorder predictors on specific datasets. These materials are included in the supplementary PDF file accompanying the publication. The metrics used for evaluation include Matthews correlation coefficient (MCC), balanced accuracy (BACC), true positive rate (TPR), true negative rate (TNR), positive predictive value (PPV), and coverage of predictions (C). However, the raw data files used to generate these evaluations are not released publicly."
}