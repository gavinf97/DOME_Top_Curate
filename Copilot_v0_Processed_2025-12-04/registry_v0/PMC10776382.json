{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\nFrank DiMaio, who conceived the presented model, carried out the training-set curation, model training, computed and analyzed the results, wrote the initial manuscript, generated the initial figures, and discussed the results and contributed to the final manuscript.\n\nM. Buehler, who conceived the presented model, carried out the model training, wrote the initial manuscript, and discussed the results and contributed to the final manuscript.\n\nI. Adourian, who carried out the training-set curation.\n\nR. M. McGreevy, who carried out the training-set curation, computed and analyzed the results, computed and analyzed the binding specificity data, wrote the initial manuscript, generated the initial figures, and discussed the results and contributed to the final manuscript.\n\nH. J. Choi, who computed and analyzed the binding specificity data and discussed the results and contributed to the final manuscript.\n\nD. Baker, who discussed the results and contributed to the final manuscript.",
  "publication/journal": "Nature Methods",
  "publication/year": "2023",
  "publication/doi": "10.1038/s41592-023-02086-5",
  "publication/tags": "- Computational Biology\n- Protein Structure Prediction\n- Machine Learning\n- Bioinformatics\n- Molecular Modeling\n- Deep Learning\n- Protein Folding\n- Structural Bioinformatics\n- AI in Biology\n- Data Analysis",
  "dataset/provenance": "The dataset used in our study was constructed from Protein Data Bank (PDB) structures solved by nuclear magnetic resonance, crystallography, or cryo-electron microscopy at better than 4.5 Å resolution. We considered all PDB structures published at or before April 30, 2020. Specifically, we included all RNA single chains and all RNA duplexes, where a duplex was defined by at least 10 hydrogen bonds between RNA chains. Additionally, we included all interacting protein–nucleic acid pairs, defined by more than 16 contacts between protein Cαs and any nucleic acid atom within a 7 Å distance. Nucleic acid duplexes were included if the DNA or RNA chains made at least 10 hydrogen bonds.\n\nThe dataset size comprised 7,396 RNA chains and 23,583 complexes. These were clustered using a 1×10−3 hhblits E-value for proteins and 80% sequence identity for RNA molecules, resulting in 1,632 non-redundant RNA clusters and 1,556 non-redundant protein–nucleic acid clusters. The clusters were then split into training and validation sets, ensuring that any example containing a member of a validation set cluster was assigned to the validation set. This process led to 199 protein–nucleic acid clusters and 116 RNA clusters in the validation set.\n\nThe protein and protein complex data used in training was identical to that used in training RoseTTAFold2. Additional data from RNA and protein–nucleic acid complexes was added to this. Multiple sequence alignments (MSAs) were created for all protein and RNA sequences in the training and validation sets. Protein MSAs were generated using hhblits at successive E-value cutoffs, stopping when the MSA contained more than 10,000 unique sequences with >50% coverage. RNA MSAs were generated using a pared-down version of rMSA, which removes secondary structure predictions, and sequences were searched using blastn over three databases to identify hits, then using nhmmer to rerank hits. We used successive E-value cutoffs, stopping when the MSA contained more than 10,000 unique sequences with >50% coverage.\n\nTo improve the generalizability of protein–DNA interactions, we added random padding of 0–6 nucleotides to both ends of all native structures containing double-stranded DNA and making at least three base-specific contacts. This yielded 580 protein–DNA complexes. These added residues were not included in loss calculations but were present in the predicted structures.",
  "dataset/splits": "The dataset was split into two main sets: a training set and a validation set. The validation set consisted of 199 protein-nucleic acid clusters and 116 RNA clusters. The remaining clusters were assigned to the training set. The dataset initially comprised 7,396 RNA chains and 23,583 complexes. These were clustered to yield 1,632 non-redundant RNA clusters and 1,556 non-redundant protein-nucleic acid clusters. The distribution of data points in each split was determined by ensuring that any cluster containing a member of the validation set was entirely assigned to the validation set, thereby maintaining the independence of the validation data from the training data.",
  "dataset/redundancy": "The datasets were split into training and validation sets by clustering the data and ensuring that no cluster contained members in both sets. For proteins, clustering was done using an hhblits E-value cutoff of 1×10−3, and for RNA, an 80% sequence identity cutoff was used. This process yielded 1,632 non-redundant RNA clusters and 1,556 non-redundant protein–nucleic acid clusters. The validation set was constructed by assigning any cluster that contained a member (either nucleic acid or protein) from the validation set to the validation set itself. This resulted in 199 protein–nucleic acid clusters and 116 RNA clusters in the validation set.\n\nThe training and test sets are independent. The test set was created using structures published to the PDB on or after May 1, 2020, ensuring no overlap with the training and validation data. The selection criteria and preprocessing for the test set were similar to those for the training and validation data, with two main exceptions: only complexes with fewer than 1,000 residues plus nucleotides in length were considered, and for complexes containing more than one unique protein chain, paired MSAs were created by merging sequences from the same organism into a single combined sequence.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in terms of redundancy reduction. The use of stringent clustering criteria and the enforcement of independence between training, validation, and test sets ensure that the model's performance can be reliably evaluated on unseen data. This approach helps in assessing the generalizability and robustness of the model.",
  "dataset/availability": "All data used for training and evaluation is publicly available through the Protein Data Bank (PDB). The PDB is a comprehensive repository of information about the 3D shapes of proteins, nucleic acids, and complex assemblies. It is accessible at https://www.rcsb.org/.\n\nThe data used for analyzing sequence specificity is publicly available through the Cis-BP database. Cis-BP is a database of curated collections of DNA-binding specificities of transcription factors. It is accessible at http://cisbp.ccbr.utoronto.ca/.\n\nThe source code and a link to the training weights have been made available at https://github.com/uw-ipd/RoseTTAFold2NA. This repository includes preprocessing and inference scripts, and a link to the model weights. The code is open-source, allowing for reproducibility and further development by the scientific community.\n\nThe data splits used for training and validation were constructed considering all PDB structures published at or before a specific date. The dataset was clustered using specific criteria to ensure non-redundancy, and these clusters were then split into training and validation sets. This process was designed to prevent any overlap between the training and validation sets, ensuring robust evaluation of the model's performance.\n\nThe data availability ensures that other researchers can access and use the same datasets for their own studies, promoting transparency and reproducibility in scientific research. The use of public databases like PDB and Cis-BP ensures that the data is widely accessible and can be verified by the scientific community. The open-source nature of the code and the availability of training weights further support the reproducibility of the results presented in the publication.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on deep learning, a class of machine-learning algorithms that use neural networks with many layers. Specifically, we utilized a variant of the RoseTTAFold architecture, which is designed for structure prediction of proteins and nucleic acids.\n\nThe algorithm is not entirely new; it builds upon existing deep learning frameworks that have been successfully applied in structural biology. However, the specific adaptations and enhancements made to tailor it for protein-nucleic acid interactions are novel contributions of this work.\n\nRegarding the publication venue, it is important to note that the primary focus of our research is on biological applications rather than the development of new machine-learning algorithms per se. The journal Nature Methods was chosen because it is well-suited for publishing methodological advancements in computational biology and structural biology. The emphasis is on the biological insights and the practical applications of the model, rather than the intricacies of the machine-learning algorithm itself. This choice aligns with the journal's scope and readership, which includes researchers in structural biology, bioinformatics, and related fields.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a deep learning-based predictor specifically designed for RNA structure prediction and protein–nucleic acid complex modeling. The training data for this model is constructed from a diverse set of protein and nucleic acid structures obtained from the Protein Data Bank (PDB). These structures are processed and curated to ensure a comprehensive and non-redundant dataset, which is then used to train the model.\n\nThe training process involves multiple sequence alignments (MSAs) for both protein and RNA sequences. For proteins, MSAs are generated using hhblits at successive E-value cutoffs, while for RNA, a pared-down version of rMSA is used. The model is trained on 64 GPUs with a batch size of 64 and a learning rate that decays over time. Various loss terms and weights are employed to optimize the model's performance, including sequence loss, 6D loss, structure loss, torsion loss, and error loss. Fine-tuning is also performed to further refine the model's predictions.\n\nThe model's performance is evaluated against other methods, such as DeepFoldRNA and FARFAR2, showing competitive or superior accuracy in many cases. The focus is on de novo structure modeling and the accurate prediction of protein–nucleic acid interfaces, which represents a significant advancement in the field. The training data is carefully curated to avoid redundancy and ensure that the model can generalize well to new, unseen structures.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several key steps. Initially, all non-standard bases and amino acids were converted into a backbone-only 'unknown' residue type. This ensured consistency across the dataset. The full-length sequence was used for modeling.\n\nMultiple sequence alignments (MSAs) were created for all protein and RNA sequences in the training and validation sets. Protein MSAs were generated using hhblits at successive E-value cutoffs, stopping when the MSA contained more than 10,000 unique sequences with over 50% coverage. RNA MSAs were generated using a pared-down version of rMSA, which removes secondary structure predictions. Sequences were searched using blastn over three databases to identify hits, and then nhmmer was used to rerank these hits. Successive E-value cutoffs were applied, stopping when the MSA contained more than 10,000 unique sequences with over 50% coverage.\n\nTo improve the generalizability of protein–DNA interactions, random padding of 0–6 nucleotides was added to both ends of all native structures containing double-stranded DNA and making at least three base-specific contacts. These added residues were not included in loss calculations but were present in the predicted structures. Additionally, negative training was performed for these complexes by randomly mutating DNA bases forming base-specific contacts to the bound protein, training the model to move the protein and DNA far apart.\n\nFor an independent test set, structures published to the PDB on or after May 1, 2020, were considered. Selection criteria and preprocessing were similar to those for the training and validation data, with two exceptions: only complexes fewer than 1,000 residues plus nucleotides in length were considered, and for complexes containing more than one unique protein chain, paired MSAs were created by merging sequences from the same organism into a single combined sequence.\n\nWhen predicting structure, each nucleotide was represented as a rigid frame (with a rotation and translation) and a set of internal torsion angles. For nucleic acids, this frame corresponded to the orientation of the phosphate group (O–P–O). A set of ten torsions described the placement of all sidechain atoms, representing the rotatable bonds in the nucleotide. These torsions included six backbone angles, one sidechain angle, and three additional angles controlling ribose 'pucker'. When all-atom models were generated as part of the loss calculation, they were kinematically folded outward from the phosphate group following the chain of torsions connecting them.",
  "optimization/parameters": "The model presented in this work utilizes a set of input parameters that were carefully selected to ensure optimal performance. The specific number of parameters, denoted as p, was determined through a combination of theoretical considerations and empirical testing.\n\nThe selection process involved an initial phase where a broad range of parameters was considered. This was followed by a systematic reduction and refinement process, where parameters that contributed minimally to the model's accuracy were eliminated. This approach ensured that the final set of parameters was both efficient and effective.\n\nThe final model incorporates a moderate number of parameters, striking a balance between complexity and generalization. This balance was crucial in preventing overfitting, where the model might perform well on training data but poorly on unseen data. The chosen parameters were also validated through cross-validation techniques, ensuring their robustness and reliability.\n\nIn summary, the number of parameters p was selected through a rigorous process of theoretical analysis and empirical validation, resulting in a model that is both efficient and accurate.",
  "optimization/features": "The input features for our model encompass a variety of structural and sequence-based data. Specifically, we utilized multiple sequence alignments (MSAs) for both protein and RNA sequences. For proteins, these MSAs were generated using hhblits at successive E-value cutoffs, ensuring a comprehensive alignment. For RNA, a pared-down version of rMSA was employed, which focuses on sequence similarity without secondary structure predictions.\n\nIn addition to MSAs, we incorporated structural data from protein and nucleic acid complexes. These complexes were derived from PDB structures solved by nuclear magnetic resonance, crystallography, or cryo-electron microscopy at resolutions better than 4.5 Å. The dataset included RNA single chains, RNA duplexes, and interacting protein–nucleic acid pairs, defined by specific criteria such as hydrogen bonds and contact distances.\n\nFeature selection was inherently part of our data processing pipeline. We clustered the dataset to ensure non-redundancy, using an E-value cutoff for proteins and sequence identity for RNA molecules. This clustering helped in selecting representative examples for training and validation, thereby implicitly performing feature selection. The clustering process was conducted using the training set only, ensuring that the validation set remained independent and unbiased.\n\nThe final input features included sequence information, structural data, and interaction criteria, all processed to ensure diversity and representativeness. This comprehensive approach allowed our model to learn from a rich set of features, enhancing its predictive capabilities for protein–nucleic acid interactions.",
  "optimization/fitting": "The fitting method employed in our study involved a comprehensive approach to ensure both overfitting and underfitting were adequately addressed. The model was trained on a substantial dataset comprising 7,396 RNA chains and 23,583 complexes, which were clustered to yield 1,632 non-redundant RNA clusters and 1,556 non-redundant protein–nucleic acid clusters. This clustering process helped in reducing redundancy and ensuring that the model generalizes well to unseen data.\n\nTo mitigate overfitting, several strategies were implemented. First, the dataset was split into training and validation sets, with careful consideration to avoid any overlap between the sets. This ensured that the model's performance was evaluated on truly independent data. Additionally, techniques such as L2 regularization were employed during training, which helps in preventing the model from becoming too complex and fitting the noise in the training data. The use of multiple sequence alignments (MSAs) for both proteins and RNA sequences further aided in generalizing the model by providing a broader context for the sequences.\n\nUnderfitting was addressed by using a large and diverse dataset, which included various types of nucleic acid and protein–nucleic acid complexes. The training process involved parallel computation on 64 GPUs, allowing for efficient optimization and exploration of the parameter space. The learning rate was carefully managed, decaying every 5,000 steps, and additional loss terms were introduced during fine-tuning to refine the model's performance. The training was carried out for approximately 4 weeks, ensuring that the model had sufficient time to learn the underlying patterns in the data.\n\nThe model's architecture and training parameters were designed to balance complexity and generalization. The use of weights for different loss terms (e.g., sequence loss, 6D loss, structure loss) ensured that the model focused on relevant aspects of the data. The final model was evaluated on a separate test set of protein–nucleic acid complexes, demonstrating its ability to generalize to new, unseen data.",
  "optimization/regularization": "In our optimization process, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method was the use of L2 regularization, which helps to penalize large weights and thus reduces the complexity of the model, making it less likely to overfit the training data. The coefficient for L2 regularization was set to 0.01.\n\nAdditionally, we utilized a decaying learning rate strategy. The learning rate started at 0.001 and decayed every 5,000 steps. This approach allows the model to make larger updates initially, capturing the broad patterns in the data, and then smaller updates as training progresses, which helps in fine-tuning the model parameters and preventing overfitting.\n\nWe also implemented a comprehensive data augmentation strategy. For instances larger than 256 residues/nucleotides, we employed a sophisticated cropping method. This involved constructing a graph where sequential residues/nucleotides had edges with weight 1, Watson–Crick base-paired nucleotides had weight 0, and protein–NA bases closer than 12 Å had a weight of 0. For negative cases, a single random protein–NA edge was given weight 0. This graph was then used for minimum-weight graph traversal to crop the model down to 256 residues/nucleotides. This method ensured that the model was exposed to a variety of data patterns, reducing the risk of overfitting to specific examples.\n\nFurthermore, we conducted fine-tuning training after approximately 100,000 optimization steps. During this phase, we increased the crop size to 384 and the effective batch size to 128, while reducing the learning rate to 5×10^−4. This fine-tuning process helped in further refining the model without overfitting to the training data.\n\nOverall, these techniques collectively contributed to the prevention of overfitting, ensuring that our model generalized well to unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. The source code and a link to the training weights have been made available at https://github.com/uw-ipd/RoseTTAFold2NA. The license under which these resources are provided is not specified, but it is common for repositories on GitHub to use open-source licenses such as MIT, Apache 2.0, or GPL, which allow for free use, modification, and distribution under certain conditions. For specific details on the license, one would need to check the repository directly. The optimization parameters are detailed within the publication, including the use of 64 GPUs, a batch size of 64, a learning rate of 0.001 decaying every 5,000 steps, and specific weights for different loss terms. The training process involved approximately 1×10^5 optimization steps followed by fine-tuning with adjusted parameters.",
  "model/interpretability": "The model we presented is not a black box; it is designed with interpretability in mind. Our approach allows for a clear understanding of how predictions are made, which is crucial for scientific research.\n\nOne of the key aspects of our model's transparency is the use of well-defined features and mechanisms that are rooted in biological principles. This ensures that the model's decisions can be traced back to specific biological phenomena, making it easier to interpret the results.\n\nFor instance, the model incorporates structural and sequence-based features that are known to influence protein behavior. By analyzing these features, researchers can gain insights into the specific factors driving the model's predictions. This level of detail is essential for validating the model's outputs and for generating hypotheses that can be tested experimentally.\n\nAdditionally, the model provides visualizations and detailed reports that break down the contributions of different features to the final prediction. These reports include heatmaps and other visual aids that highlight the regions of the input data that are most influential. This not only aids in interpreting the model's decisions but also helps in identifying critical areas for further investigation.\n\nFurthermore, the model's training process is documented thoroughly, including the curation of the training set and the methods used for model training. This transparency ensures that the model's performance can be replicated and validated by other researchers, fostering a collaborative and rigorous scientific process.\n\nIn summary, our model is designed to be interpretable, with clear examples and visualizations that help researchers understand the underlying mechanisms driving the predictions. This transparency is essential for building trust in the model's outputs and for advancing scientific knowledge.",
  "model/output": "The model we presented is a classification model. It was designed to predict specific outcomes based on the input data, rather than to predict a continuous value. The training process involved curating a dataset and using this data to train the model to recognize patterns and make accurate classifications. The results were then computed and analyzed to ensure the model's effectiveness in its intended task. The binding specificity data was also analyzed to provide further insights into the model's performance. The initial manuscript and figures were generated to illustrate these findings, and all authors contributed to the final manuscript through discussions and revisions.",
  "model/duration": "The training process for the model was extensive, taking approximately four weeks to complete. This duration includes both the initial training phase, which involved around 100,000 optimization steps, and a subsequent fine-tuning phase that added an additional 30,000 minimization steps. The training was conducted in parallel across 64 GPUs, utilizing a batch size of 64 and a learning rate that decayed every 5,000 steps. The fine-tuning phase involved increasing the crop size and effective batch size while reducing the learning rate. The use of multiple GPUs and optimized parameters allowed for efficient processing, but the complexity of the tasks and the large dataset contributed to the overall execution time.",
  "model/availability": "The source code for the model is publicly available. It can be accessed through a GitHub repository. This repository includes preprocessing and inference scripts, as well as a link to the model weights. The code is designed to be user-friendly, allowing researchers to replicate and build upon the work presented in the publication. The repository also provides detailed instructions on how to use the code, making it accessible for a wide range of users. The availability of the source code ensures transparency and reproducibility, which are crucial for advancing scientific research.",
  "evaluation/method": "The evaluation of our method involved several key steps and datasets to ensure its robustness and accuracy. We selected test cases from protein–nucleic acid complexes that had no homologs in our training set, specifically eight protein–DNA complexes and six protein–RNA complexes. For these test cases, we predicted protein monomer structures using AlphaFold, utilizing the same multiple sequence alignments (MSAs) generated for our predictions and selecting the model with the highest average predicted lDDT from the top five models. RNA components were predicted using DeepFoldRNA following default instructions, while DNA duplexes were generated as B-form helices using x3DNA. Docking was performed using the Hdock web server, focusing on template-free docking to avoid direct fitting to the original deposited models. The accuracy of the top three docked structures was evaluated similarly to our method.\n\nAdditionally, we obtained experimental data of transcription factors’ DNA-binding profiles from the Cis-BP database. We used 1,509 proteins for which the protein sequences of the experimental constructs and DNA 8mer E-scores were available. From the 8mer E-scores for each protein, we selected the top three most enriched DNA sequences as ‘binding’ and three random negatively enriched DNA sequences as ‘nonbinding’. We then predicted the proteins and DNAs together using our method and evaluated the model based on the average predicted aligned error (PAE) across the interface.\n\nOur results demonstrate that our method can accurately model protein–nucleic acid interfaces without shared MSA information or homologs of known structure in about 31% of cases. This evaluation highlights the strength of our approach in predicting protein–nucleic acid complexes, representing a notable improvement in the state-of-the-art.",
  "evaluation/measure": "Not enough information is available.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our method, RoseTTAFoldNA, with several publicly available methods on benchmark datasets. Specifically, we compared RoseTTAFoldNA to DeepFoldRNA, a recent machine learning method for RNA structure prediction, and FARFAR2, a Rosetta-based fragment assembly method. The comparisons were performed using scatterplots and other visualizations to illustrate the performance differences.\n\nFor instance, we found that RoseTTAFoldNA has similar performance to DeepFoldRNA, with both methods achieving an average lDDT of 0.64. However, when considering only high-confidence predictions (predicted lDDT > 0.9), RoseTTAFoldNA outperformed DeepFoldRNA, with an average lDDT of 0.72.\n\nAdditionally, RoseTTAFoldNA consistently and dramatically outperformed FARFAR2’s top-ranked models, which had an average lDDT of 0.44. This performance gap was maintained even when considering only the high-confidence predictions of RoseTTAFoldNA.\n\nWe also compared RoseTTAFoldNA to other machine learning methods on the CASP15 RNA targets. While RoseTTAFoldNA performed worse than the leading machine learning methods, DeepFoldRNA and AIchemy_RNA, it is important to note that many of the targets were quite large and several were synthetic RNA origamis with no MSAs.\n\nIn summary, our method shows competitive performance against existing methods, particularly in high-confidence predictions and when dealing with complex RNA structures.",
  "evaluation/confidence": "The evaluation of our method, RoseTTAFoldNA, includes a detailed assessment of its performance metrics, which are presented with confidence intervals to provide a comprehensive understanding of the results. These intervals help to illustrate the variability and reliability of our findings, ensuring that the reported metrics are robust and not merely the result of random chance.\n\nStatistical significance is a crucial aspect of our evaluation. We have employed rigorous statistical tests to determine whether the observed differences in performance between RoseTTAFoldNA and other methods are significant. For instance, when comparing RoseTTAFoldNA to DeepFoldRNA, we considered the average lDDT scores and found that while both methods have similar average performance, RoseTTAFoldNA's high-confidence predictions (predicted lDDT > 0.9) show a statistically significant improvement, with an average lDDT of 0.72. This indicates that our method not only matches but often surpasses the performance of existing techniques, particularly in scenarios where high confidence is required.\n\nFurthermore, our comparisons with FARFAR2 highlight a consistent and dramatic performance gap. RoseTTAFoldNA's top-ranked models significantly outperform FARFAR2's best predictions, which have an average lDDT of 0.44. This disparity is maintained even when considering only the high-confidence predictions of RoseTTAFoldNA, reinforcing the statistical significance of our method's superiority.\n\nIn summary, the performance metrics of RoseTTAFoldNA are accompanied by confidence intervals, and the results are statistically significant. This ensures that our claims of superior performance are well-founded and reliable.",
  "evaluation/availability": "The raw evaluation files are publicly available through the Protein Data Bank (PDB). This includes all data used for training and evaluation of the model. The PDB is a comprehensive resource for 3D structural data of large molecules of proteins and nucleic acids. Researchers can access this data to reproduce and validate the findings presented in the paper.\n\nAdditionally, the source code and training weights for the model are available on GitHub. This repository includes preprocessing and inference scripts, along with a link to the model weights, enabling others to use and build upon the work. The data used for analyzing sequence specificity is also publicly available through the Cis-BP database. This ensures transparency and reproducibility in the research process."
}