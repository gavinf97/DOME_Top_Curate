{
  "publication/title": "Machine Learning Made Easy (MLme): a comprehensive toolkit for machine learning–driven data analysis",
  "publication/authors": "The authors who contributed to the article are:\n\nAkshay Akshay, who is affiliated with the Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern, and the Graduate School for Cellular and Biomedical Sciences, University of Bern.\n\nMitali Katoch, who is associated with the Institute of Neuropathology, Universitätsklinikum Erlangen, Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU).\n\nNavid Shekarchizadeh, who is part of the Department of Medical Data Science, Leipzig University Medical Centre, and the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig.\n\nMasoud Abedi, who is also affiliated with the Department of Medical Data Science, Leipzig University Medical Centre.\n\nAnkush Sharma, who is connected to the KG Jebsen Centre for B-cell Malignancies, Institute for Clinical Medicine, University of Oslo, and the Department of Cancer Immunology, Institute for Cancer Research, Oslo University Hospital.\n\nFiona C. Burkhard, who is part of the Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern, and the Department of Urology, Inselspital University Hospital.\n\nRosalyn M. Adam, who is affiliated with the Urological Diseases Research Center, Boston Children’s Hospital, the Department of Surgery, Harvard Medical School, and the Broad Institute of MIT and Harvard.\n\nKatia Monastyrskaya, who is associated with the Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern, and the Department of Urology, Inselspital University Hospital.\n\nAli Hashemi Gheinani, who is the corresponding author and is affiliated with the Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern, the Urological Diseases Research Center, Boston Children’s Hospital, the Department of Surgery, Harvard Medical School, and the Broad Institute of MIT and Harvard.\n\nAkshay Akshay and Mitali Katoch contributed equally to the paper.",
  "publication/journal": "GigaScience",
  "publication/year": "2024",
  "publication/doi": "10.1093/gigascience/giad111",
  "publication/tags": "- Machine Learning\n- Data Analysis\n- Classification Problems\n- AutoML\n- CustomML\n- Data Exploration\n- Visualization\n- Biomedical Research\n- Feature Selection\n- Python Programming\n- Data Mining\n- Hyperparameter Tuning\n- Research Tools\n- Bioinformatics\n- Data Science",
  "dataset/provenance": "The datasets utilized in our study were sourced from various reputable repositories and studies. The first dataset comprised messenger RNA (mRNA) patient data from a study on chronic lymphocytic leukemia (CLL), which measured transcriptome profiles of 136 patients. The second dataset was collected from a cervical cancer study that analyzed the expression levels of 714 microRNAs (miRNAs) in 58 human samples. The third and fourth datasets were obtained from The Cancer Genome Atlas (TCGA), consisting of mRNA and miRNA sequencing data from patients with invasive breast carcinoma (BRCA). These datasets included 1,219 mRNA and 1,207 miRNA samples, respectively. The fifth dataset consists of single-cell RNA (scRNA) sequencing data obtained from peripheral blood mononuclear cells (PBMCs) that were sequenced using 10 × Chromium technology. This dataset specifically utilized scRNA datasets of CD8+ naive, CD14+, and CD16+ monocytes, totaling 1,500 data points. The sixth dataset was the widely recognized glass identification dataset, obtained from the University of California, Irvine ML repository, comprising 214 glass samples. The seventh dataset comprised body signal data collected from 100,000 individuals through the National Health Insurance Service in Korea. This dataset includes 21 essential biological signals related to health, such as measurements of systolic blood pressure and total cholesterol levels. These datasets have been previously used in the community and are well-documented in the literature, ensuring their reliability and relevance for our study.",
  "dataset/splits": "The dataset is initially split into training and independent test sets, provided the user has activated the test set option. If the test set option is not activated, the entire dataset is used for training.\n\nThe training dataset is then divided into n bins of equal size through stratified sampling. From these bins, k-1 are designated as training sets while the remainder becomes the test set. This process is repeated for each unique bin in the k-fold cross-validation method.\n\nThe number of data splits depends on the value of k chosen for the k-fold cross-validation. For each fold, k-1 bins are used for training, and 1 bin is used for testing. The distribution of data points in each split is designed to be equal, ensuring that each bin contains an approximately equal number of data points.\n\nThe specific number of data points in each split is not explicitly stated, as it depends on the size of the dataset and the value of k chosen for the cross-validation. However, the process ensures that the data is divided in a way that maintains the overall distribution of the dataset in each split.",
  "dataset/redundancy": "The datasets utilized in our study were carefully split to ensure robust and reliable model training and evaluation. Initially, if the user opted for a test set, the input dataset was divided into training and independent test sets. This independence was crucial for evaluating the model's performance on unseen data, mimicking real-world scenarios. If the test set option was not activated, the entire dataset was used for training.\n\nIn the subsequent step, the training dataset was divided into n bins of equal size through stratified sampling. This method ensures that each bin maintains the same proportion of class labels as the original dataset, which is particularly important for handling class imbalances. From these bins, k-1 were designated as training sets, while the remaining bin served as the validation set. This process was repeated for each unique bin in the k-fold cross-validation (CV) method, ensuring that every data point was used for both training and validation.\n\nThe distribution of our datasets compares favorably to previously published ML datasets. We included datasets of varying sizes, from small (such as those from chronic lymphocytic leukemia and cervical cancer studies) to large (like the invasive breast carcinoma and body signal datasets). This range allows for a comprehensive assessment of the tool's scalability and efficiency. Additionally, we addressed the challenge of high-dimensional features and low sample sizes, known as the curse of dimensionality, by including datasets that exemplify these issues. This approach ensures that our tool is thoroughly tested and validated across diverse data scales and complexities.",
  "dataset/availability": "All supporting data, including the input datasets, \"inputParameters.pkl,\" and \"results.pkl\" files for all evaluated datasets, are available on Zenodo. This ensures that the data used in the study is publicly accessible and can be verified by other researchers. The \"results.pkl\" files can be visualized using the Visualization feature of MLme, providing transparency and reproducibility.\n\nAdditionally, an archival copy of the source code and supporting data is available via the GigaScience database, GigaDB. This further ensures that the methods and data used in the study are accessible and can be replicated by others.\n\nThe data availability is enforced through these public repositories, which provide a permanent and citable link to the datasets and code. This approach ensures that the data and methods are transparent, reproducible, and can be used by other researchers for further studies or validation. The use of Zenodo and GigaDB also ensures that the data is preserved and accessible in the long term, supporting the principles of open science and data sharing.",
  "optimization/algorithm": "The optimization algorithm discussed in this publication is not a novel machine-learning algorithm but rather a tool designed to streamline and simplify machine learning processes. This tool, named MLme, integrates various functionalities such as data exploration, automated machine learning (AutoML), customizable machine learning pipelines (CustomML), and visualization. It is built to enhance efficiency and productivity by reducing the need for extensive coding efforts, making it accessible for researchers who may not have deep domain expertise in machine learning.\n\nMLme leverages existing machine-learning algorithms and techniques, combining them into a user-friendly interface. It supports a wide array of machine-learning algorithms for classification problems, which are essential for the datasets analyzed in this study. These algorithms include various classifiers, evaluation methods, and metrics that are commonly used in the field.\n\nThe decision to publish this work in a scientific journal rather than a machine-learning-specific journal is driven by the tool's broad applicability and the focus on its practical use in research. MLme is designed to address the challenges faced by researchers in integrating machine learning into their workflows, particularly in fields where traditional statistical approaches fall short. By providing a comprehensive tool that simplifies the machine-learning process, it aims to facilitate insightful data analysis and enhance research outcomes across various disciplines.\n\nFuture developments for MLme include plans to incorporate hyperparameter tuning capabilities, allowing users to fine-tune their models for improved performance. Additionally, the tool will support the integration of user-defined algorithms, expanding its applicability and customization options. These enhancements will further solidify MLme's role as a valuable resource for machine learning practitioners, making it a versatile and reliable tool for data analysis.",
  "optimization/meta": "The model described in the publication does not explicitly function as a meta-predictor. Instead, it focuses on utilizing various machine learning algorithms and pipelines to analyze different datasets. The tool, referred to as MLme, allows for the construction of distinct machine learning pipelines tailored to specific datasets. These pipelines can include a variety of processing steps, such as data scaling, resampling, multiple machine learning classifiers, and diverse evaluation methods.\n\nThe tool supports both CustomML and AutoML features. CustomML enables users to design their own machine learning pipelines, selecting from an array of algorithms and steps. AutoML, on the other hand, automates the process of training multiple models, optimizing performance across different datasets.\n\nWhile the model does not explicitly use data from other machine-learning algorithms as input in a meta-predictor sense, it does compare the performance of different machine learning tools, including TPOT and hyperopt-sklearn. This comparison helps in assessing the reliability and consistency of the results produced by MLme.\n\nThe datasets used in the study are independent and include a variety of sources, such as the glass identification dataset from the University of California, Irvine ML repository, and the body signal dataset from the National Health Insurance Service in Korea. The independence of the training data is maintained throughout the analysis, ensuring that the performance metrics are accurately reflected.\n\nIn summary, while the model does not operate as a traditional meta-predictor, it leverages a comprehensive approach to machine learning, allowing for flexible and optimized analysis of diverse datasets. The independence of the training data is a key aspect of the model's design, ensuring robust and reliable results.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. For all datasets, the input data was structured with samples as rows and features as columns, with sample names in the first column and target classes in the last column. This format facilitated consistent data handling across different datasets.\n\nPreprocessing involved several key steps. Initially, low-variance features were removed to focus on the most informative data. This step was followed by data scaling to normalize the feature values, which is essential for algorithms sensitive to feature magnitude. Additionally, resampling techniques were employed to address class imbalances, particularly in datasets like the invasive breast carcinoma (BRCA) dataset. This ensured that the models were not biased towards the majority class.\n\nFeature selection was another critical preprocessing step. The SelectPercentile univariate feature selection method was used to identify and retain the most important features. This method helped in reducing dimensionality and improving model performance by focusing on the most relevant features.\n\nFor the glass identification dataset, which comprised 10 distinct features representing oxide content, the primary objective was to classify different types of glass based on these features. Similarly, for the body signal dataset, which included 21 essential biological signals, the goal was to determine alcohol consumption based on these signals.\n\nIn the case of the peripheral blood mononuclear cell (PBMC) dataset, feature selection was particularly important. By selecting the top 10% of the original input of 500 highly variable genes, we identified 50 genes sufficient for classifying CD8+ naive, CD14+, and CD16+ monocyte cell populations. This selection process highlighted key markers for these cell types, excluding 13 ribosomal genes that showed similar expression levels across all cell types.\n\nOverall, the preprocessing steps ensured that the data was in an optimal state for training machine-learning models, leading to robust and reliable performance across various datasets.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the dataset and the specific machine learning pipeline employed. For the AutoML feature, the default pipeline includes several preprocessing steps such as data scaling, resampling, and feature selection, which inherently involve multiple parameters. Specifically, the SelectPercentile univariate feature selection method is applied to select important features, and five machine learning classification algorithms are trained. Each of these algorithms has its own set of hyperparameters that are optimized during the training process.\n\nFor the CustomML feature, users have the flexibility to design and customize their machine learning pipelines. This includes choosing from seven algorithms for data resampling, nineteen algorithms for scaling, and a diverse array of feature selection algorithms. Additionally, there are sixteen classification algorithms available, each with its own set of hyperparameters. Users can select the parameter values for all the provided algorithms, giving them greater control over the behavior of their developed pipeline.\n\nThe selection of parameters was guided by the need to ensure that the input data were properly prepared for the machine learning algorithms, thereby enhancing the performance and reliability of the trained models. The AutoML feature handles essential preprocessing steps such as data resampling, scaling, and feature selection, which are crucial for optimizing the model's performance. For the CustomML feature, the extensive range of preprocessing options and classification algorithms allows users to refine their pipeline to align with their research requirements, ensuring that the most relevant features are selected and the models are trained effectively.",
  "optimization/features": "In our study, the number of input features varied across different datasets. For instance, the glass identification dataset utilized 10 distinct features representing the oxide content of glass samples. The body signal dataset included 21 essential biological signals related to health, such as measurements of systolic blood pressure and total cholesterol levels. For the PBMC dataset, we initially considered 500 highly variable genes and then selected the top 10% for classification, resulting in 50 genes. Feature selection was performed using the training set only, ensuring that the evaluation remained unbiased. This approach helped in identifying the most relevant features for classifying different cell populations and other target variables in our datasets.",
  "optimization/fitting": "In our study, we employed a comprehensive approach to ensure that our models were neither overfitting nor underfitting the data. The datasets used varied significantly in size and complexity, which necessitated careful consideration of model complexity and regularization techniques.\n\nFor datasets where the number of parameters was potentially much larger than the number of training points, such as the single-cell RNA sequencing data from peripheral blood mononuclear cells (PBMCs), we implemented several strategies to mitigate overfitting. These included rigorous cross-validation techniques, such as k-fold cross-validation, which helped in assessing the model's performance on different subsets of the data. Additionally, we utilized feature selection methods, like SelectPercentile, to reduce the dimensionality of the data and focus on the most informative features. Regularization techniques, such as L1 and L2 regularization, were also applied to penalize complex models and prevent overfitting.\n\nTo address underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. This involved experimenting with different machine learning algorithms and hyperparameters. For instance, we used a variety of classifiers, including tree-based methods, support vector machines, and neural networks, each with its own set of hyperparameters tuned through grid search or random search. Furthermore, we employed data augmentation techniques and resampling methods to enhance the diversity of the training data, thereby improving the model's ability to generalize.\n\nThe performance of our models was evaluated using multiple metrics, including accuracy, precision, recall, and F1 score, across different datasets. This multi-faceted evaluation ensured that our models were not only accurate but also robust and reliable. The consistent performance of our top models across various datasets, achieving scores exceeding 90% for most metrics, indicates that we successfully balanced the trade-off between overfitting and underfitting.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our machine learning models. One of the key steps in our default AutoML pipeline is the removal of low-variance features. This process helps in reducing the dimensionality of the data and focusing on the most informative features, thereby minimizing the risk of overfitting.\n\nAdditionally, we employed data scaling and resampling techniques as part of our preprocessing steps. Data scaling ensures that all features contribute equally to the model, preventing any single feature from dominating the learning process. Resampling techniques, such as stratified sampling, help in creating balanced training and test sets, which is crucial for the model's generalizability.\n\nFurthermore, we utilized the SelectPercentile univariate feature selection method to select the most important features. This method ranks features based on their statistical significance and selects the top percentage, which helps in retaining only the most relevant features for model training.\n\nOur models were evaluated using k-fold cross-validation, which involves splitting the data into k subsets and training the model on k-1 subsets while testing on the remaining subset. This process is repeated k times, ensuring that each subset is used for testing once. Cross-validation helps in assessing the model's performance on different subsets of the data, providing a more reliable estimate of its generalization capability.\n\nWe also compared the performance of our models with dummy classifiers, which serve as baselines to ensure that our models are not overfitting to the training data. The consistent performance of our top models across various datasets, except for the glass identification and body signal datasets, indicates that our techniques were effective in preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters are not explicitly detailed in the publication. However, the tool does plan to incorporate hyper-parameter tuning capabilities to allow users to fine-tune their models and improve overall performance. This feature is intended to enhance the effectiveness and reliability of the tool.\n\nThe source code and supporting data, including input datasets and results files, are available on Zenodo and the GigaScience database, GigaDB. These resources can be used to understand the optimization processes and parameters employed. The source code is licensed under the GNU GPL, ensuring that users have access to the underlying mechanisms and can modify or extend the tool as needed.\n\nFor those interested in the specific configurations and optimization schedules, the Visualization feature of the tool can be used to interpret the results files. This feature provides a range of plots and tables that cover fundamental and advanced visualization options, facilitating a deeper understanding of the model's performance and the optimization processes involved.\n\nAdditionally, the tool's project homepage on GitHub provides further details on the operating system requirements, programming language, and other necessary components for running the tool. This information can be crucial for users looking to replicate or build upon the optimization strategies outlined in the publication.",
  "model/interpretability": "The model developed within the Machine Learning Made Easy (MLme) toolkit is designed with a strong emphasis on transparency and interpretability, ensuring that users can understand and trust the outcomes of their machine learning processes. Unlike many black-box models, MLme provides clear insights into how decisions are made, which is crucial for researchers who need to validate and interpret their results.\n\nOne of the key features that enhance the transparency of MLme is its comprehensive visualization capabilities. Through the Visualization interface, users can generate a wide range of plots and tables that illustrate the performance and behavior of their models. These visualizations include bar plots, heatmaps, and spider plots, among others, which help users to compare and understand the performance of different models. For instance, heatmaps can reveal the importance of various features in the dataset, while bar plots can show the accuracy and other metrics of different classification algorithms.\n\nAdditionally, the Data Exploration feature allows users to gain a preliminary understanding of their datasets through interactive visualizations such as density plots, scatter matrix plots, and class distribution plots. These visual tools help users identify patterns, trends, and potential outliers in their data, which is essential for designing effective machine learning models. For example, a density plot can show how data are distributed, while a scatter matrix plot can identify potential correlations between variables.\n\nThe AutoML feature further supports interpretability by conducting training and evaluation of multiple classification models, including a dummy classifier. This approach provides users with a comprehensive understanding of their data by comparing the performance of diverse models. After the pipeline is completed, users have various options for examining and interpreting the results, including intuitive and interactive plots that help them gain a deeper understanding of the performance characteristics of the models.\n\nFor users with moderate to advanced knowledge of the machine learning domain, the CustomML feature offers even greater control and transparency. Users can design and customize their machine learning pipelines, selecting specific preprocessing steps, algorithms, and evaluation metrics. This level of customization ensures that the models align with the users' research requirements and domain knowledge, making the process more transparent and interpretable.\n\nIn summary, MLme is not a black-box model but a transparent and interpretable toolkit that empowers researchers to understand and validate their machine learning processes. Through its visualization capabilities, data exploration tools, and customizable pipelines, MLme provides clear insights into how models make decisions, ensuring that users can trust and interpret their results effectively.",
  "model/output": "The model in question is designed for classification problems. It focuses on extracting meaningful information from datasets through machine learning techniques. The toolkit offers both automated and customizable machine learning pipelines, which include essential preprocessing steps such as data resampling, scaling, and feature selection. These steps ensure that the input data is properly prepared for the classification algorithms, enhancing the performance and reliability of the trained models. The toolkit supports a variety of classification algorithms, allowing users to identify the most effective ones for their specific datasets. Additionally, it provides comprehensive evaluation methods and metrics to assess the performance of the trained models. The results can be visualized using interactive plots, and the output files can be downloaded for further analysis.",
  "model/duration": "The execution time for our models varied depending on the dataset and the specific configurations used. For instance, when employing the AutoML feature, the preprocessing steps such as data resampling, scaling, and feature selection were handled automatically, which streamlined the process. This automated pipeline ensured that the input data were properly prepared for the machine learning algorithms, thereby enhancing the performance and reliability of the trained models.\n\nIn our comparative analysis, we configured hyperopt-sklearn to explore all classification algorithms and data transformations comprehensively. For TPOT, we set a runtime limit of 5 minutes, with a population size of 50 and 5 generations. These settings allowed us to evaluate the performance of MLme against other tools effectively.\n\nThe top-performing models consistently achieved scores exceeding 90% for all computed metrics across most datasets, except for the glass identification and body signal datasets. This indicates that the models were efficient in terms of both time and performance.\n\nOverall, the execution time was optimized to balance thorough evaluation with practical runtime constraints, ensuring that users could obtain reliable results without excessive computational overhead.",
  "model/availability": "The source code for Machine Learning Made Easy (MLme) is publicly available. It is hosted on GitHub, under the project name \"Machine Learning Made Easy (MLme)\". The tool is platform-independent and can be run using Docker or Python. The programming language used is Python, version 3.9. The software is licensed under the GNU GPL, ensuring that users can freely use, modify, and distribute the code. Additionally, the tool is assigned a BioTool ID and a SciCrunch ID for easy identification and referencing. All supporting data, including input datasets and results, are available on Zenodo and GigaDB, providing users with the necessary resources to replicate and build upon the work. The DOME-ML annotation supporting the study is also available through the DOME Wizard, further enhancing the transparency and reproducibility of the research.",
  "evaluation/method": "The evaluation of the method involved a comprehensive assessment using multiple datasets and comparison with other tools. We utilized the CustomML feature to construct distinct machine learning pipelines for various datasets, including those related to chronic lymphocytic leukemia, cervical cancer, body signals, and The Cancer Genome Atlas datasets. These pipelines included diverse processing steps such as data scaling and resampling, multiple machine learning classifiers, and various evaluation methods and metrics.\n\nAdditionally, the AutoML feature was employed to train multiple models for the peripheral blood mononuclear cell and glass identification datasets. The performance of the top-performing models was consistently high, exceeding 90% for all computed metrics across most datasets, except for the glass identification and body signal datasets.\n\nTo ensure the reliability and consistency of the results, a comparative analysis was conducted against the Tree-based Pipeline Optimization Tool (TPOT) and hyperopt-sklearn. All three tools demonstrated similar performance across most datasets, underscoring the effectiveness of the method. For hyperopt-sklearn, a comprehensive exploration of classification algorithms and data transformations was configured, utilizing the tree-structured Parzen estimator algorithm for hyperparameter search. For TPOT, specific parameters were set, including a 5-minute runtime limit, a population size of 50, and 5 generations.\n\nFurthermore, the feature selection functionality from AutoML was used to identify important genes for classifying different cell populations from the peripheral blood mononuclear cell dataset. This involved selecting the top 10% of highly variable genes, resulting in a list of 50 genes that were sufficient for classifying these cell types. The identified genes showed strong correspondence with their respective cell populations, with some classic markers being discovered for CD8+ naive and CD16+ cell populations.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our machine learning models. These metrics were chosen to provide a thorough assessment of model performance across various dimensions.\n\nFor each machine learning algorithm, we computed multiple performance metrics, including F1 score, accuracy, and recall. These metrics were evaluated on both training and testing datasets, ensuring a robust assessment of model generalization. The F1 score, which is the harmonic mean of precision and recall, provides a balanced measure of a model's accuracy, especially when dealing with imbalanced datasets. Accuracy, the ratio of correctly predicted instances to the total instances, gives a general idea of the model's performance. Recall, or sensitivity, measures the model's ability to identify positive instances correctly.\n\nThe performance metrics were visualized using 2-dimensional polar coordinates, where each vertex represents a specific performance metric. This visualization allows for an intuitive comparison of the top and worst-performing models. A larger shaded area in the polar plots indicates better overall performance, providing a clear and concise way to compare different models.\n\nAdditionally, we compared our tool, MLme, with other machine learning automation tools, such as TPOT and hyperopt-sklearn, across multiple datasets. The performance comparison included the same set of metrics, ensuring a fair and representative evaluation. The results demonstrated that MLme achieved similar performance to these established tools, underscoring its reliability and effectiveness.\n\nIn summary, the performance metrics reported in our study are representative of standard practices in the field. They provide a comprehensive evaluation of model performance, ensuring that our findings are both reliable and comparable to existing literature.",
  "evaluation/comparison": "In the evaluation of MLme, a comprehensive comparison was conducted with other publicly available machine learning automation tools. Specifically, MLme was benchmarked against TPOT and hyperopt-sklearn across multiple datasets. These datasets included cervical cancer, TCGA mRNA, TCGA miRNA, peripheral blood mononuclear cell (PBMC), glass identification, and body signal data. The performance metrics evaluated were F1 score, accuracy, and recall.\n\nThe results demonstrated that MLme, TPOT, and hyperopt-sklearn exhibited similar performance across most datasets, indicating the reliability and consistency of MLme's results. For hyperopt-sklearn, a thorough exploration of all classification algorithms and data transformations within the library was configured, utilizing the tree-structured Parzen estimator algorithm for hyperparameter search. TPOT was employed with a 5-minute runtime limit, a population size of 50, and 5 generations, with default values for other parameters.\n\nAdditionally, simpler baselines were included in the evaluation. Dummy classifiers were used as a baseline to assess the performance of MLme. As expected, these dummy classifiers performed the worst among all the datasets, highlighting the superior performance of MLme and the other compared tools.\n\nThis rigorous comparison underscores the effectiveness of MLme in handling diverse datasets and its competitiveness with established machine learning automation tools.",
  "evaluation/confidence": "In our study, we conducted a thorough evaluation of the MLme tool using various datasets and compared its performance with other machine learning automation tools such as TPOT and hyperopt-sklearn. The performance metrics, including F1 score, accuracy, and recall, were computed for each dataset during both training and testing phases. However, specific details about confidence intervals for these metrics are not provided in the current evaluation.\n\nThe results demonstrated that MLme, along with TPOT and hyperopt-sklearn, showed similar performance across multiple datasets, indicating the reliability and consistency of MLme's results. For the glass identification and body signal datasets, the performance was notably different, but this was expected due to the unique characteristics of these datasets.\n\nStatistical significance was not explicitly discussed in the context of claiming superiority over other methods and baselines. However, the consistent performance across multiple datasets and the comparison with established tools suggest that MLme is a robust and reliable tool for machine learning-driven data analysis. Further statistical analysis could be conducted to provide more definitive evidence of MLme's superiority in specific contexts.",
  "evaluation/availability": "All supporting data, including the input dataset, “inputParameters.pkl,” and “results.pkl” files, for all evaluated datasets, are available on Zenodo. The “results.pkl” files can be visualized using the Visualization feature of MLme. An archival copy of the source code and supporting data is also available via the GigaScience database, GigaDB. The link to the DOME annotations for this study is available on GigaDB. The datasets used in this study are diverse, including both biological and non-biological examples, such as the glass identification dataset and body signal data. These datasets vary in sample size and complexity, ensuring a comprehensive evaluation of MLme's performance. The datasets are publicly available, and the tools used for comparison, such as TPOT and hyperopt-sklearn, are also well-documented. The performance of MLme was rigorously tested and compared with other tools, demonstrating its reliability and consistency. The source code and supporting data are accessible, allowing for reproducibility and further analysis."
}