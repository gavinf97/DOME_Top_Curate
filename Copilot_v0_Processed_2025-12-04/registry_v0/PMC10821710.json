{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- J.C. This author planned the project and implemented most of the software. Additionally, they wrote the paper together with J.S.\n- J.S. This author tested the software and ran all the experiments. They also contributed minor fixes to the code and collaborated in writing the paper.\n- Y.Z. This author added Docker support to the software and contributed minor fixes to the code. They also contributed to proofreading the paper.",
  "publication/journal": "GigaScience",
  "publication/year": "2024",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Deep Learning\n- Image-to-Image Transformation\n- Biomedical Applications\n- Microscopy\n- Convolutional Neural Networks\n- Image Processing\n- Fluorescence Microscopy\n- Computational Biology",
  "dataset/provenance": "The datasets utilized in our work were sourced from various open-accessible public repositories, all released under open-source licenses. These datasets were exclusively used for research purposes, ensuring compliance with their intended use.\n\nFor the label-free prediction of nuclear structure from 2D/3D bright-field images, we used data from two sources. The first source contained a time-lapse tiff with 240 time steps, each featuring five channels. Only channels 3 and 5 were utilized in our study. The second source provided snapshots sliced from different time-lapse data, saved as six different tiff files. We specifically used the bright field and the second channel of the fluorescence files.\n\nFor 3D data, we downloaded images from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket. This dataset includes multic hannel 3D images, where the brightfield and corresponding structure channels were used as input and ground truth, respectively. Experiments were conducted on four different cell lines: fibrillarin, nucleophosmin, lamin b1, and histone H2B.\n\nIn the 2D semantic segmentation of tissues from H&E images, we used data originally from the MICCAI GlaS challenge. This dataset includes one training set with 85 images and two test sets with 60 and 20 images, respectively. We maintained the same train/test split as in the original challenge.\n\nFor instance segmentation in microscopy images, we used data from a dataset for segmenting C. elegans from widefield images. All images from this dataset were utilized, with 5% of the data held out for testing. Additionally, for 3D instance segmentation, we used the lamin b1 cell line from the hiPSC single-cell image dataset. Each raw field of view (FOV) is a multic hannel 3D image, with instance segmentation of all nuclei available.\n\nIn comparing semantic segmentation and instance segmentation of organelles from 3D confocal microscopy images, we used the fibrillarin cell line from the hiPSC single-cell image dataset. Each raw FOV is a multic hannel 3D image, and the input is always the structure channel. We generated the semantic segmentation ground truth using the FBL_fine workflow in the Allen Cell and Structure Segmenter.\n\nFor generating synthetic microscopy images from binary masks, we used the nucleophosmin cell line for 2D experiments and the histone H2B cell line for 3D experiments. The input for these experiments was binary segmentation, while the ground truth was the raw image.\n\nIn image denoising for microscopy images, we used two datasets: “Denoising_Planaria.tar.gz” and “Denoising_Tribolium.tar.gz.” The original train/test splitting in these datasets was maintained.\n\nFor imaging modality transformation from 3D confocal microscopy images to stimulated emission depletion (STED) microscopy images, we used two datasets: Microtubule and Nuclear_Pore_complex from “Confocal_2_STED.zip.” The original train/test splitting in these datasets was also kept.\n\nIn staining transformation for multiplex experiments, we used the dataset “BC-Dee pLIIF_Training_Set.zip” and “BC-Dee pLIIF_Validation_Set.zip.” In our experiments, the IHC image was used as the input, with different images serving as ground truth depending on the experiment.\n\nAll models and sample data used in the manuscript, as well as additional supporting data, are deposited at Zenodo to ensure the reproducibility of our work. The scripts used to download and reorganize the data can be found in the release branch called “paper_version” within our repository. Detailed information about each dataset is listed in the same order as the Results section.",
  "dataset/splits": "In our study, we utilized various datasets for different experiments, each with specific data splits. For most of the experiments involving 3D data from the hiPSC single-cell image dataset, we held out 20% of the data for testing. This means that 80% of the data was used for training, and the remaining 20% was reserved for testing purposes.\n\nFor instance, in the experiments involving Golgi and mitochondria, we used the corresponding structure segmentation from the dataset. Similarly, for histone H2B, we converted the released nuclear instance segmentation to binary as semantic segmentation results. In all these cases, 20% of the data was set aside for testing.\n\nIn the 2D semantic segmentation of tissues from H&E images, we had one training set consisting of 85 images and two test sets with 60 and 20 images, respectively. We maintained the same train/test split as in the original MICCAI GlaS challenge.\n\nFor instance segmentation in microscopy images, we used all images from the dataset for segmenting C. elegans from widefield images, with 5% of the data held out for testing. In another 3D instance segmentation experiment, we used the lamin b1 cell line, where 20% of the data was reserved for testing.\n\nIn the label-free prediction of nuclear structure from 2D/3D bright-field images, we used data from two sources. For the first source, which contained a time-lapse tiff of 240 time steps, 15% of the data was held out for testing. For the second source, the data was already split into training and testing sets, with each snapshot saved as multiple tiff files.\n\nIn summary, the data splits varied depending on the specific experiment and dataset used. However, a common practice was to reserve a portion of the data for testing, typically around 20%, to evaluate the performance of our models.",
  "dataset/redundancy": "In our work, we carefully managed dataset redundancy to ensure the independence of training and test sets, which is crucial for the validity of our machine learning models. For most of our experiments, we held out a specific percentage of the data for testing purposes. For instance, in several 3D experiments using the hiPSC single-cell image dataset, 20% of the data was reserved for testing. This split was done to ensure that the training and test sets were independent, preventing any data leakage that could artificially inflate the performance metrics.\n\nTo enforce this independence, we shuffled the correspondence between images and their segmentations, simulating unpaired ground truth. This approach helped in creating a more robust evaluation framework. Additionally, we maintained the original train/test splitting in datasets that were released with specific publications, ensuring consistency with previously established benchmarks.\n\nThe distribution of our datasets compares favorably with previously published machine learning datasets in bioimaging. By using well-established datasets like the Allen Cell Quilt Bucket and adhering to standard splitting methods, we ensured that our results are comparable and reproducible. This attention to detail in dataset management is a key aspect of our methodology, aiming to provide reliable and generalizable findings in the field of image-to-image transformations for bioimaging applications.",
  "dataset/availability": "All data used in this work were sourced from open-accessible public repositories, released under open-source licenses. These datasets were utilized solely for research purposes, ensuring no commercial or noncommercial misuse. The scripts employed to download and reorganize the data are available in the release branch named “paper_version” within our repository. This approach ensures transparency and reproducibility.\n\nTo enhance accessibility and traceability, the toolbox has been registered with biotools (bio.tools ID: biotools:mmv_im2im) and workflow hub. Detailed information about each dataset is provided in the same order as presented in the Results section. Snapshots of our code and additional supporting data are openly available in the GigaScience repository, GigaDB. Furthermore, all trained models and sample data have been deposited at Zenodo to ensure the reproducibility of our work. This comprehensive approach guarantees that all necessary resources are publicly accessible, facilitating verification and further research.",
  "optimization/algorithm": "Not enough information is available.",
  "optimization/meta": "Not enough information is available.",
  "optimization/encoding": "For the data encoding and preprocessing in our machine-learning algorithm, we employed several strategies tailored to microscopy images. We utilized a persistent dataset approach with partial loading and sampling support, which allowed for efficient handling of large datasets. This method, powered by aicsimageio, enabled delayed image reading, making it feasible to train models on extensive datasets, such as those containing over 125,000 3D images, within a reasonable timeframe.\n\nWe implemented common frameworks, like fully convolutional networks (FCNs) and conditional generative models, in a generic manner to easily switch between different dimensionalities for training. This flexibility allowed us to handle up to 5D images (channel × time × Z × Y × X) directly during inference without presplitting into smaller chunks.\n\nTo address the specific characteristics of microscopy data, we incorporated specialized image normalization methods. For 3D microscopy images, we calculated the mean and standard deviation of image intensity using only the middle chunk along the Z dimension. This approach ensured that the normalization process was robust to the anisotropic nature of 3D light microscopy images, which typically have much lower resolution along the Z dimension compared to the XY dimensions.\n\nAdditionally, we adopted an anisotropic variation of the U-Net architecture to better handle the anisotropic dimensions of 3D microscopy images. This modification improved the model's performance and applicability to 3D anisotropic microscopy data.\n\nFor deployment in production, we combined efficient data handling with sliding window inference and Gaussian weighted blending. This approach allowed us to apply models trained on small 3D patches to much larger images without visible stitching artifacts, ensuring smooth and efficient inference in practical applications.",
  "optimization/parameters": "In our work, the number of parameters (p) in the model varies depending on the specific architecture and task at hand. For instance, when using a basic UNet model for 2D label-free predictions, the number of parameters is relatively modest compared to more complex architectures. In contrast, for 3D label-free predictions, models like the anisotropic UNet or fully convolutional networks (FCNs) are employed, which typically have a higher number of parameters to capture the intricate details in 3D data.\n\nThe selection of p was guided by a balance between model complexity and computational efficiency. For 2D tasks, simpler models with fewer parameters were sufficient to achieve accurate predictions, as seen in the comparison between the basic UNet and the fnet model. For 3D tasks, more parameters were necessary to handle the increased dimensionality and complexity of the data. The use of frameworks like MONAI and aicsimageio allowed for efficient data handling and training, even with large datasets and high-dimensional images.\n\nIn summary, the number of parameters was tailored to the specific requirements of each task, ensuring that the models were neither underfitted nor overfitted. This approach allowed us to achieve robust and efficient image-to-image transformations across various biomedical applications.",
  "optimization/features": "The input features for our models are derived from biomedical images, specifically from microscopy data. The exact number of features (f) can vary depending on the specific dataset and task, as we have demonstrated the applicability of our MMV_Im2Im package on more than 10 different problems or datasets. For instance, in 2D label-free experiments, we used images from the HeLa \"Kyoto\" cells dataset, focusing on the brightfield and mCherry-H2B channels. In 3D label-free experiments, we utilized the human induced pluripotent stem cell (hiPSC) single-cell image dataset, extracting the brightfield channel and various structure channels.\n\nFeature selection was not explicitly performed in the traditional sense of selecting a subset of features from a larger set of candidates. Instead, our approach involves selecting relevant channels or modalities from the microscopy images based on the specific biological structures or phenomena of interest. This selection process is guided by domain knowledge and the goals of the particular experiment.\n\nThe selection of input features was done carefully to ensure that the models could learn meaningful representations from the data. For example, in the 3D label-free experiments, we chose channels that correspond to different nuclear structures, histones, and nucleoli, which are crucial for understanding cellular processes. This careful selection of input features is integral to the success of our image-to-image transformation models in biomedical imaging applications.",
  "optimization/fitting": "The fitting method employed in our work leverages advanced machine learning techniques to handle both 2D and 3D segmentation problems effectively. The models used, such as those based on the EmbedSeg framework, are designed to manage a large number of parameters relative to the training data points. To address potential overfitting, several strategies were implemented. Firstly, data augmentation techniques, including on-the-fly augmentation and patch sampling, were utilized to increase the diversity of the training data. This helps in generalizing the model's performance across different scenarios. Additionally, exclusion masks were employed to ignore certain parts of the images during training, which is particularly useful for partially annotated ground truth and large images where full annotation is impractical. This approach ensures that the model focuses on relevant data, reducing the risk of overfitting to noise or irrelevant features.\n\nTo further mitigate overfitting, efficient half-precision training was used to reduce the GPU memory workload for each sample, allowing for larger batch sizes or patch sizes. This not only improves training efficiency but also helps in better generalization. Moreover, the modular design of the toolbox allows for easy integration of state-of-the-art ML engineering methods, enabling the use of multiple GPUs to scale up training. This scalability ensures that the model can be trained on larger datasets, further reducing the risk of overfitting.\n\nUnderfitting was addressed by carefully selecting appropriate network architectures and training strategies. For 3D problems, anisotropic neural networks like the anisotropic U-Net were used to handle the challenges posed by anisotropic microscopy images. This ensures that the model can capture the necessary spatial information without being limited by the dimensional constraints. Additionally, the use of advanced normalization techniques, such as percentile, center, and standard normalization, helps in improving the model's ability to learn from the data effectively. The combination of these strategies ensures that the model is neither too complex nor too simple, striking a balance that avoids both overfitting and underfitting.",
  "optimization/regularization": "Regularization methods were employed to prevent overfitting in our models. Techniques such as dropout layers were integrated into our neural network architectures to randomly deactivate a fraction of neurons during training, which helps in reducing overfitting by preventing the model from becoming too reliant on specific neurons. Additionally, we utilized data augmentation techniques to artificially expand our training dataset by applying random transformations like rotations, flips, and zooms. This approach ensures that the model generalizes better to unseen data by exposing it to a wider variety of input variations. Furthermore, early stopping was implemented to halt the training process when the model's performance on a validation set ceased to improve, thereby avoiding overfitting to the training data. These regularization strategies collectively contribute to the robustness and generalizability of our models.",
  "optimization/config": "The MMV_Im2Im package is designed to be highly configurable and user-friendly, catering to both experienced and novice users in machine learning and Python programming. The toolbox is modularized, allowing for various levels of configurability. Users can run the toolbox with a simple command or customize details directly from a human-readable configuration file. This flexibility enables users to choose specific settings, such as batch normalization or instance normalization in certain layers of the model, and add extra data augmentation steps as needed.\n\nThe package supports multiple ways of data loading, covering common scenarios to ensure ease of use. It handles large datasets efficiently by using delayed loading from aicsimageio and the PersistentDataloader from MONAI. This approach optimizes the efficiency of training by avoiding repetitive heavy computations on large files.\n\nFor training, the package fully adopts the Trainer from PyTorch Lightning, which is widely used and tested in both research and industrial applications. Users can specify training parameters in a YAML file to set up multi-GPU training, half-precision training, automatic learning rate finder, automatic batch size finder, early stopping, stochastic weight averaging, and more. This allows researchers to focus on their specific problems without worrying about the underlying machine learning engineering.\n\nThe package is open-source and available under the MIT license. All models used in the manuscript, along with sample data, are deposited at Zenodo to ensure reproducibility. The scripts used to download and reorganize the data are available in the release branch called “paper_version” within the repository. Detailed information about each dataset is provided, and snapshots of the code and other supporting data are openly available in the GigaScience repository, GigaDB. This ensures that all necessary configurations, optimization schedules, model files, and optimization parameters are accessible and reproducible.",
  "model/interpretability": "The model presented in our work is designed with a focus on interpretability and transparency, rather than being a black box. We have developed a toolbox that facilitates image-to-image transformation for biomedical applications, and one of its key features is modularization and a human-readable configuration system. This design choice ensures that users, regardless of their extensive experience with machine learning and Python programming, can understand and interact with the model.\n\nThe toolbox allows for a range of configurability, enabling users to customize various aspects of the model directly from a human-readable configuration file. For instance, users can choose between different normalization methods, such as batch normalization or instance normalization, in specific layers of the model. Additionally, users can add extra data augmentation steps or adjust other parameters to suit their needs. This level of configurability makes the model transparent, as users can see exactly how changes in the configuration file affect the model's behavior.\n\nMoreover, the toolbox is built on the PyTorch-Lightning framework, which provides a universal boilerplate for image-to-image transformation. This framework ensures that all methods share the same modularized code structure, making it easier for users to read, understand, and modify the code. The use of PyTorch-Lightning also allows users to enjoy different state-of-the-art machine learning training techniques without changing the code, further enhancing the model's transparency.\n\nIn summary, the model is not a black box but a transparent and configurable tool that allows users to understand and customize its behavior through a human-readable configuration system and a modularized code structure. This design choice is intended to lower the learning curve for users and make the model accessible to a wider range of researchers in the biomedical field.",
  "model/output": "The MMV_Im2Im toolbox is designed for image-to-image transformation tasks, which are primarily regression problems in the context of biomedical imaging. The toolbox focuses on predicting various types of images from input microscopy data, such as label-free prediction of nuclear structures from bright-field images. This involves generating continuous output images rather than classifying them into discrete categories.\n\nThe toolbox supports a range of models and frameworks, including fully convolutional networks (FCNs), conditional generative models, and cycle-consistent generative models. These models are reimplemented in a generic way to handle different dimensionalities, from 2D to 5D images, making them versatile for various biomedical applications.\n\nFor example, the toolbox can predict 3D fluorescence images from transmitted-light microscopy, demonstrating its capability in regression tasks. The evaluation metrics used, such as Pearson correlation, structural similarity, and peak signal-to-noise ratio, further indicate that the toolbox is geared towards regression problems where the goal is to produce high-quality, continuous output images.\n\nIn summary, the MMV_Im2Im toolbox is primarily used for regression tasks in biomedical image analysis, focusing on transforming input microscopy images into desired output images with high accuracy and efficiency.",
  "model/duration": "The execution time for training our models was optimized through several strategies. By employing the PersistentDataset in MONAI with partial loading and sampling support, along with delayed image reading powered by aicsimageio, we were able to efficiently handle large datasets. This setup allowed us to train a 3D nuclei instance segmentation model with over 125,000 3D images in a single day, even with limited computational resources. The toolbox supports training on up to 5D images, enabling direct loading of large images without presplitting into smaller chunks. Additionally, the use of efficient data handling and sliding window inference with Gaussian weighted blending ensures that inference can be performed without visible stitching artifacts, making the process both time-efficient and high-quality.",
  "model/availability": "The source code for our toolbox, MMV_Im2Im, is publicly available and can be accessed via a GitHub repository. This repository contains all the necessary code to implement the image-to-image transformation techniques described in our work. The toolbox is designed to be versatile, supporting a range of biomedical applications from 2D to 5D microscopy images.\n\nTo facilitate ease of use and reproducibility, we have also provided detailed documentation and tutorials within the repository. These resources guide users through the setup process, including installation instructions and examples of how to run the algorithms. The toolbox is built using Python and leverages state-of-the-art machine learning engineering techniques, making it accessible for researchers to integrate into their workflows.\n\nFor those who prefer containerized environments, Docker support has been added to the software, ensuring consistency across different operating systems. Additionally, all models used in the manuscript, along with sample data, have been deposited in a public repository to help researchers get started quickly.\n\nThe toolbox is released under the MIT license, which allows for free use, modification, and distribution, provided that the original work is properly cited. This open-source approach encourages collaboration and further development within the community, fostering innovation in biomedical image analysis.",
  "evaluation/method": "In our evaluation of the MMV_Im2Im toolbox, we focused on demonstrating its versatility and general applicability across various biomedical research and development use cases. We conducted over 10 different experiments using publicly available datasets to showcase the toolbox's capabilities. These experiments were designed to highlight the different features of the package rather than to achieve the best performance on each individual task, which may require further hyperparameter tuning.\n\nFor the label-free prediction of nuclear structures from 2D/3D brightfield images, we used visual inspection as our primary evaluation method, supplemented by Pearson correlation and structural similarity as rough quantitative references. This approach allowed us to demonstrate the utility of our toolbox while leaving more specific and systematic evaluations to users for their particular problems.\n\nIn the 2D label-free experiments, we used images from the HeLa \"Kyoto\" cells dataset, comparing a basic UNet model and a 2D version of the fnet model. The fnet model achieved slightly more accurate predictions. For 3D label-free experiments, we tested with images from the human induced pluripotent stem cell (hiPSC) single-cell image dataset, predicting various nuclear structures from brightfield images. We found that center normalization combined with pix2pix and fnet as the generator yielded the best overall qualitative performance.\n\nWe also conducted experiments on generating synthetic microscopy images from binary masks, image denoising for microscopy images, imaging modality transformation from 3D confocal microscopy images to stimulated emission depletion (STED) microscopy images, and staining transformation in multiplex experiments. Each of these experiments used publicly available datasets and followed similar evaluation strategies, focusing on visual inspection and general applicability.\n\nIt is important to note that while we recognize the importance of systematic evaluation, such analysis falls outside the scope of this article. We argue that an appropriate evaluation methodology should depend on specific downstream quantitative analysis goals. For example, if the aim is to quantify the size of nucleoli, one must compare the segmentation derived from real nucleoli signals to that of the predicted nucleoli segmentation. Alternatively, if the goal is to localize the nucleoli roughly within the cell, Pearson correlation may be a more appropriate metric. Our intent is to demonstrate the utility of our MMV_Im2Im package and leave appropriate evaluations to users in their specific problems in real studies.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to assess the quality of our image-to-image transformations. For visual inspection, we primarily used Pearson correlation and structural similarity as rough quantitative references. These metrics provided a general sense of how well our predicted images matched the ground truth.\n\nFor more specific evaluations, we calculated the Pearson correlation, structural similarity, and peak signal-to-noise ratio on holdout validation sets. These metrics were chosen to provide a comprehensive assessment of the image quality and the accuracy of the predicted structures. The results of these evaluations were summarized in tables within our publication, offering a clear view of the performance across different tasks.\n\nAdditionally, we mentioned the use of error plots to visually compare the predicted images with the reference images. These plots helped in identifying any discrepancies and understanding the areas where the model performed well or needed improvement.\n\nIt is important to note that while these metrics are commonly used in the literature, the choice of evaluation methodology should depend on the specific goals of the downstream quantitative analysis. For instance, if the aim is to quantify the size of nucleoli, a comparison of segmentation derived from real nucleoli signals to that of the predicted nucleoli segmentation would be more appropriate. Alternatively, for localizing nucleoli roughly within the cell, Pearson correlation might be more suitable.\n\nOur intent was to demonstrate the utility of our MMV_Im2Im package, leaving the appropriate evaluations to users based on their specific problems in real studies. We believe that these metrics, along with visual inspections and error plots, provide a representative and comprehensive evaluation of our models' performance.",
  "evaluation/comparison": "In the evaluation of our MMV_Im2Im toolbox, we conducted a thorough comparison with both publicly available methods and simpler baselines to ensure the robustness and versatility of our approach. For the 2D label-free prediction task, we utilized the HeLa \"Kyoto\" cells dataset and compared our toolbox against a basic UNet model and a 2D version of the fnet model. The fnet model demonstrated slightly more accurate predictions than the basic UNet, highlighting the effectiveness of our toolbox in handling 2D image-to-image transformations.\n\nFor the 3D label-free prediction task, we tested our toolbox on the human induced pluripotent stem cell (hiPSC) single-cell image dataset. We extracted various nuclear structures from the brightfield images and compared different image normalization methods, network backbones, and model types. This comprehensive comparison allowed us to evaluate the performance of our toolbox across multiple dimensions and configurations, ensuring its applicability to a wide range of biomedical research scenarios.\n\nAdditionally, we focused on visual inspection using metrics such as Pearson correlation and structural similarity as rough quantitative references. This approach allowed us to demonstrate the utility of our toolbox while leaving more specific evaluations to users for their particular problems in real studies. By conducting these comparisons, we aimed to provide a holistic view of image-to-image transformation concepts, inspiring researchers to integrate AI into traditional assay development strategies and enabling new biomedical studies that were previously unfeasible.",
  "evaluation/confidence": "The evaluation of the transformed images in our study provides valuable insights into the localization and expression patterns of specific biomolecules spatially. However, it is important to note that while we present metrics as an example of evaluation, we leave application-specific evaluations to the appropriate researchers. This is because the appropriate evaluation methodology should depend on specific downstream quantitative analysis goals.\n\nFor instance, if the aim is to quantify the size of nucleoli, one must compare the segmentation derived from real nucleoli signals to that of the predicted nucleoli segmentation, ensuring that measurements from both are consistent. Alternatively, if the goal is to localize the nucleoli roughly within the cell, Pearson correlation may be a more appropriate metric.\n\nIn our work, we concentrate on visual inspection, using Pearson correlation and structural similarity as a rough quantitative reference. Our intent is to demonstrate the utility of our MMV_Im2Im package and leave appropriate evaluations to users in their specific problems in real studies.\n\nRegarding confidence intervals and statistical significance, our focus was on demonstrating the flexibility and general applicability of the MMV_Im2Im toolbox across various biomedical applications. We did not perform detailed statistical analyses to claim superiority over other methods or baselines. Instead, we aimed to provide a holistic view of image-to-image transformation concepts to biomedical researchers, hoping to inspire computational and experimental codesign methods for new biomedical studies.",
  "evaluation/availability": "The raw evaluation files used in our work are not directly available. However, all data used in this work were from open-accessible public repositories, released with other publications under open-source licenses. The scripts used to download and reorganize the data can be found in the release branch called “paper_version” within our repository. Detailed information about each dataset is listed in the same order as the Results section. Snapshots of our code and other data further supporting this work are openly available in the GigaScience repository, GigaDB. Additionally, all the trained models and sample data have been deposited at Zenodo to ensure the reproducibility of our work. This approach allows researchers to access the necessary data and scripts to reproduce our results and adapt them to new problems of interest."
}