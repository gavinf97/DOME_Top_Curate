{
  "publication/title": "AIUPred: combining energy estimation with deep learning for the enhanced prediction of protein disorder",
  "publication/authors": "The authors who contributed to this article are Gábor Erdős and Zsuzsanna Dosztányi. Zsuzsanna Dosztányi is the corresponding author, and her contact information is provided for correspondence. She is affiliated with the Department of Biochemistry at Eötvös Loránd University. The specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Nucleic Acids Research",
  "publication/year": "2024",
  "publication/doi": "10.1093/nar/gkae385",
  "publication/tags": "- Intrinsically Disordered Proteins\n- Protein Disorder Prediction\n- Deep Learning\n- AIUPred\n- IUPred\n- Energy Estimation\n- Protein Structure\n- Bioinformatics\n- Computational Biology\n- Machine Learning",
  "dataset/provenance": "The dataset utilized for training and testing the transformer neural network in our study was derived from the PISCES database, specifically version PISCES_cullpdb_pc40.0_res0.0–2.2_len40-10000_R0.25_Xray_d2022_05_22_chains18454. This dataset consists of 17,282 non-redundant structures. To ensure the quality and relevance of the data, sequences that could not be mapped to Uniprot sequences, those with a length below 30, and sequences containing transmembrane regions (as identified by the PDBTM database) were filtered out. This filtering process resulted in a dataset comprising 4,276,509 positions.\n\nThe dataset was split into two parts: 80% of the positions were used for training, and the remaining 20% were reserved for testing. This split ensures that the model is trained on a substantial amount of data while also having a robust testing set to evaluate its performance.\n\nFor the disorder propensity calculation, the DisProt database (version 2023-06) was used as the positive dataset. This dataset was filtered for redundancy using the CD-Hit algorithm and then divided into an 80-20 ratio for training and testing. Additionally, a set of globular domains, which includes sequence regions encoding single structural domains with determined monomeric structures in the PDB, was used as the negative dataset. This dataset also comprises 4,549 protein regions and was split into an 80-20 ratio for training and testing.\n\nFor validation purposes, the CAID1-PDB and CAID2-PDB datasets were employed. Any entries that were already part of the DisProt database were manually removed to ensure the validation process was unbiased. All these datasets are accessible at the web server under the statistics section, providing transparency and reproducibility for our research.",
  "dataset/splits": "The dataset used for training and testing the transformer neural network consisted of 4,276,509 positions from 17,282 non-redundant structures derived from the PISCES database. These positions were split into two main parts: one for training and one for testing. The training set contained 80% of the positions, while the testing set contained the remaining 20%.\n\nFor the transfer learning phase, where estimated energies were translated into disorder tendencies, the DisProt database (version 2023-06) was used as the positive dataset. This dataset was also split into training and testing sets, with 80% of the data used for training and 20% for testing. Additionally, a set of globular domains, comprising sequence regions that encode single structural domains with determined monomeric structures in the PDB, was used as the negative dataset. This set was similarly split into 80% for training and 20% for testing.\n\nFor validation purposes, the CAID1-PDB and CAID2-PDB datasets were employed. Each entry that was already part of the DisProt database was manually removed to ensure independence. These validation datasets were used to assess the performance of the model on unseen data.",
  "dataset/redundancy": "The datasets used in our study were carefully curated and split to ensure independence between training and test sets. For the energy prediction network, we utilized a dataset of 17,282 non-redundant structures from the PISCES database. This dataset was filtered to remove sequences that could not be mapped to Uniprot sequences, those with a length below 30, and sequences containing transmembrane regions based on the PDBTM database. The resulting dataset consisted of 4,276,509 positions, which were then split into training and testing sets containing 80% and 20% of the positions, respectively.\n\nFor the disorder propensity calculation, we used the DisProt database as the positive dataset and a set of globular domains as the negative dataset. Both datasets were filtered for redundancy using the CD-Hit algorithm and split into training and testing sets in an 80-20 ratio. This splitting ensured that the training and test sets were independent, preventing any data leakage that could artificially inflate the performance metrics.\n\nThe distribution of our datasets is comparable to previously published machine learning datasets in the field of protein disorder prediction. We ensured that our datasets were representative of the diversity found in naturally occurring proteins, which is crucial for the generalizability of our models. The use of non-redundant structures and the application of filtering criteria helped in maintaining a balanced and representative dataset, which is essential for training robust and accurate predictive models.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages well-established machine-learning techniques, specifically neural networks, rather than introducing a novel algorithm. The core of our approach is a transformer architecture, which is a type of model that has gained significant traction in various fields, including natural language processing and protein structure prediction. This architecture is not new but has been adapted and optimized for our specific task of energy prediction and disorder propensity calculation.\n\nThe transformer architecture used in our energy predictor consists of a token vector input, positional and feature encoders, multi-headed attention layers, and a linear decoder. This setup allows the model to capture complex patterns in the sequential data of protein residues. The optimization process involves training the transformer to predict positional energies using a dataset derived from the PISCES database. The training was conducted on a commercial NVidia 1080 GPU utilizing PyTorch, with a batch size of 512. The Adam optimizer was used with a learning rate of 1e-6 and a decay of 0.9 every 10 steps. The loss function converged in just 50 epochs, indicating efficient training.\n\nFor the transfer learning phase, where the estimated energies are translated into disorder tendencies, a smaller neural network was built on top of the frozen energy prediction network. This network consists of three fully connected layers with 16, 8, and 4 neurons respectively, using a rectified linear unit activation for the hidden layers and a sigmoid function for the output layer. The Adam optimizer was again employed, this time with a learning rate of 0.001, and the loss was measured using binary cross-entropy.\n\nThe choice to use these established algorithms and frameworks was driven by their proven effectiveness and efficiency in handling sequential data and high-dimensional feature spaces. The transformer architecture, in particular, has shown remarkable success in capturing long-range dependencies and contextual information, making it well-suited for our task. The decision to publish this work in a scientific journal rather than a machine-learning journal is due to the focus on the biological application and the improvements in disorder prediction, rather than the introduction of a new machine-learning algorithm. The primary contribution lies in the application and optimization of these algorithms for protein disorder prediction, demonstrating significant advancements in accuracy and speed.",
  "optimization/meta": "The AIUPred model does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a transformer neural network for energy prediction and a subsequent neural network for translating these energies into disorder propensities. The transformer architecture processes a window of amino acids centered around a given position, converting this sequence into a token vector. This vector is then encoded using positional and feature encoders, followed by a multi-headed attention layer. The output of this transformer is fed into a smaller neural network consisting of fully connected layers to predict disorder tendencies.\n\nThe training data for the energy prediction network is derived from non-redundant protein structures, filtered to exclude sequences that could not be mapped to Uniprot sequences, those with lengths below 30, and those containing transmembrane regions. This dataset is split into training and testing sets, ensuring independence between the two. For the disorder propensity calculation, the model uses the DisProt database as the positive dataset and a set of globular domains as the negative dataset, both split into training and testing sets. Additionally, validation datasets like CAID1-PDB and CAID2-PDB are used to assess the model's performance, with entries already part of DisProt removed to maintain independence.",
  "optimization/encoding": "For the energy prediction transformer architecture, the input data for predicting a given position consisted of a window of 101 residues, centered on the position in question. This window was transformed into a token vector using standard integer-based tokenization, where each of the 20 standard amino acids was represented by a unique integer from 1 to 20, and non-standard residues were represented by a 21st token. To ensure uniformity in the input size, zero padding was added to the token vectors for N- and C-terminal residues.\n\nThe token vector was then processed by a positional and feature encoder. The feature encoder had 22 dimensions, while the positional encoder had a dimensionality of 32. This encoded data was subsequently fed into a multi-headed attention layer, which contained two heads, each with 256 hidden layers. Two layers of encoders were utilized to further process the data. To mitigate overfitting, a dropout rate of 0.2 was applied.\n\nThe transformer's decoder was replaced with a linear layer consisting of 3232 neurons. This layer was designed to handle the output dimensionality, which was calculated as (window size * 2 + 1) * model size. The learning rate for the training process was set to 1e-6, with a decay of 0.9 every 10 steps. The training was conducted on a commercial NVidia 1080 GPU using PyTorch version 2.0.1 with CUDA version 11.7. A batch size of 512 was used, and the optimization was performed using the Adam optimizer with a Mean Squared Error loss function. The loss function converged in just 50 epochs, with both training and test losses falling just below 1.5.\n\nFor the transfer learning phase, the weights of the energy prediction network were frozen, and a smaller network was built on top of it. This network translated the estimated energies into disorder tendencies. The input for this network was the output of the transformer within the same 101-residue window. The network consisted of three fully connected layers with 16, 8, and 4 neurons, respectively. A rectified linear unit (ReLU) activation function was used for each hidden layer, while the output layer employed a sigmoid function to obtain propensity values. The Adam optimizer was used with a learning rate of 0.001, and the loss was measured using binary cross-entropy.",
  "optimization/parameters": "The model utilizes a transformer architecture for energy prediction, which is optimized to minimize the number of parameters for rapid and accurate energy prediction. The transformer network is designed with a window size of 101 (50 residues on each side of the central residue plus the residue itself). The feature encoder has 22 dimensions, and the positional encoder has a dimensionality of 32. The attention layer contains two heads, each with 256 hidden layers. The decoder is replaced with a linear layer consisting of 3232 neurons, derived from the formula ((window size * 2) + 1) * model size.\n\nThe network architecture was optimized to keep the amount of optimizable parameters to a minimum, ensuring efficient training and inference. The specific number of parameters is not explicitly stated, but the design choices indicate a focus on balancing model complexity and performance.\n\nFor the transfer learning component, a small neural network is built on top of the frozen energy prediction network. This network consists of three fully connected layers with 16, 8, and 4 neurons respectively. The activation function used is a rectified linear unit (ReLU) for the hidden layers and a sigmoid function for the output layer to obtain propensity values.\n\nThe learning rate for the energy prediction network is set to 1e-6 with a decay of 0.9 every 10 steps. For the transfer learning network, the learning rate is 0.001. The Adam optimizer is used for both networks, with the energy prediction network using mean squared error loss and the transfer learning network using binary cross-entropy loss.\n\nThe model was trained on a commercial NVidia 1080 GPU using PyTorch version 2.0.1 with CUDA version 11.7, and a batch size of 512. The training process converged in 50 epochs, with both training and test loss just below 1.5. The Pearson correlation coefficient for the average values is above 0.99, indicating high precision in energy estimation.",
  "optimization/features": "The input features for the prediction of a given position consist of a window of 101 residues, centered on the position in question. This window is transformed into a token vector using standard integer-based tokenization, where each of the 20 standard amino acids is represented by a unique integer from 1 to 20, and non-standard residues are represented by a 21st token. To ensure uniformity, a zero-padding is added to the token vector for N and C-terminal residues.\n\nThe feature encoder used in the model has 22 dimensions. This encoder, along with a positional encoder of 32 dimensions, processes the token vector. The positional encoder helps the model understand the position of each residue within the sequence.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model relies on the transformer architecture to weigh the importance of different features implicitly during training. The use of a multi-headed attention layer allows the model to focus on different parts of the input sequence, effectively selecting relevant features for each prediction.\n\nThe training process involved a dataset of 17,282 non-redundant structures derived from the PISCES database. Sequences that could not be mapped to Uniprot sequences, those with a length below 30, and sequences with transmembrane regions were filtered out. The dataset consisted of 4,276,509 positions, which were split into training and testing sets in an 80-20 ratio. This splitting ensures that the model's performance is evaluated on unseen data, providing a robust measure of its generalization capability.",
  "optimization/fitting": "The fitting method employed in our study utilized a transformer neural network for energy prediction, which was subsequently used to estimate disorder tendencies. The transformer architecture was designed to keep the number of optimizable parameters to a minimum, ensuring that the model did not become overly complex relative to the number of training points.\n\nThe dataset consisted of 4,276,509 positions, split into training and testing sets with an 80-20 ratio. This substantial number of training points helped mitigate the risk of overfitting. Additionally, a dropout rate of 0.2 was implemented during training to further prevent overfitting by randomly setting a fraction of input units to zero at each update during training time.\n\nTo address underfitting, the model's architecture was carefully optimized. The transformer included a positional and feature encoder followed by a multi-headed attention layer with two heads, each having 256 hidden layers. Two layers of encoders were utilized, and the decoder was switched to a linear layer with 3232 neurons. This configuration ensured that the model had sufficient capacity to learn the underlying patterns in the data.\n\nThe training process was conducted on a commercial NVidia 1080 GPU using PyTorch, with a batch size of 512. The Adam optimizer was used with a learning rate of 1e-6 and a decay of 0.9 every 10 steps. The loss function converged in just 50 epochs, with both training and test loss dropping below 1.5. This rapid convergence indicated that the model was neither underfitting nor overfitting the data.\n\nFurthermore, the Pearson correlation coefficient for the average values was above 0.99, demonstrating the high precision of the energy estimations. The model's performance was also validated on multiple datasets, showing consistent improvement over previous methods. These measures collectively ensured that the fitting method was robust and reliable.",
  "optimization/regularization": "In our optimization process, we implemented several techniques to prevent overfitting. One of the key methods used was dropout. Specifically, a dropout rate of 0.2 was applied. This technique involves randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent over-reliance on specific neurons and promotes the learning of more robust features.\n\nAdditionally, we utilized a learning rate decay strategy. The learning rate was set to 1e-6 initially and decayed by a factor of 0.9 every 10 steps. This gradual reduction in the learning rate helps to fine-tune the model parameters more effectively as training progresses, further aiding in the prevention of overfitting.\n\nThese regularization techniques, combined with our training approach, contributed to the model's ability to generalize well to unseen data, as evidenced by the convergence of the loss function and the high precision in energy estimation.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the transformer architecture used in our study are detailed in the supplementary material. This includes specifics such as the window size, tokenization method, dimensionality of the model, attention layers, dropout rate, learning rate, and decay schedule. The training was conducted on a commercial NVidia 1080 GPU using PyTorch with a specified batch size. The optimization was performed using the Adam optimizer with a Mean Squared Error loss function.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the supplementary material provides a comprehensive description of the network architecture and training process, which can be replicated by researchers interested in using or building upon our work. The supplementary material is accessible alongside the main publication, ensuring that the methodological details are transparent and reproducible.\n\nRegarding the license, the supplementary material and the main publication are typically subject to the standard licensing terms of the publishing journal or platform. These terms usually allow for academic use and citation, with specific guidelines on redistribution and modification. Researchers are encouraged to refer to the journal's policies for detailed information on licensing and usage permissions.",
  "model/interpretability": "The AIUPred model, while leveraging advanced neural network architectures, incorporates several features that enhance its interpretability. Unlike traditional black-box models, AIUPred maintains a level of transparency through its design and additional components.\n\nOne key aspect of interpretability in AIUPred is the use of a transformer neural network for energy prediction. This network is trained to predict positional energies, which are then used to estimate disorder propensities. The transformer architecture, with its attention mechanisms, allows for a more interpretable model compared to other deep learning approaches. The attention layers can highlight which parts of the input sequence are most influential in making predictions, providing insights into the model's decision-making process.\n\nAdditionally, AIUPred includes a smoothing function using the Savitzky-Golay filter. This filter helps to reduce noise in the predictions, making the output more interpretable by providing smoother disorder propensity profiles. The parameters of the filter are set to balance between preserving the important features of the data and reducing noise, which aids in the clarity of the results.\n\nThe model also benefits from the integration of various databases and annotations. For instance, the output includes information from DisProt, DIBS, MFIB, and ELM databases, which provide experimentally verified disordered regions and known motifs. This integration allows users to compare the model's predictions with established biological knowledge, enhancing the interpretability of the results.\n\nFurthermore, the web server interface of AIUPred is designed to be user-friendly and informative. It provides visualizations of the predictions using the PlotlyJS library, which allows users to interact with the data and gain a better understanding of the model's outputs. The interface also includes options to display additional annotations, such as post-translational modifications and PFAM annotations, which further aid in the interpretation of the results.\n\nIn summary, AIUPred strikes a balance between leveraging advanced neural network architectures for improved accuracy and maintaining interpretability through attention mechanisms, smoothing functions, and integration with biological databases. These features make AIUPred a more transparent and interpretable model compared to many other deep learning-based prediction tools.",
  "model/output": "The model is primarily a regression model, designed to predict the energy of a residue directly from its sequential neighborhood. It utilizes a transformer architecture to estimate these energies with high precision. The output of this energy prediction is then translated into disorder propensities using a neural network, which involves a classification aspect. The final output indicates disordered regions where values greater than 0.5 signify disorder. The model's performance is visualized using the PlotlyJS library, integrating various annotations from databases like DisProt, DIBS, MFIB, and ELM to aid in result interpretation. Additionally, the model includes a smoothing function using the Savitzky-Golay filter to enhance prediction capabilities and result interpretability. The output is designed to be user-friendly, with a single interface combining input and output pages, and features like dark mode for prolonged analysis sessions. The server calculates results on-the-fly and returns them asynchronously, ensuring faster loading times and immediate access to results.",
  "model/duration": "The model's execution time varies depending on the hardware used. On a CPU, it can analyze the human proteome in approximately 1.5 hours, processing about 3.5 proteins per second. However, utilizing a commercial-grade GPU, such as an NVidia 1080, significantly accelerates this process. With a GPU, the analysis time is reduced to just 3 minutes, equivalent to processing 100 proteins per second. This substantial speedup is due to the model's ability to scale exceptionally well on GPUs, even with consumer-level hardware. The efficiency of the model allows for rapid and extensive proteome analysis, making it a powerful tool for large-scale studies.",
  "model/availability": "The AIUPred software is publicly available and can be accessed through a dedicated web server. The web server provides a user-friendly interface where users can input protein sequences or UniProt accessions to perform disorder predictions. This interface combines both input and output pages into a single, streamlined design, which includes a 'dark mode' to reduce eye strain during extended analysis sessions.\n\nThe web server is designed to handle various types of inputs, including standard UniProt accessions, identifiers, and plain text protein sequences in FASTA format. It also supports predictions for disordered binding regions and redox state-dependent disordered regions. The server utilizes the latest Django-based backend for calculations and PlotlyJS for visualizing the results. Additionally, the server integrates with the UniProt resource to provide additional information about the requested proteins when available.\n\nFor those who prefer to run the algorithm locally or integrate it into their own workflows, a downloadable package is available. This package allows users to analyze whole genomes in mere seconds, leveraging the speed and accuracy of AIUPred. The software is released under the Creative Commons Attribution-NonCommercial License, which permits non-commercial re-use, distribution, and reproduction, provided the original work is properly cited. For commercial re-use, users are advised to contact the appropriate authorities.\n\nThe web server and downloadable package ensure that AIUPred is accessible to a wide range of users, from researchers needing quick predictions to developers looking to integrate advanced disorder prediction capabilities into their own tools.",
  "evaluation/method": "The evaluation of AIUPred involved several rigorous steps to ensure its performance and reliability. Initially, we compared AIUPred with its predecessor, IUPred, on various validation datasets, including CAID1-PDB and CAID2-PDB. This comparison showed significant improvements in accuracy, with AIUPred achieving higher AUC values across different datasets.\n\nTo assess the statistical significance of these improvements, we conducted a bootstrapped validation. This involved randomly selecting 100 sequences from an independent validation dataset and evaluating the resulting AUC values. AIUPred consistently outperformed IUPred, yielding an average AUC value of 0.9226 compared to IUPred’s 0.8992. The disparity between these mean values was found to be statistically significant, as determined by a Mann-Whitney test yielding a p-value of 2.5e-4.\n\nAdditionally, we tested the precision of AIUPred on the prediction of fully disordered proteins and short disordered regions. Using the DisProt database, AIUPred demonstrated clear improvements in predicting fully disordered proteins and short disordered regions, correctly identifying 44% of such regions compared to IUPred’s 37%. We also studied missing residues from X-ray structures in the PDB database, which often originate from short disordered segments. AIUPred correctly identified 44% of these regions, while IUPred identified only 36%.\n\nFurthermore, we compared AIUPred to other top-scoring single sequence methods from the CAID2 evaluation. AIUPred achieved an AUC of 0.912 on the CAID2-PDB set, while SPOT-Disorder-Single, the highest-ranking single sequence-based method, achieved an AUC of 0.917. However, AIUPred demonstrated superior speed, processing the human proteome in just 1.5 hours on a CPU and even faster with a commercial-grade GPU, reducing the time to just 3 minutes.\n\nIn summary, the evaluation of AIUPred involved comprehensive comparisons with existing methods, statistical validation, and performance testing on various datasets, all of which highlighted its superior accuracy and speed.",
  "evaluation/measure": "In the evaluation of AIUPred, several key performance metrics were reported to assess its effectiveness in predicting protein disorder. The primary metric used was the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. This metric was employed to evaluate the general accuracy of the method on various validation datasets, including those from the Critical Assessment of Intrinsic Protein Disorder (CAID) Prediction Portal. Specifically, AIUPred demonstrated superior performance with an average AUC value of 0.9226 compared to its predecessor, IUPred, which had an average AUC of 0.8992. This improvement was statistically significant, as indicated by a Mann-Whitney test yielding a p-value of 2.5e-4.\n\nIn addition to the AUC, the precision of AIUPred in predicting fully disordered proteins and short disordered regions was also assessed. For fully disordered proteins derived from the DisProt database, AIUPred outperformed IUPred, correctly identifying a higher percentage of such proteins. Similarly, AIUPred showed significant improvement in predicting short disordered regions, which are typically between 10 and 30 residues in length. It correctly predicted 44% of these regions, compared to IUPred's 37%. Furthermore, AIUPred was able to accurately identify 44% of missing residues in X-ray structures from the Protein Data Bank (PDB), which often originate from short disordered segments, whereas IUPred identified only 36%.\n\nThe reported metrics are representative of the current literature in the field of protein disorder prediction. The use of AUC as a primary metric is standard practice, as it provides a comprehensive measure of a method's ability to discriminate between ordered and disordered regions. The inclusion of precision metrics for specific types of disordered regions ensures that the method's performance is evaluated across a range of relevant biological scenarios. Overall, the set of metrics used in this evaluation is both thorough and aligned with established practices in the field.",
  "evaluation/comparison": "In the evaluation of AIUPred, a comprehensive comparison was conducted against publicly available methods using benchmark datasets. Specifically, AIUPred was compared to IUPred, its predecessor, on various validation datasets, including those from the Critical Assessment of Intrinsic Protein Disorder (CAID) Prediction Portal. The comparison focused on the Area Under the Curve (AUC) for predicting fully disordered proteins and short disordered regions. AIUPred consistently outperformed IUPred, demonstrating higher AUC values and better precision in identifying disordered regions.\n\nAdditionally, AIUPred was benchmarked against other top-scoring single-sequence methods from CAID2, such as SPOT-Disorder-Single and SETH. AIUPred achieved competitive AUC values while significantly reducing the computational time required for analysis. For instance, AIUPred processed the human proteome in just 1.5 hours on a CPU and even faster when utilizing a commercial-grade GPU, highlighting its efficiency and scalability.\n\nThe evaluation also included a bootstrapped validation, where 100 sequences were randomly selected from an independent validation dataset. AIUPred's average AUC value of 0.9226 was statistically significantly higher than IUPred's 0.8992, as determined by a Mann-Whitney test. This rigorous comparison underscores AIUPred's superior performance in predicting protein disorder, both in terms of accuracy and speed.",
  "evaluation/confidence": "To evaluate the confidence in our method's performance, we conducted a bootstrapped validation. This involved randomly selecting 100 sequences from an independent validation dataset and assessing the resulting AUC values. Across all instances, our method, AIUPred, consistently outperformed the previous version, IUPred, yielding an average AUC value of 0.9226 compared to IUPred’s 0.8992. The disparity between the mean values of the two sets is statistically significant, as determined by a Mann-Whitney test yielding a p-value of 2.5e-4. This statistical significance indicates that the improvement in performance is not due to random chance, but rather a genuine enhancement in the method's accuracy.\n\nAdditionally, we tested the precision of AIUPred on the prediction of fully disordered proteins as well as short disordered regions. On fully disordered proteins derived from the DisProt database, AIUPred vastly outperforms its predecessor. It also shows clear improvement in terms of predicting short disordered regions, correctly predicting 44% of such regions compared to IUPred's 37%. We also studied missing residues from X-ray structures, which often originate from short disordered segments. AIUPred correctly identified 44% of such regions, while IUPred was only able to assess 36%.\n\nThese results, along with the statistical significance of our findings, provide a high level of confidence in the superior performance of AIUPred over previous methods and baselines. The consistent improvement across different validation datasets and the statistically significant p-value from the Mann-Whitney test support the claim that AIUPred is a more accurate and reliable tool for predicting protein disorder.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved proprietary datasets and specific configurations that are integral to our research infrastructure. These datasets include sensitive information and are used under specific licenses that restrict public distribution. However, we have provided detailed descriptions of our methods and results in the publication, allowing other researchers to replicate our findings using their own datasets. For those interested in collaborating or accessing specific evaluation data, we encourage reaching out directly to discuss potential partnerships or data-sharing agreements. This approach ensures that the integrity and confidentiality of our evaluation process are maintained while fostering scientific collaboration."
}