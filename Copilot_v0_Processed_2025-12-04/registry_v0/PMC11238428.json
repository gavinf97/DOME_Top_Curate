{
  "publication/title": "Omada: An unsupervised machine learning toolkit for automated sample clustering of gene expression profiles",
  "publication/authors": "The authors who contributed to this article are:\n\n- S.K. and D.W. were responsible for conceiving the tools used in the study. S.K. also undertook the computational work and drafted the manuscript with D.W.\n- A.L., C.R., T.P.F., and M.W. participated in the data acquisition for the work.\n- All authors revised the manuscript critically for important intellectual content and gave final approval of the version submitted for publication.\n\nThe GUSTO study group includes:\n\n- Allan Sheppard\n- Amutha Chinadurai\n- Anne Eng Neo Goh\n- Anne Rifkin-Graboi\n- Anqi Qiu\n- Arjit Biswas\n- Bee Wah Lee\n- Birit Froukje Philipp Broekman\n- Boon Long Quah\n- Chai Kiat Chng\n- Cheryl Shufen Ngo\n- Choon Looi Bong\n- Christiani Jeyakumar Henry\n- Daniel Yam Thiam Goh\n- Doris Ngiuk Lan Loh\n- Fabian Kok Peng Yap\n- George Seow Heong Yeo\n- Helen Yu Chen\n- Hugo P. S. van Bever\n- Iliana Magiati\n- Inez Bik Yun Wong\n- Ivy Yee-Man Lau\n- Jeevesh Kapur\n- Jenny L. Richmond\n- Jerry Kok Yen Chan\n- Joanna Dawn Holbrook\n- Johan G. Eriksson\n- Joshua J. Gooley\n- Keith M. Godfrey\n- Kenneth Yung Chiang Kwek\n- Kok Hian Tan\n- Krishnamoorthy Naiduvae\n- Leher Singh\n- Lin Lin Su\n- Lourdes Mary Daniel\n- Lynette Pei-Chi Shek\n- Marielle V. Fortier\n- Mark Hanson\n- Mary Fong-Fong Chong\n- Mary Rauff\n- Mei Chien Chua\n- Michael J. Meaney\n- Maya Thway Tint\n- Neerja Karnani\n- Ngee Lek\n- Oon Hoe Teoh\n- P. C. Wong\n- Peter David Gluckman\n- Pratibha Keshav Agarwal\n- Rob Martinus van Dam\n- Salome A. Rebello\n- Seang Mei Saw\n- Shang Chee Chong\n- Shirong Cai\n- Shu-E Soh\n- Sok Bee Lim\n- Stephen Chin-Ying Hsu\n- Victor Samuel Rajadurai\n- Walter Stunkel\n- Wee Meng Han\n- Wei Wei Pang\n- Yap Seng Chong\n- Yin Bun Cheung\n- Yiong Huak Chan\n- Yung Seng Lee",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Omada\n- Unsupervised machine learning\n- Gene expression profiles\n- Sample clustering\n- Kolmogorov's D statistics\n- Dataset simulation\n- Clustering feasibility\n- Spectral clustering\n- K-means clustering\n- Hierarchical clustering\n- Whole blood RNA-seq data\n- Pregnant mothers\n- Chi-square statistical analysis\n- Generalized linear regression\n- Clinical variables\n- Maternal phenotypes\n- Scalability performance\n- Runtime and memory usage\n- Simulated datasets\n- Feature selection",
  "dataset/provenance": "The datasets utilized in this study originate from various sources, each serving specific analytical purposes. The single-class simulated dataset, comprising 100 samples and 100 genes, was generated using specific mean and standard deviation values. Similarly, the multiclass simulated dataset consists of 359 samples and 300 genes, also simulated with specific means and standard deviations.\n\nThe pan-cancer dataset, which includes 2,244 samples and 253 genes, was sourced from three distinct cancer types: breast cancer, colorectal cancer, and lung cancer. This dataset aggregates data from multiple cancer studies, providing a comprehensive view of gene expression across different cancer types.\n\nThe second TCGA-LUAD dataset contains 240 samples and 60,661 genes. This dataset focuses on lung adenocarcinoma, with primary tumors diagnosed as adenocarcinoma, NOS. The data was accessed through the Genomic Data Commons (GDC) Data Portal under the study accession phs000178.\n\nThe PAH dataset, comprising 359 samples and 300 genes, is available in the European Genome-phenome Archive (EGA) database under the accession code EGAS00001005532. Access to this dataset is restricted and requires permission from the Data Access Committee.\n\nThe GUSTO dataset includes 238 samples and 24,070 genes. This dataset is available in the NCBI Gene Expression Omnibus (GEO) under the accession number GSE182409. The dataset consists of RNA whole-blood samples obtained from mothers during mid-gestation.\n\nThe single-cell PBMC dataset, available in two versions (8k and 4k cells from a healthy donor), was sourced from the 10X Genomics website. This dataset underwent preprocessing using the Cell Ranger output and the Seurat pipeline, focusing on identifying the top 2,000 most variable genes for downstream analysis.\n\nThese datasets have been used in previous studies and by the community, providing a robust foundation for our analyses. The single-class and multiclass simulated datasets were generated specifically for this study, while the other datasets have been utilized in various research contexts, ensuring their reliability and relevance.",
  "dataset/splits": "In our study, we utilized several datasets, each with its own specific splits and distributions. For the simulated single-class dataset, we generated a dataset composed of 100 samples and 100 genes drawn from a single distribution. This dataset was used to evaluate the clustering feasibility and stability of our tool.\n\nFor the simulated multi-class dataset, we created a dataset with 359 samples and 300 genes. This dataset was used to assess the performance of different clustering methods and to generate a confusion matrix, as the actual labels were known.\n\nThe pan-cancer dataset consisted of 2,244 samples and 253 genes, sourced from three different cancer types: breast cancer, colorectal cancer, and lung cancer. This dataset was used to evaluate the tool's performance on a diverse set of cancer types.\n\nThe PAH dataset contained 359 samples and 300 genes. This dataset was used to explore the gene expression profiles of IPAH patients and to discover potential subgroups within the data.\n\nThe GUSTO dataset included 238 samples and 24,070 genes, sourced from healthy maternal whole blood. This dataset was used to evaluate the tool's performance on a dataset with no known subgroups and to assess the potential biases in the clustering results.\n\nAdditionally, we used single-cell PBMC datasets with 8,000 and 4,000 cells from healthy donors. These datasets were used to benchmark the scalability of our tool.\n\nIn summary, we utilized multiple datasets with varying numbers of samples and genes to evaluate the performance and scalability of our tool. The specific splits and distributions of data points in each dataset were designed to address different research questions and to assess the tool's robustness and versatility.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data used in this study are available through various public and restricted-access forums. The simulated datasets, both single-class and multiclass, can be accessed and downloaded. The Pan-cancer tissue expression data is also publicly available. A second TCGA LUAD dataset can be accessed through the Genomic Data Commons (GDC) Data Portal under the study accession phs000178.\n\nThe transcriptomic data used in this study, specifically the PAH dataset, is available through the European Genome-phenome Archive (EGA) database under the accession code EGAS00001005532. Access to this data is restricted and requires approval from the Data Access Committee. Researchers must agree to the conditions of use, which include keeping the data secure and using it only for approved purposes. To apply for access, interested parties should contact cohortcoordination@medschl.cam.ac.uk. The application process involves receiving an application form within 30 days and having the request reviewed within 3 months. All requesters must adhere to the data access conditions specified by the EGA.\n\nThe GUSTO expression dataset is available in the NCBI Gene Expression Omnibus under the accession number GSE182409. The Single-cell PBMC 8k and 4k data are publicly available. Additionally, snapshots of the code and other supporting data are openly available in the GigaScience repository, GigaDB.\n\nThe data used to generate statistics, plots, and figures are accessible through an interactive portal. The expression datasets used in this work can be accessed through the specified sources, ensuring transparency and reproducibility of the findings.",
  "optimization/algorithm": "The optimization algorithm employed in our work is rooted in unsupervised machine learning, specifically designed for automated sample clustering of gene expression profiles. The core of our approach involves several established clustering methods, including spectral clustering, k-means, and hierarchical clustering. These algorithms are well-known in the machine-learning community and have been extensively studied and applied in various domains.\n\nThe choice of these algorithms is driven by their ability to handle high-dimensional data, which is characteristic of gene expression profiles. Spectral clustering, for instance, leverages the eigenvalues of similarity matrices to perform dimensionality reduction before clustering, making it particularly effective for complex datasets. K-means is a classic partitioning method that aims to divide the data into k clusters, each represented by the mean of the points in the cluster. Hierarchical clustering, on the other hand, builds a tree of clusters by either merging or dividing them successively.\n\nWhile the individual algorithms themselves are not new, their integration and application within the context of gene expression analysis are novel. The toolkit, named Omada, is designed to automate the process of sample clustering, which involves selecting the optimal number of clusters, features, and stability thresholds. This automation is crucial for handling the large and complex datasets typical in genomics.\n\nThe reason for not publishing this work in a machine-learning journal is that the primary focus is on the application and integration of these algorithms in the field of bioinformatics, rather than the development of new machine-learning techniques. The toolkit's effectiveness is demonstrated through its application to various datasets, including simulated single-class and multi-class datasets, as well as real-world datasets like the PanCan and I/PAH datasets. The performance criteria and stability metrics used in our evaluations are tailored to the specific challenges and requirements of gene expression analysis.\n\nIn summary, the optimization algorithm used in Omada is based on established unsupervised machine-learning techniques, adapted and integrated to address the unique challenges of gene expression profiling. The toolkit's innovation lies in its automated and comprehensive approach to sample clustering, making it a valuable resource for researchers in the field of genomics.",
  "optimization/meta": "The optimization process in our toolkit employs an ensemble learning approach, which can be considered a form of meta-predictor. This method integrates multiple internal cluster indexes to make decisions, rather than relying on a single metric. By doing so, it avoids bias from specific metrics and frees the user from making assumptions about the optimal number of clusters.\n\nThe ensemble learning approach uses 15 different indexes to evaluate the clustering quality. These indexes assess various aspects of the generated clusters, such as compactness and the distance between cluster centers. The toolkit calculates the value of these indexes for each potential number of clusters (k) within a specified range. The means of these indexes over the range of k are then computed, and the optimal k is estimated by majority voting among the indexes that evaluate compactness and/or the distance between different subgroups.\n\nThis approach ensures that the decision-making process is robust and generalizable, as it considers multiple perspectives on cluster quality. The use of an ensemble of indexes helps to mitigate the risk of selecting an ineffective index and provides a more comprehensive evaluation of the clustering results.\n\nThe training data for this meta-predictor is independent, as the indexes are calculated based on the clustering results of the dataset itself, without relying on external data or assumptions. This independence ensures that the meta-predictor can be applied to a wide range of datasets and scenarios, providing consistent and reliable results.",
  "optimization/encoding": "The data encoding and preprocessing steps were designed to ensure optimal performance of the machine-learning algorithm. For the multi-issue pan-cancer dataset, RNA-seq expression data was downloaded for 2,244 samples and 253 genes representing three types of cancers: breast, lung, and colon/rectal. The mRNA expression was in the form of z-scores relative to normal samples, and an additional step of arcsine normalization was applied. This transformation calculates a proportional pseudocount per gene, creating a compression effect that accommodates genes with low expression values. After filtering for tissue-specific genes for the three cancer types, 243 genes were retained.\n\nFor the simulated datasets, the feasibilityAnalysisDataBased function was used to generate a multi-class dataset with 359 samples and 300 genes, composed of five groups of samples drawn from five different distributions. The feasibilityAnalysis function simulated a single-class dataset of 100 samples and 100 genes drawn from a single distribution.\n\nThe preprocessing steps included determining the optimal number of features and clusters. The minimum and maximum number of clusters to be tested were specified, along with the feature step, which defines the number of features by which the generated datasets grow. The optimal features and clusters were selected based on stability measures, ensuring robust and reliable clustering results.\n\nAdditionally, for the whole blood RNA-seq dataset from pregnant mothers, 238 samples and 24,070 features were used. The data was preprocessed to determine the optimal number of features and clusters, with the spectral clustering method showing the highest performance.\n\nIn summary, the data encoding and preprocessing involved normalization, filtering, and optimization of features and clusters to enhance the performance of the machine-learning algorithm.",
  "optimization/parameters": "In our study, the number of parameters used in the model varies depending on the specific clustering method and the dataset being analyzed. The primary parameters include the number of clusters (k), the number of features, and the specific algorithm parameters such as kernels in k-means and spectral clustering, or the linkage method in hierarchical clustering.\n\nThe selection of these parameters is automated to ensure optimal clustering without the need for manual tuning. We utilize cluster stabilities to determine the best parameters. This process involves measuring the average bootstrap stability of the clusters using the previously determined optimal k and feature set for each parameter. The parameter that yields the highest stability is then selected for the optimal clustering run.\n\nFor the number of clusters (k), we test a range starting from 2 up to a maximum value (maxclusters), which is dataset-specific. The optimal k is identified through a voting mechanism that considers multiple internal indexes, each evaluating different aspects of cluster compactness and separation. This approach ensures that the chosen k is robust and not biased by the dataset.\n\nThe number of features is also optimized by testing different feature sets and selecting the one that provides the highest stability. This step is crucial for ensuring that the clustering results are reliable and meaningful.\n\nIn summary, the model parameters are selected through a systematic process that prioritizes cluster stability and robustness, ensuring that the clustering results are both accurate and biologically relevant.",
  "optimization/features": "The input features used in our analysis vary depending on the dataset. For instance, in one dataset, we utilized 300 genes, while in another, we used 243 genes. In other cases, the number of features was much higher, such as 25,955 features in a different dataset. The optimal number of features was determined through a process that considered the stability of the generated test clusters and the range of cluster numbers.\n\nFeature selection was indeed performed to ensure that only the most informative features were used for clustering. This process involved ranking features based on their variance across samples, with the most variable features being selected. This approach helps to exclude features that remain stable across samples and do not provide discriminatory power to our unsupervised machine learning models. The feature selection procedure is exhaustive and incremental, considering all genes in the feature set. It does not require deep knowledge or filtering decisions from the user. This step is crucial because many clustering algorithms are heavily affected by a large number of features, both computationally due to input size and in performance due to misdirecting data noise. By selecting the most variable features, we aim to improve the computational efficiency and performance of our clustering algorithms. The feature selection was performed using the training set only, ensuring that the selected features are representative of the data used for clustering.",
  "optimization/fitting": "In the context of our toolkit, the fitting method involves several key considerations to ensure that neither overfitting nor underfitting occurs. The number of parameters is indeed larger than the number of training points, but we employ several strategies to mitigate the risk of overfitting.\n\nFirstly, we use a modular approach that allows for the addition of various clustering methods and parameters. This flexibility ensures that we can test multiple configurations and select the most appropriate one based on stability and agreement scores. The agreement score, calculated using the adjusted Rand Index, helps in evaluating the consistency of clustering results across different runs and parameters. This method ensures that the chosen parameters capture an underlying signal in the data rather than fitting noise.\n\nTo further rule out overfitting, we perform multiple comparisons between runs of the same approach. Each comparison involves two variations of the same algorithm with different parameters. This process helps in identifying parameters that consistently produce stable and reliable clusters. Additionally, we use a predefined set of parameters selected randomly with replacement, ensuring that the same parameters are not used within a single comparison. This random selection helps in avoiding overfitting to specific parameter settings.\n\nFor underfitting, we consider a logical range of clusters (k) that is most probable to exist within our data. This range is determined by prior knowledge, previous studies, or domain expertise. By testing a range of k values, we ensure that the clustering method can capture the underlying structure of the data without being too simplistic. The optimal number of clusters is selected based on the highest intramethod agreement over this logical range, ensuring that the chosen number of clusters is neither too few nor too many.\n\nMoreover, we automate the selection of parameters for each clustering method to ensure optimal performance. This automation involves measuring the average bootstrap stability of the clusters using the previously determined optimal k and feature set for each parameter. The parameter that produces the highest stability is used for the optimal clustering run, further reducing the risk of underfitting.\n\nIn summary, our fitting method involves a combination of modularity, multiple comparisons, random parameter selection, and automated parameter tuning. These strategies collectively help in ruling out both overfitting and underfitting, ensuring robust and reliable clustering results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our clustering results. One of the key methods used was parameter tuning based on cluster stability. By automating the selection of parameters for each clustering method, we aimed to minimize the risk of overfitting. This process involved measuring the average bootstrap stability of the clusters using the previously determined optimal number of clusters (k) and feature set for each parameter. The parameter that produced the highest stability was then used for the optimal clustering run. This approach helped in selecting the most reliable parameters that generalize well to unseen data.\n\nAdditionally, we utilized different clustering methods such as spectral clustering, k-means, and hierarchical clustering. By comparing the performance of these methods, we could identify the most suitable algorithm for each dataset, further reducing the likelihood of overfitting to a specific method's biases. The performance of these methods was evaluated using adjusted Rand index (PA), which provided a measure of the agreement between the predicted and true cluster labels.\n\nAnother important aspect of our approach was the use of simulated datasets with known distributions. This allowed us to validate the feasibility of clustering and assess the performance of our methods under controlled conditions. By generating datasets with varying numbers of samples and features, we could evaluate the scalability and robustness of our clustering algorithms.\n\nFurthermore, we conducted extensive testing on multiple real-world datasets, including multi-class and single-class simulated datasets, as well as datasets from different biological contexts such as cancer and pregnancy studies. This diverse range of datasets helped in assessing the generalizability of our methods and ensured that they were not overfitted to a specific type of data.\n\nIn summary, our study incorporated several overfitting prevention techniques, including parameter tuning based on cluster stability, the use of multiple clustering methods, and extensive validation on diverse datasets. These measures collectively contributed to the reliability and robustness of our clustering results.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the supplementary materials. Specifically, the parameters for dataset clustering feasibility, clustering method selection, sample set selection, and k-estimation are provided in Supplementary Table 1. This table includes details such as the maximum and average stability for clustering, the comparison of different clustering methods like Spectral, K-means, and Hierarchical, and the optimal number of features and clusters identified.\n\nThe results generated from our tool's application on various datasets, including simulated single-class and multi-class datasets, as well as real-world datasets like the PANCAN, I/PAH, and GUSTO datasets, are available in Supplementary Tables 2, 4, 5, 7, and 8, respectively. Additionally, the confusion matrix for the simulated multi-class dataset, which includes the actual labels, is presented in Supplementary Table 6.\n\nThese supplementary tables provide a comprehensive overview of the configurations and results, ensuring reproducibility and transparency. The supplementary materials are accessible under the same licensing terms as the main publication, allowing researchers to utilize and build upon our findings.",
  "model/interpretability": "Not enough information is available.",
  "model/output": "The model described in this publication is an unsupervised machine learning toolkit named Omada, designed for automated sample clustering of gene expression profiles. Therefore, it is neither a classification nor a regression model in the traditional sense. Instead, it focuses on clustering, which is an unsupervised learning technique used to group similar data points together based on certain features or characteristics.\n\nOmada evaluates the feasibility of clustering datasets composed of samples and genes, determining optimal parameters such as the number of clusters and features. The toolkit employs various clustering methods, including Spectral, K-means, and Hierarchical, to identify the most suitable approach for a given dataset. The performance of these methods is assessed using purity and adjusted rand index metrics.\n\nThe scalability of Omada has been evaluated across different scenarios using simulated datasets. This evaluation includes measuring runtime and memory usage as the number of samples per feature and the number of features per sample vary. The results indicate that the toolkit can handle datasets of varying sizes and complexities, making it a versatile option for gene expression profile analysis.\n\nOmada's performance has been tested on multiple datasets, including simulated single-class and multi-class datasets, as well as real-world datasets such as PanCan, I/PAH, and GUSTO. The results generated from these datasets can be found in the supplementary tables provided in the publication. Additionally, the confusion matrix for the simulated multi-class dataset is presented, offering insights into the toolkit's clustering accuracy.\n\nIn summary, Omada is an unsupervised machine learning toolkit designed for clustering gene expression profiles. It evaluates clustering feasibility, selects optimal parameters, and assesses performance using various metrics. The toolkit's scalability and versatility have been demonstrated through extensive testing on simulated and real-world datasets.",
  "model/duration": "The execution time of the Omada pipeline varies depending on the number of samples and features analyzed. For simulated datasets, the runtime increases significantly with the number of samples. For instance, processing 100 samples took around 0.3 minutes on average, while processing 3000 samples took approximately 320.2 minutes. The increase in runtime is more pronounced when the number of samples is large, such as 3000, where the average runtime can exceed 280 minutes.\n\nWhen examining the effect of the number of features, the runtime increase is less substantial. For example, with 1000 samples, the runtime varied from about 18.4 minutes with 20 features to 22.3 minutes with 5000 features. This indicates that the number of samples has a more significant impact on runtime than the number of features.\n\nFor single-cell PBMC data, the average time taken to process 1000 samples with 1000 features was approximately 11.0 minutes. This is notably less than the time taken for simulated datasets with the same number of samples and features, which was around 20.3 minutes. This difference can be attributed to the complexity and sparsity of the input matrix, which can vary between different types of datasets.\n\nIt is important to note that there can be slight variations in runtime across different iterations due to factors such as the selection of genes during the feature selection step. These variations are generally minor and do not significantly affect the overall performance of the pipeline.\n\nIn summary, the execution time of the Omada pipeline is primarily influenced by the number of samples, with a smaller but noticeable impact from the number of features. The pipeline demonstrates efficient scalability, handling large datasets with reasonable runtime, although the exact time can vary based on the dataset's characteristics.",
  "model/availability": "The source code for the tools developed in this study is not publicly released. However, snapshots of our code and other supporting data are openly available in the GigaScience repository, GigaDB. This repository provides access to the necessary resources for further exploration and validation of our work. Additionally, the data used to generate statistics, plots, and figures are accessible through our interactive portal. For specific datasets, such as the GUSTO expression dataset, it is available in the NCBI Gene Expression Omnibus under the accession number GSE182409. The Single-cell PBMC 8k and 4k data are publicly available through other specified sources.",
  "evaluation/method": "The evaluation of our method involved several key steps and metrics to ensure its robustness and scalability. We assessed the performance across different scenarios using multiple simulated datasets and real-world data.\n\nWe measured runtime and memory usage as the number of samples per feature and the number of features per sample varied. This evaluation helped us understand the scalability of our method under different conditions. The average runtime and memory usage were tracked to provide a clear picture of performance across varying datasets.\n\nTo determine the significance of differences between simulated distributions, we utilized Kolmogorov's D statistics. This statistical measure was crucial in understanding the variability and reliability of our simulated data.\n\nWe also evaluated the clustering feasibility of datasets by simulating datasets based on input data and user-estimated classes. This process involved generating datasets with specific means and standard deviations, allowing us to assess how well our method could handle different types of data.\n\nFor unsupervised learning, we implemented a tool to calculate an average agreement score per clustering approach. This tool compared multiple runs within each clustering method, using various parameters specific to the dataset provided. The agreement score helped us understand the consistency and robustness of different clustering algorithms.\n\nAdditionally, we considered the stability of clusters using the clusterboot function in R package fpc v2.2–3. The number of clusters k was considered within a specific range, and the maximum and average stabilities were reported. This provided a stability-based quality score, offering insights into whether a dataset was suitable for clustering studies.\n\nIn summary, our evaluation method involved a combination of statistical measures, simulated datasets, and real-world data to assess the performance, scalability, and robustness of our clustering toolkit.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our clustering methods. These metrics include the Adjusted Rand Index, which measures the agreement between the predicted and true clusters, and is used to compare the performance of different clustering algorithms such as hierarchical clustering, k-means, and spectral clustering. We also report the stability of clusters, which is assessed over a range of possible numbers of clusters (k) and different subsets of features. Stability is a crucial metric as it indicates the reliability of the clustering results across various conditions.\n\nAdditionally, we evaluate the partitioning agreement, which shows how well the clustering algorithms agree with each other and with the true class labels. This metric is particularly important for understanding the robustness of the clustering solutions. We also consider the number of clusters identified by the algorithms and compare it to the true number of clusters in the dataset. This helps in assessing the accuracy of the clustering methods in determining the correct number of clusters.\n\nThe contingency tables provide a visual representation of the combinations between generated clusters and real classes, highlighting the frequency of correct and incorrect assignments. This visual aid complements the numerical metrics and offers insights into the performance of the clustering algorithms.\n\nFurthermore, we assess the performance criteria for both single-class and multiclass simulated datasets. For the single-class dataset, the results demonstrate low scores for most steps, indicating unstable clustering runs. In contrast, for the multiclass dataset, the clustering tools show higher stability and better performance, with spectral clustering achieving the highest average partitioning agreement.\n\nOverall, the reported metrics are comprehensive and representative of the current literature on clustering evaluation. They provide a thorough assessment of the clustering methods' performance, stability, and agreement with true class labels, ensuring that the results are reliable and comparable to other studies in the field.",
  "evaluation/comparison": "In the evaluation of our toolkit, we conducted a comprehensive comparison with publicly available clustering methods on various benchmark datasets. Specifically, we assessed the performance of our tool against hierarchical clustering, k-means, and spectral clustering algorithms. These comparisons were performed on both simulated and real-world datasets, including single-class and multi-class simulated datasets, as well as high-dimensional single-cell PBMC datasets. The performance criteria included measures such as the adjusted Rand Index, which evaluates the agreement between predicted and true clusters, and cluster stability across different numbers of samples and features.\n\nFor the simulated multiclass dataset, which contained artificial samples from five distinct clusters, and the PANCAN dataset, composed of three different cancer types, we applied the three clustering algorithms. The results showed that spectral clustering generally achieved the highest average score in terms of partitioning agreement, indicating its robustness in capturing the underlying structure of the data. Additionally, we evaluated the real number of clusters in the datasets and compared it with the most likely number of clusters estimated by our tool, providing insights into the accuracy of our clustering method.\n\nFurthermore, we tested the ability of our clustering tools to produce stable clusters in various contexts by applying them sequentially on strategically simulated data. The data were composed of distinct classes based on class mean and standard deviation, ensuring a strong signal for accurate clustering. The clustering feasibility tool demonstrated high stability, with the highest stability reaching 78%, which indicated strong cluster consistency across our datasets. This stability was maintained even when considering a range of cluster numbers, k, from 2 to 6, ensuring that our tool could handle a broad range of clustering scenarios.\n\nIn summary, our evaluation included a thorough comparison with established clustering methods on benchmark datasets, demonstrating the effectiveness and robustness of our toolkit in various clustering scenarios. The use of multiple performance metrics and real-world datasets ensured that our comparisons were comprehensive and reliable.",
  "evaluation/confidence": "To evaluate the confidence in our results, we employed several statistical measures and methods. For determining the significance of differences between simulated distributions, we used Kolmogorov's D statistics. This statistic helps us understand the empirical cumulative distribution function (ECDF) and assess the differences between the generated classes.\n\nIn our clustering feasibility assessments, we reported both the maximum and average stabilities over all possible cluster numbers (k). These stability metrics provide a quality score that indicates whether a dataset is suitable for clustering studies. The stability is calculated using the clusterboot function in the R package fpc v2.2–3, ensuring that our results are robust and not due to random assignments.\n\nFor comparing different clustering methods, we calculated an average agreement score per clustering approach. This score measures the consistency of different clustering runs, indicating the robustness of the method. A high agreement score suggests that the clustering method is driven by the underlying structure of the data rather than random assignments.\n\nAdditionally, we performed chi-square and generalized linear regression (GLM) analyses on clinical variables and maternal phenotypes from the GUSTO dataset. The p-values from these analyses help us determine the statistical significance of the relationships between variables. For instance, the sequenced flow and machine variables showed highly significant p-values, indicating strong statistical evidence.\n\nOverall, our evaluation includes confidence intervals and statistical significance tests to ensure that our claims about the superiority of our methods are well-founded. The use of stability metrics, agreement scores, and statistical tests provides a comprehensive assessment of our toolkit's performance and reliability.",
  "evaluation/availability": "The raw evaluation files used to generate statistics, plots, and figures are accessible through an interactive portal. This portal provides a comprehensive way to explore the data and results presented in the publication.\n\nAdditionally, the datasets used for evaluation are available through various sources. The single-class and multiclass simulated datasets can be accessed and downloaded. The Pan-cancer tissue expression data is also available through specific sources. A second TCGA LUAD dataset can be accessed through the Genomic Data Commons (GDC) Data Portal under the study accession phs000178. The transcriptomic data used in this study can be accessed through the European Genome-phenome Archive (EGA) database under the accession code EGAS00001005532. Access to this data is restricted and requires approval from the \"UK National PAH Cohort Study Data Access Committee.\" Researchers must agree to the data access conditions found in EGA and apply for access through a specific email address. The GUSTO expression dataset is available in the NCBI Gene Expression Omnibus under the accession number GSE182409. The Single-cell PBMC 8k and 4k data are publicly available. Snapshots of the code and other supporting data are openly available in the GigaScience repository, GigaDB."
}