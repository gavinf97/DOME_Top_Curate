{
  "publication/title": "RNAVirHost: A Hierarchical Host Classification Framework for Emergent Novel RNA Viruses",
  "publication/authors": "The authors who contributed to this article are G.C., Y.S., and J.J. G.C. was involved in data collection and analysis, experiment execution, framework design, and paper writing. Y.S. conceptualized the study and oversaw the project. J.J. also oversaw the project. All authors reviewed, contributed to, and approved the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2024",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- RNA viruses\n- Host prediction\n- Machine learning\n- Genomic traits\n- Viral metagenomics\n- Cross-validation\n- XGBoost\n- Feature selection\n- Bioinformatics\n- Viral classification",
  "dataset/provenance": "The dataset used in our study is a comprehensive collection of RNA viruses curated from multiple sources. We initially gathered 6,735 viruses from the Virus–Host Database, which is a well-known repository covering complete viral genomes from NCBI RefSeq and other reliable sources. To enhance the diversity of our dataset, we supplemented it with 126,417 complete viral sequences with host annotations from NCBI GenBank. To ensure the quality and relevance of these sequences, we filtered them for \"complete genome\" or \"complete cds\" with lengths ranging from 3 to 50 kbp, which is typical for RNA viruses.\n\nTo remove redundancy, we combined these two datasets and used CD-HIT to de-replicate the reference sequences at 90% average nucleotide identity and 80% coverage. This process helped us maintain a high-quality dataset. Additionally, we cross-referenced the host annotations from GenBank with the NCBI Taxonomy database and performed manual validation to standardize host tags to their corresponding scientific names and remove any ambiguous annotations.\n\nThe final dataset was used as a high-quality reference for our label screening step. This curated dataset is essential for the accurate prediction of host lineages and ensures that our models are trained on reliable and diverse viral sequences. For more detailed information on the data collection and preprocessing steps, please refer to the Supplementary Information.",
  "dataset/splits": "The dataset was split into four distinct datasets, each corresponding to different host types. These splits were designed to cover a wide range of hosts, ensuring comprehensive evaluation. The first dataset included 21 viruses whose hosts are from six orders under Magnoliopsida plants. The second dataset focused on invertebrates, the third on fungi, and the fourth on fishes, specifically chordates. Each dataset comprised viral sequences originating from diverse hosts, including economically important species like shrimp and salmon, as well as less-studied species such as seahorses, Stellaria aquatica, and Cnidium officinale. This partitioning allowed for a detailed assessment of the tool's performance across different host types, providing a robust evaluation of its accuracy and utility in real-world scenarios.",
  "dataset/redundancy": "The dataset was meticulously curated to ensure high quality and minimal redundancy. Initially, a comprehensive collection of RNA viruses was gathered from the Virus–Host Database and NCBI GenBank, totaling over 133,000 sequences. To maintain data integrity, sequences were filtered for completeness and typical genome size ranges for RNA viruses. Redundancy was addressed using CD-HIT to de-replicate sequences at 90% average nucleotide identity and 80% coverage, ensuring that the reference dataset was non-redundant.\n\nTo validate the reliability of host annotations, the dataset was cross-referenced with the NCBI Taxonomy database and manually validated. Host tags were standardized to scientific names, and ambiguous annotations were removed. This process ensured that the dataset was of high quality and ready for label screening.\n\nThe dataset was then split into training and test sets to evaluate the performance of RNAVirHost. For cross-validation, a stratified 5-fold approach was employed, where each virus order was divided into non-overlapping folds based on host labels. This method ensured that the training and test sets were independent, with each fold being used once for testing and four times for training. This strategy helped in assessing the model's performance across different host types and virus orders.\n\nThe distribution of the dataset was designed to cover a wide range of hosts, including economically important species and less-studied ones. This comprehensive coverage aimed to replicate real-world scenarios that potential users of RNAVirHost might encounter. The dataset included sequences from various host types such as plants, invertebrates, fungi, and fishes, ensuring a diverse and representative sample.\n\nIn summary, the dataset was split using a stratified 5-fold cross-validation approach, ensuring independence between training and test sets. The distribution was designed to be comprehensive and representative, covering a broad spectrum of host types and virus orders, which is a significant improvement over previously published machine learning datasets that often focus on human- and mammalian-associated viruses.",
  "dataset/availability": "All supporting data and materials are available in the GigaScience repository, GigaDB. This ensures that the data is publicly accessible and can be used by other researchers for verification or further studies. The repository provides a reliable platform for data sharing, promoting transparency and reproducibility in scientific research. The data is made available under a license that allows for its use, ensuring that it can be accessed and utilized by the broader scientific community. The availability of the data in GigaDB enforces the principle of open science, making it easier for others to build upon the findings presented in the publication.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is gradient boosting. Specifically, we employed XGBoost, which is a popular and efficient implementation of gradient boosting decision trees. This algorithm was chosen due to its high performance in predictive modeling tasks.\n\nThe XGBoost algorithm is not new; it has been widely used and studied in the machine learning community. It was developed by Tianqi Chen and his team and has been extensively optimized for speed and performance. Given its established reputation and widespread use, it was deemed suitable for our host prediction tasks.\n\nThe reason XGBoost was not published in a machine-learning journal in this context is that our primary focus is on the application of machine learning to biological data, specifically for predicting hosts of RNA viruses. The innovation lies in the application and optimization of XGBoost for this specific biological problem, rather than in the development of a new algorithm. Our work demonstrates the effectiveness of XGBoost in handling complex biological data and provides insights into feature selection and model optimization for viral host prediction.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on various feature sets and their combinations to predict hosts for RNA viruses. The primary machine-learning method used is XGBoost, which was chosen due to its high accuracy in the experiments conducted. Other learning architectures evaluated include gradient boosting decision tree (GBDT), random forest (RF), support vector machine with RBF kernel, logistic regression, k-nearest neighbors, and Gaussian naive Bayes. However, these were not combined into a meta-predictor approach.\n\nThe training data used for the model is independent and follows a standard cross-validation strategy. Specifically, stratified 5-fold cross-validation was employed, where the data was split into non-overlapping folds based on host labels. This ensures that the model's performance is evaluated on unseen data, maintaining the independence of the training and test sets. The experiments also included leave-one-taxon-out validation to assess the model's performance on novel viruses, further ensuring the independence of the training data.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the quality and effectiveness of our machine-learning models. We began by curating a comprehensive dataset of RNA viruses, collecting 6,735 viruses from the Virus–Host Database and supplementing it with 126,417 complete viral sequences from NCBI GenBank. To maintain data quality, we filtered these sequences for \"complete genome\" or \"complete cds\" with lengths between 3 and 50 kbp, which is typical for RNA viruses. We then used CD-HIT to de-replicate the reference sequences at 90% average nucleotide identity and 80% coverage, removing redundancy.\n\nNext, we cross-referenced the host annotations from GenBank with the NCBI Taxonomy database and performed manual validation to ensure reliability. Host tags were standardized to corresponding scientific names, and ambiguous annotations were removed. This process resulted in a high-quality reference dataset.\n\nFor feature encoding, we considered various genomic traits, including nucleotide preference, dinucleotide preference, codon usage, codon pair bias, and amino acid usage. These traits were categorized into feature sets, such as \"Bias\" and its subset \"sBias,\" which excludes codon pair bias. Additionally, we evaluated other features like BLASTN, digital signal processing-based structural patterns (M-SP), and the frequency of k-mers and amino acid patterns.\n\nWe conducted stratified 5-fold cross-validation to assess the performance of different feature sets and their combinations. For each virus order, we stratified the viruses into non-overlapping 5-fold sets by host labels, trained models using 4 out of 5 folds, and tested them on the remaining fold. This process was repeated for all 5 folds to present the overall performance.\n\nThe genomic traits and sequence homologies of the query viruses were encoded and input into the models. Our hierarchical host prediction framework consists of two classification layers. In the first layer, we predict hosts at the kingdom and phylum levels, including Chordata, Invertebrate, Viridiplantae, Fungi, and Bacteria. In the second layer, we further predict specific host groups under Chordata at the class and order levels.\n\nWe evaluated several machine-learning architectures, including XGBoost, gradient boosting decision tree (GBDT), random forest (RF), support vector machine with RBF kernel, logistic regression, k-nearest neighbors, and Gaussian naive Bayes. Using the scikit-learn package with default parameters, we found that XGBoost delivered the top performance, achieving the highest accuracy of 94.0% in layer 1 and 88.0% in layer 2. This made XGBoost the preferred choice as the default architecture for RNAVirHost.\n\nIn summary, our data encoding and preprocessing involved rigorous curation, standardization, and feature evaluation to ensure the robustness and accuracy of our machine-learning models.",
  "optimization/parameters": "In our study, the number of parameters used in the model varies depending on the feature sets and their combinations. We evaluated 11 different feature sets, including genomic traits such as nucleotide preference, dinucleotide preference, codon usage, codon pair bias, and amino acid usage, as well as other features like BLASTN, digital signal processing-based structural patterns (M-SP), and various k-mer frequencies.\n\nThe selection of these features was based on comprehensive benchmark experiments. We conducted stratified 5-fold cross-validation to assess the performance of different feature sets and their combinations. This process involved training models using 4 out of 5 folds and testing them on the remaining fold, repeating this for all 5 folds to ensure robust evaluation.\n\nAmong the evaluated feature sets, the subset of genomic traits, particularly the one excluding codon pair bias (referred to as sBias), demonstrated superior performance. This subset was chosen for its effectiveness in host prediction, as it achieved the highest accuracy in both layer 1 and layer 2 of our classification tasks.\n\nAdditionally, we assessed the impact of combining different feature sets. The combination of sBias and BLASTN (referred to as sBias_Blast) yielded the best order-wise accuracy and F1 scores, indicating that this combination significantly improved prediction performance. This selection process ensured that the model parameters were optimized for accuracy and efficiency in host prediction.",
  "optimization/features": "In our study, we evaluated a total of 11 different feature sets for host prediction. These feature sets included various genomic traits such as nucleotide preference, dinucleotide preference, codon usage, codon pair bias, amino acid usage, BLASTN, digital signal processing-based structural patterns (M-SP), and the frequency of different k-mers (6-mer, 7-mer, 8-mer, amino acid 3-mer, AA4, physiochemical 5-mer, and PC6).\n\nFeature selection was indeed performed to identify the most effective features for prediction. Initially, we assessed the performance of each feature set individually using stratified 5-fold cross-validation. This process involved training models on 4 out of 5 folds and testing them on the remaining fold, ensuring that the performance was evaluated across all 5 folds.\n\nThe results indicated that the subset of genomic traits, excluding codon pair bias (referred to as sBias), achieved the highest accuracy. This subset was further validated through machine learning strategies, confirming its effectiveness. Additionally, we evaluated combinations of feature sets, finding that the combination of sBias and BLASTN (sBias_Blast) yielded the best performance in terms of both accuracy and F1 score.\n\nIt is important to note that the feature selection process was conducted using the training data only, ensuring that the evaluation was unbiased and that the selected features were truly representative of the underlying patterns in the data. This rigorous approach allowed us to identify the most relevant features for accurate host prediction.",
  "optimization/fitting": "The fitting method employed in our study involved a comprehensive evaluation of various feature sets and learning architectures to ensure robust host prediction for RNA viruses. We utilized stratified 5-fold cross-validation to assess the performance of different models, which helped in mitigating overfitting by ensuring that each fold contained a representative sample of the data.\n\nTo address the potential issue of overfitting, given the high-dimensional nature of some feature sets, we conducted feature selection and reduction. For instance, we created a subset feature set named \"sBias\" by excluding the codon pair bias from the \"Bias\" set, which included nucleotide preference, dinucleotide preference, codon usage, codon pair bias, and amino acid usage. This reduction significantly improved prediction performance, validating the effectiveness of feature selection in preventing overfitting.\n\nAdditionally, we compared the performance of different learning architectures, including XGBoost, gradient boosting decision tree (GBDT), random forest (RF), support vector machine with RBF kernel, logistic regression, k-nearest neighbors, and Gaussian naive Bayes. XGBoost emerged as the most accurate model, achieving the highest accuracy in both layer 1 and layer 2 predictions. This comparison ensured that the chosen model was not only fitting the training data well but also generalizing effectively to unseen data.\n\nTo rule out underfitting, we evaluated the performance of various feature combinations and learning architectures. The combination of sBias and BLASTN, trained with XGBoost, demonstrated superior accuracy, indicating that the model was complex enough to capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, we conducted statistical tests, such as the 1-sided Wilcoxon test, to compare the accuracy distribution of different models across virus orders. This analysis provided strong evidence that the observed improvements in prediction accuracy were statistically significant and not due to random chance, further confirming the robustness of our fitting method.\n\nIn summary, our approach involved careful feature selection, model comparison, and statistical validation to ensure that our fitting method was neither overfitting nor underfitting the data. The use of cross-validation and rigorous performance evaluation across different scenarios and feature sets contributed to the reliability and generalizability of our host prediction model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation, specifically stratified 5-fold cross-validation. This technique involves dividing the dataset into five folds, where each fold is used once as the validation set while the remaining four folds form the training set. This process is repeated five times, ensuring that each fold is used as the validation set once. This approach helps in assessing the model's performance more reliably and reduces the risk of overfitting to a particular subset of the data.\n\nAdditionally, we evaluated various feature sets and their combinations to identify the most effective ones. By comparing different feature sets, we could determine which features contributed most significantly to the model's performance. This feature selection process helps in reducing the dimensionality of the data and focusing on the most relevant features, thereby preventing the model from overfitting to noise or irrelevant information.\n\nWe also conducted experiments to assess the impact of sequence completeness on host prediction. By generating viral sequences with varying levels of completeness, we could understand how the model performs under different conditions and ensure that it generalizes well to real-world scenarios.\n\nFurthermore, we compared different learning architectures, including XGBoost, gradient boosting decision tree (GBDT), random forest (RF), support vector machine with RBF kernel, logistic regression, k-nearest neighbors, and Gaussian naive Bayes. The use of multiple learning architectures allowed us to select the one that performed best, reducing the risk of overfitting to a specific model.\n\nIn summary, our study incorporated cross-validation, feature selection, and the evaluation of different learning architectures to prevent overfitting and ensure the robustness and generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. The specific configurations and schedules employed for the XGBoost model, which achieved the highest accuracy, are outlined in the results section. These include the evaluation of different feature sets and their combinations through 5-fold cross-validation.\n\nThe model files and optimization parameters are not explicitly provided in the main text but can be inferred from the described methods and results. For instance, the use of XGBoost with default parameters from the scikit-learn package is mentioned, which implies that standard settings were applied unless otherwise specified.\n\nRegarding the availability of these configurations, the study is published under the MIT license, which allows for the free use, modification, and distribution of the work, provided that the original authors are credited. All supporting data and materials, including details on data collection, feature descriptions, and optimization strategies, are available in the GigaScience repository, GigaDB. This repository serves as a comprehensive resource for accessing the underlying data and methodologies used in our research.\n\nFor those interested in replicating or building upon our work, the GigaDB repository provides the necessary files and information to understand and utilize the optimization parameters and model configurations described in the publication. This ensures transparency and reproducibility, aligning with the principles of open science.",
  "model/interpretability": "The model, RNAVirHost, is not entirely a black box. It leverages several interpretable features and machine learning techniques that allow for some level of transparency in its predictions. The model uses a combination of genomic traits, sequence homologies, and virus taxonomy to predict hosts. These features include nucleotide preference, dinucleotide preference, codon usage, codon pair bias, and amino acid usage, which are all biologically meaningful and can be interpreted in the context of viral genomics.\n\nAdditionally, the model employs XGBoost, a gradient boosting framework that provides feature importance scores. These scores indicate the relative importance of each feature in making predictions. A higher fold value in the feature importance scores signifies greater importance demonstrated by the feature. This allows researchers to understand which genomic traits are most influential in predicting the hosts of RNA viruses.\n\nThe use of BLASTN, a tool for sequence alignment, also adds a layer of interpretability. BLASTN provides alignment scores that can be directly linked to sequence similarities between the query virus and known hosts. This helps in understanding why certain predictions are made based on sequence homology.\n\nFurthermore, the hierarchical classification framework of RNAVirHost allows for a stepwise interpretation of predictions. The first layer of the model predicts broad host categories, such as Chordata, Invertebrate, Plant, Fungi, and Bacteria. The second layer refines these predictions to provide more precise host classification information. This hierarchical approach makes it easier to trace the decision-making process of the model.\n\nIn summary, while RNAVirHost is a complex model that integrates multiple features and machine learning techniques, it is not a black box. The use of interpretable features, feature importance scores, and a hierarchical classification framework provides insights into how the model makes its predictions.",
  "model/output": "The model, RNAVirHost, is a classification model designed to predict the hosts of RNA viruses. It operates in a hierarchical manner, first categorizing viruses into broad host types and then refining these predictions in a second layer for more precise classification. The model uses a combination of genomic traits and sequence homology features, leveraging machine learning techniques to achieve high accuracy in host prediction. The primary output of RNAVirHost is the predicted host type for a given viral genome, which can include categories such as Chordata (Vertebrates), Invertebrates, Plants, Fungi, and Bacteria. The model's performance has been evaluated through various benchmarks, including cross-validation and leave-one-taxon-out experiments, demonstrating its effectiveness across diverse viral landscapes. The use of XGBoost as the learning architecture further enhances the model's accuracy, making it a robust tool for predicting the hosts of emergent novel RNA viruses.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our model is publicly available. It is written in Python and requires Python 3.8 to run. Additionally, it depends on several other tools and libraries, including BLAST 2.12.0+, Prodigal 2.6.3+, xgboost 2.0.3, pandas 2.0.3, biopython 1.83, and numpy 1.23.5. The software is released under the MIT license, which allows for free use, modification, and distribution.\n\nTo facilitate easy deployment and use, we have also made available supplementary files and additional information. These include various figures and tables that provide insights into the model's performance and feature importance. For instance, Supplementary Fig. S1 and S6 show the host prediction performance using different feature sets, while Supplementary Table S1 details the contribution of genomic traits to host prediction.\n\nFor those interested in running the algorithm, the necessary data and materials are available in the GigaScience repository, GigaDB. This ensures that users have access to all the resources needed to replicate our results or apply the model to their own datasets. The repository includes details on data collection, benchmark features, feature selection, and the trade-off between prediction rate and precision.\n\nWe have also provided information on the authors' contributions to the project, ensuring transparency and accountability. The study was supported by the Hong Kong Research Grants Council General Research Fund and the Hainan Provincial Natural Science Foundation of China. All authors have reviewed and approved the manuscript, and we declare that there are no competing interests.",
  "evaluation/method": "The evaluation of RNAVirHost was conducted through a series of comprehensive benchmark experiments designed to assess its performance across various scenarios. Initially, we employed stratified 5-fold cross-validation to compare different feature sets and their combinations. This involved dividing the data into five folds, training the model on four folds, and testing it on the remaining fold, ensuring that each fold was used for testing exactly once. This method allowed us to evaluate the model's performance consistently across different subsets of the data.\n\nWe tested 11 feature sets using the XGBoost ensemble learning algorithm. These feature sets included nucleotide preference, dinucleotide preference, codon usage, codon pair bias, amino acid usage, BLASTN, digital signal processing-based structural patterns, and various frequencies of k-mers. We also evaluated combinations of these features to determine their collective impact on prediction accuracy.\n\nTo assess the impact of sequence completeness on host prediction, we generated viral sequences with varying levels of completeness and evaluated the model's performance on these fragmented sequences. This involved creating contigs with different length ratios (90%, 75%, 60%, and 45%) and assessing how the model's accuracy varied with decreasing sequence completeness.\n\nFor evaluating the model's capability to identify hosts of novel viruses, we used the leave-one-genus-out strategy. This involved training the model without including specific genera and then testing its performance on those genera. This approach simulates the scenario where a novel query with an unknown genus label is used as input. We compared RNAVirHost against BLASTN and two null models to benchmark its performance.\n\nAdditionally, we evaluated RNAVirHost's performance on recently identified viruses by retraining the model on all reference viruses and assessing its accuracy on identifying hosts for these newly discovered viruses. This process was designed to replicate real-world scenarios that potential users of RNAVirHost might encounter. We collected datasets from various studies focusing on different host types, including plants, invertebrates, fungi, and fishes, to ensure a comprehensive evaluation.",
  "evaluation/measure": "In our evaluation of RNAVirHost, we employed a comprehensive set of performance metrics to ensure a thorough assessment of its predictive capabilities. The primary metrics we reported include accuracy, precision, F1 score, and prediction rate. Accuracy serves as a fundamental metric, representing the proportion of correctly predicted queries out of the total number of queries. This metric provides a straightforward measure of the model's overall performance.\n\nTo offer a more nuanced evaluation across different taxonomic levels, we introduced rank-wise accuracy, which is computed by averaging the accuracy of respective virus orders, families, and genera. This approach allows us to assess the model's performance at various levels of taxonomic detail.\n\nPrecision captures the ratio of correctly predicted queries to the total number of output predictions, providing insight into the model's reliability in making correct predictions. The prediction rate quantifies the ratio of output predictions to the total number of queries, indicating the model's coverage and ability to make predictions across a wide range of inputs.\n\nRecognizing the potential bias in reference labels towards human and vertebrate hosts, we included the macro F1 score as an additional evaluation metric. The macro F1 score is calculated by averaging the F1 scores of each individual host label, where the F1 score for a specific label is the harmonic mean of its precision and recall. This metric ensures that the evaluation is not skewed by the prevalence of certain host labels in the dataset.\n\nThese metrics collectively provide a comprehensive and granular evaluation of RNAVirHost's performance, enabling a robust analysis of its predictive capabilities and strengths. The choice of these metrics is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of RNAVirHost by comparing it against publicly available methods and simpler baselines. We benchmarked RNAVirHost against BLASTN, an alignment-based method, and two null models. The first null model randomly assigns host labels based on the distribution in the training data, while the second uses the dominant host label from the training data. These comparisons were performed using a leave-one-genus-out strategy to simulate the prediction of novel viruses.\n\nAdditionally, we assessed the performance of various feature sets and their combinations. We tested 11 different feature sets, including nucleotide preference, dinucleotide preference, codon usage, codon pair bias, amino acid usage, BLASTN scores, and digital signal processing-based structural patterns (M-SP). The feature set named \"sBias,\" which excludes codon pair bias, showed the highest accuracy and F1 scores. We also evaluated combinations of these features, finding that the combination of sBias and BLASTN (sBias_Blast) achieved the best performance.\n\nThe results demonstrated that RNAVirHost outperformed simpler baselines and was comparable to more complex methods like BLASTN. The improvements in prediction accuracy were statistically significant, as validated by the Wilcoxon test. This comprehensive evaluation underscores the effectiveness of RNAVirHost in predicting hosts for RNA viruses across diverse datasets.",
  "evaluation/confidence": "To ensure the robustness of our findings, we conducted a thorough statistical analysis. We employed the 1-sided Wilcoxon test to compare the performance of our method with other approaches, such as BLASTN and Bias_Blast. This test helped us demonstrate that the observed improvements in host prediction accuracy are statistically significant and not merely due to random chance. A small P-value (P < 0.05) from these tests provides strong evidence that the median accuracy of our method is significantly larger than that of the compared methods.\n\nAdditionally, we evaluated the performance metrics across different virus orders and host groups. This comprehensive assessment included metrics like accuracy, precision, F1 score, and prediction rate. By averaging these metrics at various taxonomic levels (order-wise, family-wise, and genus-wise), we ensured a nuanced and granular evaluation of our method's performance. This approach allowed us to offer a confident and reliable analysis of our method's predictive capabilities and strengths.\n\nWe also considered the potential bias in reference labels towards human and vertebrate hosts by including the macro F1 score as an additional evaluation metric. This score is calculated by averaging the F1 scores of each individual host label, providing a balanced assessment of our method's performance across different host types.\n\nFurthermore, we conducted experiments to assess the impact of sequence completeness on host prediction. By generating viral sequences with various levels of completeness, we demonstrated that our method can effectively handle different levels of sequence data, further boosting our confidence in its reliability.\n\nIn summary, our evaluation strategy involved rigorous statistical testing, comprehensive performance metrics, and consideration of potential biases. These steps collectively ensure that our claims of superior performance are well-founded and statistically significant.",
  "evaluation/availability": "All supporting data and materials are available in the GigaScience repository, GigaDB. This includes the raw evaluation files necessary for reproducing the results presented in the study. The data is publicly released under the MIT license, ensuring that it can be freely used, modified, and distributed by anyone. This open-access approach facilitates transparency and reproducibility in scientific research."
}