{
  "publication/title": "Deep learning links localized digital pathology phenotypes with transcriptional subtype and patient outcome in glioblastoma",
  "publication/authors": "The authors who contributed to this article are:\n\n- Thomas R. P. (Conceptualization, Methodology, Formal analysis and investigation, Writing—original draft preparation, Funding acquisition, Resources)\n- M. R. (Conceptualization, Methodology)\n- B. B. (Conceptualization, Methodology, Supervision)\n- G. L. (Conceptualization, Methodology, Supervision, Funding acquisition)\n- A. W. (Conceptualization, Methodology, Writing—review and editing, Funding acquisition, Resources, Supervision)\n- K.-H. N. (Methodology)\n- B. K. (Resources)\n- J. K. (Resources)\n\nAll authors contributed to the writing—review and editing of the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2024",
  "publication/doi": "10.1093/gigascience/giae057",
  "publication/tags": "- Deep Learning\n- Digital Pathology\n- Glioblastoma\n- Transcriptional Subtypes\n- Patient Outcome\n- Convolutional Neural Networks\n- Medical Imaging\n- Cancer Pathology\n- Phenotype-Genotype Correlations\n- Artificial Intelligence in Medicine",
  "dataset/provenance": "The dataset used in this study is primarily composed of digital slide scans, including both H&E-stained slides and immunohistochemistry-stained slides. The complete slide scan library, along with intermediate annotations such as tissue segmentations, is publicly available online via the GBMatch supplementary website. This includes preselected image tiles used for training, along with their corresponding annotations and segmentations, which are available via a Zenodo repository. Additionally, the external TCGA validation dataset is accessible through cBioPortal and the GDC Data Portal. Snapshots of the code and other supporting data are openly available in the GigaScience repository, GigaDB. The dataset has been used to train convolutional neural networks for predicting transcriptional subtypes and risk scores in glioblastoma histopathology images. The study also includes manual segmentations and quantitative characterizations of the tumor microenvironment, which are provided via GitHub. The dataset has been utilized to complement histologic assessments by adding spatially resolved information on transcriptional subtypes and prognostic patient information. The availability of these resources ensures reproducibility and facilitates further research by the community.",
  "dataset/splits": "The dataset was split into two main cohorts: the discovery cohort and the TS subcohort. The discovery cohort included a total of 276 patients with digital histology and outcome data. This cohort was used to train the RS-CNN using overall survival as a label. For 189 tumors in the TS subcohort, transcriptional subtype information was also available. This information was used as ground truth for training. Both cohorts featured a similar age range and female-to-male ratio. However, the TS cohort was slightly biased towards an increased receipt of temozolomide-based radiochemotherapy and prolonged survival.\n\nEach cohort was further divided into 5 equally large folds with comparable characteristics for internal 5-fold cross-validation. This means that both the discovery cohort and the TS subcohort were each split into 5 folds, resulting in a total of 10 folds across both cohorts. The specific distribution of data points in each fold was designed to be comparable, ensuring that each fold had similar characteristics to maintain the integrity of the cross-validation process.",
  "dataset/redundancy": "The dataset used in this study consisted of digital histology and outcome data from 276 patients, forming the discovery cohort. For 189 of these tumors, transcriptional subtype information was also available, creating a subcohort used for training the transcriptional subtype convolutional neural network (TS-CNN). Both cohorts featured a similar age range and female-to-male ratio. However, the transcriptional subtype (TS) cohort had a slight bias towards increased receipt of temozolomide-based radiochemotherapy and prolonged survival.\n\nTo ensure independence between training and test sets, we employed 5-fold cross-validation. Each cohort was split into five equally large folds with comparable characteristics. This method helps to mitigate overfitting and ensures that the model's performance is evaluated on different subsets of the data, enhancing the generalizability of the results.\n\nWe took additional steps to enforce independence between training and validation data. For instance, we avoided using models pretrained with histological data from The Cancer Genome Atlas (TCGA), as we used TCGA data for independent external validation. This precaution prevented the mixing of training and test data, which could otherwise lead to inflated performance metrics.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in histopathology. By including a diverse range of patients and ensuring a balanced representation of different transcriptional subtypes, we aimed to create a robust and generalizable model. The use of 5-fold cross-validation further ensures that the model's performance is not overly dependent on any single split of the data, providing a more reliable estimate of its true capabilities.",
  "dataset/availability": "The complete slide scan library, including H&E-stained slides and intermediate annotations such as corresponding tissue segmentations, is publicly available online via the GBMatch supplementary website. This includes all preselected image tiles used for training, along with their corresponding annotations and segmentations for the immunohistochemically stained slides, which are available via an accompanying Zenodo repository. The external TCGA validation dataset is accessible via cBioPortal and the GDC Data Portal. Additionally, snapshots of our code and other supporting data are openly available in the GigaScience repository, GigaDB. The code for CNN training is also available via GitHub, including code for the initial training of CV-folds and corresponding exemplary histological data and clinical annotation. A final fully trained predictor is provided as gbm_predictor.py, which can be used for assessing new digital slides in supported formats such as ndpi and svs. Furthermore, QuPath groovy-scripts for the analysis of the tumor microenvironment are included. The project is platform-independent and utilizes Python and Groovy programming languages.",
  "optimization/algorithm": "The machine-learning algorithm class used is convolutional neural networks (CNNs). Specifically, the Xception architecture was employed, which is a well-established model in the field of deep learning. This architecture was chosen for its high efficiency and performance, comparable to or even surpassing more commonly used architectures like ResNet.\n\nThe Xception model used was not new; it was pre-trained on ImageNet data, which is a large dataset commonly used for training and evaluating computer vision models. The decision to use a pre-trained model was strategic, as it allowed for leveraging a model that has already been trained on a vast amount of generic image data, thereby improving the model's ability to generalize to new tasks.\n\nThe reason the model was not published in a machine-learning journal is that the focus of the study was on applying deep learning techniques to histopathology in glioblastoma, rather than on developing a new machine-learning algorithm. The Xception architecture is widely recognized and has been extensively documented in the literature, making it a reliable choice for the specific application in this study. The innovation lies in the application of this established model to the domain of glioblastoma histopathology, rather than in the development of a new algorithm.",
  "optimization/meta": "The model leverages predictions from two convolutional neural networks (CNNs) as input: the transcriptional subtype CNN (TS-CNN) and the risk score CNN (RS-CNN). These CNNs are trained independently to predict transcriptional subtypes and risk scores, respectively. The TS-CNN outputs probabilities for three transcriptional subtypes, while the RS-CNN produces a risk score using a Cox regression layer.\n\nThe TS-CNN and RS-CNN are based on the Xception architecture, pretrained on ImageNet, ensuring that the training data for these models is independent of the validation data from The Cancer Genome Atlas (TCGA). This independence is crucial to avoid data leakage and ensure robust validation.\n\nThe predictions from these CNNs are then integrated to create a comprehensive feature space. This integration involves dimensionality reduction techniques, such as Uniform Manifold Approximation and Projection (UMAP), to visualize and analyze the combined outputs. The mean risk score for each transcriptional subtype is calculated, revealing significant differences in risk scores across different subtypes.\n\nIn summary, the model is a meta-predictor that combines the outputs of two independently trained CNNs, ensuring that the training data remains independent and that the predictions are robust and reliable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, the hematoxylin and eosin (H&E) stained slides were digitized using a Hamamatsu NanoZoomer 2.0 HT slide scanner. These digital slides were then manually annotated by a board-certified neuropathologist to segment out areas of necrosis, preexisting brain parenchyma, bleeding, scar tissue, and deformed tissue. The remaining areas were designated as tumor regions.\n\nEach digital slide was converted into multiple 1,024 × 1,024-pixel tiles at 20× magnification, with a 64-pixel overlap, using a custom MATLAB script. An accompanying spreadsheet was created to store the coordinates of each tile along with the relative areas of the segmented regions. Tiles containing more than 50% tumor tissue were retained for classifier training, while those with less than 50% tumor tissue were excluded. Additionally, patients with fewer than 50 different tiles were excluded from further analysis.\n\nFor the machine-learning pipeline, the tiles were randomly cropped to 512 × 512 pixels and underwent automated data augmentation using an H&E-specific algorithm at runtime. The input to the convolutional neural network (CNN) consisted of these randomly sampled and augmented tiles, with no additional information introduced to the model.\n\nThe CNN architecture used was Xception, pretrained on ImageNet, which was chosen for its high efficiency and performance. For transcriptional subtype (TS) prediction, a fully connected 3-neuron layer with softmax activation was added to the base model. The target for TS prediction consisted of the probabilities for each of the three transcriptional subtypes. The mean squared error was used as the loss function and backpropagated to update the weights. For risk score (RS) prediction, a single 1-neuron Cox regression layer with a linear activation function was added. The negative log-likelihood was used as the loss function and backpropagated to update the weights.\n\nDuring training, a new random image tile was selected per patient for each cycle through the digital slides. The TS-CNN and RS-CNN were trained independently of each other. The models were initially trained for 25 epochs with 150 steps per epoch and a batch size of 64, using the Adam optimizer with a learning rate of 0.001 and exponential learning rate decay. For fine-tuning, the last two convolutional layers of the Xception model were set trainable, and the model was trained for an additional 10 epochs with the same steps per epoch and batch size, but with a reduced learning rate of 0.0001 and a different decay rate. During training, 20 random batches were loaded into memory for validation at the start of each fold, and the mean squared error or c-index was calculated for the validation batches at the end of each epoch to monitor model performance.",
  "optimization/parameters": "In our study, we utilized the Xception architecture as the base model for our deep learning pipeline. This architecture is known for its efficiency and high performance, particularly in image classification tasks. The Xception model has a total of 20,861,480 parameters. During the initial training phase, we froze all the weights of the pre-trained model and added an extra layer depending on the target. For transcriptional subtype (TS) prediction, we added a fully connected 3-neuron layer with softmax activation. For risk score (RS) prediction, we added a single 1-neuron Cox regression layer with a linear activation function.\n\nFor fine-tuning, we set the last 2 convolutional layers of the Xception model to be trainable, which amounts to 4,741,632 parameters out of the total. This approach allowed us to leverage the pre-trained weights while also adapting the model to our specific dataset and tasks.\n\nThe selection of the Xception architecture was driven by its proven performance on the ImageNet classification task and its efficiency compared to other commonly used architectures like ResNet. We chose not to use models pre-trained with histological data to avoid mixing training and validation data, as our validation dataset included data from The Cancer Genome Atlas (TCGA). This decision ensured that our model's performance was evaluated on an independent and external dataset, providing a more robust assessment of its generalizability.",
  "optimization/features": "The input to the model consisted of a single whole slide image tile, randomly sampled and cropped to 512 × 512 pixels. No other information was introduced to the model. Feature selection was not performed, as the model directly used the image tiles as input features. The tiles were randomly selected per patient for each new cycle through the digital slides, ensuring that the model did not rely on predefined features but rather learned directly from the pixel data. This approach leverages the full spatial information contained within the histological images, allowing the model to capture complex patterns and structures relevant to the prediction tasks.",
  "optimization/fitting": "The model used in this study is based on the Xception architecture, which is a convolutional neural network (CNN) known for its efficiency and performance. The base model was pre-trained on ImageNet and then fine-tuned for our specific tasks. This approach helps in leveraging the features learned from a large dataset, reducing the risk of overfitting on the smaller, task-specific dataset.\n\nThe input to the Xception network consisted of randomly sampled and augmented tiles from whole slide images (WSIs), ensuring a diverse set of training examples. The model was trained using 5-fold cross-validation, which helps in assessing the model's performance and generalizability by rotating the training and validation sets.\n\nTo mitigate overfitting, several strategies were employed. First, data augmentation was used to artificially increase the diversity of the training set. This included random cropping and H&E-specific augmentation algorithms. Second, the model was fine-tuned by unfreezing only the last two convolutional layers, allowing the model to learn task-specific features while retaining the general features learned from ImageNet. Third, early stopping was implemented based on the performance on validation batches, preventing the model from overfitting to the training data.\n\nUnderfitting was addressed by ensuring the model had sufficient capacity to learn the complex patterns in the data. The Xception architecture, with its deep and wide structure, provides a large number of parameters, allowing it to capture intricate details in the histopathology images. Additionally, the model was trained for an adequate number of epochs, with learning rate decay to ensure convergence.\n\nThe final validation was performed on all validation tiles with center cropping to 512 × 512 pixels and no augmentation, providing an unbiased estimate of the model's performance. The risk score predictions were z-scored, and all predictions were concatenated into a single spreadsheet for further statistical analysis. This approach ensures that the model's performance is not overestimated due to data leakage or overfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our convolutional neural networks (CNNs). One of the primary methods used was 5-fold cross-validation, which helps to ensure that the model generalizes well to unseen data by training and validating on different subsets of the data.\n\nAdditionally, we utilized data augmentation techniques, such as random cropping and H&E-specific algorithms, to increase the diversity of the training data. This approach helps the model to learn more robust features that are invariant to minor variations in the input images.\n\nWe also implemented early stopping based on the performance on validation batches. During training, at the start of each fold, 20 random batches were loaded into memory for validation. At the end of each epoch, the mean squared error for transcriptional subtype (TS) prediction or the c-index for survival prediction was calculated for these validation batches to monitor the model's performance. This allowed us to stop training when the performance on the validation set started to degrade, indicating potential overfitting.\n\nFurthermore, we used a pretrained Xception architecture, which was initially trained on a large and diverse dataset (ImageNet). This transfer learning approach provides the model with a good starting point, reducing the risk of overfitting to the specific dataset used in our study.\n\nThe learning rate was adjusted using exponential decay, which helps in fine-tuning the model by reducing the learning rate as training progresses. This technique allows the model to make smaller updates to the weights, leading to more stable convergence and better generalization.\n\nLastly, we froze the initial layers of the pretrained model and only fine-tuned the last few layers. This approach ensures that the model retains the general features learned from the large dataset while adapting to the specific task at hand.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed an Xception architecture pretrained on ImageNet, which was chosen for its efficiency and performance. The model was trained using TensorFlow 2.1.0/keras. Key hyper-parameters included an initial learning rate of 0.001 with exponential decay, a batch size of 64, and training for 25 epochs initially, followed by fine-tuning for an additional 10 epochs. The Adam optimizer was used throughout the training process.\n\nThe model files and optimization parameters are not directly available in the publication but can be accessed through the provided code repository. The code for CNN training, including the initial training of cross-validation folds and corresponding exemplary histological data and clinical annotations, is available on GitHub under the project name GBMatch_CNN. This repository includes a final fully trained predictor, gbm_predictor.py, which has been trained with the complete discovery dataset. This predictor can be used for assessing new digital slides in supported formats such as ndpi and svs.\n\nAdditionally, QuPath Groovy scripts for the analysis of the tumor microenvironment are provided. The code is released under a permissive license, allowing for easy adaptation to similar problems. The project homepage on GitHub provides further details and access to the necessary files and scripts.\n\nThe availability of these resources ensures that other researchers can replicate our findings and apply the models to their own datasets, promoting transparency and reproducibility in the field.",
  "model/interpretability": "The models employed in this study, specifically the convolutional neural networks (CNNs) based on the Xception architecture, are inherently black-box models. This means that while they can provide accurate predictions, the internal decision-making processes are not easily interpretable. The CNNs process input data through multiple layers of convolutions and non-linear transformations, making it challenging to trace back how a specific prediction was made.\n\nHowever, efforts were made to enhance interpretability through various analyses. For instance, the spatial distribution of predicted targets was visualized directly on digital slides. This involved mapping predictions to the coordinates of windows covering the entire slide, allowing for a visual understanding of where certain predictions were made. Additionally, misclassified samples were qualitatively assessed to understand potential reasons for errors. It was found that many misclassifications were \"near correct,\" indicating that the model's predictions were close to the true values. Furthermore, many samples with misclassifications had relatively little tumor tissue, suggesting that the amount of tumor tissue present could influence the model's performance.\n\nAnother approach to interpretability involved analyzing the features learned by the model. A feature landscape was created using UMAP projections, highlighting different regions corresponding to various histological phenotypes. This provided insights into how the model differentiates between different types of tissue and subtypes.\n\nWhile these methods do not make the model fully transparent, they offer ways to understand and validate the model's predictions, providing some level of interpretability in an otherwise black-box system.",
  "model/output": "The model developed in this study is primarily a classification model, but it also incorporates regression elements. It is designed to predict both the transcriptional subtypes of glioblastoma and the risk scores associated with patient survival. The classification aspect involves determining the transcriptional subtype (mesenchymal, classical, or proneural) from histological images. The regression component is used to predict the risk scores, which are continuous values indicating the likelihood of patient survival. The model's outputs include heatmaps that visualize the spatial distribution of predicted targets within digital slides, providing a comprehensive view of the tumor's characteristics. Additionally, the model generates risk scores and transcriptional subtype predictions for each image tile, which are then used for further statistical analysis and visualization through techniques like UMAP plotting. The final outputs are used to assess the model's performance in predicting survival outcomes and transcriptional subtypes, demonstrating its utility in both classification and regression tasks.",
  "model/duration": "The execution time for our deep learning pipeline was influenced by several factors, including the computational resources and the complexity of the tasks performed. We utilized a TITAN Xp GPU, which significantly accelerated the training process. The models were trained for a total of 35 epochs, with the initial 25 epochs using a custom 150 steps per epoch and a batch size of 64. This phase was followed by an additional 10 epochs for fine-tuning, again with 150 steps per epoch and the same batch size. The Adam optimizer was employed with an initial learning rate of 0.001, which decayed exponentially every 400 steps. During training, validation was performed at the start of each fold by loading 20 random batches into memory. The mean squared error for transcriptional subtype (TS) prediction and the c-index for survival prediction were calculated at the end of each epoch to monitor model performance. The use of 5-fold cross-validation ensured that the model was thoroughly validated, contributing to the overall execution time. Additionally, the final validation involved predicting all validation tiles with center cropping to 512 × 512 pixels and no augmentation, which further added to the computational load. While specific execution times were not detailed, the combination of these factors indicates a substantial but efficient use of computational resources to achieve robust model performance.",
  "model/availability": "The source code for the convolutional neural network (CNN) training is publicly available via GitHub. This includes the code for the initial training of cross-validation folds and corresponding exemplary histological data and clinical annotations. Additionally, a final fully trained predictor, named gbm_predictor.py, is provided. This predictor has been trained with the complete discovery dataset and can be used for assessing new digital slides in supported formats such as ndpi and svs. The code is released under a permissive license, allowing for easy adaptation to similar problems.\n\nFor the analysis of the tumor microenvironment, QuPath Groovy scripts are also provided. These scripts can be used in conjunction with QuPath, a popular open-source software for digital pathology image analysis.\n\nThe project is platform-independent and can be run on any operating system. It requires Python 3.6 or higher, with additional dependencies listed on the project's home page. QuPath version 0.3.0 or higher is also required.\n\nThe project is registered as a software application on SciCrunch and biotools, ensuring its accessibility and credibility within the scientific community. The project's home page and additional resources, including workflows and supplementary data, are available online. This comprehensive release of code and resources aims to facilitate reproducibility and further research in the field of deep learning for histopathology in glioblastoma.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and generalizability. We employed 5-fold cross-validation during the model training process. This technique helps in assessing the model's performance by dividing the data into five subsets, training the model on four subsets, and validating it on the remaining one. This process is repeated five times, with each subset serving as the validation set once.\n\nFor the final validation, the trained models were used to predict all validation tiles. These tiles were center-cropped to 512 × 512 pixels without any augmentation. The risk score (RS) predictions were z-scored, while the transcriptional subtype (TS) predictions were used as they were. All predictions from the validation set were then concatenated into a single spreadsheet for further statistical analysis.\n\nTo visualize the spatial distribution of the predicted targets directly in the digital slides, predictions were performed on a set of windows covering the entire digital slide. These predictions were then mapped to the coordinates of those windows, allowing for the creation of heatmaps that represent the three transcriptional subtypes or a single map depicting the risk score.\n\nStatistical analysis was conducted using Python 3.8.5. Permutation tests by label shuffling were performed to compare our predicted risk scores to random guesses. To determine the significance of the RS and TS predictions, label shuffling was used to generate a null distribution. Mann–Whitney U and Wilcoxon tests were calculated using the scipy library. Kaplan–Meier survival analysis and Cox proportional hazards models were performed using the lifelines library. Harrel’s c-index was calculated using the sksurv library. Figures were drawn using matplotlib and seaborn. The confusion matrix and ROC analysis were performed using sklearn.\n\nFor Uniform Manifold Approximation and Projection (UMAP) plotting, the outputs of the penultimate CNN layers of all models were concatenated, resulting in 20,480 features for each image tile. The umap package was then used to plot UMAPs.\n\nAdditionally, the fraction of high-risk tiles (z-scored risk > 1) was calculated per digital slide, and their distribution was plotted as a heatmap. In the validation folds, the risk scores were strongly associated with survival upon univariate and multivariate analyses.\n\nExternal validation was also performed using an independent dataset from The Cancer Genome Atlas (TCGA). Applying the previously defined cutoff of 25% high-risk tiles resulted in a statistically significant separation of survival curves. The accuracy for predicting the transcriptional subtypes was 56.2% compared to a random guess accuracy of 34.3% ± 0.4% in the validation set. The accuracy was highest for predicting the mesenchymal subtype compared to the classical and proneural subtypes.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the effectiveness of our models. For the risk score (RS) predictions, we reported the Harrel’s c-index, which provides a measure of the model's discriminative ability. Additionally, we conducted permutation tests by label shuffling to compare our predicted risk scores to random guesses, calculating P-values to determine the significance of the RS predictions.\n\nFor the transcriptional subtype (TS) predictions, we evaluated the accuracy and compared it to a random guess accuracy. We also performed a confusion matrix and ROC analysis to further assess the performance of our models in predicting the transcriptional subtypes. The area under the curve (AUC) was reported for each subtype, highlighting the model's ability to distinguish between different subtypes.\n\nTo understand the relationship between risk scores and transcriptional subtypes, we calculated the mean risk score for each subtype using a winner-takes-all approach. This involved assigning each tile to the subtype with the highest predicted score and then calculating the mean risk score for the tiles associated with each subtype.\n\nStatistical tests such as the Mann–Whitney U test and Wilcoxon test were used to compare different groups, ensuring the robustness of our findings. Kaplan–Meier survival analysis and Cox proportional hazards models were performed to assess the prognostic value of our predictions. These metrics collectively provide a thorough evaluation of our models' performance, aligning with standard practices in the literature for similar studies.",
  "evaluation/comparison": "In our study, we extensively evaluated our deep learning models by comparing them to various publicly available methods and simpler baselines. We utilized benchmark datasets to ensure a rigorous assessment of our models' performance. Specifically, we compared our convolutional neural networks (CNNs) to other CNN architectures, which have shown strong performance on tasks like ImageNet classification. This comparison allowed us to gauge how our models stack up against established standards in the field.\n\nAdditionally, we explored the use of different deep learning models as backbones for computational pathology tasks. The variability among these pretrained models enabled us to tailor solutions to a wide range of requirements and tasks, ultimately leading to improved performance. We also considered the potential of foundation models for computational pathology, recognizing that their generalizability across different organs and disease types needs thorough evaluation. This comparative analysis helped us understand the strengths and limitations of our models in the context of existing technologies.\n\nWe also performed comparisons to simpler baselines to ensure that the complexity of our models was justified by their performance gains. This involved evaluating basic statistical methods and traditional machine learning algorithms to see if they could achieve similar results with less computational overhead. By doing so, we confirmed that our deep learning approaches provided significant advantages in terms of accuracy and robustness.\n\nOverall, our evaluation process was comprehensive, involving comparisons to both advanced and simpler methods, ensuring that our models offer a meaningful contribution to the field of computational pathology.",
  "evaluation/confidence": "The evaluation of our models included several statistical analyses to ensure the robustness and significance of our results. We employed permutation tests by label shuffling to compare our predicted risk scores to random guesses, generating a null distribution to calculate P-values. This approach helped us determine the significance of both risk score (RS) and transcriptional subtype (TS) predictions.\n\nFor survival analysis, we used Kaplan-Meier plots and Cox proportional hazards models, which provided hazard ratios (HR) with confidence intervals. For instance, the HR for age was calculated for each 1-year increase, and the HR for radiochemotherapy (TMZ) was found to be significant with a P-value of less than 0.001. The RS-CNN also showed a significant HR of 1.32 with a P-value of 0.013, indicating its prognostic value.\n\nWe utilized the Mann–Whitney U and Wilcoxon tests for comparing different groups, such as high-risk versus low-risk regions across histological and immunohistochemical parameters. These tests provided P-values that helped us assess the statistical significance of our findings. For example, the differences in cellularity and circularity between high-risk and low-risk regions were statistically significant with P-values of less than 0.001.\n\nAdditionally, we performed receiver operating characteristic (ROC) analysis and calculated the area under the curve (AUC) to evaluate the performance of our models in predicting transcriptional subtypes. The AUC values for different subtypes (mesenchymal, classical, and proneural) were compared, with the mesenchymal subtype showing the highest AUC of 0.746.\n\nTo further validate our results, we conducted a comparative analysis between high- and low-risk regions using the Wilcoxon signed-rank test and the Mann–Whitney U test. This analysis provided median values with interquartile ranges (IQR) and corresponding P-values, ensuring that our conclusions were statistically sound.\n\nOverall, the performance metrics in our study included confidence intervals and were subjected to rigorous statistical testing to claim the superiority of our method over baselines and other approaches. The results were statistically significant, providing strong evidence for the effectiveness of our deep learning models in prognostic and predictive tasks.",
  "evaluation/availability": "The raw evaluation files are publicly available for reuse. The complete slide scan library, including H&E-stained slides and intermediate annotations such as corresponding tissue segmentations, is accessible online via the GBMatch supplementary website. Additionally, all preselected image tiles used for training, along with their corresponding annotations and segmentations for the immunohistochemically stained slides, are available via an accompanying Zenodo repository. The external TCGA validation dataset can be accessed through cBioPortal and the GDC Data Portal. Snapshots of the code and other supporting data are openly available in the GigaScience repository, GigaDB. The code for CNN training is also available via GitHub, including code for the initial training of CV-folds and corresponding exemplary histological data and clinical annotation. A final fully trained predictor is provided as gbm_predictor.py, which can be used for assessing new digital slides in supported formats such as ndpi and svs. The project is licensed under GPL-3.0, ensuring permissive use and adaptation."
}