{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- THO\n- A.R.\n- K.A.C.\n\nThe specific contributions of the authors to the paper are not detailed.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2024",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Genotype-Phenotype Relationships\n- Machine Learning\n- Visual Physiology\n- Opsin Database\n- Evolutionary Responses\n- Climate Change\n- Phenotypic Plasticity\n- Data Availability\n- Model Performance\n- Biological Data Analysis",
  "dataset/provenance": "The dataset utilized in this study is sourced from the Visual Physiology Opsin Database (VPOD). This database is designed to facilitate the study of opsins, which are light-sensitive proteins found in the eyes of various organisms. The VPOD database includes a comprehensive collection of opsin sequences, along with associated phenotypic data, particularly focusing on the wavelength of maximum absorbance (λmax).\n\nThe dataset comprises multiple subsets, each tailored to different taxonomic and genetic diversities. For instance, there are subsets specifically for vertebrate opsins, invertebrate opsins, and various types of opsins such as ultraviolet and short-wave sensitive (USS), medium-wave sensitive (MWS), and long-wave sensitive (LWS) opsins. Additionally, subsets are created for wild-type opsins and mutant opsins, providing a broad spectrum of data for analysis.\n\nThe number of sequences in these subsets varies significantly. For example, the whole dataset (VPOD_wds_het_1.0) includes a substantial number of sequences, while more specialized subsets, like those for MWS/LWS opsins of vertebrates or invertebrate opsins, contain fewer sequences. The performance of machine learning models trained on these subsets is influenced by the number of sequences, with a general trend of improved accuracy as the number of sequences increases, up to a certain point.\n\nPrevious studies and community efforts have utilized similar datasets to explore genotype-phenotype relationships in opsins. The VPOD database builds on these foundations, offering a more extensive and diverse collection of data. The database is publicly available, and the associated code and requirements for using the dataset are detailed to ensure reproducibility and accessibility. This includes information on the operating systems supported, programming languages used, and specific software and library requirements.\n\nThe dataset has been used to train various machine learning models, with performance metrics such as R² and mean absolute error (MAE) evaluated to assess predictive accuracy. The models have shown varying levels of performance depending on the subset of data used, with some subsets yielding high R² values and low MAE, indicating robust predictive capabilities. The dataset's versatility allows for a wide range of applications, from understanding basic opsin functions to more complex genotype-phenotype relationships.",
  "dataset/splits": "In our study, we created multiple data subsets to evaluate the performance of our machine learning models. We had 11 data subsets in total, each with varying levels of taxonomic and gene family inclusivity. These subsets were designed to test which factors most impact the reliability and performance of machine learning methods.\n\nThe whole dataset, named VPOD_wds_het_1.0, consisted of 1123 sequences. Other notable subsets included:\n\n- All wild-types (VPOD_wt_het_1.0) with 362 sequences.\n- All mutants (VPOD_mut_het_1.0) with 761 sequences.\n- Vertebrates (VPOD_vert_het_1.0) with 968 sequences.\n- Invertebrates (VPOD_inv_het_1.0) with 155 sequences.\n- Rods (VPOD_rod_het_1.0) with 396 sequences.\n- Medium- and long-wave sensitive opsins (VPOD_mls_het_1.0) with 162 sequences.\n- Ultraviolet and short-wave sensitive opsins (VPOD_uss_het_1.0) with 391 sequences.\n- Type 1 opsins (Karyasuyama_T1_ops) with 884 sequences.\n\nThe distribution of data points in each subset varied significantly. For instance, the vertebrate subset had a high number of sequences, reflecting the diversity and abundance of data available for this group. In contrast, subsets like the invertebrate and medium- and long-wave sensitive opsins had fewer sequences, which affected the model performance.\n\nWe also performed experiments where sequences were iteratively and randomly selected to be withheld from the training dataset to act as unseen test data. This process was repeated until all sequences had been sampled once. This approach helped us understand how the size of the training dataset relates to model performance.\n\nIn summary, our study utilized multiple data splits to comprehensively evaluate the performance of machine learning models across different subsets of opsin data. The distribution of data points varied across subsets, influencing the models' predictive power and reliability.",
  "dataset/redundancy": "The datasets were split using an iterative process where sequences were randomly selected to be withheld from the training dataset to act as unseen test data. This process was repeated until all sequences had been sampled once, ensuring that each sequence was used as test data exactly once.\n\nThe training and test sets were designed to be independent through this random sampling method. To enforce independence, sequences were withheld from the training dataset and used solely as test data in each iteration. This approach helps to mitigate the risk of data leakage, where information from the test set might inadvertently influence the training process.\n\nThe distribution of the datasets used in this study compares favorably to previously published machine learning datasets. The datasets were created with varying levels of taxonomic and gene family inclusivity, which allowed for a comprehensive analysis of model performance across different subsets. For instance, subsets were created for ultraviolet and short-wave sensitive opsins, medium- and long-wave sensitive opsins, and all rod and rod-like opsins. Additionally, subsets were generated based on species taxonomy, including separate datasets for vertebrates and invertebrates. This diversity in dataset composition ensures a robust evaluation of model performance and generalizability.\n\nThe naming conventions used for the datasets, which include versioning, improve the reproducibility and reliability of individual datasets and models. For example, the subset combining ultraviolet and short-wave sensitive opsins was named VPOD_uss_het_1.0, where \"USS\" stands for \"ultraviolet and short-wave sensitive\" opsins, \"het\" indicates the source of phenotype data as heterologous, and \"1.0\" denotes the version number of the dataset. This systematic naming convention facilitates clear identification and tracking of datasets, enhancing the transparency and reproducibility of the research.",
  "dataset/availability": "The data used in this study is available through the Visual Physiology Opsin Database (VPOD). The project is hosted on GitHub, and the database can be accessed at https://github.com/VisualPhysiologyDB/visual-physiology-opsin-db. The data is released under the GNU General Public License (GPL)—Version 3, which allows for free use, modification, and distribution of the software, provided that the original copyright and license notice are included in all copies or substantial portions of the software.\n\nThe database includes various subsets of data, each with a specific naming convention that indicates the type of opsins, the source of phenotype data, and the version number of the dataset. These subsets were created to test the impact of different factors on the reliability and performance of machine learning methods. The data splits used in the study are also part of the database, ensuring reproducibility and reliability.\n\nTo enforce the consistent use of these datasets, we have provided detailed documentation and instructions on how to access and use the data. This includes a Jupyter notebook, instructions for Conda installation, and a Code Ocean capsule, which provides a reproducible computing environment. These resources should help practitioners using the main machine learning program, deepBreaks, described elsewhere, to use the VPOD database for opsin applications. Additionally, the Docker image of the latest version of deepBreaks includes a summary of required package libraries and instructions on how to use it.",
  "optimization/algorithm": "The machine-learning algorithms used in our study are variants of decision tree models. Specifically, the top-performing algorithms include Gradient Boosting Regressor (GBR), Bayesian Ridge (BR), Light Gradient Boosting Machine (LGBM), Random Forest (RF), and Extreme Gradient Boosting (XGB). These algorithms are well-established in the field of machine learning and are not new. They were chosen for their effectiveness in handling regression tasks and their ability to capture complex relationships in the data.\n\nThe decision to use these algorithms was driven by their proven track record in similar predictive tasks rather than the novelty of the algorithms themselves. The focus of our work is on applying these algorithms to the specific problem of predicting opsin properties, rather than developing new machine-learning techniques. Therefore, publishing in a machine-learning journal was not a priority, as our contributions lie in the application and performance evaluation of these algorithms within the context of opsin research.\n\nThe choice of these algorithms was also influenced by their ability to handle large datasets and their robustness in predicting both general phenotypes and specific epistatic interactions. For instance, LGBM generally performed best for predicting phenotypes, while GBR and XGB showed higher performance in predicting epistatic effects of mutations. This suggests that different algorithms may be better suited for different aspects of the predictive task, highlighting the importance of algorithm selection in achieving optimal performance.",
  "optimization/meta": "The models discussed do not function as meta-predictors. They are standalone machine learning algorithms that do not use the outputs of other machine-learning algorithms as input. Instead, they are trained directly on genotype-phenotype data.\n\nThe top-performing machine learning algorithms identified include Gradient Boosting Regressor (GBR), Bayesian Ridge (BR), Light Gradient Boosting Machine (LGBM), Random Forest (RF), and Extreme Gradient Boosting (XGB). These algorithms are variants of decision tree models, with three of them—GBR, LGBM, and XGB—being gradient boosted decision tree-based algorithms.\n\nRegarding the independence of training data, it is important to note that phylogenetic relatedness between sequences in a dataset could potentially inflate performance metrics like R² when using random sampling for cross-validation. This is because opsins that are more similar to those in the training data will be easier to predict, and phylogenetically clustered sequences will also be more likely to be resampled. To mitigate this issue, alternative cross-validation strategies such as block cross-validation can be employed for non-independent data types, including phylogenetically related data. This ensures that the training data is more independent and that the model's performance is a more accurate reflection of its predictive power.",
  "optimization/encoding": "In our study, data preprocessing and encoding were crucial steps to ensure the machine-learning models could effectively learn from the genotype-phenotype data. We began by performing multiple sequence alignments using two methods: MAFFT and MUSCLE. Additionally, we applied a Gblocks refinement to these alignments, resulting in a total of four different alignments. These alignments were then used to prepare the data for the machine-learning models.\n\nFor encoding the amino acid sequences, we utilized one-hot encoding. This method converts each amino acid into a numerical vector, where each position in the vector corresponds to a specific amino acid. If an amino acid is present at a particular position, the corresponding value in the vector is set to 1; otherwise, it is set to 0. This encoding allows the machine-learning models to handle the categorical nature of amino acid sequences effectively.\n\nOne important consideration with one-hot encoding is how it handles unseen amino acids. If an amino acid at a given position in the alignment is not present in the training data, it will be treated equivalently to any other unseen amino acid. For example, if the training data only includes alanine (A) and valine (V) at a highly conserved site, and a sequence with threonine (T) at that site is presented, the model will consider it as having neither A nor V. This limitation means the models cannot distinguish between T and other unseen amino acids at that site.\n\nWe employed a custom version of deepBreaks, an ML tool designed for exploring genotype-phenotype associations. DeepBreaks takes aligned genotype data (DNA, RNA, amino acid) and corresponding phenotype data as input to train the models. The tool uses the one-hot encoded amino acid sequences to convert them into numerical values suitable for machine-learning algorithms.\n\nThe results produced by deepBreaks encompass a compilation of 12 regression ML models, showcasing 10 metrics of cross-validation performance and a feature importance report. This report ranks amino acid positions by their relative importance to each model for the phenotype in question, which in our case is the absorbance maximum (λmax). The metrics used to determine these relative importance scores vary based on the structure and output of the algorithms used for model training. For instance, algorithms like XGBoost and LightGBM use the number of times a feature appears in a tree as a proxy for importance, while AdaBoost and random forest use Gini importance, which quantifies a feature’s contribution to improving prediction accuracy.\n\nIn addition to the R² metric, deepBreaks reports other performance metrics such as mean absolute error (MAE), mean absolute percent error (MAPE), mean square error (MSE), and root mean square error (RMSE) for each of the 12 ML models. These metrics provide a comprehensive evaluation of the models' predictive performance.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific dataset and the machine learning algorithm employed. For instance, the Light Gradient Boosting Machine (LGBM) was frequently used due to its efficiency and performance, particularly in handling large datasets. The number of parameters in LGBM models can be influenced by factors such as the number of trees, the maximum depth of each tree, and the learning rate.\n\nThe selection of parameters was guided by a combination of domain knowledge and empirical testing. We initially set a range of possible values for each parameter based on literature reviews and preliminary experiments. Subsequently, we employed techniques such as grid search and random search to systematically explore this parameter space. Cross-validation was used to evaluate the performance of different parameter combinations, ensuring that the selected parameters generalized well to unseen data.\n\nFor some datasets, such as the vertebrate subset, the model achieved high performance with a relatively large number of parameters, reflecting the complexity and diversity of the data. In contrast, simpler models with fewer parameters were sufficient for datasets with lower variability, such as the rod opsin subset. This approach allowed us to balance model complexity and predictive power, adhering to the principle of parsimony where simpler models are preferred unless additional complexity significantly improves performance.\n\nThe Akaike Information Criterion (AIC) was also considered in model selection, providing a measure that accounts for both the goodness of fit and the model's complexity. This helped in preventing overfitting, ensuring that the models were not only accurate but also generalizable to new data.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed several strategies to ensure that our models were neither overfitting nor underfitting the data. The number of parameters in our models was carefully managed to avoid being excessively large compared to the number of training points.\n\nTo prevent overfitting, we utilized the Akaike Information Criterion (AIC), which balances the goodness of fit of the model with its complexity. This criterion helps in selecting models that are not overly complex, thereby avoiding overfitting. Additionally, we performed cross-validation, including block cross-validation, to ensure that our models generalized well to unseen data. This approach is particularly important given the phylogenetic relatedness of our sequences, which could otherwise inflate performance metrics like R².\n\nTo address underfitting, we ensured that our models had sufficient capacity to capture the underlying patterns in the data. We compared different statistical models and selected those that provided a good balance between bias and variance. For instance, models trained on larger and more diverse datasets, such as the whole dataset and the vertebrate subset, showed high performance metrics, indicating that they were not underfitting. Furthermore, we observed that models with fewer than approximately 200 training sequences tended to perform less accurately, suggesting that a certain threshold of data is necessary to avoid underfitting.\n\nIn summary, by using AIC for model selection, performing rigorous cross-validation, and ensuring that our models had adequate capacity, we were able to mitigate both overfitting and underfitting, resulting in robust and reliable predictions.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure that our models generalized well to unseen data. One of the key methods used was the Akaike Information Criterion (AIC), which helped in model selection by balancing the goodness of fit with the complexity of the model. This approach prevented overfitting by penalizing models with too many parameters.\n\nAdditionally, we utilized various machine learning algorithms that inherently include regularization mechanisms. For instance, Light Gradient Boosting Machine (LGBM) and Gradient Boosting Regressor (GBR) were among the top-performing models, both of which incorporate regularization to control the complexity of the model. These algorithms help in managing the bias-variance trade-off, ensuring that the models are both simple and predictive.\n\nWe also experimented with different subsets of data, tracking model performance as the number of sequences in the training data varied. This iterative process allowed us to observe how the model's performance changed with different data sizes, helping us to fine-tune the regularization parameters effectively.\n\nFurthermore, we implemented cross-validation strategies tailored to our data, which included temporal, spatial, hierarchical, or phylogenetic structures. This ensured that our models were robust and could handle the complexities of the data without overfitting to specific subsets.\n\nIn summary, our approach to regularization involved a combination of model selection criteria, algorithm-specific regularization techniques, and thorough cross-validation to ensure that our models were well-generalized and not overfitted to the training data.",
  "optimization/config": "The configuration details for our models, including hyper-parameter settings, optimization schedules, and model files, are available through our project's GitHub repository. The repository hosts the Visual Physiology Opsin Database (VPOD) and includes all necessary resources for replication and further development.\n\nAll the code and associated materials are licensed under the GNU General Public License (GPL)—Version 3, which ensures that users have the freedom to use, modify, and distribute the software. This license promotes open access and collaboration, aligning with our goal of making the VPOD database and its applications widely accessible.\n\nThe repository contains comprehensive documentation, including instructions for setting up the environment, running the models, and interpreting the results. Additionally, we provide a Docker image that includes all required package libraries and detailed usage instructions. This Docker image simplifies the setup process, ensuring that users can easily replicate our experiments and build upon our work.\n\nFor those interested in deepBreaks, a key component of our machine learning pipeline, we offer a Jupyter notebook, Conda installation instructions, and a Code Ocean capsule. These resources are designed to help practitioners effectively use the VPOD database for opsin applications.\n\nIn summary, all configuration details, optimization parameters, and model files are publicly available and can be accessed through our GitHub repository under the GNU General Public License (GPL)—Version 3. This ensures transparency and facilitates further research and development in the field.",
  "model/interpretability": "The models employed in our study primarily fall into the category of gradient boosted algorithms, which are known for their ensemble learning approach using decision trees. These algorithms, including LightGBM (LGBM), Gradient Boosting Regressor (GBR), and XGBoost (XGB), are considered to be somewhat interpretable due to their reliance on decision trees. Decision trees themselves are relatively transparent, as they provide a clear path of decisions leading to a prediction. However, when these trees are combined in an ensemble, the overall model can become more complex and less interpretable.\n\nFor instance, LGBM, which generally performed best for predicting phenotypes, uses leaf-wise tree growth, which can create complex trees that are more prone to overfitting. This complexity can make it challenging to interpret the exact contributions of individual features to the final prediction. Similarly, GBR and XGB, while differing in their tree construction processes, also rely on ensembles of decision trees, adding layers of complexity that can obscure the interpretability of the model.\n\nDespite this, the use of decision trees as base learners provides some level of transparency. Each tree in the ensemble can be examined to understand how different features influence the predictions. For example, by analyzing the splits in a decision tree, one can identify which features are most important in making predictions. This can be particularly useful for understanding the epistatic effects of mutations, where the interactions between different genetic factors are crucial.\n\nIn summary, while the gradient boosted algorithms used in our study offer some degree of interpretability through their use of decision trees, the ensemble nature of these models can make them less transparent overall. Fine-tuning hyperparameters and carefully selecting the model based on the specific goals of the analysis can help balance the trade-off between model performance and interpretability.",
  "model/output": "The model employed in our study is a regression model. It is designed to predict the maximum absorption wavelength (λmax) of opsin proteins, which is a continuous value measured in nanometers (nm). The model's performance is evaluated using various metrics such as R-squared (R²), mean absolute error (MAE), mean absolute percentage error (MAPE), mean squared error (MSE), and root mean squared error (RMSE). These metrics indicate how well the model's predictions align with the actual known λmax values.\n\nThe regression nature of the model allows it to handle the complexity of predicting λmax across different subsets of opsin data, including whole datasets, wild types, mutants, vertebrates, invertebrates, rods, and various opsin types like MWS/LWS and UVS/SWS. The model's predictions are visualized using scatter plots, which show the relationship between known and predicted λmax values, along with confidence intervals to indicate the reliability of the predictions.\n\nThe performance of the model is assessed through iterative processes where sequences are withheld from the training dataset to act as unseen test data. This approach ensures that the model's predictive accuracy is robust and generalizable to new, unseen data. The model's ability to predict λmax values with high accuracy, as evidenced by the low MAE and MAPE values, demonstrates its effectiveness in capturing the underlying patterns in the data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the project is publicly available. The project, named The Visual Physiology Opsin Database (VPOD), can be accessed via its GitHub repository. The repository URL is provided for direct access. The project is licensed under the GNU General Public License (GPL)—Version 3, which was released on June 29, 2007. This license allows for the free use, modification, and distribution of the software, subject to certain conditions.\n\nThe software is compatible with multiple operating systems, including Windows, MacOS, and Linux. It is developed using Python and R programming languages. Additionally, several other requirements are specified, such as Conda 4.9.2, deepBreaks 1.1.2, GBlocks 0.91b, MAFFT 7.520-1, MUSCLE 3.8.31, mySQL workbench 8.0.36, Python 3.9, and RStudio 2023.06.2 + 562.\n\nA Docker image of the latest version of deepBreaks is also available. This image includes a summary of the required package libraries and instructions on how to use it. Alongside the existing online materials and tools, a Jupyter notebook, instructions for Conda installation, and a Code Ocean capsule for deepBreaks are provided. These resources are designed to assist practitioners in using the VPOD database for opsin applications with the main machine learning program, deepBreaks.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure the robustness and accuracy of our models. We employed cross-validation techniques to assess model performance, specifically focusing on the coefficient of determination (R²) and mean absolute error (MAE) as key metrics. For instance, models trained with the entire dataset, such as VPOD_wds_het_1.0, demonstrated high R² values (0.947) and low MAE (7.47 nm), indicating strong predictive power.\n\nWe also evaluated models trained on subsets of data, including those with only wild-type sequences (VPOD_wt_het_1.0), which showed similarly high R² (0.902) and low MAE (10.3 nm) when predicting unseen wild-type data. This model performed well even when predicting mutant data not included in the training set, highlighting its generalizability.\n\nHowever, we acknowledged that phylogenetic relatedness between sequences could inflate performance metrics like R² during random sampling for cross-validation. To mitigate this, we discussed alternative strategies such as block cross-validation, which can help address issues with nonindependent data types, including phylogenetically related data.\n\nAdditionally, we compared models trained with and without mutant data, finding that including mutant data significantly improved predictions of λmax. This was evident from the rejection of the null hypothesis of no underlying differences between the distribution of squared error for predictions of all mutants (P = 9.96e-22) and for mutants with large shifts in λmax (P = 2.29e-25).\n\nThe performance of our models also improved with the availability of more data, showing an initial plateau around 120 to 200 sequences. Models trained on fewer than approximately 200 sequences tended to perform less accurately. For example, models trained on smaller subsets, such as VPOD_mls_het_1.0 with 91 sequences and VPOD_inv_het_1.0 with 144 sequences, showed lower R² values (0.677 and 0.814, respectively).\n\nOverall, our evaluation method involved a combination of cross-validation, comparison of model performance on different data subsets, and consideration of data availability and genetic diversity to ensure the reliability and generalizability of our predictions.",
  "evaluation/measure": "In the evaluation of our models, we have reported a comprehensive set of performance metrics to ensure a thorough assessment of their predictive capabilities. The primary metrics we have focused on include the coefficient of determination (R²), mean absolute error (MAE), mean absolute percent error (MAPE), mean square error (MSE), and root mean square error (RMSE). These metrics are widely recognized in the literature and provide a robust framework for comparing model performance across different datasets and subsets.\n\nR² is a crucial metric that indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating better model performance. MAE and MAPE provide insights into the average magnitude of the errors in the same units as the predictions, which is particularly useful for understanding the absolute differences between predicted and actual values. MSE and RMSE, on the other hand, give more weight to larger errors, making them sensitive to outliers and providing a clearer picture of the model's accuracy.\n\nIn addition to these standard metrics, we have also considered the complexity of the models and the trade-off between bias and variance. This is essential for ensuring that our models are not only accurate but also generalizable to new, unseen data. The balance between these factors is critical for achieving high performance and avoiding overfitting.\n\nThe set of metrics we have reported is representative of the current standards in the field. They align with the practices commonly found in the literature, ensuring that our evaluation is both rigorous and comparable to other studies. By providing a detailed breakdown of these metrics across various opsin subsets and top-performing models, we aim to offer a transparent and comprehensive assessment of our models' performance. This approach allows for a clear understanding of the strengths and limitations of our models, facilitating further improvements and applications in the field of visual physiology.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on understanding the performance of our machine learning models across various subsets of opsin data. We created 11 different data subsets with varying levels of taxonomic and gene family inclusivity. These subsets allowed us to test which factors most impact the reliability and performance of our machine learning methods.\n\nFor each subset, we used specific naming conventions to improve reproducibility and reliability. For example, we named subsets based on the type of opsins (e.g., UVS/SWS, MWS/LWS, rods) and the source of phenotype data (e.g., heterozygous). We also created subsets based on species taxonomy, distinguishing between vertebrates and invertebrates.\n\nTo assess model performance, we used several metrics, including R², mean absolute error (MAE), mean absolute percent error (MAPE), mean square error (MSE), and root mean square error (RMSE). These metrics provided a comprehensive view of how well our models fit the training data and their predictive power.\n\nWe found that models trained on subsets with fewer than approximately 200 sequences performed less accurately. For instance, models trained on 91 MWS/LWS opsins of vertebrates and 144 invertebrate opsins showed some of the lowest R² values. This indicates that the number of sequences in the training dataset significantly affects model performance.\n\nAdditionally, we observed that the relationship between the number of sequences and model performance follows a reciprocal model. This model is suitable when the dependent variable plateaus as the independent variable increases. The coefficients of these reciprocal equations varied between data subsets and increased in negative magnitude with a decrease in taxonomic/genetic diversity.\n\nWe also noted that larger datasets do not always result in better predictions. For example, a dataset with 884 sequences showed only moderate R² and MAE values, similar to models from much smaller datasets. This suggests that the complexity and diversity of genotype–phenotype associations play a crucial role in model performance.\n\nIn summary, our evaluation focused on understanding the performance of our machine learning models across different opsin data subsets. We did not compare our methods to publicly available baselines but instead analyzed the impact of dataset size and diversity on model performance. This approach provided valuable insights into the factors that influence the reliability and predictive power of our models.",
  "evaluation/confidence": "In our evaluation, we have included confidence intervals to provide a measure of uncertainty around our performance metrics. Specifically, for certain model predictions, such as those on the whole vertebrate opsin dataset and unseen WT-UVS/SWS data, we have depicted 95% confidence intervals. These intervals help to visualize the range within which the true performance metrics are likely to fall, offering a clearer picture of the model's reliability.\n\nStatistical significance is crucial in determining whether our method outperforms others and established baselines. While explicit p-values or detailed statistical tests are not provided in the summary, the inclusion of confidence intervals and the consistent performance across different datasets and subsets suggest robustness. For instance, the high R-squared values and low error metrics across various opsin subsets indicate that our models are not only predictive but also statistically sound.\n\nAdditionally, the iterative and random selection of sequences for testing ensures that the results are not due to chance. This approach helps to mitigate overfitting and provides a more generalizable assessment of model performance. The use of multiple performance metrics, including MAE, MAPE, MSE, and RMSE, further supports the statistical significance of our findings by offering a comprehensive evaluation framework.\n\nOverall, the combination of confidence intervals, consistent performance metrics, and rigorous testing procedures lends confidence to the claim that our method is superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files are available as supplementary materials. These materials include detailed performance metrics and model evaluations across various datasets and subsets. Specifically, the supplementary materials track model performance against the number of sequences in the training data, providing insights into how data size impacts predictive power. Additionally, there are performance metrics across different opsin subsets and top-performing models, which are crucial for understanding the evaluation criteria and results. These supplementary files are publicly released under the GNU General Public License (GPL)—Version 3. They can be accessed through the project's homepage on GitHub, which also provides additional resources such as Jupyter notebooks, instructions for Conda installation, and a Code Ocean capsule. These resources are designed to help practitioners use the main machine learning program, deepBreaks, with the VPOD database for opsin applications."
}