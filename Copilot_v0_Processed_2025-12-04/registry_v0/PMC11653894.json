{
  "publication/title": "Multiomic landscape of the PFC in ALS",
  "publication/authors": "The authors who contributed to this article are:\n\n- P.L. and L.C.G. were instrumental in conceiving the project, designing the sample collection methodology, reviewing the sample/data quality, and coordinating the acquisition of human tissue samples from selected brain banks.\n- S.H., F.H., R.K., S.O., and S.B. played key roles in conceptualizing the data analysis.\n- L.C.G., M.P., L.T., L.P., M.G., P.Z., and Q.Z. were responsible for processing post-mortem brain material for multiomics experiments.\n- S.B. was involved in the funding and conceptualization of the study.\n- S.H. received funding from BMBF and DFG, CRC1192.\n- S.O. was funded by CRC1286 SP02 and KFO296 P8.\n- R.K. was funded by FOR5068 P9.\n- F.H. was funded by the M3I excellence initiative and a UKE postdoctoral stipend.\n- S.B. was funded by CRC1286 SP02, CRC1192 PB8, and PC3.\n- L.C.G., M.P., L.T., and P.L. were supported by the Bundesministerium für Bildung und Forschung and the Munich Cluster for Systems Neurology (SyNergy).",
  "publication/journal": "GigaScience",
  "publication/year": "2024",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- amyotrophic lateral sclerosis\n- multiomics\n- prefrontal cortex\n- RNA sequencing\n- proteomics\n- data analysis\n- personalized medicine\n- neurodegenerative disease\n- sex differences\n- molecular subclusters\n- Data Version Control\n- mouse models\n- transcriptomics\n- computational biology\n- bioinformatics\n- variant analysis\n- transcription factor activity\n- RNA stability\n- cross-species analysis\n- therapeutic targets",
  "dataset/provenance": "The dataset presented in this study is derived from a comprehensive multi-omics analysis focused on the prefrontal cortex in amyotrophic lateral sclerosis (ALS). The data includes both mouse and human samples, providing a rich resource for understanding the molecular mechanisms underlying ALS.\n\nThe mouse RNA-seq and small RNA-seq data, along with the corresponding processed data, are openly available in the National Center for Biotechnology Information Gene Expression Omnibus database under the accession numbers GSE234245 and GSE234243, respectively. These datasets include raw data in FASTQ format and processed data in CSV format, ensuring accessibility for further analysis by the research community.\n\nFor human samples, the raw data in FASTQ format are encrypted and stored in the European Genome Phenome Archive, registered under the study ID EGAS00001007318. Access to these data is restricted and requires a formal request to the archive. Detailed instructions on how to request access are available online.\n\nIn addition to RNA-seq data, proteomics data for both human and mouse samples are deposited in the ProteomeXchange Consortium database under the accession number PXD043300. These data are also openly available, and the results from an open modification search can be accessed via Figshare.\n\nThe workflow used to generate these datasets is available on WorkflowHub under the identifier 10.48546/workflowhub.workflow.1191.1. This workflow includes scripts for automatic download of mouse sequencing data from the Sequence Read Archive (SRA) and can also be used with manually downloaded files, which is necessary for human samples due to restricted access.\n\nAll supporting data and materials are available in the GigaScience database, GigaDB, and in WorkflowHub. The project's source code and requirements are hosted on GitHub, providing a platform-independent pipeline that can be executed in a dockerized environment. This ensures reproducibility and comparability of the analysis across all generated datasets.\n\nThe datasets include sex-specific and cross-species information, highlighting male and female differences in ALS. This resource is valuable for investigating early ALS mechanisms and identifying new therapeutic targets. The robust analytical pipeline and high-quality data ensure that the findings are reproducible and comparable across different omics layers and species.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The datasets generated and analyzed in our study are publicly available through several repositories, ensuring accessibility and reproducibility. The raw and processed RNA-seq data for mice are deposited in the National Center for Biotechnology Information Gene Expression Omnibus database under the accession numbers GSE234245 and GSE234243, respectively. These datasets are openly available for download and use.\n\nFor human data, due to restricted access, the raw data in FASTQ format are encrypted and stored at the European Genome Phenome Archive under the registered study EGAS00001007318. Access to these data can be requested through the European Genome Phenome Archive, with detailed instructions available online.\n\nProteomics data for both human and mouse samples are deposited in the ProteomeXchange Consortium database under the accession number PXD043300. These data are also openly available. Additionally, the results from the open modification search are available via Figshare.\n\nAll supporting data and materials are available in the GigaScience database, GigaDB, and in WorkflowHub. The workflow contains scripts for the automatic download of mouse sequencing data from the Sequence Read Archive (SRA) to be used as input. However, for human samples, manual download is required due to restricted access.\n\nThe workflow is designed to be platform-independent and includes scripts written in Python, R, and Bash, which can be executed in a dockerized environment. This ensures that the correct package versions are used, maintaining the reproducibility of the analysis. The execution order of the scripts is provided as a Data Version Control (DVC) workflow, which can be automatically executed with DVC. All parameters are provided in the params directory.\n\nThe datasets and workflows are made available under the MIT license, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. This approach ensures that the data and methods are accessible to the scientific community, fostering further research and collaboration in the field of ALS.",
  "optimization/algorithm": "Not applicable.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent several preprocessing steps to ensure quality and consistency. Initially, quality checks were performed using FastQC for RNA-seq and small RNA-seq data, and miRTrace for small RNA-seq data. This involved adapter trimming and quality filtering to remove low-quality reads and artifacts.\n\nFor RNA-seq data, pseudo-alignment and quantitation were conducted using Salmon, with indices built from GENCODE annotations. Count matrices were then filtered to retain features with at least 10 counts in 50% of samples for any condition or sex. Normalization was performed using variance-stabilizing transformation (VST) implemented in DESeq2, ensuring consistent and comparable expression values across samples.\n\nSmall RNA-seq data were processed similarly, with normalization achieved through quantile normalization using the preprocessCore R package. For proteomics data, protein peaks were assigned using trypsin/P specificity against an in-house-generated protein sequence database. Low-abundance proteins detected in less than 50% of samples were filtered out, and missing values were imputed using the missForest algorithm. Intensities were log2-transformed for variance stabilization.\n\nThese preprocessing steps ensured that the data were of high quality and appropriately formatted for input into the machine-learning algorithms, facilitating robust and reproducible analyses.",
  "optimization/parameters": "In our study, the optimization process involved several parameters, but the specific number of parameters (p) used in the model is not explicitly stated. The selection of parameters was guided by the requirements of the specific tools and methods employed in our analysis.\n\nFor instance, in the proteomics data processing, parameters were set for the mass spectrometry (MS) precursor tolerance at 20 ppm and MS/MS fragment tolerance at 0.02 Da. Variable modifications included methionine oxidation and protein N-term acetylation, while cysteine carbamidomethylation was set as a fixed modification. These settings were chosen based on standard practices and the recommendations of the software used, such as MaxQuant and IonBot.\n\nIn the RNA-seq data processing, parameters were set for the Nextflow Core RNA-seq pipeline, including the use of Salmon for pseudo-alignment and quantitation, with indices built from GENCODE annotations. Quality checks were conducted with FastQC, and preprocessing steps included adapter trimming and quality filtering to remove low-quality reads and artifacts. These parameters were selected to ensure the accuracy and reliability of the RNA-seq data analysis.\n\nThe choice of parameters was also influenced by the need to ensure reproducibility and comparability across different datasets and models. For example, the use of default values for error tolerances in the IonBot software helped to maintain consistency in the proteomics data analysis. Similarly, the use of standard databases and annotations for RNA-seq data processing ensured that the results were comparable across different studies.\n\nOverall, the selection of parameters was driven by a combination of standard practices, software recommendations, and the need for reproducibility and comparability. While the exact number of parameters (p) is not specified, the parameters used were carefully chosen to optimize the analysis and ensure the reliability of the results.",
  "optimization/features": "In our study, we utilized a comprehensive set of input features derived from multiomic data to ensure a thorough analysis. For transcriptomics data, we focused on the 500 most variable genes, which were selected based on their expression variability across samples. This approach helped in capturing the most informative genes that could potentially drive the observed biological differences. For proteomics data, all detected proteins were included in the analysis, providing a broad spectrum of protein-level information.\n\nFeature selection was indeed performed, but it was done in a manner that ensured the integrity and independence of the training and testing datasets. Specifically, the selection of the 500 most variable genes for transcriptomics was based on the entire dataset before any splitting into training and testing sets. This method ensures that the feature selection process does not introduce any bias that could affect the performance evaluation of our models. For proteomics, since all proteins were included, no additional feature selection was necessary beyond the initial filtering steps that removed low-abundance proteins detected in less than 50% of samples.\n\nThe use of these selected features allowed us to conduct robust differential expression and enrichment analyses, which were crucial for identifying molecular subclusters and understanding the underlying disease mechanisms in ALS. The inclusion of both transcriptomic and proteomic data provided a multi-layered perspective, enhancing the reliability and comprehensiveness of our findings.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are all available and well-documented. All the scripts used for preprocessing and analyzing the data are accessible online. These scripts are written in Python, R, and Bash, and they can be executed in a dockerized environment, which is also provided on GitHub. The execution order of these scripts is defined in Data Version Control (DVC) workflows, which can be automatically executed using DVC. This ensures that the entire workflow is reproducible.\n\nAll important parameters are found in `params.yaml` files, and the dependencies and outputs of the scripts are defined in `dvc.yaml` files. This structured approach allows for easy maintenance and updates. The use of Docker images further ensures that the correct package versions are used, preventing issues related to package version mismatches.\n\nThe workflow also includes scripts for the automatic download of mouse sequencing data from the Sequence Read Archive (SRA), making it easy to use the workflow with the provided data. For human samples, manual download is required due to restricted access, but detailed instructions are available online.\n\nThe project is licensed under the MIT license, which allows for free use, modification, and distribution of the code. All supporting data and materials are available in the GigaScience database (GigaDB) and WorkflowHub. The raw and processed data for mouse RNA-seq and small RNA-seq are deposited in the National Center for Biotechnology Information Gene Expression Omnibus database and are openly available. Human data is stored in the European Genome Phenome Archive and is available upon request. Proteomics data is deposited in the ProteomeXchange Consortium database and is also openly available.\n\nIn summary, the configuration available for this study is comprehensive and designed to ensure reproducibility and ease of use. All necessary files, scripts, and parameters are documented and accessible, making it straightforward for others to replicate the analysis or adapt it for their own research.",
  "model/interpretability": "The model employed in our study is not a blackbox but rather a transparent approach that allows for interpretability through various analytical methods. We utilized decoupleR to estimate transcription factor activity, which provides clear insights into the regulatory mechanisms at play. This tool uses a combination of univariate linear models, weighted sums, and multivariate linear models to estimate activity, making the process transparent and interpretable.\n\nAdditionally, the DoRothEA database was used to identify potential transcription factor targets, ensuring that only targets with at least category C confidence were considered. This filtering process enhances the reliability and interpretability of the results.\n\nFor RNA stability analysis, we employed REMBRANDTS, which requires the extraction of exon and intron regions from GENCODE annotations. This method involves quantifying exon and intron abundance using htseq, providing a clear and interpretable measure of RNA stability.\n\nIn the context of proteomics, we used MaxQuant for data processing, which includes an open modification search to identify various protein modifications. This approach allows for a detailed understanding of the proteomic landscape, making the model transparent and interpretable.\n\nFurthermore, the verification of transgenic animals involved aligning RNA-seq reads against the human genome, which confirms the expression of transgenic transcripts. This method provides a clear and interpretable way to validate the transgenic models used in the study.\n\nOverall, the transparency of our model is evident through the use of well-established tools and databases, clear analytical methods, and interpretable results. This ensures that the findings are reliable and can be easily understood and replicated by other researchers.",
  "model/output": "The model discussed in this publication is not a traditional machine learning model used for classification or regression. Instead, it refers to various biological models, specifically transgenic mouse models, used to study amyotrophic lateral sclerosis (ALS). These models include different genetic modifications such as SOD1, FUS, TDP43, and C9orf72, which are associated with ALS. The data generated from these models include RNA-seq, small RNA-seq, and proteomics data, which were processed and analyzed to understand the molecular mechanisms underlying ALS.\n\nThe quality of the transformed proteomics data was evaluated by calculating the fraction of measured zero values and creating histograms of protein abundance values. No significant differences were observed between the sexes and conditions in terms of protein abundance. Similarly, the quality of the transformed small RNA-seq data was assessed, and no strong differences were found between the sexes and conditions in the expression of mature miRNAs.\n\nThe data processing workflow involves multiple scripts written in R and Python, which can be executed in a dockerized environment. This workflow ensures robust comparability and reproducibility of the analysis across all generated datasets, both within omics layers for the analyzed cohorts and across species. The workflow is available on GitHub, and all parameters are provided in a dedicated directory. Further details can be found in the README file of the GitHub repository.\n\nThe study provides a valuable data resource, including sex-specific and cross-species datasets. The stratified multi-omics data from ALS prefrontal cortices highlight male and female differences, which have implications for future personalized treatment approaches. Additionally, the study offers a robust analysis pipeline and high-quality data for investigating early ALS mechanisms. The datasets generated can help in understanding ALS pathogenesis and identifying new and personalized therapeutic targets for this neurodegenerative disease.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our project, named MAXOMOD, is publicly available. It can be accessed through the project's homepage on GitHub. The code is written in Python, R, and Bash, and it is designed to be platform-independent. To ensure reproducibility and ease of use, we have containerized the applications using Docker. This allows users to automatically execute the workflow without struggling to install the correct package versions. The workflow consists of multiple scripts that can be executed in a dockerized environment. The execution order is provided as a Data Version Control workflow, which can be automatically executed with DVC. All parameters are provided in the params directory. Additionally, the workflow is available on WorkflowHub, and further information can be found in the README file in the GitHub repository. The project is licensed under the MIT license.",
  "evaluation/method": "The evaluation of our methods involved several rigorous steps to ensure the quality and reliability of the data. For RNA-seq data, we performed quality checks and validated the annotation of sex by investigating the expression of XIST, a long noncoding RNA involved in X chromosome inactivation. This approach confirmed that there were no mismatched sex annotations in the human or mouse samples. Additionally, we validated the expression of transgenic variants in FUS, SOD1, and TDP43 mouse models by comparing the fraction of reads aligning to the region of interest against the total number of reads in that region. For the C9orf72 mouse model, we used an indirect approach to detect GFP expression, which indicated the presence of the introduced repeat expansion.\n\nThe quality of small RNA-seq data was evaluated using miRTrace as part of the Nextflow smrnaseq pipeline. We also conducted variant calling on the RNA-seq data using Bcftools based on STAR alignments. For proteomics data, we performed differential protein abundance analysis and a calibration analysis to verify that the obtained P-values followed the assumptions of classical FDR control. We detected a high differential protein abundance concentration and low uniformity underestimation, indicating no violations of the FDR control assumptions.\n\nTo assess potential batch effects, we used PCA and sample distance heatmaps, investigating factors such as brain bank, sex, case/control condition, and age at death. Sex-related differences were found, and all analyses were performed separately for each sex. No evidence of batch effects related to other factors was detected.\n\nOverall, our evaluation methods ensured robust comparability and reproducibility of the analysis across all generated datasets, both within omics layers for the analyzed cohorts and across species. This comprehensive evaluation provides a unique resource for the (re)analysis of ALS, considering multiple known ALS mouse models and human samples.",
  "evaluation/measure": "In the evaluation of our proteomics data, we focused on several key performance metrics to ensure the quality and reliability of our findings. One of the primary metrics we reported is the differential protein abundance concentration. This metric indicates the proportion of proteins that show significant differences in abundance between case and control samples. We aimed for a high differential concentration, ideally close to 100%, which would signify robust differences between the groups being compared.\n\nAnother crucial metric we evaluated is the uniformity underestimation. This metric assesses how well the P-values from our differential abundance analysis adhere to the assumptions of classical false discovery rate (FDR) control procedures. A low uniformity underestimation, close to 0, is preferred as it indicates that our statistical methods are accurately controlling for false positives. Our results showed a high differential protein abundance concentration (greater than 83%) and a low uniformity underestimation (less than 0.02) across all models, suggesting that our data meets the necessary statistical standards.\n\nAdditionally, we performed an open modification search to identify any post-translational modifications in the proteins. While we did not detect any striking differences between the mouse models and human samples, this analysis further validated the consistency and reliability of our proteomics data.\n\nOur evaluation also included assessing the fraction of measured zero values in the proteomics experiments. This metric helps to identify any potential systematic biases that might affect sample quality. We found no significant differences in the fraction of zero measurements between samples, indicating that there is no systematic bias impacting our data.\n\nFurthermore, we visualized the normalized protein abundance values using histograms, which showed no strong differences between sexes and conditions. This consistency across different sample groups reinforces the reliability of our proteomics data.\n\nIn summary, the performance metrics we reported—differential protein abundance concentration, uniformity underestimation, fraction of measured zero values, and histograms of normalized protein abundances—provide a comprehensive evaluation of our proteomics data. These metrics are representative of standard practices in the field and ensure that our findings are robust and reliable.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "In our study, we conducted a thorough evaluation of the differential protein abundance analysis using calibration plots. These plots helped us verify that the obtained P-values adhered to the assumptions of classical false discovery rate (FDR) control procedures. We detected a high differential protein abundance concentration, exceeding 83%, and a low uniformity underestimation, which was less than 0.02 across all models. This indicates that there are likely no violations of the FDR control assumptions, providing confidence in our results.\n\nWe also performed an open modification search using ionbot for four mouse models and human samples. The top 10 modifications for each model were selected, and the occurrences of the union of these modifications were displayed. Fixed modifications and sequence variations were removed for clarity. The results showed no striking differences between the mouse models and human samples, further supporting the reliability of our dataset.\n\nAdditionally, we assessed the possibility of batch effects in both transcriptomics and proteomics data using principal component analysis (PCA) and sample distance heatmaps. We investigated factors such as brain bank, sex, case/control condition, and age at death. While sex-related differences were found, all analyses were performed separately for each sex to mitigate this effect. For the other factors, no evidence of influence on the results was detected, suggesting that batch effects did not significantly impact our findings.\n\nOverall, our evaluation indicates a high level of confidence in the dataset, with no systematic biases detected and low intersample intramodel variability, especially for the mouse samples. The expression of transgenes was also proven, making this dataset a unique resource for the (re)analysis of ALS considering multiple known ALS mouse models and human samples.",
  "evaluation/availability": "The raw evaluation files are available through various databases and repositories. For mouse RNA-seq and small RNA-seq data, the raw data in FASTQ format, along with processed data in CSV format, have been deposited in the National Center for Biotechnology Information Gene Expression Omnibus database. These datasets are openly available under the accession numbers GSE234245 and GSE234243, respectively.\n\nHuman RNA-seq and small RNA-seq data are encrypted and stored at the European Genome Phenome Archive under the registered study EGAS00001007318. Access to these data is available upon request, with detailed instructions on how to request the data provided online.\n\nProteomics data for both human and mouse samples are deposited in the ProteomeXchange Consortium database under the accession number PXD043300. These data are also openly available. Additionally, the results from the open modification search are available via Figshare.\n\nAll supporting data and materials, including scripts and workflows, are available in the GigaScience database, GigaDB, and in WorkflowHub. The workflow consists of multiple scripts written in R, Python, and Bash, which can be executed in a dockerized environment. The execution order and parameters are provided in the Data Version Control workflow, which can be automatically executed with DVC. The project is licensed under the MIT license, ensuring open access and the ability to reuse, distribute, and reproduce the work with proper citation."
}