{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\nLiqian Zhou, who was involved in conceptualization, methodology, and writing the original draft and review.\n\nXiaohui Peng, who contributed to conceptualization, methodology, and writing the original draft and review.\n\nXiaozhe Huang, who contributed to the methodology.\n\nGuang Tian, who contributed to the methodology.\n\nLihong Peng, who contributed to conceptualization, supervision, and writing the original draft and review.\n\nMin Chen, who contributed to conceptualization and writing the review and editing.\n\nJian-Liang Yang, who contributed to supervision and writing the review and editing.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Spatial transcriptomics\n- Single-cell RNA sequencing\n- Machine learning\n- Deep learning\n- Graph neural networks\n- Clustering algorithms\n- Bioinformatics\n- Computational biology\n- Data analysis\n- Gene expression",
  "dataset/provenance": "The datasets utilized in our study are sourced from various reputable platforms. The Adult Mouse Brain (FFPE), Human Breast Cancer (DCIS), and Human Breast Cancer (Block A Section 1) datasets are obtained from the 10x Genomics website. These datasets have been previously used in the scientific community, ensuring their reliability and relevance.\n\nThe Adult Mouse Brain (FFPE) dataset contains 2,264 spots and 19,465 genes. The Human Breast Cancer (DCIS) dataset includes 3,798 spots and 36,601 genes. The Human Breast Cancer (Block A, Section 1) dataset detects 2,518 spots and 19,743 genes. Additionally, the Human Dorsolateral Prefrontal Cortex (DLPFC) dataset contains 12 tissue slices, capturing 33,538 genes with varying spot numbers ranging from 3,460 to 4,789 in each slice. Each slice contains 5 to 7 regions by manual annotation.\n\nThe mouse visual cortex STARmap dataset provides expression information of 1,020 genes from 1,207 cells. The Stereo-seq dataset from mouse embryos at E9.5 is obtained using high-resolution full-transcriptome coverage technologies, specifically the Stereo-seq technology. This dataset includes 5,913 spots and 25,568 genes for one sample and 4,356 spots and 24,107 genes for another.\n\nThese datasets have been carefully selected to ensure a comprehensive analysis, leveraging both labeled and unlabeled data to enhance the robustness of our findings. The use of well-established datasets from trusted sources underscores the rigor and reliability of our research.",
  "dataset/splits": "There are four primary datasets used in our study, each with distinct characteristics and data splits. The Adult Mouse Brain (FFPE) dataset contains 2,264 spots and 19,465 genes. The Human Breast Cancer (DCIS) dataset includes 3,798 spots and 36,601 genes. The Human Breast Cancer (Block A, Section 1) dataset detects 2,518 spots and 19,743 genes. The Human Dorsolateral Prefrontal Cortex (DLPFC) dataset contains 12 tissue slices, with each slice capturing between 3,460 to 4,789 spots and 33,538 genes.\n\nThe first two datasets, Adult Mouse Brain (FFPE) and Human Breast Cancer (Block A, Section 1), do not have clustering labels. The latter two datasets, Human Breast Cancer (DCIS) and DLPFC, are known to be labeled. The DLPFC dataset is particularly notable for its manual annotations, with each slice containing 5 to 7 regions.\n\nAdditionally, the mouse visual cortex STARmap dataset provides expression information of 1,020 genes from 1,207 cells. The Stereo-seq dataset from mouse embryos at E9.5, obtained using high-resolution full-transcriptome coverage technologies, includes two splits: E9.5_E1S1 with 5,913 spots and 25,568 genes, and E9.5_E2S2 with 4,356 spots and 24,107 genes.\n\nThe datasets were collected from various sources, including the 10x Genomics website and the GigaScience repository. The Adult Mouse Brain (FFPE), Human Breast Cancer (DCIS), and Human Breast Cancer (Block A Section 1) datasets are specifically from the 10x Genomics website. The DLPFC dataset is from the spatialLIBD website. The Stereo-seq dataset is from a study using DNA nanoball-patterned arrays.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets utilized in our study are publicly available, ensuring transparency and reproducibility. The Adult Mouse Brain (FFPE), Human Breast Cancer (DCIS), and Human Breast Cancer (Block A Section 1) datasets can be accessed from the 10x Genomics website. Additionally, an archival copy of the code and supporting data is available via the GigaScience repository, GigaDB. The license for the code is the MIT license, which allows for free use, modification, and distribution. The filtered spatial transcriptomic data is released under the Creative Commons CC0 1.0 Public Domain Dedication, ensuring that the data is freely available for any purpose without restrictions.\n\nTo enforce the availability and accessibility of these datasets, we have made them accessible through well-known repositories and provided clear licensing information. This approach ensures that other researchers can easily access and utilize the datasets for their own studies, promoting collaboration and further advancements in the field. The DOME-ML annotations are also available via a link in GigaDB and through accession cji1mirt7b in the DOME registry, providing additional resources for researchers interested in machine learning applications in spatial transcriptomics.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages deep learning techniques, specifically focusing on neural networks and clustering methods. The core of our approach involves a self-supervised module that utilizes clustering labels to guide the learning of spot embeddings. This module is designed to enhance the accuracy of spatial clustering by incorporating multiscale information through a deep subspace clustering strategy.\n\nThe algorithm is not entirely new but builds upon established methods in the field of machine learning and spatial transcriptomics. It integrates several well-known techniques, such as Louvain clustering, Leiden clustering, and mclust, which are adapted and optimized for our specific datasets. The use of these established methods ensures robustness and reliability in our spatial clustering results.\n\nThe decision to publish in a journal focused on computational biology and genomics, rather than a machine-learning journal, is driven by the application and impact of our work. Our primary goal is to advance the understanding of tissue organization and biological functions through accurate spatial domain detection and differential gene expression analysis. While the optimization algorithm is a crucial component, the biological insights and applications are the main contributions of this study. Therefore, it is more appropriate to share our findings in a context where the biological significance can be fully appreciated.",
  "optimization/meta": "The model does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a self-supervised module that utilizes clustering labels to guide the learning of spot embeddings. This approach enhances the model's clustering ability by leveraging the inherent structure of the data.\n\nThe model incorporates several key components to improve spatial clustering performance. These include a multiscale deep subspace clustering module, which fully utilizes embedded multiscale information, and an adaptive fusion method that significantly improves spatial clustering. Additionally, an optimization step is used to further refine the spatial clustering results, ensuring that spots are accurately assigned to their respective domains.\n\nThe model's design focuses on enhancing the accuracy of spatial clustering by integrating various strategies and techniques. These strategies are applied to different datasets, such as DLPFC, STARmap, and Human Breast Cancer, to demonstrate their effectiveness. The results indicate that the model outperforms other clustering methods, such as Louvain clustering, mclust, Leiden clustering, and subspace clustering, across various datasets.\n\nThe model's performance is evaluated using the Adjusted Rand Index (ARI), which measures the similarity between the predicted and true cluster assignments. The ablation studies conducted on different loss terms, multiscale strategies, and additional optimization steps provide insights into the model's robustness and effectiveness. The results demonstrate that the model's components work synergistically to achieve superior spatial clustering performance.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We began by normalizing gene expressions for each spot, which served as the initial spot embeddings. This normalization process was essential to standardize the data and mitigate the effects of varying expression levels across different genes and spots.\n\nWe employed a graph attention mechanism to construct an embedding feature matrix. For each spot, an encoder with multiple layers was used to generate embedding features by incorporating information from neighboring spots. The initial embedding for spot i, denoted as h(0)i, was simply the normalized gene expression xi. In subsequent layers, the embedding for spot i in the k-th layer, h(k)i, was computed using a weighted sum of the embeddings of its neighbors, with weights determined by an attention mechanism. This process allowed the model to adaptively focus on relevant neighbors, enhancing the quality of the embeddings.\n\nThe attention mechanism itself was implemented as a 1-layer feed-forward neural network, parametrized by a weight vector. It computed the similarity between neighboring spots in an adaptive manner, ensuring that the most informative neighbors contributed more to the embedding of a given spot.\n\nIn the decoder part of our model, we transformed the learned latent embeddings back into normalized expression profiles to reconstruct the spot features. This involved using a self-expression matrix to map the embeddings from the encoder's final layer and then feeding this mapped data into the decoder. The decoder layers mirrored the encoder layers, using similar attention mechanisms to reconstruct the embeddings. To avoid overfitting, we set the weights in the decoder to be the transpose of the weights in the encoder.\n\nAdditionally, we utilized a self-supervised learning approach to further refine the spot embeddings. This involved using clustering labels to guide the learning process, ensuring that spots with similar labels had similar embeddings. This self-supervised module significantly improved the accuracy of our clustering results.\n\nFor datasets with known labels, we manually tuned the resolution parameter to ensure that the number of clusters matched the ground truth. This step was crucial for validating our model's performance and ensuring that it could accurately capture the underlying structure of the data. For other clustering methods, we used their default settings to maintain consistency in our comparisons.",
  "optimization/parameters": "In our model, several key parameters were used to optimize the performance of spatial clustering. The specific parameters varied depending on the dataset, but generally included `rad_cutoff`, `cost_ssc`, and `α`. These parameters were carefully selected and tuned to ensure optimal clustering performance.\n\nThe `rad_cutoff` parameter, which defines the radius for constructing the nearest neighbor network, was set to different values for each dataset. For instance, it was set to 150 for the DLPFC dataset, 300 for the Human Breast Cancer (Block A, Section 1) and Adult Mouse Brain (FFPE) datasets, 400 for the Mouse visual cortex dataset, and 3 for the Stereo-seq mouse embryo dataset. This parameter is crucial as it determines the spatial resolution of the clustering.\n\nThe `cost_ssc` parameter, which likely influences the self-supervised learning component, was set to 0.1 for most datasets except for the Human Breast Cancer (Block A, Section 1) and Human Breast Cancer (DCIS) datasets, where it was set to 1. This parameter helps in balancing the trade-off between the self-supervised loss and other components of the model.\n\nThe `α` parameter, which might control the contribution of different loss terms or regularization strengths, was set to 0 for datasets like DLPFC and Stereo-seq mouse embryo, 0.7 for Human Breast Cancer (Block A, Section 1), 0.5 for Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS), and 0 for Mouse visual cortex. This parameter fine-tunes the model's sensitivity to different aspects of the data.\n\nAdditionally, the clustering method was specified for each dataset. For example, Louvain clustering was used for DLPFC, Adult Mouse Brain (FFPE), Human Breast Cancer (DCIS), and Stereo-seq mouse embryo datasets. Leiden clustering was used for the Human Breast Cancer (Block A, Section 1) dataset, and mclust was used for the Mouse visual cortex dataset. These methods were chosen based on their performance in preliminary experiments and their suitability for the specific characteristics of each dataset.\n\nThe selection of these parameters was guided by an ablation study, where different combinations of parameters were tested to identify the optimal settings. This systematic approach ensured that the chosen parameters provided the best clustering performance for each dataset.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed a self-supervised module with a rectified linear units (ReLu) activation function, which is known for its simplicity and effectiveness in preventing overfitting by introducing non-linearity without adding too many parameters. This helps in managing the complexity of the model relative to the number of training points.\n\nTo address potential overfitting, we utilized several strategies. Firstly, we conducted ablation studies to evaluate the impact of different components and loss terms on the model's performance. These studies helped in identifying the most effective configurations and ensuring that the model was not overly complex for the given data. Secondly, we implemented an additional optimization step to refine the spatial clustering results, which further improved the model's accuracy without increasing the risk of overfitting.\n\nUnderfitting was mitigated through the use of a multiscale strategy, which allowed the model to capture embedding features at different scales. This approach ensured that the model could learn from various levels of detail in the data, thereby improving its ability to generalize. Additionally, we employed adaptive fusion methods that significantly enhanced the spatial clustering performance, indicating that the model was capable of learning from the data without being too simplistic.\n\nThe use of different clustering methods for different datasets, such as Louvain clustering for DLPFC and Leiden clustering for Human Breast Cancer, also contributed to preventing underfitting. These methods were selected based on ablation experiments that demonstrated their effectiveness in improving clustering accuracy for specific datasets.\n\nOverall, our approach balanced the complexity of the model with the need to avoid both overfitting and underfitting, resulting in robust and accurate spatial clustering performance.",
  "optimization/regularization": "In our study, we implemented a regularization method to prevent overfitting. Specifically, we introduced a regularization loss to ensure that the coefficient matrices in our multiscale self-expression module were not overly sparse. This regularization loss is defined to penalize the sparsity of these matrices, thereby encouraging a more robust and generalizable model. By incorporating this regularization term into our total loss function, we aimed to enhance the model's ability to generalize to unseen data, thus mitigating the risk of overfitting. This approach helped in maintaining a balance between fitting the training data accurately and ensuring that the model performs well on new, unseen data.",
  "optimization/config": "In our study, we have made efforts to ensure that our methodology and findings are reproducible. The hyper-parameter configurations used in our experiments are detailed within the publication. Specifically, for the self-supervised module, we utilized rectified linear units (ReLU) as the activation function. For Louvain clustering, the radius parameter was set to 50, which yielded the best clustering performance on the DLPFC dataset.\n\nThe data preprocessing steps adopted in our study, STMSGAL, mirror those used in SCANPY, including log-normalization and the construction of the nearest neighbor network. The resolution parameter for each dataset with labels was manually tuned to ensure the cluster number matched the ground truth. This approach was consistent across all methods to maintain fairness in comparison.\n\nRegarding the optimization schedule and model files, while the specific details of these are not explicitly outlined in the main text, the code and supporting data are available via the GigaScience repository, GigaDB. This repository includes an archival copy of the code and the necessary data to replicate our experiments. Additionally, DOME-ML (Data, Optimization, Model, and Evaluation in Machine Learning) annotations are accessible through GigaDB and the DOME registry, providing further transparency and reproducibility.\n\nThe license under which these resources are made available is not specified in the provided context, but typically, repositories like GigaDB provide open access to data and code, allowing researchers to use and build upon the work with appropriate citation. For precise licensing details, one would need to refer directly to the GigaDB repository.\n\nIn summary, while the hyper-parameter configurations and optimization details are reported within the publication, the specific model files and optimization schedules are available through the GigaScience repository. This ensures that other researchers can replicate and build upon our findings.",
  "model/interpretability": "The model employed in our study is designed with a focus on interpretability, aiming to provide transparency in its operations and outputs. Unlike many black-box models, our approach incorporates several mechanisms that enhance interpretability.\n\nOne key aspect of our model's transparency is the use of self-attention mechanisms. These mechanisms allow the model to compute the similarity between neighboring spots adaptively. By examining the attention weights, one can understand which spots are considered most relevant to each other at different stages of the encoding and decoding processes. This provides insights into the model's decision-making process and helps in interpreting the relationships between different data points.\n\nAdditionally, the model utilizes a multiscale deep subspace clustering algorithm. This algorithm obtains cluster labels based on multiscale information from each encoder layer for spots. The self-expression property of the data is crucial here, as it allows each datum to be represented as a linear combination of other data points. This linear combination can be analyzed to understand how different features contribute to the clustering of spots, making the clustering process more interpretable.\n\nThe use of a self-expression matrix in the decoder part further aids in interpretability. This matrix reconstructs the spot features by transforming the learned latent embeddings back into normalized expression profiles. By examining the self-expression matrix, one can gain insights into how the model reconstructs the original data from its latent representations, providing a clearer understanding of the model's internal workings.\n\nMoreover, the model incorporates a gradient boosted neural network and an interpretable boosting machine for identifying potential ligand-receptor interactions. These components enhance the model's ability to provide clear and understandable explanations for its predictions, making it more transparent and interpretable.\n\nIn summary, our model is designed to be transparent and interpretable, with several mechanisms in place to provide clear insights into its operations and outputs. The use of self-attention mechanisms, multiscale deep subspace clustering, and interpretable boosting machines all contribute to making the model's decision-making process more understandable.",
  "model/output": "The model described is primarily focused on clustering and feature learning rather than traditional classification or regression tasks. It employs a deep learning framework to analyze spatial transcriptomics data, aiming to uncover patterns and identify spatial domains within the data.\n\nThe output of the model involves several key components. Initially, the model constructs an embedding feature matrix for spots using a graph attention mechanism. This matrix is then used to obtain spot cluster labels through subspace clustering. Subsequently, the model learns robust latent features for the spots via a self-supervised learning module. This module includes a classification step where spots are classified based on three fully connected layers, and the classification results are constrained using cross-entropy loss between the classification and clustering results.\n\nThe total loss function of the model integrates multiple loss components, including a multiscale self-expression loss, a regularization loss to prevent sparsity, and a supervised loss from the self-supervised module. This comprehensive loss function ensures that the model effectively captures the multiscale features and robustly learns the latent representations of the spots.\n\nIn terms of biological application, the model first identifies spatial domains using clustering algorithms such as Leiden, Louvain, or mclust. It then performs differential expression analysis using the t-test in the Scanpy package and conducts trajectory inference. The model has been applied to various datasets, including those from the 10x Genomics website, and the results are available via the GigaScience repository.\n\nThe output of the model is thus a combination of spatial domain identification, differential expression analysis, and trajectory inference, providing a holistic view of the spatial transcriptomics data.",
  "model/duration": "The execution time of the STMSGAL model varied depending on the dataset and the specific tasks performed. The model incorporates several components, including a graph attention autoencoder, multiscale deep subspace clustering, and a self-supervised module, each contributing to the overall computational load.\n\nFor the DLPFC dataset, the model was optimized to achieve the best clustering performance with a radius of 50 for Louvain clustering. The execution time for this dataset included the preprocessing steps, such as log-normalization and the construction of the nearest neighbor network, which were handled similarly to SCANPY. The actual clustering and embedding processes were computationally intensive, but the specific execution time was not explicitly detailed.\n\nThe ablation studies conducted on the DLPFC sections from 151507 to 151510 provided insights into the contributions of different modules. For instance, the combination of the multiscale deep subspace clustering module and the self-supervised module significantly improved clustering performance. However, the exact execution times for these studies were not provided.\n\nIn summary, while the model demonstrated efficient performance in terms of clustering accuracy, the specific execution times for different datasets and tasks were not thoroughly documented. Further details on execution time would require additional benchmarking and profiling of the model across various datasets and computational environments.",
  "model/availability": "The source code for the STMSGAL project is publicly available. It can be accessed via the project's GitHub repository. The repository is platform-independent and the code is written in Python. The project is licensed under the MIT license, which allows for free use, modification, and distribution of the software, subject to the terms of the license. This ensures that users can integrate and build upon the code for their own research and applications. Additionally, the filtered spatial transcriptomic data is available under the Creative Commons CC0 1.0 Public Domain Dedication, making it freely available for any purpose without restrictions.",
  "evaluation/method": "The evaluation of our method, STMSGAL, involved a comprehensive assessment using various datasets and metrics to ensure its robustness and accuracy in spatial clustering.\n\nFor datasets with labeled spatial domains, such as Human Breast Cancer (Block A, Section 1), DLPFC, and mouse visual cortex STARmap, we employed the adjusted Rand index (ARI) to evaluate performance. ARI measures the similarity between predicted clustering labels and reference cluster labels, with higher scores indicating better performance. This metric was chosen for its ability to account for chance grouping, providing a more reliable evaluation of clustering accuracy.\n\nFor datasets without labeled spatial domains, such as Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS), we utilized three clustering metrics: Davies–Bouldin (DB) score, Calinski–Harabasz (CH) score, and S_Dbw score. The DB score averages all cluster similarities, where similarity is computed by the ratio of within-cluster distances to between-cluster distances. The CH score measures cluster validity by averaging the squares of within- and between-cluster distance sums. The S_Dbw score evaluates intraclass compactness and interclass density. Optimal clustering is indicated by small DB and S_Dbw scores and a large CH score.\n\nIn addition to these metrics, we compared STMSGAL with six other clustering algorithms—SCANPY, SEDR, CCST, DeepST, STAGATE, and GraphST—on the Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS) datasets. STMSGAL demonstrated superior performance, achieving the smallest DB and S_Dbw scores and the highest CH score on the Adult Mouse Brain (FFPE) dataset, and the highest CH and smallest S_Dbw scores on the Human Breast Cancer (DCIS) dataset.\n\nFurthermore, we conducted an ablation study to investigate the contributions of different components in STMSGAL, including the graph attention autoencoder (GATE), multiscale deep subspace clustering, and the self-supervised module. The results showed that these components significantly improved clustering performance. We also evaluated the impact of an additional optimization step, finding that it enhanced the method's accuracy across different DLPFC sections.\n\nOverall, our evaluation approach combined established clustering metrics with comparative analyses and ablation studies to thoroughly assess STMSGAL's performance and validate its effectiveness in spatial clustering tasks.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the effectiveness of our spatial clustering algorithms. For datasets with labeled spatial domains, such as Human Breast Cancer (Block A, Section 1), DLPFC, and mouse visual cortex STARmap, we utilized the Adjusted Rand Index (ARI). ARI is a robust metric that measures the similarity between predicted clustering labels and reference cluster labels, providing a clear indication of clustering accuracy. A higher ARI score signifies better performance.\n\nFor datasets lacking spatial domain annotations, such as Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS), we evaluated performance using three clustering metrics: Davies–Bouldin (DB) score, Calinski–Harabasz (CH) score, and S_Dbw score. The DB score assesses cluster separation by averaging the similarity between each cluster and its most similar cluster, with lower values indicating better separation. The CH score measures cluster validity by evaluating the ratio of within-cluster distances to between-cluster distances, where higher values denote better-defined clusters. The S_Dbw score evaluates intraclass compactness and interclass density, with smaller values indicating optimal clustering.\n\nThese metrics are well-established in the literature and provide a thorough assessment of clustering performance across different datasets and conditions. By using both ARI for labeled datasets and DB, CH, and S_Dbw for unlabeled datasets, we ensure a representative and comprehensive evaluation of our clustering algorithms. This approach allows us to compare our methods against existing state-of-the-art techniques and demonstrate their effectiveness in identifying spatial domains in various biological contexts.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of our proposed method, STMSGAL, by comparing it against several publicly available and widely used spatial clustering algorithms. This comparison was performed on benchmark datasets to ensure a fair and comprehensive assessment.\n\nFor datasets with labeled spatial domains, such as Human Breast Cancer (Block A, Section 1), DLPFC, and mouse visual cortex STARmap, we employed the adjusted Rand index (ARI) to evaluate the performance of different spatial clustering algorithms. ARI measures the similarity between predicted clustering labels and reference cluster labels, with higher scores indicating better performance.\n\nFor datasets without labeled spatial domains, such as Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS), we used three clustering metrics: Davies–Bouldin (DB) score, Calinski–Harabasz (CH) score, and S_Dbw score. The DB score averages all cluster similarities, where similarity is computed by the ratio of within-cluster distances to between-cluster distances. The CH score measures cluster validity by averaging the squares of within- and between-cluster distance sums. The S_Dbw score evaluates intraclass compactness and interclass density. Optimal clustering is indicated by small DB and S_Dbw scores and a large CH score.\n\nWe compared STMSGAL with six other clustering methods: SCANPY, SEDR, CCST, DeepST, STAGATE, and GraphST. These methods were chosen because they are broadly applied in single-cell and spatial clustering. The results demonstrated that STMSGAL achieved the best performance in terms of DB, CH, and S_Dbw scores on the Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS) datasets, suggesting its superior clustering performance.\n\nAdditionally, we performed an ablation study to evaluate the impact of different clustering methods on various datasets. For DLPFC, Louvain clustering was used, while Leiden clustering was applied to Human Breast Cancer, and mclust was used for STARmap. The ablation analysis showed that STMSGAL significantly improved spatial clustering accuracy when using these specific clustering methods for each dataset.\n\nIn summary, our evaluation involved a detailed comparison with publicly available methods and simpler baselines on benchmark datasets, providing a robust assessment of STMSGAL's performance.",
  "evaluation/confidence": "In our evaluation, we employed several metrics to assess the performance of our method, STMSGAL, and compared it with other clustering algorithms. For datasets with labeled spatial domains, such as Human Breast Cancer (Block A, Section 1), DLPFC, and mouse visual cortex STARmap, we used the Adjusted Rand Index (ARI) to evaluate the similarity between predicted clustering labels and reference cluster labels. ARI provides a measure of the agreement between two data clusterings, adjusted for chance. A higher ARI score indicates better performance.\n\nFor datasets without labeled spatial domains, such as Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS), we utilized three clustering metrics: Davies–Bouldin (DB) score, Calinski–Harabasz (CH) score, and S_Dbw score. The DB score measures the average similarity ratio of each cluster with the cluster most similar to it, with lower values indicating better clustering. The CH score evaluates the cluster validity by considering the ratio of within-cluster dispersion and between-cluster dispersion, with higher values indicating better-defined clusters. The S_Dbw score assesses intraclass compactness and interclass density, with smaller values suggesting optimal clustering.\n\nTo ensure the statistical significance of our results, we conducted ablation studies and comparisons across multiple datasets. For instance, we demonstrated that STMSGAL with an additional optimization step significantly outperformed the version without this step on DLPFC datasets, as evidenced by higher ARI values. Similarly, we showed that STMSGAL achieved the best performance in terms of DB, CH, and S_Dbw scores on Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS) datasets compared to six other clustering methods.\n\nWhile we did not explicitly provide confidence intervals for the performance metrics, the consistent superiority of STMSGAL across different datasets and metrics suggests robust and statistically significant results. The use of multiple evaluation metrics and comparisons with established methods further supports the claim that STMSGAL is superior in spatial clustering performance.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the source codes and datasets of STMSGAL are accessible in the GitHub repository. Additionally, an archival copy of the code and supporting data can be found via the GigaScience repository, GigaDB. The license for the code is the MIT license, and the filtered spatial transcriptomic data is under the Creative Commons CC0 1.0 Public Domain Dedication. This ensures that the data and code are freely available for use and distribution."
}