{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Janine Bijsterbosch, who contributed to conceptualization, funding acquisition, supervision, writing the original draft, and reviewing and editing.\n- Ty Easley, who contributed to conceptualization, data curation, formal analysis, funding acquisition, methodology, writing the original draft, and reviewing and editing.\n- Kayla Hannon, who contributed to methodology, visualization, and writing the original draft.\n- Petra Lenzini, who contributed to data curation, visualization, and writing the original draft.\n- Xiaoke Luo, who contributed to data curation, formal analysis, methodology, visualization, and writing the original draft and reviewing and editing.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Machine Learning\n- Neuroimaging\n- Classification Models\n- Random Forest\n- Support Vector Classifier\n- K-Nearest Neighbors\n- Structural MRI\n- Functional MRI\n- Sociodemographic Features\n- UK Biobank\n- Diagnostic Classification\n- Mental Health\n- Data Analysis\n- Hyperparameter Tuning\n- Principal Component Analysis",
  "dataset/provenance": "The dataset used in this study is sourced from the UK Biobank (UKB), a large-scale biomedical database and research resource containing anonymized genetic, lifestyle, and health information from half a million UK participants. The UK Biobank offers a unique neuroimaging dataset, which adopts an epidemiological approach with a prospective recruitment strategy and a large sample size. At the time of writing, neuroimaging data for tens of thousands of participants had been acquired and released, with ongoing data acquisition aiming for a total of 100,000 participants.\n\nThe study utilized neuroimaging data from participants with a wide range of ICD-10 diagnoses, derived from clinical records. This inclusivity allows for a comprehensive analysis across various diseases. The dataset includes structural and functional neuroimaging features, as well as sociodemographic information. The structural features comprise surface-based and volumetric measures, while functional features are extracted through methods such as independent component analysis and probabilistic functional modes.\n\nThe study focused on 17 ICD-10 diagnostic groups from Chapters V (mental and behavioral disorders) and VI (diseases of the nervous system). For each diagnostic group, cases were matched with controls to achieve balanced case-control groups for classification. The total number of UK Biobank participants with complete neuroimaging data but without ICD-10 labels in the relevant chapters was 31,225, forming the pool of healthy controls. Each case was matched to a unique control participant across all diagnostic groups, ensuring balance in sex, age, and head motion.\n\nThe dataset has been previously used in various studies, and the UK Biobank is a well-established resource in the scientific community. The data is available following an access application process, and more information can be found on the UK Biobank website. The study provides a benchmark for future research on diagnostic classification models using the UK Biobank dataset, leveraging its longitudinal follow-up of participants and capture of hospital and death records.",
  "dataset/splits": "In our study, we employed several data splits to ensure robust and comprehensive analysis. For the multiclass classification task, we categorized samples into 18 possible labels, including 17 ICD-10 groups and a control group. To avoid multiple diagnostic labels per individual, we used a unique and matched sample size subject list. This resulted in a balanced dataset where each of the 17 diagnostic groups was matched with controls based on sex, age, and resting state head motion.\n\nFor the case-control classifications, we achieved 17 fully balanced groups, with each diagnostic group containing an equal number of cases and controls. The total number of UK Biobank participants with complete neuroimaging data but no ICD-10 labels in relevant chapters was 31,225, forming our pool of healthy controls. From this pool, controls were selected to match each case, resulting in perfectly matched groups for sex and no significant group differences for age or head motion.\n\nTo test the impact of sample size on classification performance, we repeated analyses after subsampling each ICD-10 diagnostic group to match the minimum sample size of 125 cases and 125 controls. This subsampling procedure ensured that potential differences in confounding variables between diagnostic groups were removed.\n\nAdditionally, we compiled a balanced group of 2,656 subjects to match the size of the largest ICD-10 diagnostic group and subsampled 252 subjects to match the smallest group. This allowed us to benchmark classification effect sizes across all ICD-10 diagnostic groups against the classification effect size of age. The random forest classification models were applied to both structural feature sets (surface and volume) and functional data extracted through various methods.\n\nIn summary, our data splits included:\n1. **Multiclass Classification**: 18 labels (17 ICD-10 groups + control group) with matched samples.\n2. **Case-Control Classifications**: 17 balanced groups with equal cases and controls.\n3. **Subsampled Groups**: Each diagnostic group subsampled to 125 cases and 125 controls.\n4. **Age Benchmarking**: Balanced groups of 2,656 and 252 subjects to match the largest and smallest ICD-10 groups, respectively.",
  "dataset/redundancy": "The datasets were split using a shuffle-split approach across 100 repeats, with 80% of the data allocated for training and 20% for validation. This method ensures that the training and test sets are independent in each iteration, providing a robust evaluation of the classification models.\n\nTo enforce independence and avoid data leakage, we employed a matching procedure. For each of the 17 diagnostic groups, cases were matched to controls based on sex, age, and head motion. This resulted in perfectly matched groups for sex and no significant differences in age or head motion. Additionally, to test the impact of sample size on classification performance, we repeated analyses after subsampling each diagnostic group to match the smallest sample size, further ensuring that potential confounding variables were controlled.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. By matching cases and controls on key demographic and confounding variables, we aimed to create a balanced and representative dataset. This approach helps mitigate biases and ensures that the classification results are generalizable. The use of multiple feature sets, including structural, functional, and socioeconomic features, also aligns with best practices in the literature, providing a comprehensive evaluation of diagnostic classification.",
  "dataset/availability": "The data used in this study is available through the UK Biobank, which requires an access application process. Interested researchers can find more information and apply for access at the UK Biobank website. The specific research was conducted under UK Biobank application number 47267.\n\nThe data splits used in the study are not explicitly released in a public forum. However, the methodology and details about the data processing and splits are described in the publication, allowing other researchers to replicate the process.\n\nThe source code for the project, named WAPIAW3, is publicly available on GitHub under the MIT license. This ensures that the code can be freely used, modified, and distributed. The code has also been archived in Software Heritage, providing a permanent record of the software used in the study.\n\nThe availability of the data and the open-source nature of the code promote transparency and reproducibility in the research. The UK Biobank's access control ensures that the data is used responsibly and ethically, in line with the guidelines and regulations set by the UK Biobank and relevant authorities.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the ensemble learning method, specifically the random forest classifier. This algorithm was chosen for its flexibility in handling data of varied units, its suitability for nonlinear classification tasks, and its scalability. The random forest classifier is not new; it is well-established and widely used in the machine learning community. It was implemented using the scikit-learn library, which is a popular and reliable toolkit for machine learning in Python.\n\nThe reason the random forest classifier was not published in a machine-learning journal is that it is a well-known and extensively studied algorithm. Our work focuses on applying this established method to a specific problem in neuroimaging and diagnostic classification, rather than developing a new algorithm. The primary contributions of our study lie in the application of the random forest classifier to classify ICD-10 diagnostic groups using neuroimaging data, and in comparing its performance against other classifiers and feature sets.\n\nIn addition to the random forest classifier, we also tested two other classifiers: the support vector classifier and the k-nearest neighbors classifier. These classifiers were implemented using scikit-learn as well. For the support vector classifier, we tuned the regularization parameter C and the kernel type. For the k-nearest neighbors classifier, we tuned the number of neighbors, the weight function, and the distance metric. The classification pipeline, including principal component analysis, nested folds for hyperparameter estimation, and shuffle-splits, was identical for all three classifiers. This approach allowed us to benchmark the performance of the random forest classifier against these alternative methods.",
  "optimization/meta": "The model employed in our study does not function as a meta-predictor. Instead, it primarily relies on a single machine-learning algorithm, specifically the random forest classifier, which is implemented using scikit-learn. This classifier was chosen for its flexibility in handling varied data units, its suitability for nonlinear classification tasks, and its scalability.\n\nThe random forest classifier was used to perform the primary analyses, and its performance was benchmarked against other classification algorithms, including the support vector classifier and the k-nearest neighbors classifier. However, these additional classifiers were not combined into a meta-predictor model. Rather, they were tested independently to compare their classification accuracy with that of the random forest classifier.\n\nThe training data for the random forest classifier was carefully managed to ensure independence. A shuffle-split resampling scheme was used to subdivide the data into 100 stratified training (80%) and validation (20%) splits. This approach helps to mitigate the risk of data leakage and ensures that the model's performance is evaluated on independent data.\n\nIn summary, the model does not use data from other machine-learning algorithms as input in a meta-predictor framework. The random forest classifier stands alone as the primary classification algorithm, and the training data's independence is maintained through rigorous resampling techniques.",
  "optimization/encoding": "The data used in our study consisted of neuroimaging data from the UK Biobank, specifically T1-weighted scans and resting-state functional MRI data. The T1-weighted scans had 1-mm isotropic voxels with a repetition time (TR) of 2,000 ms and an inversion time (TI) of 880 ms. The resting-state functional scans had 2.4-mm isotropic voxels, a TR of 735 ms, a time echo (TE) of 39 ms, and a multiband factor of 8.\n\nThe preprocessing of the data involved several steps. First, the data was transformed into MNI space using both linear and nonlinear transforms provided by the UK Biobank. This step ensured that all data was aligned to a common reference space, facilitating consistent analysis across subjects.\n\nFor the structural neuroimaging features, we defined two feature sets from the T1-weighted images. The surface-based structural feature set included 285 imaging-derived phenotypes (IDPs) from the UK Biobank pipelines, derived from Freesurfer pipelines. This set consisted of 186 cortical IDPs from Freesurfer’s Desikan-Killiany-Tourville (DKT)-based parcellation and 99 subcortical IDPs from the Automatic Segmentation of Hippocampal Subfields (ASEG). The volumetric structural feature set consisted of 153 IDPs, including 139 regional gray matter volumes segmented using FSL FAST and 14 subcortical volumes segmented using FSL FIRST.\n\nIn addition to structural features, we also explored functional neuroimaging features derived from resting-state fMRI data. We tested 17 different feature sets obtained from data-driven approaches, including independent component analysis, probabilistic functional modes, and atlas-based features. These feature sets were chosen to capture various aspects of brain function and connectivity.\n\nFor the socioeconomic feature set, we selected variables based on prior work, focusing on categories such as age, sex, education, early life, and lifestyle. We excluded variables that directly reflected key diagnostic symptoms of ICD-10 labels to avoid circular reasoning in our classification analyses.\n\nThe classification pipeline included scaling, feature space dimension reduction using principal component analysis (PCA), and classification. We employed nested 5-fold cross-validation to tune the PCA dimensionality and classifier-specific hyperparameters. A shuffle-split resampling scheme was used to subdivide the data into 100 stratified training (80%) and validation (20%) splits. This approach ensured robust and generalizable performance of our machine-learning models.",
  "optimization/parameters": "In our study, we employed several classifiers, each with its own set of parameters that were tuned to optimize performance. For the support vector classifier, the regularization parameter C and the kernel type were adjusted. The k-nearest neighbors classifier had the number of neighbors, the weight function, and the distance metric tuned. The specific values included in the tuning process can be found in the supplementary materials.\n\nFor the random forest classifier, which was used extensively in our analyses, the number of trees was set to 250, the criterion for splitting was set to \"gini,\" and the random state was set to 42 to ensure reproducibility. These parameters were chosen based on prior literature and preliminary testing to balance model complexity and performance.\n\nThe selection of these parameters was guided by a combination of domain knowledge and empirical testing. We used nested folds for hyperparameter estimation, which helps in preventing overfitting and ensures that the model generalizes well to unseen data. This approach allowed us to systematically explore the parameter space and identify the optimal settings for each classifier.\n\nIn addition to the classifier-specific parameters, we also considered the dimensionality of the feature sets. For structural neuroimaging data, we used surface and volume features. For functional neuroimaging data, we explored features derived from independent component analysis, probabilistic functional modes, and atlas-based features. The socioeconomic feature set consisted of 36 variables, which were one-hot encoded to create a 36-dimensional predictive feature set.\n\nOverall, the number of parameters used in the model varied depending on the classifier and the feature set. However, the tuning process was rigorous and aimed at selecting the most effective parameters for accurate classification.",
  "optimization/features": "In our study, we utilized multiple feature sets derived from various data sources. The primary structural neuroimaging feature sets included two types: a surface-based set with 285 features and a volumetric set with 153 features. These features were extracted from T1-weighted images using established pipelines.\n\nAdditionally, we explored alternative feature sets to enhance classification accuracy. This included 17 different feature sets derived from resting-state functional MRI data, reflecting combinations of brain parcellations, feature types, and dimensionalities. Furthermore, we incorporated a sociodemographic feature set comprising 36 variables across categories such as age, sex, education, early life, and lifestyle.\n\nFeature selection was not explicitly performed as a separate step. Instead, we employed principal component analysis (PCA) for dimension reduction within our classification pipeline. This approach helped in handling the high-dimensional data and ensuring that the most relevant features were considered for classification. The PCA dimensionality was tuned using nested 5-fold cross-validation, ensuring that the feature space was optimized without overfitting.\n\nThe use of shuffle-split resampling and nested cross-validation ensured that the feature selection and model tuning were performed using the training set only, maintaining the integrity of the validation process. This methodology allowed us to robustly evaluate the performance of our classifiers across different feature sets and diagnostic groups.",
  "optimization/fitting": "In our study, we employed a robust framework to ensure that our models were neither overfitting nor underfitting the data. The number of parameters in our models, particularly those involving functional neuroimaging features, was indeed much larger than the number of training points. To mitigate the risk of overfitting, we implemented a rigorous shuffle-split validation framework. This involved training our models across 100 shuffle-split repeats, each with 80% of the data used for training and 20% for validation. This approach helped to ensure that our models generalized well to unseen data.\n\nAdditionally, we utilized nested folds for hyperparameter estimation, which further helped in preventing overfitting by providing an unbiased estimate of model performance. For the support vector classifier, we tuned the regularization parameter C and the kernel type, while for the k-nearest neighbors classifier, we tuned the number of neighbors, the weight function, and the distance metric. These hyperparameters were carefully selected to optimize model performance without overfitting to the training data.\n\nTo address the potential issue of underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We compared multiple classifiers, including random forest, support vector, and k-nearest neighbors, to find the best-performing model for our classification tasks. The random forest classifier, with 250 trees and the Gini criterion for splitting, was found to be effective in capturing the nuances in the data.\n\nFurthermore, we conducted a benchmarking analysis by classifying age groups, which showed high accuracy across all feature types and observed sample sizes. This comparison helped to validate our classification pipeline and ensured that our models were not underfitting the data. Overall, our approach balanced model complexity and generalization, leading to reliable and robust classification results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our classification models. One key method involved the use of nested cross-validation, which helps in tuning hyperparameters while maintaining an unbiased estimate of model performance. This approach involves an inner loop for hyperparameter tuning and an outer loop for performance evaluation, ensuring that the model generalizes well to unseen data.\n\nAdditionally, we utilized principal component analysis (PCA) for dimensionality reduction. By reducing the feature space, PCA helps in mitigating the risk of overfitting, especially when dealing with high-dimensional data. This step was integrated into our classification pipeline, ensuring that only the most informative components were used for training the models.\n\nFurthermore, we implemented a shuffle-split resampling scheme, which subdivides the data into multiple training and validation splits. This technique enhances the stability and reliability of our models by providing a more comprehensive evaluation across different subsets of the data.\n\nFor the random forest classifier, we fixed the number of trees at 250, following established practices. This decision was based on prior work that demonstrated the effectiveness of this parameter in balancing model complexity and performance. Additionally, we tuned other hyperparameters such as the depth of the trees and the number of variables considered for splitting, further optimizing the model to prevent overfitting.\n\nIn summary, our approach included nested cross-validation, PCA for dimensionality reduction, shuffle-split resampling, and careful tuning of hyperparameters to ensure that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available in the supplementary materials. Specifically, the hyper-parameter search grids for all scikit-learn classification models, including the random forest classifier, support vector classifier, and k-nearest neighbors classifier, are detailed in Supplementary Table S3. This table includes the values considered for tuning parameters such as the regularization parameter C and kernel type for the support vector classifier, and the number of neighbors, weight function, and distance metric for the k-nearest neighbors classifier.\n\nThe code used for the classification pipeline, including the implementation of principal component analysis, nested folds for hyper-parameter estimation, and shuffle-splits, is available on GitHub under the project name W APIA W3. The project is platform-independent and written in Python and shell scripting. The code is licensed under the MIT license, which allows for free use, modification, and distribution. Additionally, the code has been archived in Software Heritage for long-term preservation and accessibility.\n\nThe supplementary materials also include detailed information on the feature sets used, such as structural neuroimaging features, functional neuroimaging features, and socioeconomic features. Supplementary Table S4 provides an overview of the structural MRI feature sets, resting state functional MRI feature sets, and the sociodemographic feature set. Supplementary Table S5 lists the sociodemographic variables used in the study.\n\nFor those interested in replicating or building upon our work, all necessary information and code are publicly available, ensuring transparency and reproducibility.",
  "model/interpretability": "The model employed in this study is primarily a random forest classifier, which is inherently more interpretable than many other machine learning models, such as deep neural networks. Random forests are considered transparent to a certain extent because they consist of multiple decision trees, each of which can be visualized and understood individually. This transparency allows for the examination of feature importance, which indicates the significance of each feature in making predictions.\n\nIn our analysis, we utilized the random forest classifier to handle data of varied units and to perform nonlinear classification tasks effectively. The model's flexibility and scalability make it suitable for complex datasets like those derived from neuroimaging features. By tuning hyperparameters such as the depth of the trees and the number of variables considered for splitting, we ensured that the model was optimized for our specific dataset.\n\nAdditionally, we employed principal component analysis (PCA) for dimension reduction, which helps in visualizing the data in a lower-dimensional space. This step further aids in interpretability by allowing us to understand the underlying structure of the data.\n\nWhile random forests provide a level of interpretability through feature importance and decision tree visualization, it is important to note that the model is not entirely transparent. The ensemble nature of random forests means that the final prediction is an aggregation of many trees, making it challenging to trace back the exact decision path for a given prediction. However, the ability to assess feature importance and visualize individual trees offers valuable insights into the model's behavior.\n\nIn summary, the random forest classifier used in this study strikes a balance between complexity and interpretability. It allows for a deeper understanding of the data and the model's decision-making process, making it a suitable choice for neuroimaging classification tasks.",
  "model/output": "The model employed in this study is primarily a classification model. Specifically, random forest classifiers were used for diagnostic classifications across various ICD-10 groups. Additionally, support vector classifiers and k-nearest neighbors classifiers were tested and compared against the random forest classifier. The classification tasks included both binary case-control classifications and multiclass classifications, aiming to categorize samples into 17 ICD-10 diagnostic groups and a control group. The output from these classifiers was used to assess the accuracy of diagnosing different conditions based on various feature sets, including structural neuroimaging features, functional neuroimaging features, and socioeconomic features. The results indicated that while some diagnostic groups could be classified significantly above chance, the overall performance varied across different feature sets and models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the project has been made publicly available. The project, named W APIA W3, can be accessed via its GitHub repository at https://github.com/tyo8/WAPIAW3. The code is platform-independent and is written in Python and shell scripting. There are no additional requirements beyond the standard libraries for these languages. The project is licensed under the MIT license, which allows for free use, modification, and distribution of the software, even for commercial purposes, as long as the original copyright and license notice are included in all copies or substantial portions of the software.\n\nThe code has also been archived in Software Heritage, ensuring its long-term preservation and accessibility. This archiving process helps to maintain a historical record of the software, which can be useful for future reference and reproducibility of the research. The RRID (Research Resource Identifier) for the software is SCR_026093, which provides a unique and persistent identifier for the resource, making it easier to cite and reference in scientific publications.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and generalizability of the classification models. Random forest classification models were trained separately for each diagnostic category across 100 shuffle-split repeats, with 80% of the data used for training and 20% for validation. This shuffle-split method helps in assessing the model's performance by providing multiple train-test splits, thereby reducing the risk of overfitting.\n\nThe primary neuroimaging features used to drive the classification algorithms included two sets of structural measures: 285 surface-based measures and 153 volumetric measures. Significance testing was performed by comparing the resulting 100 classification accuracies for each shuffle-split against the chance level of 0.5. This rigorous testing ensures that the classification results are statistically significant and not due to random chance.\n\nFollow-up analyses were conducted to further validate and improve the primary classification results. These analyses included classifications using matched sample sizes, classifications using a multiclass algorithm, alternative classification models such as support vector and k-nearest neighbors classifiers, and alternative feature sets derived from functional neuroimaging or sociodemographic information. These additional analyses provide a thorough evaluation of the model's performance under different conditions and feature sets.\n\nMoreover, diagnostic classification accuracies were benchmarked based on age classification by differentiating between the extreme ends of the age distribution (youngest vs. oldest). Two age classification groups were generated to match the sample sizes of the largest and smallest ICD-10 diagnostic groups. This benchmarking helps in understanding the classification effect size across all ICD-10 diagnostic groups against the classification effect size of age.\n\nThe evaluation method also included correction for multiple comparisons across feature sets using the false discovery rate. This statistical correction helps in controlling the rate of Type I errors, ensuring that the significant results are reliable and not due to multiple testing.\n\nIn summary, the evaluation method involved a combination of shuffle-split cross-validation, significance testing, follow-up analyses with alternative models and feature sets, and benchmarking against age classification. These steps collectively ensure a robust and comprehensive evaluation of the classification models.",
  "evaluation/measure": "In our evaluation, we primarily focused on classification accuracy as our key performance metric. This metric was chosen because it directly measures the proportion of correctly classified instances, providing a clear indication of the model's effectiveness in distinguishing between different diagnostic groups.\n\nTo ensure the robustness of our results, we employed a statistical significance measure derived from the distribution of split-wise accuracy scores. This approach allowed us to assess whether the classification performance was significantly above chance. Specifically, we used an empirical probability of classifying above chance, computed from the fitted Student’s t-distribution. This method is more stringent than a standard 1-sample t-test against chance, as it uses the sample standard deviation rather than the sample standard error.\n\nIn addition to accuracy, we also considered the effect size of classification across different diagnostic groups. This was particularly important for benchmarking our models against age classification, where we compared the classification effect size of various ICD-10 diagnostic groups against the classification effect size of age.\n\nWe also reported mean classification accuracy across different models and feature sets, providing a comprehensive view of model performance. This included comparisons between different classifiers such as random forest, support vector classifier, and k-nearest neighbors, as well as various feature sets derived from structural and functional neuroimaging data, and sociodemographic information.\n\nThe use of these metrics is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By focusing on accuracy, statistical significance, and effect size, we aimed to provide a thorough assessment of our classification models' performance.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison against alternative classification models and feature sets to benchmark our primary methods. We tested three different classifiers: the random forest classifier, the support vector classifier, and the k-nearest neighbors classifier. For the support vector classifier, we tuned the regularization parameter and the kernel type. For the k-nearest neighbors classifier, we adjusted the number of neighbors, the weight function, and the distance metric. This allowed us to compare the performance of our primary random forest classifier against these alternative models.\n\nIn addition to comparing different classification models, we also evaluated the performance of alternative feature sets. We repeated our classification analyses using functional neuroimaging features and socioeconomic features, in addition to the primary structural neuroimaging features. For functional neuroimaging, we compared feature sets obtained from data-driven approaches such as independent component analysis, probabilistic functional modes, and atlas-based features. We also included a feature set comprising demographic information, which has been shown to outperform neuroimaging-derived features in phenotype prediction.\n\nTo further benchmark our classification analyses, we repeated the same random forest regression model to classify older versus younger groups based on the same feature sets. This allowed us to compare the classification effect size across all ICD-10 diagnostic groups against the classification effect size of age. We compiled a balanced group of subjects, ensuring that older and younger groups were matched for sex and head motion.\n\nStatistical significance was used as our measure of successful classification. We computed statistical significance from the distribution of split-wise accuracy scores as the empirical probability of classifying above chance. Correction for multiple comparisons was performed across feature sets using the false discovery rate. This rigorous evaluation process ensured that our methods were thoroughly benchmarked against alternative approaches.",
  "evaluation/confidence": "In our study, we employed statistical significance as the primary measure of successful classification. We computed this significance from the distribution of split-wise accuracy scores, treating the classification accuracy score of a given shuffle-split as a mean of independent and identically distributed Bernoulli variables. This approach assumes that the accuracy scores are asymptotically normally distributed.\n\nWe used a significance threshold of α = 0.05, which means that a feature set classified significantly above chance if its fitted Student’s t-distribution lies above the guess line within this threshold. This computation is more stringent than a 1-sample t-test against 0.5 (chance) because we used the sample standard deviation instead of the sample standard error. This makes our significance criterion more rigorous, ensuring that our results are robust.\n\nCorrection for multiple comparisons was performed across feature sets using the false discovery rate. This correction is crucial in preventing Type I errors, especially when testing multiple hypotheses. In our results, we highlighted cells in green to indicate significant results (pc < 0.05) and in yellow to indicate trend-level results (0.05 < pc < 0.10). This visual distinction helps in quickly identifying which diagnostic groups and feature sets yielded statistically significant classification accuracy.\n\nFor instance, the demyelinating diseases diagnostic group showed outsized sensitivity, suggesting a relatively larger magnitude of effect in this population. However, this result did not remain significant after correction across the expanded number of feature sets, underscoring the importance of controlling for multiple comparisons.\n\nIn summary, our evaluation confidence is high due to the rigorous statistical methods employed, including stringent significance criteria and corrections for multiple comparisons. This ensures that our findings are reliable and that the classification accuracy is genuinely above chance for the diagnostic groups identified.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the source code for the project, named W APIA W3, is publicly available on GitHub under the MIT license. This code can be accessed at https://github.com/tyo8/WAPIAW3. The code has also been archived in Software Heritage. The availability of the source code allows for reproducibility and further evaluation by other researchers."
}