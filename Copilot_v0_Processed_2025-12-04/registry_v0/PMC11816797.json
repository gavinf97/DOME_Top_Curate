{
  "publication/title": "Unlocking the power of AI for phenotyping fruit morphology in Arabidopsis",
  "publication/authors": "The authors contributing to this article are Kieran Atkins, G. G. M., A. L., J. H. D., and C. L.\n\nKieran Atkins annotated the dataset, trained the model, created the phenotyping pipeline, and performed the analyses. They also wrote the manuscript. G. G. M. designed and performed the original experiment and collected the image dataset. A. L. oversaw the QTL and gene ontology analyses. J. H. D. and C. L. obtained the funding and supervised the project. All authors proofread and agreed on the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "10.1007/s00122-021-03820-3",
  "publication/tags": "- Deep Learning\n- Plant Phenotyping\n- Arabidopsis thaliana\n- Quantitative Trait Locus (QTL) Analysis\n- Image Segmentation\n- Genomics\n- Machine Learning\n- Phenomics\n- Silique Morphology\n- High-Throughput Phenotyping",
  "dataset/provenance": "The dataset used in this study was developed using a custom plug-in called the GIMP Image Annotator (GIÀ), which allows users to save selected regions as mask images along with metadata. This tool was employed to annotate siliques in images, producing a dataset of 55 images with 2,940 annotated siliques. The dataset was then split into an 80% training set and a 20% testing set, resulting in 44 training images with 2,282 individual silique annotations and 11 testing images with 652 silique annotations.\n\nThe dataset for model development and validation with QTL analysis is available in Zenodo under a CC0 license. Additionally, DOME-ML annotations can be found in the DOME Registry. This availability ensures that the dataset is accessible for further research and replication by the scientific community.\n\nThe dataset was specifically curated to include a high number of single-class objects per image, which helped in effective fine-tuning of a COCO pre-trained model. The uniform background and the abundance of objects per image made the loss value at each training step more informative, crucial given the variability in siliques' shape, size, openness, and fertility within the same plant.\n\nThe dataset's design and the use of the GIÀ tool significantly reduced the effort required for precise annotation of each object within an image. This approach facilitated the creation of a comprehensive and high-quality dataset, which is essential for the accurate phenotyping of silique morphology in Arabidopsis thaliana. The dataset's availability and the associated genetic tools are intended to motivate others to reuse these data, fostering further advancements in plant phenomics studies.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a testing set. This split was done randomly.\n\nThe training set consists of 44 images, which contain a total of 2,282 individual silique annotations. The testing set, on the other hand, comprises 11 images with 652 silique annotations. This distribution ensures that the model's performance can be evaluated on unseen samples, providing a robust assessment of its generalization capabilities.",
  "dataset/redundancy": "The dataset used for model development and validation was split into training and testing sets to evaluate the model's performance on unseen samples. The split was done randomly, with 80% of the annotated dataset allocated to the training set and the remaining 20% to the testing set. This resulted in 44 training images containing 2,282 individual silique annotations and 11 testing images with 652 silique annotations.\n\nThe training and test sets are independent, ensuring that the model's performance can be objectively assessed. This independence was enforced through the random split, which helps to prevent data leakage and ensures that the model generalizes well to new, unseen data.\n\nThe distribution of our dataset is somewhat unique compared to previously published machine learning datasets. Our dataset consists of a relatively small number of images (55 in total), but each image contains a high number of annotated objects (siliques). This abundance of objects per image makes the loss value at each training step more informative, which is crucial given the variability in siliques' shape, size, openness, and fertility within the same plant. The uniform background and the high number of single-class objects per image also contribute to the model's ability to generalize well, despite the small training dataset. This approach differs from conventional wisdom, which suggests that small training datasets typically lead to overfitting in deep learning models. However, our strategy of using a smaller backbone model and leveraging the high number of objects per image has proven effective in achieving good detection and segmentation results.",
  "dataset/availability": "The dataset used for model development and validation, including quantitative trait locus (QTL) analysis, is publicly available. It can be accessed in Zenodo under a CC0 license, which allows for unrestricted use, distribution, and reproduction. This ensures that the data is freely available to the scientific community for further research and validation.\n\nAdditionally, the DOME-ML annotations are available in the DOME Registry. These annotations are crucial for understanding the specific details of the dataset and how the annotations were generated.\n\nTo ensure reproducibility and adherence to open science principles, the dataset, along with the code and models developed in this study, is fully available at an open data repository. This includes step-by-step documentation, allowing for ease of replication by other researchers. The availability of the dataset and associated tools in public forums promotes transparency and facilitates further research in the field of plant phenotyping.",
  "optimization/algorithm": "The machine-learning algorithm class used is instance segmentation, specifically the Cascade Mask R-CNN model. This model is not new; it is an improvement over the Mask R-CNN by utilizing a cascading architecture. This architecture introduces a series of cascaded region of interest (RoI) heads, each trained on progressively harder examples using larger Intersection over Union (IoU) thresholds for bounding box detection. This approach leads to improved performance, especially for difficult objects.\n\nThe choice of Cascade Mask R-CNN was driven by its capability to both detect each object (in this case, siliques) and output a mask that denotes the pixels belonging to that specific object instance. These outputted masks form the basis for the phenotype data.\n\nThe model was initialized with weights pre-trained on the COCO (Common Objects in Context) dataset, which is a commonly used, high-quality dataset. Models trained on COCO have already learned strong lower-level object detection features, providing a strong model initialization point. The weights were initialized without any parameter freezing to allow the model to adapt to the largely different domain of the dataset used in this study.\n\nThe training process involved stochastic gradient descent with a learning rate of 0.01, an initial linear learning rate warm-up for the first 100 iterations, and a learning rate decay by a factor of 0.1 at epochs 24 and 33. This approach allowed for fine-tuning later in the training process. The model was trained for 36 epochs with a batch size of 1, resulting in 792 training iterations. The training was conducted using 2x NVIDIA A100 48 GB GPUs, taking approximately 1 hour and 21 minutes.\n\nThe decision to use Cascade Mask R-CNN was influenced by its effectiveness in capturing morphometric data and its ability to handle the specific challenges of the dataset, such as the variability in siliques' shape, size, openness, and fertility within the same plant. The model's performance was further enhanced by the use of a smaller backbone, RegNetX 1.6GF, which has approximately half the parameter count of ResNet-50. This choice was made to reduce redundancy in parameters and to prevent overfitting on the constrained dataset.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The images used in our study were initially too large to fit on the GPU at their native resolution. To address this, we scaled the images to 4,590 × 3,240 pixels, which is 90% of the original size. This reduction in size helped to manage memory usage while preserving the necessary details of the siliques. Given that the siliques within these images are already very small, downscaling further was not feasible as it would have compromised the visibility and accuracy of the annotations.\n\nFor the annotation process, we developed a custom plug-in called the GIMP Image Annotator (GIÀ). This tool integrates seamlessly with GIMP and features an intuitive graphical user interface. GIÀ leverages existing computer vision algorithms that rely on color similarity to segment specific structures, such as siliques, within images. This approach significantly reduces the effort required for precise annotation of each object within an image. Using GIÀ, we produced a dataset consisting of 55 images with 2,940 annotated siliques.\n\nWe employed an amodal instance segmentation approach, which involves segmenting both visible and occluded pixels of an object. By saving masks separately for each instance, this approach allows overlapping regions to be attributed to multiple instances. This method is particularly useful in scenarios where objects overlap and partially occlude each other, providing a more comprehensive representation of the scene.\n\nThe annotated dataset was then split randomly into an 80% training set and a 20% testing set. This resulted in 44 training images with 2,282 individual silique annotations and 11 testing images with 652 silique annotations. This split ensured that the model's performance could be evaluated on unseen samples, providing a robust assessment of its generalization capabilities.",
  "optimization/parameters": "In our study, we chose to use a smaller backbone for the model, ultimately deciding to use RegNetX 1.6GF. This model has approximately half the parameter count of ResNet-50. The RegNet family of convolutional neural networks is designed to have more efficient-sized depth and width to reduce redundancy in parameters. We found that using a smaller backbone resulted in equal or improved detection and segmentation results compared to its larger parameter-count siblings. This implies that larger models overfit easily on this constrained dataset, and the regularization effect of reducing the model size benefits this task. Reducing model size also allowed more memory to fit the large images during training.",
  "optimization/features": "The input features for our model are derived from images of siliques. Specifically, the model processes images that have been scaled to 4,590 × 3,240 pixels, which is 90% of the original size. This scaling was done to reduce memory usage while preserving the necessary detail for accurate segmentation.\n\nFeature selection in the traditional sense was not performed, as the features are directly derived from the pixel data of the images. However, the process of annotating the images using the GIMP Image Annotator (GIÀ) tool ensured that only relevant regions, specifically the siliques, were highlighted and saved as mask images. This step effectively acts as a form of feature selection by focusing the model's attention on the relevant parts of the images.\n\nThe dataset was split into an 80% training set and a 20% testing set. The annotations and feature extraction were performed independently for each set, ensuring that the training process did not inadvertently use information from the testing set. This split was done randomly to ensure that the model's performance could be evaluated on unseen samples.\n\nThe model used for this task is Cascade Mask R-CNN, which is an instance segmentation model. It was initialized with weights pre-trained on the COCO dataset, which provides a strong starting point for object detection features. The model was then fine-tuned on our specific dataset of silique images. The use of a pre-trained model and the subsequent fine-tuning process allowed the model to adapt to the unique characteristics of our dataset without overfitting.",
  "optimization/fitting": "The model was trained on a relatively small dataset consisting of only 44 images and around 2,000 annotated siliques. Despite the small training dataset, the model generalised well to the remaining dataset. This was likely due to the uniform background and the high number of single-class objects per image, which made the loss value at each training step more informative.\n\nTo address the potential issue of overfitting, several strategies were employed. First, the model was trained using stochastic gradient descent with a learning rate that decayed by a factor of 0.1 at epochs 24 and 33. This allowed for fine-tuning later in the training process. Additionally, an initial linear learning rate warm-up was included for the first 100 iterations, starting at 0.00001 and ending at the learning rate of 0.01. This warm-up phase helped to prevent sharp initial updates from the initial COCO weights, which could lead to instability.\n\nThe use of a smaller backbone, specifically RegNetX 1.6GF, also helped to mitigate overfitting. This model has approximately half the parameter count of ResNet-50, reducing redundancy in parameters and allowing for more efficient training. The smaller model size resulted in equal or improved detection and segmentation results compared to larger models, indicating that the larger models were more prone to overfitting on the constrained dataset.\n\nFurthermore, the images were scaled to 4,590 × 3,240, which is 90% of the original size, to reduce memory usage while preserving silique detail. This scaling helped to balance image quality with the capabilities of the available GPU resources, ensuring that the model could effectively learn from the data without overfitting.\n\nTo rule out underfitting, the model was trained for 36 epochs, which was found to be sufficient for convergence on the dataset. The use of stochastic gradient descent with a momentum value of 0.9 and a weight decay value of 0.0001 also helped to ensure that the model could effectively learn from the data without underfitting. The model's performance was evaluated on a test split of the annotated dataset, and the results showed high precision and recall metrics for both silique detection and segmentation, indicating that the model was able to generalise well to unseen data.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting, given the relatively small size of our training dataset. One of the key strategies was the use of a smaller model backbone. We opted for RegNetX 1.6GF, which has approximately half the parameter count of ResNet-50. This choice was driven by the observation that larger models tend to overfit more easily on constrained datasets. By reducing the model size, we introduced a form of regularization that helped the model generalize better to unseen data.\n\nAdditionally, we utilized stochastic gradient descent (SGD) with a momentum value of 0.9 and a weight decay value of 0.0001. Weight decay acts as a form of L2 regularization, which helps to penalize large weights and encourages the model to find a simpler solution that generalizes better.\n\nWe also implemented a learning rate warm-up strategy for the initial 100 iterations, starting at a very low learning rate of 0.00001 and gradually increasing it to 0.01. This approach helped to stabilize the training process and prevent sharp initial updates that could lead to instability. Furthermore, we decayed the learning rate by a factor of 0.1 at epochs 24 and 33, allowing for fine-tuning later in the training process.\n\nAnother important aspect was the use of a high-resolution image scaling strategy. We scaled images to 4,590 × 3,240, which is 90% of the original size. This balance between preserving silique detail and reducing memory usage was crucial for effective training on our available GPU resources. Downscaling the images further resulted in poor segmentation results, highlighting the importance of maintaining a certain level of detail.\n\nOverall, these regularization techniques, combined with careful tuning of hyperparameters and image preprocessing, helped us to achieve a well-generalized model despite the small size of our training dataset.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are indeed available. The source code, including the models and the optimization parameters, has been made publicly accessible to ensure reproducibility and ease of replication by the scientific community. This adherence to FAIR principles allows researchers to replicate the study and build upon the work.\n\nThe code and models developed in this study are available at an open data repository. The pipeline is designed to be platform-independent and is programmed in Python and R. Additional requirements for running the code are specified in a public environment file released under the GNU GPL v3 license. This open-source approach not only provides immediate insights into silique morphology but also serves as a foundational tool for future open-source development. Researchers working in different phenomics environments can modify the pipeline, allowing for extended applications beyond Arabidopsis thaliana.\n\nThe availability of the source code and models ensures that the optimization parameters and configurations used in this study are transparent and accessible. This transparency is crucial for verifying the results and for other researchers to adapt the pipeline to their specific needs. The code has also been archived in Software Heritage, further ensuring its long-term accessibility and reproducibility.",
  "model/interpretability": "The model employed in our study is primarily a deep learning-based instance segmentation model, specifically Cascade Mask R-CNN, which is generally considered a black-box model. This means that the internal workings of the model, particularly the decision-making processes within its neural networks, are not easily interpretable by humans. The model takes input images and outputs masks and bounding boxes for detected objects (siliques) without providing explicit reasoning for its predictions.\n\nHowever, there are aspects of the model's operation that can be interpreted and understood. For instance, the model's architecture includes a series of cascaded region of interest (RoI) heads, each trained on progressively harder examples. This cascading approach helps in improving the model's performance, especially for difficult objects. The use of a feature pyramid network and a pre-trained backbone (RegNetX 1.6GF) on the COCO dataset provides a strong initialization point, allowing the model to adapt to the specific domain of silique detection and segmentation.\n\nThe model's outputs, such as the masks and bounding boxes, can be visually inspected to understand its performance. For example, the model's ability to handle occlusion and segment overlapping siliques can be observed in the qualitative results. The model's performance metrics, such as precision and recall, provide quantitative measures of its accuracy and reliability.\n\nAdditionally, the phenotypic data derived from the model's segmentations, such as the length, diameter, and volume of siliques, can be interpreted in the context of genetic analysis. The model's ability to detect treatment effects on silique morphology, as shown in the distribution of phenotypic traits, demonstrates its potential for high-throughput phenotyping and genetic studies.\n\nIn summary, while the deep learning model itself is a black-box, its outputs and performance can be interpreted and understood in the context of the specific application and domain. The model's architecture and training process provide some insights into its operation, and the phenotypic data derived from its segmentations offer valuable information for genetic analysis.",
  "model/output": "The model developed is primarily focused on instance segmentation, which is a form of classification task. It identifies and segments individual siliques within images, providing both bounding boxes and masks for each detected object. The model's output includes derived traits such as the length and maximum diameter of the siliques, which are overlaid on the generated masks. Additionally, a 3D visualization of the silique based on diameter values along its length is produced. The model was evaluated using precision and recall metrics for both detection and segmentation tasks. For detection, it achieved an average precision of over 95% for siliques when an Intersection over Union (IoU) threshold of 0.75 was used. For segmentation, the model achieved an average precision of 66% with the same IoU threshold. The model successfully detects and creates quality segmentations of most siliques within the unseen test set, even in situations of partial occlusion. The outputs were further used to evaluate plant-level phenotype data, assessing their ability to detect treatment effects on silique morphology. The model's performance was strong, allowing for the extraction of meaningful phenotypic data for further analysis.",
  "model/duration": "The model was trained for 36 epochs using 2x NVIDIA A100 48 GB GPUs and a batch size of 1, resulting in 792 training iterations. The total training time was approximately 1 hour and 21 minutes. This efficient execution time was achieved despite the large size of the images, which were scaled to 4,590 × 3,240 pixels to balance memory usage and preserve necessary details for accurate segmentation. The choice of a smaller backbone model, RegNetX 1.6GF, also contributed to the efficient use of computational resources, allowing for more memory to fit the large images during training.",
  "model/availability": "The source code for our deep learning pipeline, named MorphPod, is publicly available. It is designed for phenotyping the morphology of Arabidopsis fruit. The code can be accessed via a GitHub repository. The pipeline is platform-independent and is programmed in Python and R. Additional requirements for running the code are specified in a public environment file. The software is released under the GNU General Public License version 3.\n\nFor those interested in using our image annotation tool, we have developed a lightweight GIMP plug-in called GIMP Image Annotator (GIÀ). This tool is also available on GitHub and is designed to assist with computer vision-assisted image annotation. It is platform-independent and requires GIMP to operate. Like MorphPod, it is released under the GNU GPL v3.\n\nBoth tools have been archived in Software Heritage, ensuring their long-term preservation and accessibility. Additionally, they are registered with bio.tools, a registry for bioinformatics tools, under the identifiers biotools:morphpod and biotools:gimp_image_annotator, respectively. Research Resource Identifiers (RRIDs) are also provided for easy citation and tracking: MorphPod (RRID:SCR_026174) and GIMP Image Annotator (RRID:SCR_026175). The workflow for GIMP Image Annotator is available on Workflow Hub.\n\nWe encourage the scientific community to utilize these tools, modify them as needed, and contribute to their further development. By adhering to the principles of open science and the FAIR (Findable, Accessible, Interoperable, Reusable) principles, we aim to facilitate reproducibility and collaboration in plant phenomics research.",
  "evaluation/method": "The evaluation of the method involved two primary approaches to ensure its robustness and accuracy. Firstly, the model's performance was assessed on previously unseen samples by comparing its outputs against manually annotated ground truth data. This direct comparison involved evaluating the intersection over union (IoU) between the predicted objects and the ground truth annotations. Predicted objects were considered correctly identified if the IoU exceeded a specified threshold, categorizing the model's detection as either true positives or false negatives.\n\nSecondly, the model's phenotypic data was evaluated using quantitative trait locus (QTL) analysis to identify phenotype-genotype correlations. This step was crucial for verifying that the phenotypic data derived from the model could be reliably used for genetic analysis. The evaluation on unseen test data required a meticulous comparison of the model's predicted outputs with hand-collected annotation data, ensuring that the model's performance was consistent and accurate across different samples.\n\nThe model was trained using a dataset split into 80% for training and 20% for testing, resulting in 44 training images with 2,282 individual silique annotations and 11 testing images with 652 silique annotations. This split allowed for a comprehensive evaluation of the model's ability to generalize to new, unseen data. The training process involved using stochastic gradient descent with a learning rate of 0.01 over 36 epochs, with an initial linear learning rate warm-up to stabilize the training process. This approach ensured that the model converged well without overfitting, providing reliable and accurate phenotypic data for further genetic analysis.",
  "evaluation/measure": "In our evaluation, we focused on precision and recall metrics for both silique detection and segmentation. These metrics are standard in evaluating object detection models and allow us to check if the model detected all objects within the image without littering the image with spurious detections.\n\nFor detection, we achieved an average precision of over 95% for siliques when using an Intersection over Union (IoU) threshold of 0.75. This high precision indicates that the model is highly accurate in identifying siliques. We also observed similar performance across a range of IoU thresholds from 0.50 to 0.95, with an average precision of 88% and recall of 91%. These results demonstrate the model's robustness in detecting siliques under various conditions.\n\nFor segmentation, the model achieved an average precision of 66% for siliques when using an IoU threshold of 0.75. Additionally, the model achieved a segmentation IoU score of over 0.50 in 94% of detected siliques. This indicates that the model successfully creates quality segmentations of most siliques within the unseen test set.\n\nWe also reported average precision (AP) and average recall (AR) across IoU thresholds of 0.5, 0.75, and the range from 0.5 to 0.95 at 0.05 increments. These metrics provide a comprehensive evaluation of the model's performance across different levels of precision and recall.\n\nThe set of metrics we used is representative of the literature on object detection and segmentation. Precision and recall are widely used metrics in the field, and our use of IoU thresholds aligns with standard practices. This ensures that our evaluation is comparable to other studies in the literature.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of our model's performance on silique detection and segmentation was conducted using precision and recall metrics, with specific focus on average precision (AP) and average recall (AR) at various Intersection over Union (IoU) thresholds. These metrics provide a quantitative measure of the model's accuracy and reliability.\n\nConfidence intervals for these performance metrics were not explicitly stated, but the results were derived from a test split of the annotated dataset, ensuring that the evaluations were based on unseen data. This approach helps in assessing the model's generalizability and robustness.\n\nStatistical significance was addressed through permutation tests, where the number of permutations was set to 1,000 and a p-value threshold of less than 0.05 was used. This method helps in determining the confidence thresholds for the identified Quantitative Trait Loci (QTL), ensuring that the results are statistically significant.\n\nAdditionally, the model's performance was evaluated in two independent ways: first, by comparing its outputs on previously unseen samples with manually annotated ground truth data, and second, by evaluating the model's phenotypic data using QTL analysis to identify phenotype-genotype correlations. This dual evaluation approach strengthens the confidence in the model's performance and its ability to detect treatment effects on silique morphology.\n\nThe model achieved high precision and recall for silique detection, with an average precision of over 95% at an IoU threshold of 0.75. For segmentation, the model achieved an average precision of 66% at an IoU threshold of 0.75, indicating its capability to handle complex segmentation tasks. The model's success in detecting and segmenting siliques, even in situations of partial occlusion, further demonstrates its reliability and effectiveness.\n\nIn summary, while explicit confidence intervals for the performance metrics were not provided, the use of permutation tests and a dual evaluation approach ensures that the results are statistically significant and reliable. The model's high precision and recall, along with its ability to handle occlusion, provide strong evidence of its superiority over baselines and other methods.",
  "evaluation/availability": "The raw evaluation files are available for public access. The dataset used for model development and validation, including quantitative trait locus (QTL) analysis, can be found in Zenodo under a CC0 license. This license allows for unrestricted use, distribution, and reproduction of the data. Additionally, DOME-ML annotations are available in the DOME Registry. These resources ensure that the data is freely accessible and can be reused by the scientific community for further research and validation purposes."
}