{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- M.S.\n- W.A.M.B.\n- K.P.M.S.\n- M.V.\n- D.E.\n- H.L.\n- G.E.B.\n\nThe contributions of each author are as follows:\n\n- Conceptualization: M.S., W.A.M.B., K.P.M.S., M.V.\n- Methodology: M.S., W.A.M.B., M.V., H.L., G.E.B.\n- Software: M.S., D.E., H.L.\n- Investigation: M.S., H.L.\n- Resources: W.A.M.B., K.P.M.S., M.V.\n- Data curation: M.S., W.A.M.B., G.E.B.\n- Writing, original draft: M.S., H.L.\n- Writing, review & editing: W.A.M.B., K.P.M.S., M.V., D.E., G.E.B.\n- Supervision: W.A.M.B., K.P.M.S., M.V.\n- Project administration: W.A.M.B., K.P.M.S., M.V.\n- Funding acquisition: K.P.M.S., W.A.M.B., M.V.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Panoptic Segmentation\n- Nuclei Segmentation\n- Tissue Segmentation\n- Melanoma\n- Deep Learning\n- H&E Stained Slides\n- Biomarker Generation\n- Lymphocyte Segmentation\n- Histopathology\n- Machine Learning Models\n- Tumor Infiltrating Lymphocytes\n- Image Analysis\n- Medical Imaging\n- Pathology\n- Computational Biology",
  "dataset/provenance": "The dataset used in this study is focused on nuclei and tissue segmentation in melanoma. It consists of 310 regions of interest (ROIs), each accompanied by annotations of both tissue and nuclei. These annotations were created by a medical expert and verified by a dermatopathologist. The cases were digitized in a large melanoma referral center, with 76 cases being revisions or consultations from other treatment hospitals.\n\nThe dataset is divided into several subsets. A training set, which is publicly available, includes 103 primary and 103 metastatic ROIs. The remaining 104 ROIs are not publicly available as they are reserved for the PUMA challenge. Within this challenge, 10 ROIs serve as a preliminary test set for model functionality testing, while the remaining 94 samples form an independent test set for final metric calculations. The results presented in this study are specifically for this independent test set to ensure comparability with the PUMA challenge.\n\nThe public training set contains 97,429 nuclei distributed across 103 primary and 103 metastatic melanoma ROIs. The preliminary test set includes 5 primary and 5 metastatic melanoma ROIs, totaling 4,860 nuclei. The final test set, comprising 47 primary and 47 metastatic samples, contains 45,406 nuclei. The distribution of nuclei types and metastatic sample locations is visually represented in a figure.\n\nThe dataset is available in the .GeoJSON format, making it easily visualizable with the open-source pathology image viewer QuPath. The public training dataset can be accessed on Zenodo. The preliminary and final test sets will remain hidden until October 10, 2029, due to the ongoing PUMA challenge. However, earlier access can be requested for educational or collaborative purposes by contacting the corresponding author. Additionally, the DOME-ML annotation supporting this study is available through the DOME Registry.",
  "dataset/splits": "The dataset consists of three main splits: a training set, a preliminary test set, and a final test set. The training set is publicly available and includes 103 primary melanoma regions of interest (ROIs) and 103 metastatic melanoma ROIs. This set contains a total of 97,429 nuclei.\n\nThe preliminary test set is used for initial model testing and includes 5 primary and 5 metastatic melanoma ROIs, totaling 4,860 nuclei. This set is part of the PUMA challenge and is not publicly available until the challenge concludes.\n\nThe final test set, also part of the PUMA challenge, consists of 47 primary and 47 metastatic melanoma samples, containing 45,406 nuclei. This set is used for the final evaluation of models and remains hidden until the challenge ends.\n\nThe distribution of nuclei types and metastatic sample locations is visualized in a figure, showing that lymphocytes and tumor nuclei are the most common types. The tissue distribution across the training, preliminary, and final test sets is also illustrated, highlighting that primary samples have more tumor stroma compared to metastatic samples. The epidermis and necrotic areas are underrepresented in both datasets.",
  "dataset/redundancy": "The dataset was divided into three main parts: a training set, a preliminary test set, and a final test set. The training set is publicly available and consists of 103 primary and 103 metastatic melanoma regions of interest (ROIs), totaling 206 ROIs. This set includes 97,429 nuclei annotations. The preliminary test set, used for initial model testing, comprises 10 ROIs with 4,860 nuclei, split evenly between primary and metastatic samples. The final test set, which remains hidden until October 2029 due to the ongoing PUMA challenge, consists of 94 ROIs with 45,406 nuclei, again evenly divided between primary and metastatic samples.\n\nThe training and test sets are independent. This independence was enforced by ensuring that the ROIs in the test sets were not used during the training phase. The preliminary test set is used to validate the functionality of models, while the final test set is reserved for calculating final metrics, ensuring an unbiased evaluation.\n\nThe distribution of nuclei types and metastatic sample locations in the dataset is visualized in relevant figures. The dataset includes a variety of nuclei types, with lymphocytes and tumor nuclei forming the majority. The most common metastatic lesion sites are lymph nodes and skin metastases. The tissue distribution across the training, preliminary, and final test sets is also visualized, showing that primary samples contain more tumor stroma compared to metastatic samples. The epidermis and necrotic areas are underrepresented in both datasets.\n\nThe dataset's creation process involved manual selection of ROIs, focusing on immune cell subsets and less common nuclei and tissue classes. This approach aims to make the dataset and resulting models more applicable to whole-slide images, which often contain artifacts, unsharp regions, and pigmented areas that are difficult to segment, especially in melanoma cases. The dataset's annotations are in the .GeoJSON format, making them easily visualizable with the open-source pathology image viewer QuPath. This format ensures that the annotations are standardized and can be readily used by other researchers and models.",
  "dataset/availability": "The PUMA training dataset is publicly available on Zenodo. This dataset includes a training set consisting of 103 primary and 103 metastatic regions of interest (ROIs). The remaining 104 ROIs are not publicly available as they are used in the PUMA challenge. The challenge itself is accessible through the grand challenge platform. The preliminary and final test sets will remain hidden until October 10, 2029, due to the ongoing nature of the PUMA challenge. However, readers can request earlier access to the hidden dataset for educational or collaborative purposes by contacting the corresponding author via email.\n\nThe dataset is licensed under CC0 1.0 for the dataset and the MIT license for the codebase. Both licenses allow for non-commercial use. The Biobank Research Ethics Committee (TCBio) at UMC Utrecht has reviewed and approved the release of the dataset, ensuring compliance with all applicable regulations and laws. The reference number for this approval is TCBio 23-270/U-B.\n\nThe annotations in the dataset are in the .GeoJSON format, making them easily visualizable with the open-source pathology image viewer QuPath. This format ensures that the annotations can be used and verified by other researchers and developers. The dataset includes detailed annotations of both tissue and nuclei, created by a medical expert and checked by a dermatopathologist. This rigorous process ensures the accuracy and reliability of the dataset.\n\nThe PUMA challenge is designed to stimulate the development of deep learning models capable of segmenting nuclei and tissue in melanoma histopathology images. By making the training dataset publicly available and hosting a challenge, we aim to foster innovation and collaboration in the field of medical image analysis. The challenge provides a platform for researchers to test and improve their models, ultimately contributing to the development of prognostic biomarkers in melanoma treatment.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages deep learning techniques, specifically convolutional neural networks (CNNs), which are a class of machine-learning algorithms well-suited for image analysis tasks. These models are designed to automatically and adaptively learn spatial hierarchies of features from input images, making them highly effective for tasks such as nuclei and tissue segmentation in histopathology.\n\nThe algorithms used, including Hover-Net and Hover-NeXt, are not entirely new but have been adapted and optimized for our specific dataset and challenges. Hover-Net, for instance, is known for its simultaneous segmentation and classification of nuclei in multi-tissue histology images. Hover-NeXt builds upon this architecture, incorporating additional enhancements for faster and more accurate nuclei segmentation and classification.\n\nThe decision to use and further develop these algorithms in the context of our study, rather than publishing them in a machine-learning journal, is driven by the specific needs and goals of our research. Our primary focus is on advancing the field of histopathology and improving the accuracy and reliability of tumor-infiltrating lymphocyte (TIL) scoring in melanoma. By adapting and optimizing existing algorithms, we can directly address the unique challenges posed by melanoma histopathology, such as variations in staining and the presence of artifacts.\n\nMoreover, the algorithms have been tailored to work with our dataset, which includes a diverse range of samples and challenging conditions. This tailored approach ensures that the models are robust and generalizable, making them applicable to whole-slide images from different hospitals and scanner types. The adaptations and optimizations are thoroughly documented and made available to the research community, fostering collaboration and further advancements in the field.",
  "optimization/meta": "The meta-predictor model employed in this study leverages data from multiple machine-learning algorithms as input, making it a sophisticated ensemble approach. This model integrates outputs from various algorithms to enhance the accuracy and robustness of tumor-infiltrating lymphocyte (TIL) scoring in melanoma histopathology.\n\nThe constituent machine-learning methods include established algorithms such as NN192, Hover-Net, and Hover-NeXt. NN192 utilizes watershed segmentation followed by a fully connected neural network, although it has shown lower performance due to its older technique. Hover-Net and Hover-NeXt, on the other hand, are more advanced architectures that can compensate for variations in staining and scanners through extensive data augmentation, both in training and inference stages. These models have demonstrated better results, especially when trained on the PanNuke dataset, which includes skin samples.\n\nIt is crucial to ensure that the training data for these models is independent to avoid bias and overfitting. The dataset used in this study is meticulously curated, with annotations created by medical experts and checked by dermatopathologists. The dataset includes a training set of 103 primary and 103 metastatic regions of interest (ROIs), a preliminary test set of 10 ROIs, and a final test set of 94 ROIs. This separation ensures that the models are trained and tested on distinct datasets, maintaining the independence of the training data.\n\nThe use of multiple models and the careful curation of the dataset aim to provide a more precise, consistent, and fine-grained assessment of TILs, addressing the substantial inter-observer variability among pathologists. By integrating the strengths of different algorithms, the meta-predictor model offers a comprehensive solution for TIL scoring in melanoma histopathology.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure high-quality input for the models. The dataset consisted of regions of interest (ROIs) from histological images, which were annotated by medical experts. These annotations included both tissue and nuclei information, provided in the .GeoJSON format. This format allows for easy visualization using open-source pathology image viewers like QuPath.\n\nThe ROIs were selected from slides at a 40x magnification, with each ROI measuring 1,024 x 1,024 pixels. Additionally, a larger context ROI of 5,120 x 5,120 pixels was sampled to provide broader contextual information for the annotation process. This dual ROI approach helped in capturing both detailed and contextual data, which is crucial for accurate segmentation and classification.\n\nThe dataset included 310 cases, with 76 being consultation cases from referral hospitals or general practitioners. This variability in staining protocols and case origins added complexity to the data, requiring robust preprocessing techniques. The annotations were created by a medical expert and verified by a dermatopathologist, ensuring high accuracy and reliability.\n\nThe dataset was split into training, preliminary test, and final test sets. The training set consisted of 103 primary and 103 metastatic ROIs, made publicly available. The preliminary test set included 10 ROIs for initial model testing, while the final test set comprised 94 samples for final metric calculations. This division allowed for thorough evaluation and comparison of model performance.\n\nPreprocessing steps included digitizing the cases in a large melanoma referral center and ensuring that the annotations were in a format compatible with QuPath. The use of QuPath scripts facilitated the import and export of annotations as masks, streamlining the preprocessing workflow. No changes were made to the training code for the algorithms used, maintaining consistency in the preprocessing pipeline.\n\nThe dataset was licensed under CC0 1.0 for the data and MIT for the codebase, allowing for non-commercial use. This licensing ensured that the data and methods could be accessed and utilized by the research community while adhering to ethical and legal standards. The Biobank Research Ethics Committee approved the dataset, confirming compliance with all applicable regulations and laws.",
  "optimization/parameters": "In our study, we utilized several models for nuclei and tissue segmentation in melanoma, each with its own set of parameters. For nuclei segmentation, we compared models such as NN192, Hover-Net, and Hover-NeXt, trained on different datasets like PanNuke and PUMA. The specific number of parameters for each model varies. For instance, Hover-Net and Hover-NeXt are convolutional neural networks with a significant number of layers, leading to a high number of parameters. The exact count of parameters can differ based on the architecture and the specific training configurations used.\n\nThe selection of parameters was guided by established practices in the field of deep learning and medical image segmentation. We did not adjust training parameters or use data augmentation for the algorithms used in our experiments. Instead, we relied on 5-fold cross-validation on the public training dataset to ensure robust performance evaluation. This approach helped in validating the models without overfitting to the training data.\n\nFor tissue segmentation, we used nnU-Net and Mask2Former. nnU-Net is known for its automated configuration, which adapts the network architecture based on the dataset characteristics. Mask2Former, on the other hand, was pre-trained on the COCO instance segmentation task using a Swin Transformer backbone. The images were resized to 512 × 512 pixels before being loaded into the model, which is a standard preprocessing step to ensure consistency in input dimensions.\n\nIn summary, the number of parameters in our models is determined by the architecture of the neural networks used. The selection of these parameters was based on standard practices and the need for robust validation through cross-validation techniques.",
  "optimization/features": "In our study, the input features for the models were derived from the annotated regions of interest (ROIs) in the dataset. The annotations included both tissue and nuclei, with a focus on specific categories such as tumor, lymphocyte, and other nuclei types. The dataset consisted of 310 ROIs, with a public training set of 206 ROIs and a hidden test set of 104 ROIs used in the PUMA challenge.\n\nFeature selection was not explicitly performed in the traditional sense of reducing the number of features. Instead, the models were trained on the annotated features directly. The annotations were created by a medical expert and verified by a dermatopathologist, ensuring the quality and relevance of the input features. The models, such as Hover-Net and Hover-NeXt, were trained on these annotated features using 5-fold cross-validation to establish baseline benchmarks.\n\nThe input features for nuclei segmentation included categories like tumor, lymphocyte, and other, which were used to train and evaluate the models. For tissue segmentation, the input features included different tissue classes, and the models were evaluated using metrics like the Dice score. The models were trained on the public training set and evaluated on the hidden test set to ensure unbiased performance assessment.\n\nIn summary, the input features were derived from the annotated ROIs, and no traditional feature selection was performed. The models were trained on the entire set of annotated features, and cross-validation was used to ensure robust performance evaluation.",
  "optimization/fitting": "In our study, we employed a robust fitting method to ensure the reliability and generalizability of our models. The number of parameters in our models was indeed larger than the number of training points, which is a common scenario in deep learning applications. To address the potential issue of overfitting, we utilized several strategies.\n\nFirstly, we implemented 5-fold cross-validation during the training process. This technique helps in assessing the model's performance on different subsets of the data, thereby providing a more reliable estimate of its generalization capability. Additionally, we did not adjust training parameters or apply data augmentation, which further ensured that our models were not overly tailored to the training data.\n\nFor nuclei segmentation, we compared multiple models, including NN192, Hover-Net, and Hover-NeXt, trained on different datasets. This comparative analysis allowed us to evaluate the performance of various architectures and training datasets, reducing the risk of overfitting to a specific model or dataset.\n\nIn the case of tissue segmentation, we used nnU-Net and Mask2Former, both of which are state-of-the-art models known for their robustness. We also replaced the backbone of Mask2Former with the UNI pathology foundation model, which is better suited for feature extraction from H&E-stained histopathology images. This replacement and subsequent fine-tuning on the entire training dataset helped in improving the model's performance and generalization.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex to capture the underlying patterns in the data. The use of advanced architectures like Hover-Net, Hover-NeXt, nnU-Net, and Mask2Former, along with proper training and validation techniques, helped in achieving this. Furthermore, the high performance of our models on the final hidden test set of the PUMA challenge indicates that underfitting was not a significant issue.\n\nIn summary, our fitting method involved the use of cross-validation, comparative analysis of multiple models, and advanced architectures tailored for histopathology images. These strategies collectively helped in mitigating the risks of both overfitting and underfitting, ensuring the reliability and generalizability of our results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was 5-fold cross-validation. This technique involves dividing the dataset into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. By doing so, we ensured that the model's performance was evaluated on different portions of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, we did not adjust training parameters or apply data augmentation techniques during the model training process. This approach helped in maintaining the integrity of the dataset and prevented the models from becoming too specialized to the training data, thereby enhancing their generalization capabilities.\n\nFor the tissue segmentation tasks, we utilized ensemble modeling with nnU-Net. This method involves training multiple models and combining their predictions to improve overall performance and reduce overfitting. By creating an ensemble model, we leveraged the strengths of individual models and mitigated the risk of any single model overfitting to the training data.\n\nFurthermore, we evaluated the models using metrics such as the F1 score and Dice score, which provide a comprehensive assessment of model performance. The F1 score, calculated as the harmonic mean of precision and recall, offers a balanced measure of a model's accuracy. The Dice score, used for evaluating segmentation performance, ensures that the models accurately capture the spatial characteristics of the tissues and nuclei.\n\nIn summary, our regularization methods included 5-fold cross-validation, ensemble modeling, and the use of robust evaluation metrics. These techniques collectively helped in preventing overfitting and ensuring that our models generalized well to unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. These resources can be accessed through Zenodo and GitHub repositories, which have been archived in Software Heritage for long-term preservation. Specifically, the code and weights for the baseline solutions and metric calculations are available on Zenodo. The relevant repositories include the Evaluation Track 1, Evaluation Track 2, Baseline Track 1, and Baseline Track 2. These repositories contain the necessary scripts and models used for training and inference.\n\nThe codebase is licensed under the MIT license, which allows for both academic and non-commercial use. The dataset itself is licensed under the CC0 1.0 license, ensuring that it can be freely used and shared. Detailed information about the licenses and any restrictions can be reviewed in the respective repositories. Additionally, scripts for importing and exporting annotations as masks using QuPath are also available on Zenodo, providing further support for reproducibility and extension of our work.",
  "model/interpretability": "The models used in our study, particularly Hover-Net and Hover-NeXt, are designed with a focus on interpretability, making them more transparent than typical black-box models. These models provide simultaneous segmentation and classification of nuclei in histology images, which allows for a clear understanding of how they process and categorize different types of nuclei.\n\nHover-Net and Hover-NeXt utilize a combination of spatial and morphological features to differentiate between various nuclei types. For instance, Hover-Net employs a multi-branch architecture that includes a semantic branch for pixel-wise classification and an instance branch for object detection. This dual-branch approach helps in clearly distinguishing between different nuclei categories, such as tumor, lymphocyte, and other types, by leveraging both local and global context information.\n\nHover-NeXt builds upon Hover-Net by incorporating additional features and improvements, such as enhanced data augmentation techniques during training and inference stages. This ensures that the model can generalize better across different staining variations and scanner types, making its predictions more reliable and interpretable. The extensive data augmentation helps in accounting for variations in the input data, thereby providing more consistent and understandable results.\n\nMoreover, the models' ability to handle multi-tissue histology images and their capacity to segment and classify nuclei accurately contribute to their transparency. The use of clear, well-defined categories and the provision of detailed output, including the classification scores for each nucleus, further enhance the interpretability of these models. This makes it easier for researchers and clinicians to understand the decision-making process of the models and to trust their outputs.\n\nIn summary, Hover-Net and Hover-NeXt are not black-box models but rather transparent systems that provide clear insights into their segmentation and classification processes. Their architecture and training methods ensure that the models can handle various challenges in histology image analysis, making them reliable tools for research and clinical applications.",
  "model/output": "The models discussed in our study are primarily focused on segmentation tasks, which can be considered a form of classification at the pixel level. Specifically, we have employed models for nuclei and tissue segmentation in melanoma histopathology. For nuclei segmentation, we compared several models, including NN192, Hover-Net, and Hover-NeXt, which were trained to classify different types of nuclei such as tumor, lymphocyte, and other categories. The output of these models is a segmentation mask where each pixel is assigned to one of the predefined nucleus classes.\n\nIn the case of tissue segmentation, we used nnU-Net and Mask2Former models. These models were trained to segment different tissue classes, such as stroma, epidermis, blood vessels, and necrosis. The output here is also a segmentation mask, where each pixel is classified into one of the tissue types.\n\nThe evaluation metrics used for these segmentation tasks include the F1 score for nuclei segmentation and the Dice score for tissue segmentation. These metrics help in assessing the performance of the models in correctly classifying and segmenting the different nucleus and tissue types.\n\nThe models were trained using 5-fold cross-validation on a public training dataset, and inference was performed on a hidden test set from the PUMA challenge. The results were reported with a 95% confidence interval, calculated through bootstrapping the samples. This approach ensures that the models are robust and generalizable to new, unseen data.\n\nIn summary, the models discussed in our study are classification models that output segmentation masks for nuclei and tissue types in melanoma histopathology. The performance of these models was evaluated using standard metrics, and the results demonstrate their effectiveness in segmenting and classifying the different nucleus and tissue types.",
  "model/duration": "The execution time for our models varied depending on the specific tasks and datasets used. For nuclei segmentation, we performed experiments using several models, including NN192, Hover-Net, and Hover-NeXt. These models were trained using 5-fold cross-validation on the public training dataset. The inference was conducted on 94 regions of interest (ROIs) from the final hidden test set of the PUMA challenge. The remaining 10 ROIs were reserved for sanity checking of submitted models.\n\nFor tissue segmentation, we utilized nnU-Net and Mask2Former. The nnU-Net model was trained using the same 5-fold cross-validation to create an ensemble model. Mask2Former was pre-trained on the COCO instance segmentation task with a Swin Transformer backbone. Images were resized to 512 × 512 pixels before being loaded into the model. Both models were used for inference on the final hidden test set from the PUMA challenge.\n\nThe specific execution times for training and inference were not explicitly detailed, as the focus was on the performance metrics such as F1 scores and Dice scores. However, the models were evaluated using these metrics, and the results were shown with a 95% confidence interval, calculated through bootstrapping the samples. This approach ensured that the performance metrics were robust and reliable.\n\nIn summary, while the exact execution times for training and inference were not provided, the models were thoroughly evaluated on the PUMA challenge dataset, and the performance metrics were reported with high confidence.",
  "model/availability": "The source code for the PUMA challenge is publicly available. It includes the inference baseline solutions and metric calculation code, which can be accessed through Zenodo. Additionally, the code is archived in Software Heritage and can be found in the GitHub repository for various tracks, including Evaluation Track 1, Evaluation Track 2, Baseline Track 1, and Baseline Track 2.\n\nThe PUMA codebase is licensed under the MIT license, which allows for non-commercial use. The dataset is licensed under the CC0 1.0 license, also permitting non-commercial use. Detailed license terms can be reviewed for further information.\n\nFor running the algorithms, several software dependencies are required, including Hover-Net, Hover-NeXt, NN192 classification algorithm, and specific versions of QuPath (0.1.2 and 0.5.0). Additionally, nnU-Net and Mask2Former are utilized for tissue segmentation.\n\nQuPath scripts for importing .geoJSONs to view and export annotations as masks are also available on Zenodo. These scripts facilitate the integration and visualization of annotations within the QuPath software.\n\nNo modifications were made to the training code for the algorithms used in this project. The software is platform-independent and requires Python 3.10 to operate.",
  "evaluation/method": "The evaluation method employed in our study was comprehensive and multifaceted, ensuring robust validation of our models. For nuclei segmentation, we conducted two sets of experiments. The first experiment compared models that output three nuclei categories: tumor, lymphocyte, and other. The second experiment involved the segmentation of all individual nuclei categories. Model training utilized 5-fold cross-validation on the public training dataset, without adjusting training parameters or data augmentation. Inference was performed on the 94 regions of interest (ROIs) of the final hidden test set of the PUMA challenge. The remaining 10 ROIs were reserved for sanity checking of submitted models in the PUMA challenge.\n\nFor tissue segmentation, we used nnU-Net and Mask2Former to establish baseline benchmarks. The training of nnU-Net also employed 5-fold cross-validation to create an ensemble model. Mask2Former was pre-trained on the COCO instance segmentation task using a Swin Transformer backbone, with images resized to 512 × 512 pixels before loading into the model.\n\nThe evaluation metrics included the F1 score for nuclei segmentation, calculated using the center distance between predicted and ground-truth nuclei. For each ground-truth nucleus, predictions within 15 pixels (3.3 μm) were identified, with matching based on the highest predictive score or the shortest distance. Precision and recall were calculated to derive the F1 score, with micro F1 and average F1 scores used for model comparison. Dice scores were computed for tissue segmentation, both per sample and on a concatenated sample, to assess the performance of semantic segmentation.\n\nAdditionally, we evaluated inter- and intraobserver agreement on 12 randomly selected samples to understand the variability in human annotations. This helped in assessing the models' performance relative to human experts. The results were visualized in tables and figures to provide a clear comparison of model performance against human agreement.",
  "evaluation/measure": "For the evaluation of nuclei and tissue segmentation in melanoma, several performance metrics were employed to ensure a comprehensive assessment of the models.\n\nFor nuclei segmentation, the primary metric used was the F1 score. This score was calculated using the center distance between predicted nuclei and ground-truth nuclei, with a matching radius of 15 pixels (3.3 μm). The F1 score was determined by identifying true positives, false positives, and false negatives, and then calculating precision and recall. The class F1 score was computed as the harmonic mean of precision and recall, ranging from 0 to 1. Additionally, micro F1 and average F1 scores were reported to provide an aggregated view of the model's performance across all classes. These metrics are widely used in the literature for evaluating segmentation tasks, ensuring that our results are comparable to other studies in the field.\n\nFor tissue segmentation, the Dice score was the primary metric used. This score was computed for each class per sample and averaged across all samples, referred to as the average Dice. Additionally, a Dice score was calculated per class on a concatenated sample, known as the micro Dice. This approach allowed for a detailed evaluation of the model's performance in segmenting different tissue classes. The Dice score is a standard metric in medical image segmentation, making our evaluation representative of common practices in the literature.\n\nIn summary, the reported metrics—F1 score for nuclei segmentation and Dice score for tissue segmentation—are well-established in the field of medical image analysis. These metrics provide a robust evaluation of the models' performance, ensuring that our findings are both reliable and comparable to other studies.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various models to establish baseline benchmarks for nuclei and tissue segmentation in melanoma. For nuclei segmentation, we performed two sets of experiments. The first experiment involved comparing models that output three nuclei categories: tumor, lymphocyte, and other. The models evaluated included NN192, Hover-Net, and Hover-NeXt, each trained on different datasets such as PanNuke and PUMA. The second experiment focused on the segmentation of all individual nuclei categories using Hover-Net and Hover-NeXt.\n\nFor tissue segmentation, we utilized nnU-Net and Mask2Former to set baseline benchmarks. The nnU-Net model was trained using 5-fold cross-validation to create an ensemble model, while Mask2Former was pre-trained on the COCO instance segmentation task with a Swin Transformer backbone. Images were resized to 512 × 512 pixels before being loaded into the model. Additionally, we replaced the backbone of Mask2Former with the UNI pathology foundation model, which is better suited for feature extraction from H&E-stained histopathology images. Both models were then fine-tuned on the entire training dataset.\n\nTo evaluate the performance of these models, we used the F1 score for nuclei segmentation and the Dice score for tissue segmentation. The F1 score was calculated using the center distance between predicted nuclei and ground-truth nuclei, with a matching radius of 15 pixels. For tissue segmentation, we computed the Dice score for each class per sample and averaged it across all samples. We also calculated a micro Dice score by concatenating all images along one axis to create a single large image.\n\nIn summary, our evaluation involved a detailed comparison of publicly available methods on benchmark datasets, as well as a comparison to simpler baselines. This approach allowed us to assess the performance of state-of-the-art models in nuclei and tissue segmentation for melanoma, providing a robust foundation for future research in this area.",
  "evaluation/confidence": "The performance metrics presented in this study include confidence intervals, specifically 95% confidence intervals. These intervals are calculated through bootstrapping the samples, providing a measure of the variability and reliability of the results. This approach ensures that the reported metrics, such as the F1 score for nuclei segmentation and the Dice score for tissue segmentation, are robust and statistically significant.\n\nFor nuclei segmentation, the F1 scores for different models and categories are reported with their respective confidence intervals. This allows for a clear understanding of the precision and recall of each model, and how they compare to intra- and interobserver agreement. The use of confidence intervals helps in assessing whether the differences in performance between models are statistically significant.\n\nSimilarly, for tissue segmentation, the Dice scores are presented with 95% confidence intervals. This includes both the average Dice score and the micro-average Dice score, which accounts for the concatenation of all samples into a single large image. The confidence intervals provide insight into the consistency and reliability of the segmentation models, particularly in recognizing different tissue classes.\n\nThe statistical significance of the results is further supported by the comparison with intra- and interobserver agreement. The models' performance metrics, when compared to these benchmarks, indicate whether the automated segmentation methods are superior or comparable to human experts. This comprehensive evaluation ensures that the claims of superiority or equivalence are backed by robust statistical evidence.",
  "evaluation/availability": "The raw evaluation files for the PUMA challenge are available through multiple sources. The code and weights for the inference baseline solutions and metric calculation can be accessed on Zenodo. Additionally, the GitHub repository hosts the evaluation tracks, which have been archived in Software Heritage. These include Evaluation Track 1, Evaluation Track 2, Baseline Track 1, and Baseline Track 2. The PUMA codebase is licensed under the MIT license, which permits non-commercial use. The dataset itself is licensed under the CC0 1.0 license, also allowing for non-commercial use. For detailed license terms, further information can be reviewed."
}