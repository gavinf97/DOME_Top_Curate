{
  "publication/title": "SqueezeCall: nanopore basecalling using a Squeezeformer network",
  "publication/authors": "The author of this article is Z. Zhu. The specific contributions of Z. Zhu to the paper are not detailed.",
  "publication/journal": "Gigabyte",
  "publication/year": "2025",
  "publication/doi": "10.46471/gigabyte.148",
  "publication/tags": "- Software and Workflows\n- Bioinformatics\n- Molecular Genetics\n- Nanopore Sequencing\n- Basecalling\n- Deep Learning\n- Convolutional Neural Networks\n- Transformer Models\n- Sequence Analysis\n- Computational Biology",
  "dataset/provenance": "The datasets used in our study originate from various sources, each serving specific purposes in our research. For the human genome reference dataset, we utilized the NA12878/GM12878 dataset from the Ceph/Utah pedigree. This dataset includes multiple sequencing runs, from which we selected three specific experiments: FAB42828, FAF09968, and FAF04090. The training process involved approximately 80,000 reads, while the testing was conducted on 5,000 reads. This dataset has been previously used in other studies, ensuring its reliability and relevance in the scientific community.\n\nAdditionally, we incorporated a Lambda phage dataset, which was trained on around 40,000 reads and tested on 5,000 reads. This dataset has also been utilized in prior research, providing a benchmark for comparison.\n\nFor bacterial datasets, we employed the dataset released by Wicket et al. This dataset is divided into training and test sets. The training set comprises 50 individual species genomes, including 30 Klebsiella pneumoniae genomes, 10 genomes of other Enterobacteriaceae species, and 10 genomes from other families of Proteobacteria. The test dataset consists of nine species, such as Klebsiella pneumoniae, Shigella sonnei, Serratia marcescens, Haemophilus haemolyticus, Acinetobacter pittii, Stenotrophomonas maltophilia, and Staphylococcus aureus.\n\nTo ensure the accuracy and quality of our training data, we annotated the data using the Tombo resquiggle tool. This process involved aligning reads to reference sequences and discarding those that did not align properly or had poor resquiggle quality.\n\nFurthermore, we used a public dataset provided by ONT for ablation studies. This dataset is divided into chunks of equal length, with a training set of 1.22 million chunks and a development set of 20,000 chunks, each containing 3,600 current signal values. This dataset has been made available to the community, facilitating further research and validation of our methods.",
  "dataset/splits": "The datasets used in our study were split into training and testing sets. For the human genome reference dataset (NA12878/GM12878), we trained on approximately 80,000 reads and tested on 5,000 reads. The Lambda phage dataset was trained on approximately 40,000 reads and tested on 5,000 reads. The bacterial dataset consisted of a training set with 50 individual species genomes and a test set with nine species. The training dataset included 30 Klebsiella pneumoniae genomes, 10 genomes of other species of Enterobacteriaceae, and 10 genomes from other families of Proteobacteria. The test dataset comprised three Klebsiella pneumoniae, Shigella sonnei, Serratia marcescens, Haemophilus haemolyticus, Acinetobacter pittii, Stenotrophomonas maltophilia, and Staphylococcus aureus. Additionally, a public dataset provided by ONT for ablation studies was divided into chunks of equal length, consisting of a training set of 1.22 million chunks and a development set of 20,000 chunks, each with 3,600 current signal values.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in our study are publicly available, ensuring transparency and reproducibility. For the human genome reference dataset, specifically the NA12878/GM12878 from the Ceph/Utah pedigree, the raw data and basecall datasets (FAF04090, FAF09968, FAB42828) can be accessed from the sources provided by Jain et al. The bacterial raw data and Lambda phage data are available from Pagès-Gallego et al. and Wick et al. Additionally, the ONT chunk data used for ablation studies is available from ONT.\n\nTo facilitate easy access and usage, demo data is hosted on Zenodo, and code snapshots are preserved in the Software Heritage repository. Furthermore, DOME-ML annotations for the machine learning aspects are available in the DOME-registry. These resources are provided under licenses that allow for open access and use, promoting collaborative research and development.\n\nThe availability of these datasets and resources ensures that other researchers can replicate our findings and build upon our work. The public forum hosting these datasets enforces open access policies, making it straightforward for anyone to download and use the data for their own research purposes. This approach not only enhances the credibility of our study but also contributes to the broader scientific community by providing valuable resources for further exploration and innovation in nanopore sequencing and basecalling.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on a combination of well-established machine-learning techniques, primarily focusing on deep learning architectures. Specifically, we utilize a Squeezeformer-based model, which is an enhancement of the Conformer architecture. This model integrates convolutional layers to downsample raw signals and capture local dependencies, while the Squeezeformer network is employed to capture global context.\n\nThe Squeezeformer architecture is not entirely new but represents an improvement over existing models like Conformer. It addresses issues such as high temporal redundancy in feature representations, which can lead to unnecessary computational overhead. By incorporating a temporal U-Net structure, Squeezeformer achieves better efficiency and performance.\n\nThe decision to publish this work in a genomics journal rather than a machine-learning journal is driven by the specific application and contributions of our research. Our primary focus is on advancing the field of nanopore basecalling, which is a critical component in genomics and bioinformatics. The Squeezeformer model, while rooted in machine-learning principles, is tailored and optimized for the unique challenges and requirements of nanopore sequencing data. Therefore, the context and impact of our work are more aligned with the genomics community, where the practical applications and improvements in basecalling accuracy are of paramount importance.",
  "optimization/meta": "The model described, SqueezeCall, is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is an end-to-end model designed for accurate basecalling in nanopore sequencing. SqueezeCall utilizes convolution layers to downsample raw signals, a Squeezeformer network to capture global context, and a Connectionist Temporal Classification (CTC) decoder to generate the DNA sequence.\n\nThe Squeezeformer architecture consists of several key components:\n- Depthwise Separable Conv Subsampling module\n- Temporal U-Net Architecture of the encoder\n- Conformer module, which includes a feed-forward network, multi-head attention, convolution, and another feed-forward network, referred to as the FMCF structure.\n\nThe training process involves a combination of three types of loss:\n- CTC-CRF loss\n- Intermediate CTC-CRF loss\n- KL loss\n\nThese losses are combined with specific weight coefficients to optimize the model's performance. The training data used for SqueezeCall includes various datasets, such as the NA12878 Human Dataset, Lambda Phage dataset, and multiple bacterial datasets. The data was annotated using the Tombo resquiggle tool to ensure accurate label sequences and high-quality training data. The reads were sliced into non-overlapping chunks of 3,600 data points for training efficiency, and the model was trained on multiple GPUs with a specified batch size.\n\nThe independence of the training data is ensured through the use of established datasets and the application of the Tombo resquiggle tool for annotation. This process helps to maintain the quality and usability of the training data, ensuring that the model is trained on accurate and reliable information.",
  "optimization/encoding": "In our study, the raw electrical signals obtained from nanopore sequencing were initially pre-processed to ensure high-quality data. The reads were aligned to reference sequences using their basecalls, and any reads that did not align properly or had poor resquiggle quality were discarded. This step was crucial for obtaining accurate label sequences and enhancing the quality of our training data.\n\nThe convolution layers in our model were employed to downsample the raw signals, effectively modeling local dependencies within the data. This downsampling process helped in reducing the dimensionality of the input data while preserving essential features.\n\nTo further enhance the model's ability to capture global context, we incorporated a Squeezeformer network. This network is an improvement over the Conformer architecture, which combines convolutional and transformer elements to capture both local and global patterns in the data. The Squeezeformer network includes a temporal U-Net structure that downsamples the data in the middle layers and upsamples it towards the end, ensuring training stability and efficiency.\n\nAdditionally, inspired by the Wav2vec2.0 model, we introduced a mask module between the convolution network and the Squeezeformer network. This module masks a proportion of the time steps of the convolution outputs and replaces them with a trained feature vector shared across all masked time steps. This approach increased the model's resistance to noise and improved its overall accuracy.\n\nThe final output of the model predicts corresponding symbols, which include the nucleotides A, G, C, T, and a blank symbol. These predictions are then fed into a Connectionist Temporal Classification (CTC) decoder based on a beam search algorithm with a beam width of 5. The beam search decoder maintains a set of prefix sequences with the maximum probability and generates new sets of prefixes by extending all possible characters, ultimately selecting the sequence with the highest score as the final output.",
  "optimization/parameters": "In our study, we developed two model variants, SqueezeCall-M and SqueezeCall-L, with different sizes. SqueezeCall-M has eight layers and 79 million parameters, while SqueezeCall-L has ten layers and 95 million parameters. The selection of these parameters was driven by the need to balance model complexity and performance. We found that increasing the number of layers and parameters generally improved the model's accuracy, particularly in challenging regions like homopolymers and heteropolymers. The choice of 79 million and 95 million parameters was based on extensive experimentation and comparison with other state-of-the-art basecaller methods, ensuring that our models achieved high accuracy without excessive computational overhead.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed a robust training setting to ensure that our model, SqueezeCall, effectively learned from the data without overfitting or underfitting. The reads were sliced into non-overlapping chunks of 3,600 data points, which provided a substantial number of training points. We utilized the Adam optimizer to minimize the loss function, with a learning rate that was warmed up over the first 1,000 updates to a peak value of 0.0005 and then linearly decayed. This approach helped in stabilizing the training process and preventing the model from converging too quickly to a suboptimal solution, which could lead to underfitting.\n\nTraining was conducted on four GPUs with a batch size of 16 per GPU, resulting in a total batch size of 64. This parallel processing not only accelerated the training but also ensured that the model was exposed to a diverse set of data points in each batch, reducing the risk of overfitting. The model variants, SqueezeCall-M and SqueezeCall-L, have 79 million and 95 million parameters, respectively. While the number of parameters is indeed large, the extensive dataset and the use of regularization techniques, such as dropout and weight decay, helped in mitigating overfitting.\n\nTo further ensure that the model did not overfit, we performed ablation studies and compared the performance of different loss functions. The combination of CTC-CRF loss, intermediate CTC-CRF loss, and KL loss significantly improved the accuracy, indicating that the model was learning meaningful patterns from the data rather than memorizing it. Additionally, the use of a mask module inspired by the Wav2vec2.0 model increased the model's resistance to noise, which is crucial for nanopore sequencing data.\n\nThe performance of SqueezeCall was evaluated on 11 datasets, including the NA12878 Human Dataset, Lambda Phage, and nine bacterial datasets. The model achieved the lowest error rate and the highest identity rate across all datasets, demonstrating its generalizability and robustness. These results suggest that the model was neither overfitting nor underfitting but rather learning the underlying patterns in the data effectively.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the generalization of our model. One key method involved the use of label smoothing in the KL loss. Label smoothing helps to prevent the model from becoming too confident about its predictions by adjusting the true distribution labels. Specifically, we used a smoothing factor of 0.1, which means that instead of having a hard label (e.g., [0, 0, 1, 0, 0] for class 3), the labels were softened to [0.1, 0.1, 0.9, 0.1, 0.1]. This technique encourages the model to distribute its probability more evenly across classes, reducing the risk of overfitting.\n\nAdditionally, we utilized dropout layers within our neural network architecture. Dropout randomly sets a fraction of the input units to zero at each update during training time, which helps to prevent the model from relying too heavily on any single neuron. This forces the network to learn more robust features and reduces the likelihood of overfitting.\n\nWe also incorporated data augmentation techniques by masking a proportion of the time steps of the convolution outputs before feeding them into the Squeezeformer network. This approach, inspired by the Wav2vec2.0 model, increases the model's resistance to noise and improves its accuracy by making it learn to handle incomplete or noisy data.\n\nFurthermore, we employed a combination of three types of loss functions: CTC-CRF loss, intermediate CTC-CRF loss, and KL loss. The intermediate CTC-CRF loss is calculated at multiple intermediate blocks within the network, providing additional supervision and helping the model to learn more effectively at different stages of the training process. This multi-stage supervision helps in regularizing the model and preventing overfitting.\n\nIn summary, our regularization methods included label smoothing, dropout, data augmentation through masking, and the use of multiple loss functions. These techniques collectively contributed to improving the model's performance and generalization capabilities.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we trained our models using non-overlapping chunks of 3,600 data points, optimized with the Adam optimizer. The learning rate was warmed up over the first 1,000 updates to a peak value of 0.0005 and then linearly decayed. Training was conducted on 4 GPUs with a batch size of 16 per GPU, resulting in a total batch size of 64.\n\nThe model files and optimization parameters are not explicitly detailed in the publication, but the source code and requirements for reproducing the experiments are available. The project, named SqueezeCall, can be accessed on GitHub under the MIT license. This repository includes the necessary code to implement the SqueezeCall model, allowing researchers to replicate the experiments and explore the optimization parameters used.\n\nFor those interested in accessing the datasets and additional resources, demo data is available on Zenodo, and code snapshots are preserved in the Software Heritage repository. Furthermore, DOME-ML annotations for the Machine Learning are available in the DOME-registry. Human raw data and basecall datasets, as well as bacterial raw data and lambda phage data, can be obtained from specified sources. The ONT chunk data is also available from ONT. These resources collectively provide a comprehensive framework for reproducing and building upon the optimization strategies presented in our work.",
  "model/interpretability": "The SqueezeCall model, while leveraging advanced deep learning techniques, does exhibit some level of transparency through its architectural design and the use of specific loss functions. The model is not entirely a black box; several aspects of its operation can be interpreted and understood.\n\nThe SqueezeCall architecture includes a convolution module, a mask module, and a Temporal U-Net structure. The convolution module processes raw signals through a series of 1D convolution layers, which are followed by layer normalization and activation functions. This process down-samples the raw signals, making the data more manageable for subsequent layers. The mask module, inspired by the Wav2vec2.0 model, introduces a masking mechanism that helps the model become more resistant to noise, thereby improving accuracy. The Temporal U-Net structure captures global context, which is crucial for accurate basecalling.\n\nThe use of different types of loss functions—CTC-CRF loss, intermediate CTC-CRF loss, and KL loss—provides further insight into the model's behavior. The CTC-CRF loss ensures that the model can handle sequential data effectively, while the intermediate CTC-CRF loss relaxes the conditional independence assumption, improving accuracy. The KL loss measures the divergence between the predicted probability distribution and the true distribution, helping to refine the model's predictions.\n\nAdditionally, the beam search algorithm used in the CTC decoder maintains a set of prefix sequences with the maximum probability, accumulating scores in each iteration to select the final output sequence. This process is transparent and can be traced to understand how the model arrives at its predictions.\n\nIn summary, while the SqueezeCall model utilizes complex deep learning techniques, its architectural components and the use of specific loss functions provide a level of interpretability. The convolution and mask modules, along with the Temporal U-Net structure, offer insights into how raw signals are processed and how the model improves resistance to noise. The combination of different loss functions further enhances the model's transparency by showing how it refines its predictions.",
  "model/output": "The model is a classification model designed for basecalling in nanopore sequencing. It predicts the corresponding symbol for each position in a sequence, where the symbols can be one of A, G, C, T, or a blank symbol. The output of the model is then processed by a Connectionist Temporal Classification (CTC) decoder using a beam search algorithm. This decoder maintains a set of prefix sequences with the maximum probability and generates new sets of prefixes by extending the previous ones with all possible characters. The algorithm accumulates the scores of each prefix in the beam during each iteration and ultimately selects the sequence with the highest score as the final output. This process ensures that the model can accurately predict the sequence of symbols, making it suitable for tasks that require classifying each time step into one of several possible categories.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for SqueezeCall is publicly available. It can be accessed via the project's home page on GitHub. The software is licensed under the MIT license, which allows for free use, modification, and distribution. Additionally, code snapshots are archived in the Software Heritage repository. The project is platform-independent and written in Python, making it accessible for a wide range of users. The biotools ID for SqueezeCall is squeezecall, and the RRID is SCR_026317. This ensures that the software is well-documented and easily integrable into various bioinformatics workflows.",
  "evaluation/method": "The evaluation of SqueezeCall was conducted using a comprehensive set of datasets and comparisons with established basecaller methods. We utilized 11 datasets, including the NA12878 Human Dataset, Lambda Phage, and nine bacterial datasets. These datasets were chosen to represent a diverse range of sequencing challenges.\n\nFor the human dataset, we selected three specific experiments: FAB42828, FAF09968, and FAF04090. The training and testing splits were designed to ensure robustness, with approximately 80,000 reads used for training and 5,000 reads for testing. Similarly, the Lambda Phage dataset was split into 40,000 reads for training and 5,000 reads for testing.\n\nThe bacterial datasets were sourced from a study by Wicket et al., which included a training set of 50 individual species genomes and a test set of nine species. This diverse set of genomes helped to evaluate the model's performance across different bacterial strains.\n\nTo ensure the accuracy of the label sequences, we used the Tombo resquiggle tool to annotate the data. This process involved aligning reads to reference sequences and discarding any reads that did not align properly or had poor resquiggle quality.\n\nIn addition to these datasets, we also used a public dataset provided by ONT for ablation studies. This dataset consisted of reads divided into chunks of equal length, with a training set of 1.22 million chunks and a development set of 20,000 chunks.\n\nThe evaluation involved comparing SqueezeCall with four other basecaller methods: Bonito-LSTM, Bonito-Transformer, CATCaller, and SACall. The performance metrics included error rates (deletion, insertion, and mismatch rates) and identity rates. The error rate was defined as the sum of deletion, insertion, and mismatch rates, while the identity rate was the portion of the read length that aligned correctly.\n\nThe results showed that SqueezeCall achieved the lowest error rate and the highest identity rate across all datasets, demonstrating its superior performance in basecalling. The detailed analysis of homopolymer and heteropolymer regions further highlighted SqueezeCall's accuracy in challenging sequencing areas.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to comprehensively assess the effectiveness of our basecalling models. The primary metrics reported include error rates, which are further broken down into deletion rates, insertion rates, and mismatch rates. These rates are calculated as the number of deleted, inserted, and mismatched bases divided by the alignment length, respectively. The overall error rate is the sum of these three rates.\n\nAdditionally, we evaluated the identity rate, which represents the portion of the read length that aligns correctly. This metric provides insight into how accurately the basecalling models can reconstruct the original sequence.\n\nOur choice of metrics is aligned with common practices in the literature, ensuring that our evaluation is representative and comparable to other studies in the field. By reporting these detailed metrics, we aim to provide a thorough understanding of our models' performance across various datasets, including human, phage, and bacterial sequences. This approach allows for a nuanced comparison with other state-of-the-art basecalling methods, such as Bonito-LSTM, Bonito-Transformer, CATCaller, and SACall.",
  "evaluation/comparison": "In the evaluation of SqueezeCall, a comprehensive comparison was conducted with four established basecaller methods: Bonito-LSTM, Bonito-Transformer, CATCaller, and SACall. This comparison was performed across 11 diverse datasets, including the NA12878 Human Dataset, Lambda Phage, and nine bacterial datasets. These datasets encompass a wide range of sequencing challenges, ensuring a robust evaluation of SqueezeCall's performance.\n\nThe Bonito-LSTM and Bonito-Transformer are official state-of-the-art basecall tools from Oxford Nanopore Technologies, with 27 million and 79 million parameters, respectively. CATCaller and SACall represent other advanced basecalling methods in the field. By comparing SqueezeCall against these methods, we aimed to demonstrate its superiority in terms of accuracy and efficiency.\n\nThe error rates of the basecalled reads were meticulously analyzed, focusing on deletion, insertion, and mismatch rates. SqueezeCall, particularly the larger variant SqueezeCall-L, achieved the lowest error rates across all datasets. This indicates its superior ability to accurately decode the raw electrical signals into DNA sequences.\n\nAdditionally, the identity rate, which measures the portion of the read length that aligns correctly, was evaluated. SqueezeCall-L achieved the highest average identity rate at 93.97%, outperforming all other methods. This highlights SqueezeCall's robustness in maintaining high accuracy across different sequencing datasets.\n\nThe comparison also included an analysis of homopolymer and heteropolymer regions, which are known to be challenging in nanopore sequencing. SqueezeCall demonstrated higher accuracy in these regions compared to Bonito-LSTM and Bonito-Transformer, further solidifying its effectiveness.\n\nIn summary, the comparison with publicly available methods on benchmark datasets showed that SqueezeCall outperforms existing basecaller methods in terms of error rates and identity rates. This evaluation underscores SqueezeCall's potential to significantly improve the accuracy of nanopore sequencing, facilitating better downstream genome assembly and environmental sample analysis.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files used in our study are publicly available. For the human dataset, the raw data and basecall datasets, specifically FAF04090, FAF09968, and FAB42828, can be accessed from Jain et al. and the Whole Human Genome Sequencing Project data on GitHub. The bacterial raw data and lambda phage data are available from Pagès-Gallego et al. and Wick et al., also accessible via GitHub. Additionally, the ONT chunk data used for our ablation studies is available from Oxford Nanopore Technologies. All these datasets are open for public use, facilitating reproducibility and further research."
}