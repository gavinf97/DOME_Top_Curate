{
  "publication/title": "Healthy Microbiome—Moving Towards Functional Interpretation",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Gut Microbiome\n- Functional Interpretation\n- Health Index\n- Machine Learning\n- Metagenomics\n- Microbiome Analysis\n- Dysbiosis Prediction\n- Bioinformatics\n- Computational Biology\n- Health Thresholds",
  "dataset/provenance": "The dataset used in our study was expanded to include 30 datasets, encompassing 27 additional cohorts from various disease states. The core of our dataset originates from the Human Microbiome Project 2 (HMP2), with the accession code PRJNA398089. Additionally, two inflammatory bowel disease (IBD) validation cohorts can be found under accessions PRJNA389280 and PRJEB1220. The test cohorts were sourced from the curatedMetagenomicsData repository, where samples were manually selected to exclude children, reported control yet unhealthy samples, and those of patients undergoing antibiotic treatment. The COVID-19 cohort is available under the accession PRJEB64515 in the European Nucleotide Archive.\n\nThe dataset includes a diverse range of disease states, providing a robust benchmark for method validation. The Q2PD model was retrained with this expanded dataset, ensuring a comprehensive evaluation. The leave-one-cohort-out approach was employed to maintain a truly blind assessment, enhancing the reliability of our findings. This methodology placed our index in a disadvantaged position initially, as some datasets had been used to develop and train other methods. Despite this, the Q2PD achieved the highest average accuracy and AUC across all datasets, demonstrating its superior performance.\n\nThe dataset's diversity and the rigorous validation process underscore the reliability and generalizability of our results. The inclusion of various disease states and the careful selection of samples ensure that our findings are applicable to a wide range of clinical scenarios. The availability of the dataset and the associated scripts on GitHub and GigaDB facilitates reproducibility and further research.",
  "dataset/splits": "We utilized a total of 30 datasets for method validation, which included 27 additional cohorts from various disease states beyond the initial datasets. For the model training, validation, and subsequent testing, we employed a leave-one-cohort-out cross-validation approach. This method ensured a robust benchmark by retraining the model with new data while keeping the parameters of the Q2PD model consistent, as they were originally determined based on healthy samples from the HMP2.\n\nThe leave-one-cohort-out approach means that for each iteration of the cross-validation, one cohort is held out as the test set, and the remaining cohorts are used as the training set. This process is repeated for each cohort, ensuring that every cohort serves as the test set exactly once. This method helps in evaluating the model's performance across different datasets and ensures that the model is not overfitted to any specific cohort.\n\nThe distribution of data points in each split varies depending on the cohort being held out. Since the leave-one-cohort-out approach is used, the training set for each iteration consists of data from 29 cohorts, while the test set consists of data from the single held-out cohort. This ensures that the model is tested on unseen data in each iteration, providing a comprehensive evaluation of its performance.",
  "dataset/redundancy": "In our study, we extended our dataset to include 27 additional cohorts from various disease states, resulting in a total of 30 datasets used for method validation. To ensure a robust benchmark, we employed a leave-one-cohort-out approach. This method involved retraining the model with the new data while leaving out one cohort at a time for testing. This procedure placed our index at a disadvantaged position because some of the added datasets had been used to develop and train other methods, making our approach truly blind to the outcomes in these new cases.\n\nThe leave-one-cohort-out approach was chosen to avoid model overfitting and to ensure that the training and test sets were independent. By leaving out one cohort at a time, we ensured that the model was not trained on data that it would later be tested on, thus maintaining the independence of the training and test sets.\n\nRegarding the distribution of our datasets, we manually curated samples from the curatedMetagenomicsRepository to ensure consistent sample processing. This curation process involved excluding samples with incomplete metadata, use of antibiotics or other similar drugs, repeats from the same patients, young age (newborns), or healthy samples that were not fully healthy (high body mass index, chronic conditions). During filtering, we set thresholds for species abundance (≥0.1%), pathway coverage (≥20%), and function abundance (≥0.01%), with any features below these thresholds changed to 0.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. By ensuring consistent sample processing and maintaining the independence of training and test sets, we aimed to provide a robust and reliable validation of our method. The leave-one-cohort-out approach, combined with our rigorous curation process, helps to mitigate issues related to dataset redundancy and ensures that our results are generalizable to new, unseen data.",
  "dataset/availability": "The data utilized in this study is publicly available through various repositories. The HMP2 data can be accessed via the Sequence Read Archive under the accession code PRJNA398089. Additionally, two inflammatory bowel disease (IBD) validation cohorts are available under accessions PRJNA389280 and PRJEB1220. The test cohorts were sourced from the curatedMetagenomicsData repository, with specific samples manually selected to exclude children, unhealthy controls, and patients undergoing antibiotic treatment. The COVID-19 cohort is accessible through the European Nucleotide Archive under the accession PRJEB64515.\n\nThe Q2PD (q2-predict-dysbiosis) tool, along with sample accessions and scripts necessary to reproduce the results, is deposited on GitHub. The complete code and additional data required to reproduce the analyses are also available as a GigaDB dataset. Furthermore, DOME-ML (Data, Optimization, Model and Evaluation in Machine Learning) annotations can be found in the DOME registry under accession 4cstv1dfjm. An archival copy of the code is available via Software Heritage.\n\nThe data and code are released under the MIT License, ensuring open access and the ability to reproduce the findings. The manual curation process involved setting thresholds for species abundance, pathway coverage, and function abundance, with any features below these thresholds adjusted to zero. This ensures consistency in sample processing and validation of the approach. The leave-one-out cross-validation procedure was applied to predict health scores for each sample, avoiding model overfitting. The final health score is derived from the random forest’s predict_proba method, indicating the probability of each sample belonging to the healthy or unhealthy group.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest classifier. This algorithm is well-established and widely used due to its robustness, particularly in handling imbalanced data and providing interpretable results.\n\nThe random forest algorithm itself is not new; it has been extensively used and validated in various fields, including bioinformatics. The reason it was not published in a machine-learning journal is that our focus was on applying this algorithm to a specific biological context, namely the prediction of health status based on microbiome data. Our contributions lie in the innovative application of the random forest algorithm to this particular domain, rather than in the development of a new machine-learning algorithm.\n\nWe employed the Boruta algorithm, a wrapper around the random forest classifier, to determine the relevance of each index in the context of inflammatory bowel disease (IBD) predictions. Boruta extends the dataset by adding randomly permuted copies of each original variable, known as shadow variables. It then builds a predictive model using the random forest algorithm and estimates the importance of each variable through a permutation test. This process is repeated multiple times, and variables are eventually assigned to three classes: Confirmed (better than random), Rejected (no better than random), and Tentative (those that could not be assigned to the Confirmed or Rejected class).\n\nThe utility of the variable is a good indicator of the information importance carried by the variable, especially in situations where synergistic interactions are important. In our analysis, the predictions of each index were passed as parameters, and the health labels (0/1) were used as decision vectors. Index importances were redefined as ranks, with the highest importance marked as rank “1.”\n\nFor the purpose of testing, the index was retrained on a complete set of 30 datasets using a leave-one-cohort-out procedure with 5-fold cross-validation. This approach ensured a reliable benchmark by defining accuracy and AUC statistics separately for each test cohort based on the validation set, which was a subset of the training dataset. The performance of our index was evaluated by calculating its winning margin over hiPCA, the second-best index, and the mean of the other methods. Specifically, for each test dataset, the AUC of the hiPCA or the mean AUC of the other methods was subtracted from the AUC produced by the Q2PD. The winning margin was defined as the mean of the AUC differences, separately for cases when Q2PD won and when it did not.",
  "optimization/meta": "The Q2PD model is a meta-predictor that leverages data from other machine-learning algorithms as input. Specifically, it incorporates predictions from various indices, including GMHI, hiPCA, and Shannon entropy measures on both taxonomic and functional profiles. These indices serve as features in the Q2PD model, which is built using a random forest classification algorithm.\n\nThe constituent machine-learning methods of the Q2PD meta-predictor include:\n\n* GMHI (Gut Microbiota Health Index)\n* hiPCA (hierarchical PCA)\n* Shannon entropy on species\n* Shannon entropy on functions\n\nThe training data for the Q2PD model is designed to be independent, ensuring robust validation. Initially, the model was developed and validated using healthy samples from the HMP2 dataset. Subsequently, the model was retrained with an extended dataset of 30 cohorts, including 27 additional cohorts from various disease states. A leave-one-cohort-out approach was employed to ensure that the model was truly blind to the outcomes in these new cases, thereby maintaining independence in the training data.\n\nHowever, it is important to note that some of the added datasets had been used to develop and train the other methods included in the meta-predictor. This could potentially introduce some level of dependency, but the leave-one-cohort-out approach aims to mitigate this by ensuring that each cohort is independently validated.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several key steps to ensure consistency and robustness. Initially, manually curated samples from the curatedMeta genomics Repository were used. This ensured consistent sample processing using MetaPhlAn and HUMAnN, specifically with MetaCyc functional annotations. Samples were filtered based on specific thresholds: species abundance ≥0.1%, pathway coverage ≥20%, and function abundance ≥0.01%. Any features below these thresholds were set to zero.\n\nDuring the random forest training, healthy individuals were labeled as \"1\" and those with inflammatory bowel disease (IBD) as \"0.\" A leave-one-out cross-validation procedure was applied to predict health scores for each sample, which helped to avoid model overfitting and the need to split the data into separate training and testing sets. The final health score was derived from the random forest’s predict_proba method, expressing the probability of each sample belonging to the healthy or unhealthy group.\n\nThe GMHI scores were calculated using the QIIME 2 q2-health-index plugin. Predictions from hiPCA were obtained by substituting the original test file with our test samples, although this approach could potentially result in an overlap of training and test samples, possibly falsely increasing hiPCA’s accuracy. Shannon entropies were calculated separately on taxonomic and functional profiles for each sample using custom Python scripts.\n\nThe dataset was extended to include 27 additional cohorts from various disease states, totaling 30 datasets for method validation. The Q2PD model parameters, originally determined based on healthy samples from the HMP2, were not changed. Instead, the model was retrained with the new data using a leave-one-cohort-out approach to ensure a robust benchmark. This procedure placed the Q2PD index at a disadvantaged position, as some of the added datasets had been used to develop and train other methods, making the Q2PD approach truly blind to outcomes in these new cases.\n\nThe Q2PD index demonstrated the highest average accuracy and AUC across all datasets, outperforming other methods such as hiPCA, GMHI, and Shannon entropy on species and functions. The consistently poor performance of entropy-based measures suggested their limited utility as predictive indices. The Q2PD index showed significant classification improvement in areas where other indices did not perform well, particularly when datasets used for training other methods were excluded.",
  "optimization/parameters": "In our model, we utilized several parameters to capture various aspects of microbial community data. The specific parameters include the fraction of core functions found, the fraction of core functions among all functions, the fraction of species pairs commonly occurring together in healthy samples, the average number of function contributions per species, and the counts of \"good\" and \"bad\" GMHI species.\n\nThe selection of these parameters was driven by their relevance to the health status of individuals. For instance, the GMHI parameters were chosen because they are associated with known health-related microbial species. The core functions and species pairs were selected to reflect the functional and taxonomic composition of healthy microbiomes. The process involved a combination of domain knowledge and exploratory data analysis to identify features that showed promise in distinguishing between healthy and diseased states.\n\nThe model's robustness and interpretability were key considerations in parameter selection. We opted for a random forest classification algorithm due to its ability to handle imbalanced data and provide insights into feature importance. This allowed us to evaluate the contribution of each parameter to the model's predictions and refine our selection accordingly.\n\nAdditionally, we ensured consistency and reproducibility by using taxonomic and functional profiles from a curated metagenomics database. This approach helped us to validate the parameters across different datasets and cohorts, ensuring that our model generalizes well to new, unseen data.",
  "optimization/features": "The input features for our model encompass a diverse set of parameters, reflecting both taxonomic and functional information. The exact number of features (f) used as input can vary depending on the dataset and the specific cohorts being analyzed. We employed a feature selection process to identify the most informative parameters. This process was conducted using the Boruta algorithm, which is a wrapper around the random forest classifier. The Boruta algorithm extends the dataset by adding randomly permuted copies of each original variable, known as shadow variables. It then builds a predictive model using the random forest algorithm and estimates the importance of each variable through a permutation test. The importance scores of the original variables are compared to the maximal importance achieved by the shadow variables. This procedure is repeated multiple times, and a statistical test is performed to assign variables to three classes: Confirmed (better than random), Rejected (no better than random), and Tentative (those that could not be assigned to the Confirmed or Rejected class).\n\nThe feature selection was performed using the training set only, ensuring that the model's performance on unseen data is not biased. This approach helps in identifying the most relevant features that contribute to the model's predictive power. The importance of each feature was redefined as ranks, with the highest ranks (lowest values) representing the greatest importance. This ranking system allows us to prioritize features that provide the most significant information for classification tasks.",
  "optimization/fitting": "In our study, we employed a random forest classification algorithm, which is known for its robustness to imbalanced data and interpretability. This choice was crucial given the diverse and complex nature of our datasets, which included various disease states and healthy cohorts.\n\nThe number of parameters in our model was indeed large, encompassing a variety of features derived from both taxonomic and functional profiles. However, to mitigate the risk of overfitting, we implemented a leave-one-out cross-validation procedure. This approach ensured that each sample was used once as a test set while the remaining samples formed the training set. By doing so, we could validate our model's performance on unseen data, thereby reducing the likelihood of overfitting.\n\nAdditionally, we used a leave-one-cohort-out approach during the validation phase, which further strengthened our model's robustness. This method involved training the model on all but one cohort and then testing it on the excluded cohort. This process was repeated for each cohort, ensuring that our model could generalize well to new, unseen data.\n\nTo address the potential issue of underfitting, we carefully selected and engineered features that were relevant to the classification task. For instance, we included parameters such as the fraction of core functions found, the number of species pairs commonly occurring together in healthy samples, and the average number of function contributions per species. These features were derived from a curated metagenomics database, ensuring consistency and reproducibility in our analysis.\n\nFurthermore, we employed the Boruta algorithm, a feature selection method that works in conjunction with random forests. Boruta helped us identify the most informative features by comparing their importance to that of shadow variables, which were randomly permuted copies of the original features. This process ensured that only the most relevant features were retained, thereby enhancing the model's predictive power and reducing the risk of underfitting.\n\nIn summary, our approach to fitting the model involved careful feature selection, robust cross-validation techniques, and the use of advanced feature importance algorithms. These measures collectively helped us to avoid both overfitting and underfitting, ensuring that our model was both accurate and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the primary methods used was leave-one-out cross-validation. This procedure involved training the model on all but one sample and then testing it on the left-out sample. This process was repeated for each sample in the dataset, ensuring that every sample was used for both training and testing. This approach helped to avoid overfitting by providing a more rigorous evaluation of the model's performance.\n\nAdditionally, we utilized a leave-one-cohort-out approach during the validation phase. This involved training the model on all but one cohort and testing it on the left-out cohort. This method further ensured that our model was not overfitting to any specific cohort and could generalize well to new, unseen data.\n\nWe also incorporated the Boruta algorithm, which is a wrapper around the random forest classifier. This algorithm helps in feature selection by comparing the importance of each variable with shadow variables created by permuting the original variables. This process is repeated multiple times, and variables are eventually classified as confirmed (better than random), rejected (no better than random), or tentative. This method aids in selecting the most relevant features, thereby reducing the risk of overfitting.\n\nFurthermore, we set specific thresholds during the filtering process, such as species abundance ≥0.1%, pathway coverage ≥20%, and function abundance ≥0.01%. Features below these thresholds were changed to zero, which helped in focusing on the most significant features and reducing noise in the data.\n\nThese techniques collectively ensured that our model was robust, generalizable, and not overfitted to the training data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are available for public access. The complete code and additional data required to reproduce our analyses have been deposited as a GigaDB dataset. This includes all necessary scripts and sample accessions needed to replicate our results. Additionally, the project is hosted on GitHub under the name \"q2-predict-dysbiosis,\" where the source code is accessible. The project is platform-independent and requires Python, specifically scikit-learn version 1.1.3 or higher. It is licensed under the MIT License, ensuring open access and the freedom to use, modify, and distribute the code. Furthermore, the DOME-ML annotations are available in the DOME registry, and an archival copy of the code is available via Software Heritage. This comprehensive availability ensures that all aspects of our optimization process are transparent and reproducible.",
  "model/interpretability": "The model employed in our study is not a black box but rather a transparent one, primarily due to the use of a random forest classification algorithm. Random forests are known for their interpretability, as they provide insights into the importance of different features in making predictions.\n\nIn our analysis, we explored the importance of various features associated with model training on individual datasets. This revealed a significant diversity, indicating that different types of information were used to classify different cohorts. For instance, when trained on individual cohorts, a random forest would often pick \"GMHI_bad\" as its most informative parameter and \"GMHI_good\" as the least informative. The importance of function-based features varied depending on the dataset, with some features like \"Contributions_per_species\" showing variability in their ranking.\n\nTo further investigate the model's interpretability, we used the Boruta algorithm, which is a wrapper around the random forest classifier. Boruta helps in identifying the most important features by comparing the importance of each original variable with shadow variables created by random permutation. This process assigns variables to three classes: Confirmed (better than random), Rejected (no better than random), and Tentative (those that could not be definitively classified). This method provides a clear indication of which features carry the most information and are crucial for the model's predictions.\n\nAdditionally, we evaluated the importance of features contributing to the Q2PD score by training a random forest model separately for each cohort and using a feature permutation approach. The importances were then converted to ranks, with the highest ranks (lowest values) representing the greatest importance. This approach allowed us to understand which features were most influential in different contexts, enhancing the transparency of the model.\n\nOverall, the use of random forests and the Boruta algorithm ensures that our model is not a black box. Instead, it provides clear insights into the features that drive its predictions, making it a transparent and interpretable tool for classifying different cohorts based on their microbial profiles.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized a random forest classification algorithm due to its robustness in handling imbalanced data and its interpretability. This model was trained and validated using taxonomic and functional profiles from the curated Metagenomics database to ensure consistency and reproducibility. The primary goal of the model was to distinguish between healthy individuals and those with diseases, such as Crohn’s disease or ulcerative colitis. The model's performance was evaluated using metrics like accuracy and the area under the curve (AUC), demonstrating its effectiveness in classifying health status across various cohorts. Additionally, the model's parameters and their importance were analyzed to understand the key features contributing to its predictions. The model's ability to provide informative scores for health status prediction was highlighted, particularly in comparison to other methods like hiPCA and GMHI.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our project, named q2-predict-dysbiosis, is publicly available on GitHub. The project is platform-independent and requires Python to run, specifically version 3.8 or higher. Additionally, it depends on scikit-learn version 1.1.3 or higher. The code is released under the MIT License, which permits free use, modification, and distribution.\n\nThe project's home page on GitHub provides access to the complete codebase, along with any additional data required to reproduce our analyses. This includes scripts and sample accessions necessary for running the algorithms. The GitHub repository also contains detailed instructions on how to set up the environment and execute the code.\n\nFor those who prefer not to run the code locally, an archival copy of the software is available via Software Heritage. This ensures that the code can be accessed and verified even if the original repository becomes unavailable.\n\nFurthermore, the project has been deposited as a GigaDB dataset, which includes all the necessary files and documentation to replicate our results. This dataset is accessible through the GigaScience Database.\n\nThe project is also registered with bio.tools under the ID q2-predict-dysbiosis, providing another avenue for accessing the software and related information. The bio.tools registry offers a standardized way to discover and access bioinformatics tools, ensuring that our software is easily findable and usable by the scientific community.\n\nIn summary, the source code and all necessary resources to run our algorithms are publicly available and can be accessed through GitHub, Software Heritage, and the GigaDB dataset. The software is released under the MIT License, promoting open access and collaboration.",
  "evaluation/method": "The evaluation of our method, Q2PD, involved a comprehensive and robust approach to ensure its validity and performance across various datasets. Initially, we extended our dataset to include 27 additional cohorts from different disease states, totaling 30 datasets for method validation. The parameters of Q2PD, originally determined based on healthy samples from the HMP2, remained unchanged. Instead, we retrained the model with the new data and employed a leave-one-cohort-out approach to ensure a reliable benchmark. This procedure placed our index at a disadvantaged position, as some of the added datasets had been used to develop and train other methods, making our approach truly blind to the outcomes in these new cases.\n\nDespite this disadvantage, Q2PD achieved the highest average accuracy and AUC across all datasets, outperforming other methods such as hiPCA, GMHI, and Shannon entropy-based measures. The performance was evaluated using a leave-one-cohort-out procedure with 5-fold cross-validation. The accuracy and AUC statistics were defined separately for each test cohort based on the validation set, which was a subset of the training dataset. This ensured that the model's performance was assessed in a rigorous and unbiased manner.\n\nThe relevance of each index in the context of IBD predictions was calculated using the Boruta algorithm, a wrapper around the random forest classifier. This algorithm helps in identifying the importance of each variable by comparing it to shadow variables, which are randomly permuted copies of the original variables. The variables are eventually assigned to three classes: Confirmed (better than random), Rejected (no better than random), and Tentative (those that could not be assigned to the Confirmed or Rejected class).\n\nFor the final evaluation, the index was trained on the complete set of 30 cohorts using 5-fold cross-validation. The health threshold was defined based on the mean of thresholds from the training data for each iteration of the leave-one-cohort-out approach and was set at 0.38. This threshold was consistent across all iterations, ensuring the robustness of the model.\n\nAdditionally, the importance of the features contributing to the Q2PD score was evaluated by training a random forest model separately for each cohort and using a feature permutation approach. The importances were converted to ranks, with the highest ranks representing the greatest importance. This detailed evaluation process highlights the thoroughness and rigor with which Q2PD was assessed, ensuring its reliability and effectiveness in predicting health status across various disease states.",
  "evaluation/measure": "In our evaluation, we primarily focused on two key performance metrics: accuracy and the area under the curve (AUC). These metrics were chosen for their robustness and widespread use in the literature for evaluating classification models, particularly in the context of microbiome data analysis.\n\nAccuracy provides a straightforward measure of the proportion of correctly classified instances out of the total instances. It is a simple and intuitive metric that gives an overall sense of the model's performance. However, accuracy alone can be misleading if the classes are imbalanced, which is why we also report the AUC.\n\nThe AUC is a more comprehensive metric that evaluates the model's ability to distinguish between the positive and negative classes across all possible classification thresholds. It provides a single scalar value that summarizes the trade-off between the true positive rate and the false positive rate. A higher AUC indicates better model performance.\n\nIn addition to these primary metrics, we also calculated the winning margin of our index (Q2PD) over other methods, such as hiPCA and the mean of other indices. This margin is defined as the average difference in AUC when Q2PD outperforms the other methods. This metric helps to highlight the superior performance of Q2PD in scenarios where other indices struggle.\n\nWe also investigated the variability in performance across different cohorts, revealing substantial differences in how well each index classified various types of cohorts. This analysis underscores the importance of considering cohort-specific factors when evaluating model performance.\n\nOverall, the set of metrics we reported is representative of the standards in the field, providing a clear and comprehensive evaluation of our model's performance. The inclusion of accuracy, AUC, and winning margins ensures that our evaluation is both rigorous and informative, aligning with the practices commonly found in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our method, Q2PD, against several publicly available methods using a robust benchmarking approach. We extended our dataset to include 27 additional cohorts from various disease states, resulting in a total of 30 datasets used for method validation. This expansion allowed us to rigorously test the performance of Q2PD against other established methods.\n\nWe did not alter any parameters of Q2PD, which were originally determined based on healthy samples from the HMP2. Instead, we retrained the model with the new data and employed a leave-one-cohort-out approach to ensure a robust benchmark. This procedure placed our index at a disadvantaged position, as some of the added datasets had been used to develop and train the other methods, making our approach truly blind to the outcomes in these new cases.\n\nDespite this disadvantage, Q2PD achieved the highest average accuracy and AUC across all datasets. The performance metrics were as follows: Q2PD (AUC = 0.61, accuracy = 0.58) outperformed hiPCA (AUC = 0.58, accuracy = 0.57), GMHI (AUC = 0.55, accuracy = 0.55), Shannon entropy on species (AUC = 0.52, accuracy = 0.53), and Shannon entropy on functions (AUC = 0.44, accuracy = 0.43). This comparison included both complex methods and simpler baselines, such as entropy-based measures, which consistently showed poor performance, suggesting their limited utility as predictive indices.\n\nWe also observed that Q2PD classified certain cohorts with a significantly greater margin than when it performed worse than other methods. The average winning AUC margin of Q2PD over hiPCA, the second-best classifier, was 0.19, while the losing margin to hiPCA was 0.09. The average winning AUC margin of Q2PD against the mean of the other indices was 0.20, and 0.11 if any other method was better. These differences were statistically significant, indicating a notable improvement in classification performance with our method in areas where the other indices struggled.\n\nFurthermore, we investigated the accuracy and AUC of the indices for each cohort, revealing substantial variability in the classes of cohorts that each index could classify. Some cohorts, such as Liss_2016, were better classified with function-based indices, while others, like Gupta_2019, were slightly better separated with taxonomy-based indices. This diversity in feature importance across different datasets highlights the adaptability and robustness of Q2PD in handling various types of information for classification.",
  "evaluation/confidence": "The evaluation of our method, Q2PD, included a robust statistical analysis to ensure the reliability and significance of our results. We employed a leave-one-cohort-out approach with 5-fold cross-validation to benchmark our method's performance. This procedure helped in assessing the model's generalizability and robustness across different datasets.\n\nTo determine the statistical significance of Q2PD's performance compared to other methods, we calculated the winning margins over the second-best index, hiPCA, and the mean of other methods. The average winning AUC margin of Q2PD over hiPCA was 0.19, while the losing margin was 0.09. These differences were found to be statistically significant with P-values of 0.03 and 0.02, respectively, indicating a significant improvement in classification performance.\n\nFurthermore, we observed substantial variability in the accuracy and AUC of the indices for each cohort, suggesting that different indices performed better for different types of cohorts. This variability was analyzed using the Boruta algorithm, which helped in identifying the relevance of each index in the context of IBD predictions. The Boruta algorithm works by comparing the importance of original variables with shadow variables, ensuring that the identified features are truly informative.\n\nThe performance metrics, such as accuracy and AUC, were calculated separately for each test cohort based on the validation set. This approach provided a comprehensive evaluation of Q2PD's performance across various datasets and disease states. The final health threshold was defined based on the mean of thresholds from the leave-one-cohort-out approach, ensuring consistency and reliability in the model's predictions.\n\nIn summary, the evaluation of Q2PD included rigorous statistical analysis and cross-validation techniques to ensure the confidence and significance of the results. The performance metrics were accompanied by statistical tests, and the results demonstrated that Q2PD outperformed other methods with statistically significant margins.",
  "evaluation/availability": "The raw evaluation files are not directly available for download. However, the complete code and additional data required to reproduce our analyses have been deposited as a GigaDB dataset. This dataset includes the necessary scripts and sample accessions to reproduce the results. Additionally, the Q2PD is deposited on GitHub, providing access to the sample accessions and scripts needed to reproduce our results. The DOME-ML annotations are available in the DOME registry, and an archival copy of the code is available via Software Heritage. The project is licensed under the MIT License, ensuring open access and the ability to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, provided that the original copyright and permission notice are included in all copies or substantial portions of the software."
}