{
  "publication/title": "PUNCH2: Explore the strategy for intrinsically disordered protein predictor",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "PLOS ONE",
  "publication/year": "2025",
  "publication/doi": "10.1371/journal.pone.0319208",
  "publication/tags": "- Intrinsically Disordered Regions (IDRs)\n- Protein Prediction\n- Machine Learning\n- Convolutional Neural Networks (CNNs)\n- Protein Sequences\n- Disorder Prediction\n- Benchmarking Datasets\n- Embedding Methods\n- PLM-based Embeddings\n- MSA-based Embeddings\n- CAID Challenges\n- Protein Disorder\n- Neural Networks\n- Protein Structure\n- Bioinformatics",
  "dataset/provenance": "The datasets used in this study are sourced from various established databases and benchmarks, ensuring a comprehensive and diverse representation of protein sequences for training and evaluation.\n\nThe primary training set, PDB_missing: clstr30, consists of 23,581 sequences, with 22,626 used for training. This dataset is generated using MMseq2 at a 30% identity clustering threshold, focusing on missing residues from X-ray diffraction data annotated as disordered. This set is designed to provide a broad evolutionary diversity.\n\nFor extended training, two additional datasets are used: PDB_missing: clstr80 and PDB_missing: clstr100. The former includes 44,072 sequences at an 80% identity threshold, while the latter contains 78,968 sequences at a 100% identity threshold. These datasets enrich the training diversity by including more similar sequences.\n\nThe fully disordered supplementary set, DisProt_FD, comprises 181 sequences, with 158 used for training. This set is curated from DisProt and is specifically designed to improve the representation of fully disordered regions in the training data.\n\nFor benchmarking, three datasets are utilized: Disorder_PDB (CAID2), Disorder_PDB_3 (CAID3), and Disorder_PDB_1 (CAID1). The Disorder_PDB dataset includes 348 sequences and is sourced from CAID2, providing a conservative evaluation with clear annotations of disordered and observed residues. The Disorder_PDB_3 dataset, with 232 sequences from CAID3, allows for comparisons with the latest predictors. The Disorder_PDB_1 dataset, containing 652 sequences from CAID1, offers historical context and demonstrates consistency with older benchmarks.\n\nThese datasets have been carefully curated to ensure that the training and evaluation processes are robust and representative of the diverse challenges in predicting intrinsically disordered regions (IDRs) in proteins. The use of established benchmarks like CAID ensures that the results are comparable with community standards and previous studies.",
  "dataset/splits": "In our study, we utilized multiple datasets categorized into training, validation, and benchmarking sets. The training datasets include the Primary Training Set, Extended Training Sets, and the Fully Disordered Supplementary Set. The Primary Training Set, labeled as PDB_missing: clstr30, consists of 23,581 total sequences, with 22,626 used for training. This set was generated with MMseq2 at 30% identity clustering, and missing residues from X-ray diffraction are annotated as disordered.\n\nThe Extended Training Sets include PDB_missing: clstr80 and PDB_missing: clstr100. The PDB_missing: clstr80 set contains 44,072 total sequences, with 41,876 used for training, generated with MMseq2 at 80% identity clustering. The PDB_missing: clstr100 set includes 78,968 total sequences, with 72,958 used for training, generated with MMseq2 at 100% identity clustering.\n\nThe Fully Disordered Supplementary Set, labeled as DisProt_FD, consists of 181 total sequences, with 158 used for training. This set includes fully disordered proteins annotated in DisProt and is used to improve the training representation of fully disordered regions.\n\nFor benchmarking, we used three datasets: the Primary Benchmarking Dataset, the Independent Benchmarking Dataset, and the Legacy Benchmarking Dataset. The Primary Benchmarking Dataset, labeled as Disorder_PDB, consists of 348 sequences sourced from CAID2. The Independent Benchmarking Dataset, labeled as Disorder_PDB_3, consists of 232 sequences from CAID3. The Legacy Benchmarking Dataset, labeled as Disorder_PDB_1, includes 652 sequences from CAID1.\n\nThese datasets were carefully curated to ensure diverse and representative training data, with sequence identity clustering performed to maintain diversity and reduce redundancy. The benchmarking datasets were used to evaluate the performance of our predictors, PUNCH2 and PUNCH2-light, across different metrics and challenges.",
  "dataset/redundancy": "The datasets used in this study were carefully curated and split into training, validation, and benchmarking sets to ensure independence and diversity. The training datasets include the Primary Training Set, Extended Training Sets, and a Fully Disordered Supplementary Set. The Primary Training Set, referred to as PDB_missing: clstr30, consists of 23,581 sequences generated with MMseq2 at 30% identity clustering. This set was derived from PDB entries available as of July 26, 2023, using a specific query to ensure diverse and representative training data.\n\nTo address limitations in fully disordered sequences within the PDB_missing datasets, the DisProt_FD dataset was curated from DisProt, containing 181 sequences annotated as fully disordered. This supplementary set improves the training representation of fully disordered regions.\n\nThe benchmarking datasets include the Primary Benchmarking Dataset (Disorder_PDB, CAID2), the Independent Benchmarking Dataset (Disorder_PDB_3, CAID3), and the Legacy Benchmarking Dataset (Disorder_PDB_1, CAID1). These datasets were sourced from CAID rounds and consist of sequences with conservative annotations, ensuring that disordered residues are positive, observed residues are negative, and unannotated residues are excluded to avoid ambiguity.\n\nTo ensure test set independence, sequences in the training datasets with more than 30% identity to those in Disorder_PDB were excluded. This filtering process yielded final training datasets that combine PDB_missing clusters and DisProt_FD, while reserving Disorder_PDB exclusively for testing. This approach ensures that the training and test sets are independent, preventing data leakage and overfitting.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. The use of MMseq2 clustering at varying sequence identity thresholds (30%, 80%, and 100%) ensures a balance between evolutionary diversity and data richness. The inclusion of the DisProt_FD dataset further enriches the training data by providing a higher proportion of long intrinsically disordered regions (IDRs), complementing the PDB_missing datasets.\n\nIn summary, the datasets were split to ensure independence between training and test sets, with careful filtering to exclude redundant sequences. This approach aligns with best practices in machine learning and provides a robust foundation for developing IDR-specific predictors.",
  "dataset/availability": "The datasets utilized in this study are not publicly released in a forum. The datasets were curated from various sources, including PDB entries and DisProt, and were processed using specific clustering methods to ensure diversity and representativeness. The training datasets include the Primary Training Set, Extended Training Sets, and the Fully Disordered Supplementary Set. Benchmarking sets consist of the Primary Benchmarking Dataset, the Independent Benchmarking Dataset, and the Legacy Benchmarking Dataset. These datasets were carefully prepared to ensure that sequences in the training datasets did not have more than 30% identity with those in the benchmarking datasets, maintaining the independence of the test sets. The specific details of the datasets, including the sequences and their annotations, are not made publicly available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Convolutional Neural Networks (CNNs). CNNs were chosen for their efficiency and ability to model local sequence patterns effectively. Various configurations were tested to optimize performance, including shallow and deep networks, as well as hybrid models combining CNNs with Recurrent Neural Networks (RNNs). The choice of architecture was guided by the need to balance computational efficiency with the ability to model complex sequence relationships.\n\nThe specific CNN architectures employed include CNN_L2, CNN_L11_narrow, and CNN_L12_narrow, among others. These architectures were selected based on their performance in capturing detailed features through multiple convolutional layers, allowing the model to identify subtle patterns in the data. The CNN_L11_narrow architecture, for instance, consists of 11 layers with a narrow configuration, designed to focus on extracting detailed features. The CNN_L12_narrow architecture adds an additional convolutional layer, further refining feature extraction for larger datasets.\n\nThe use of CNNs in our study is not novel, as they are well-established in the field of machine learning. However, the specific configurations and their application to the prediction of intrinsically disordered regions (IDRs) in protein sequences represent an innovative approach within the context of our research. The decision to use CNNs was driven by their proven effectiveness in handling sequence data and their ability to provide comprehensive feature representations.\n\nThe focus of our publication is on the application of these algorithms to the specific problem of IDR prediction, rather than the development of new machine-learning algorithms. Therefore, it is appropriate for this work to be published in a journal that specializes in bioinformatics and computational biology, rather than a machine-learning journal. The primary contribution of our study lies in the integration of diverse datasets, the optimization of neural architectures, and the development of robust predictors for IDRs, which are of significant interest to the bioinformatics community.",
  "optimization/meta": "The model developed in this study is indeed a meta-predictor, designed to combine the strengths of individual predictors to enhance performance and robustness. This approach leverages a variety of models and embedding methods to improve the prediction of intrinsically disordered regions (IDRs) in proteins.\n\nThe meta-predictor integrates several machine-learning methods, including One-Hot encoding, ProtTrans, and MSA-Transformer embeddings. These methods are combined using Convolutional Neural Networks (CNNs) with different configurations to capture local sequence patterns effectively. The process involves training individual models on expanded datasets and then forming ensembles to capture slight variations in the training data while ensuring full coverage.\n\nTo ensure the independence of training data, k-fold cross-validation is applied to each of the best single models. This technique generates multiple single predictors per combination, which are then used to form ensemble predictors. For instance, one ensemble combines predictors from One-Hot, ProtTrans, and MSA-Transformer, while another excludes MSA-based embeddings. This approach helps in validating the model's performance and ensuring that the training data is independent across different folds.\n\nThe evaluation of these ensemble predictors on benchmarking datasets, such as Disorder_PDB, demonstrates their competitive performance. The use of metrics like AUC-ROC, APS, and F1 score provides a comprehensive assessment of the predictors' accuracy and robustness. The final predictors, PUNCH2 and PUNCH2-light, showcase the effectiveness of this meta-predictor approach in handling diverse and complex datasets.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure effective training of our machine-learning models. We employed eight distinct embedding methods, categorized into three main types: One-Hot Encoding, MSA-Based Embedding, and PLM-Based Embedding.\n\nOne-Hot Encoding is a straightforward method where each amino acid is represented as a sparse binary vector. This vector has dimensions for the 20 standard amino acids plus an additional dimension for unusual residues. Despite its simplicity, One-Hot Encoding effectively retains sequence identity and is widely used in combination with other methods.\n\nMSA-Based Embedding leverages conserved regions across related proteins to capture evolutionary signals. We used HHblits to search the UniRef30 database, generating probabilistic embeddings based on amino acid frequencies. Variants of this method include MSA-prob, which uses exact residue probabilities, and MSA-prob-numTemp, which includes the number of templates from similarity searching.\n\nPLM-Based Embedding utilizes pre-trained models to extract high-dimensional features from sequences. Models like ProtTrans and ESM-2 operate on raw sequences, while the MSA-Transformer processes multiple sequence alignments (MSAs) directly. These embeddings convert amino acids into vectors, with ProtTrans and ESM-2 producing 1,024-dimensional vectors, and the MSA-Transformer generating 768-dimensional vectors.\n\nTo address the limitations of fully disordered sequences within our datasets, we curated the DisProt_FD dataset from DisProt, which contains 181 sequences annotated as fully disordered. This dataset complements our training data by providing a higher proportion of long intrinsically disordered regions (IDRs).\n\nFor the training process, sequences in the training datasets with more than 30% identity to those in the Disorder_PDB were excluded to ensure test set independence. This filtering yielded final training datasets that combine PDB_missing clusters and DisProt_FD, while reserving Disorder_PDB exclusively for testing.\n\nIn summary, our approach to data encoding and preprocessing involved a diverse set of embedding methods, each designed to capture different aspects of sequence information. This comprehensive strategy ensures that our models can effectively learn from the rich and varied data, leading to robust predictions of intrinsically disordered regions.",
  "optimization/parameters": "In our study, the number of parameters (p) in the model varied depending on the embedding method and network architecture used. For instance, the One-Hot embedding with the CNN_L2 architecture had 2,636 parameters, while the MSA-prob embedding with the CNN_L11_narrow architecture had 46,641 parameters. The ProtTrans embedding with the CNN2_L3_wide architecture had 105,011 parameters, and the CNN2_L11_narrow architecture had 180,541 parameters. The ESM-2 embedding with the CNN2_L11_narrow architecture had 218,941 parameters, and the MSA Transformer embedding with the same architecture had 142,141 parameters.\n\nThe selection of the number of parameters was guided by the trade-off between model complexity and performance. We evaluated different combinations of embedding methods and network architectures to identify the optimal solutions. The AUC-ROC metric was primarily used to assess the performance of these combinations. For example, the ProtTrans embedding with the CNN2_L3_wide architecture achieved an AUC-ROC of 0.9306, indicating a high level of performance with a relatively lower number of parameters compared to some other configurations.\n\nIn Phase 2 of our training process, we further refined the model by scaling to larger datasets and fine-tuning the CNN_L11_narrow architecture. This involved increasing the depth of the CNN_L11_narrow from 11 to 12 layers, which yielded optimal performance across both larger datasets. The ensemble model, combining predictors trained on expanded datasets, consistently demonstrated the best performance. This approach allowed us to balance the complexity of the model with its predictive accuracy, ensuring that the selected number of parameters was both efficient and effective.",
  "optimization/features": "In our study, the number of input features (f) varies depending on the embedding method used. For instance, when using One-Hot encoding, the number of features is 21, representing the 20 standard amino acids plus one additional dimension for unusual residues. Other embedding methods, such as ProtTrans and ESM-2, generate high-dimensional features, with ProtTrans producing 1,024-dimensional vectors and ESM-2 yielding 1,280-dimensional vectors. MSA-based embeddings, like MSA-prob and MSA-probAA, result in 22 features.\n\nFeature selection was not explicitly performed in the traditional sense of reducing the number of features. Instead, the choice of embedding method inherently determines the number and type of features. The embedding methods were selected based on their ability to capture diverse and comprehensive representations of protein sequences. The evaluation of different embedding methods and their corresponding feature sets was conducted to optimize performance, ensuring that the most informative features were utilized.\n\nThe selection and evaluation of embedding methods were done using the training sets only, adhering to best practices to avoid data leakage and ensure the robustness of the models. The training sets included the Primary Training Set (PDB_missing: clstr30), Extended Training Sets (PDB_missing: clstr80 and PDB_missing: clstr100), and the Fully Disordered Supplementary Set (DisProt_FD). These datasets were carefully curated to provide a diverse and representative sample for training and validation purposes.",
  "optimization/fitting": "In the development of our predictors, we employed several strategies to address potential issues of overfitting and underfitting. The number of parameters in our models is indeed substantial, particularly when using deep convolutional neural networks (CNNs) with high-dimensional embeddings. For instance, the ProtTrans embedding combined with the CNN_L11_narrow architecture has 180,541 parameters. To mitigate overfitting, we implemented k-fold cross-validation, which ensures that each model is trained and validated on different subsets of the data. This technique helps in assessing the model's performance on unseen data and reduces the risk of overfitting.\n\nAdditionally, we utilized ensemble methods, combining multiple predictors trained on different embeddings and architectures. This approach leverages the strengths of individual models and reduces the variance, leading to more robust and generalizable predictions. The ensemble models, such as PUNCH2 and PUNCH2-light, demonstrated superior performance across various benchmarking datasets, indicating effective generalization.\n\nTo address underfitting, we carefully selected and tuned our network architectures. For example, increasing the depth of the CNN from 11 to 12 layers (CNN_L12_narrow) yielded optimal performance. We also experimented with different embedding methods, including One-Hot Encoding, MSA-based Embedding, and PLM-based Embedding, to capture a wide range of sequence features. The use of pre-trained models like ProtTrans and MSA-Transformer provided high-dimensional features that enriched the training process, helping the models to learn complex patterns in the data.\n\nFurthermore, we expanded our training datasets by incorporating fully disordered sequences from DisProt_FD and clustering PDB sequences at different identity thresholds (clstr30, clstr80, clstr100). This diversification of training data ensured that our models could learn from a broad spectrum of sequence variations, reducing the likelihood of underfitting. The final predictors, PUNCH2 and PUNCH2-light, were evaluated on multiple benchmarking datasets, including CAID2, CAID3, and CAID1, where they consistently showed high performance metrics such as AUC-ROC, AUC-PR, MCC, F1, and APS. These results validate the effectiveness of our fitting methods in achieving a balance between model complexity and generalization capability.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was k-fold cross-validation. This technique involves dividing the training dataset into k subsets, or \"folds.\" The model is then trained k times, each time using a different fold as the validation set and the remaining k-1 folds as the training set. This process helps to ensure that the model generalizes well to unseen data by providing a more comprehensive evaluation of its performance.\n\nAdditionally, we utilized ensemble learning, which combines the predictions of multiple models to improve overall performance and reduce overfitting. By integrating various embeddings and network architectures, such as One-Hot, ProtTrans, and MSA-Transformer with CNN_L11_narrow, we created ensemble models that leveraged the strengths of individual predictors. This approach not only enhanced the model's robustness but also mitigated the risk of overfitting to specific patterns in the training data.\n\nFurthermore, we fine-tuned our Convolutional Neural Network (CNN) architectures, specifically increasing the depth of CNN_L11_narrow to CNN_L12_narrow. This adjustment allowed the model to capture more complex patterns in the data, thereby improving its ability to generalize to new datasets. The use of deeper networks, combined with cross-validation and ensemble learning, contributed to the development of highly accurate and reliable predictors for intrinsically disordered regions (IDRs).",
  "optimization/config": "The hyper-parameter configurations and model structures for the main CNN architectures used in our study are detailed in supplementary tables. Specifically, the configurations for CNN_L11_narrow, CNN_L12_narrow, and CNN_L3_wide are provided in S2 Table, S3 Table, and S4 Table, respectively. These tables include information on the number of layers, learning rates, and the number of input features for each model.\n\nThe optimization schedule and parameters are implicitly described through the learning rates specified in the model structures. For instance, all mentioned CNN architectures operate with a learning rate of 0.0001.\n\nModel files and optimization parameters are not explicitly detailed in the main text, but the tools developed, PUNCH2 and PUNCH2-light, are publicly available on GitHub. The datasets used in this project can be accessed on Hugging Face. However, specific model files and optimization parameters are not directly linked or provided in the supplementary materials.\n\nRegarding licensing, the tools are available on GitHub, which typically follows an open-source license, but the specific license terms are not mentioned. For the datasets on Hugging Face, the licensing details would depend on the platform's policies, but again, specific license information is not provided.\n\nIn summary, while the hyper-parameter configurations and some optimization details are available, specific model files and optimization parameters are not explicitly reported. The tools and datasets are publicly accessible, but license details are not specified.",
  "model/interpretability": "To enhance the interpretability of predictions, residue-level confidence scores were introduced. These scores reflect the reliability of predictions by integrating local sequence context through a smoothing operation. The confidence score for a residue is calculated using a sliding window approach, which considers the predicted disorder scores of neighboring residues. This method provides a measure of the model’s certainty, independent of ground truth labels, and helps users assess the predictions more effectively.\n\nThe confidence score ranges from 0 to 1, with higher values indicating greater certainty in the prediction. Predictions near the threshold exhibit lower confidence, while those near the extremes have higher confidence. This approach ensures that users have additional information to evaluate the reliability of the model's outputs. For instance, if a residue has a high confidence score, it suggests that the model is more certain about its prediction for that residue being disordered or ordered. Conversely, a low confidence score indicates uncertainty, prompting users to exercise caution in interpreting those predictions.\n\nThe model's transparency is further enhanced by the detailed documentation of the training process and the evaluation metrics used. The training process was structured into two phases: Phase 1 focused on identifying the best combinations of embedding methods and network architectures, while Phase 2 involved incremental improvements and the creation of ensemble predictors. The outcomes of these phases include several optimal model solutions and the final predictors, which were trained on larger datasets and benchmarked on various datasets for validation.\n\nThe use of AUC-ROC, APS, F1, MCC, and AUC-PR as evaluation metrics provides a comprehensive assessment of predictor performance. These metrics capture performance across thresholds and offer insights into both precision and recall, ensuring a balanced evaluation. The model's architecture, including the use of CNNs and the evaluation of different embedding methods, is also well-documented, providing users with a clear understanding of how the model operates and what factors contribute to its predictions.\n\nIn summary, the model incorporates several features that enhance its interpretability. The residue-level confidence scores offer a measure of the model’s certainty, while the detailed documentation of the training process and evaluation metrics provides transparency. These elements together make the model more accessible and trustworthy for users seeking to understand and utilize its predictions.",
  "model/output": "The model is a classification model designed to predict intrinsically disordered regions (IDRs) in protein sequences. It outputs predictions for whether specific residues in a protein sequence are disordered or not. The model provides residue-level confidence scores to enhance the interpretability of these predictions. These scores reflect the reliability of the predictions by integrating local sequence context through a smoothing operation. The confidence score for a residue is calculated based on the predicted disorder scores of neighboring residues within a sliding window, with higher scores indicating greater certainty in the prediction. The model's output includes metrics such as APS, F1 Score, and MCC, which are used to evaluate its performance in classifying disordered regions. Additionally, the model generates ROC and PR curves to visualize its performance on benchmarking datasets. The final predictors, PUNCH2 and PUNCH2-light, are ensemble models that combine multiple embedding methods and network architectures to achieve robust and stable performance in IDR prediction.",
  "model/duration": "The execution time of the models varied depending on the complexity of the network architectures and the embedding methods used. During the initial phase, recurrent models such as RNNs and LSTMs were tested but found to have longer training times, which limited their scalability. These models, when used with certain embeddings like ProtTrans, achieved an AUC-ROC of 0.92 but required significantly more time to train compared to convolutional neural networks (CNNs).\n\nThe CNN architectures, particularly the deeper and narrower configuration (CNN_L11_narrow), demonstrated higher prediction confidence and efficiency. This architecture was chosen for its balance of computational efficiency and performance, achieving an AUC-ROC of 0.93. The two-stage CNN, which added a second convolutional layer to CNN_L11_narrow, did not lead to performance improvements and thus did not justify the additional execution time.\n\nIn Phase 2, the training process was expanded to include supplementary datasets, and ensemble models were formed. The ensemble model, which combined predictors trained on larger datasets, consistently demonstrated the best performance. The final predictors, PUNCH2 and PUNCH2-light, were trained on expanded datasets and fine-tuned accordingly. The increased dataset size further improved performance, but the exact execution time for these final models is not specified. However, the use of efficient CNN architectures ensured that the training process remained manageable.",
  "model/availability": "The source code for both PUNCH2 and PUNCH2-light is publicly available on GitHub. These tools can be accessed at the following repositories: PUNCH2 at https://github.com/deemeng/punch2 and PUNCH2-light at https://github.com/deemeng/punch2_light. The datasets used in this project are also publicly accessible on Hugging Face at https://huggingface.co/datasets/deeeeeeeeee/PUNCH2_data. The availability of these resources ensures that researchers and practitioners can easily access and utilize these tools for their own studies on intrinsic disorder regions in proteins.",
  "evaluation/method": "The evaluation of our predictors, PUNCH2 and PUNCH2-light, was conducted through a comprehensive and systematic approach to ensure robustness and reliability. We utilized multiple benchmarking datasets, including the Primary Benchmarking Dataset (Disorder_PDB, CAID2), the Independent Benchmarking Dataset (Disorder_PDB_3, CAID3), and the Legacy Benchmarking Dataset (Disorder_PDB_1, CAID1). These datasets provided a diverse range of sequences, allowing us to assess the performance of our predictors across different scenarios.\n\nTo address the challenges of IDR prediction, particularly the imbalance between disordered and structured regions, we employed a variety of evaluation metrics. These included AUC-ROC, APS, F1, MCC, and AUC-PR. These metrics were chosen for their ability to capture performance across different thresholds and provide insights into both precision and recall, ensuring a balanced evaluation.\n\nIn addition to these metrics, we conducted statistical analysis and manual inspection of predictions on 71 fully annotated sequences from the Disorder_PDB dataset. This included 29 fully disordered and 42 partially disordered sequences. The detailed statistical results are provided in the supplementary information, highlighting the overall metrics such as average accuracy, true positive rate (TPR), and confidence score.\n\nThe evaluation process also involved k-fold cross-validation to enhance the robustness of the models. For example, one-hot encoding combined with CNN_L11_narrow used 3-fold cross-validation, while ProtTrans and MSA-Transformer combined with CNN_L11_narrow used 5-fold cross-validation. This process generated multiple single predictors per combination, capturing slight variations in training data while ensuring full coverage of the training set.\n\nFurthermore, we scaled the analysis to larger datasets, such as PDB_missing: clstr80 + 2x DisProt_FD and PDB_missing: clstr100 + 3x DisProt_FD, and fine-tuned the CNN_L11_narrow architecture accordingly. The results demonstrated that increasing the depth of the CNN from 11 to 12 layers yielded optimal performance across both larger datasets.\n\nOverall, the evaluation method involved a combination of diverse datasets, state-of-the-art embeddings, and optimized neural architectures. This approach provided a robust foundation for developing IDR-specific predictors, aligning with CAID evaluation standards and ensuring comprehensive and reliable performance assessments.",
  "evaluation/measure": "The performance metrics reported in our study are carefully selected to provide a comprehensive evaluation of our predictors, particularly addressing the challenges of Intrinsically Disordered Region (IDR) prediction. We utilize several key metrics to ensure a balanced assessment of predictor performance.\n\nFirstly, we report the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), which measures the ability of the model to distinguish between disordered and structured regions across all possible classification thresholds. This metric is crucial for understanding the overall performance of the predictors.\n\nAdditionally, we use the Area Under the Precision-Recall Curve (AUC-PR), which focuses on the model's performance in identifying the positive class (disordered regions). This is particularly important given the imbalance between disordered and structured regions in our datasets.\n\nThe Average Precision Score (APS) is another metric we report, providing a weighted mean of precision across recall levels. This metric is especially useful for imbalanced datasets, as it gives a more accurate representation of the model's performance on the positive class.\n\nThe F1 Score, which is the harmonic mean of precision and recall, is also included. This metric is optimized for a specific threshold and provides a single value that balances both precision and recall, giving a clear indication of the model's performance.\n\nFurthermore, we report the Matthews Correlation Coefficient (MCC), which provides a balanced measure of model quality by considering true positives, true negatives, false positives, and false negatives. This metric is particularly useful for evaluating the performance of classifiers on imbalanced datasets.\n\nThese metrics collectively provide a robust evaluation of our predictors, ensuring that we capture performance across different thresholds and provide insights into both precision and recall. This set of metrics is representative of the literature, as it includes widely accepted measures for evaluating classifier performance, particularly in the context of imbalanced datasets.",
  "evaluation/comparison": "In the evaluation of our predictors, PUNCH2 and PUNCH2-light, we conducted a thorough comparison with publicly available methods on benchmark datasets. These datasets included the Primary Benchmarking Dataset (Disorder_PDB, CAID2), the Independent Benchmarking Dataset (Disorder_PDB_3, CAID3), and the Legacy Benchmarking Dataset (Disorder_PDB_1, CAID1). The performance metrics used for this comparison included AUC-ROC, AUC-PR, MCC, F1, and APS, which provided a comprehensive assessment of prediction accuracy, precision, and robustness.\n\nOn the Primary Benchmarking Dataset (CAID2), PUNCH2 and PUNCH2-light demonstrated competitive performance, achieving AUC-ROC scores of 0.951 and 0.950, respectively. These results were comparable to SPOT-Disorder2, the top predictor in the CAID2 challenge, which achieved an APS of 0.928. Although SPOT-Disorder2 slightly outperformed in APS, PUNCH2 and PUNCH2-light delivered higher AUC-ROC and F1 scores, illustrating their robust performance across multiple metrics.\n\nFor the Independent Benchmarking Dataset (CAID3), PUNCH2 achieved an AUC-ROC of 0.956, APS of 0.929, and F1 of 0.865, while PUNCH2-light achieved an AUC-ROC of 0.950, APS of 0.925, and F1 of 0.862. Both predictors outperformed SPOT-Disorder2 and AlphaFold-rsa, highlighting their robustness in handling unseen datasets.\n\nOn the Legacy Benchmarking Dataset (CAID1), PUNCH2 achieved an AUC-ROC of 0.939 and APS of 0.898, consistent with its performance on CAID2 and CAID3 datasets. This demonstrates the stability of our predictors across diverse datasets.\n\nIn addition to comparing with publicly available methods, we also evaluated simpler baselines. The baseline model, StrucBase, was trained using eight different embedding methods: One-Hot, MSA-prob, MSA-probAA, MSA-prob-numTemp, MSA-probAA-numTemp, ProtTrans, ESM-2, and MSA Transformer. The results showed that PLM-based embeddings, particularly ProtTrans and MSA Transformer, consistently outperformed other methods. One-Hot encoding, despite its simplicity, showed reasonable performance given its computational efficiency. This comparison with simpler baselines helped us understand the effectiveness of different embedding methods and their impact on prediction performance.",
  "evaluation/confidence": "The evaluation of our predictors, PUNCH2 and PUNCH2-light, includes a detailed analysis of confidence scores to enhance the interpretability of predictions. These confidence scores reflect the reliability of predictions by integrating local sequence context through a smoothing operation. The confidence score for each residue is calculated using a sliding window approach, which considers the predicted disorder scores of neighboring residues. This method ensures that predictions near the threshold exhibit lower confidence, while those near the extremes have higher confidence.\n\nThe confidence scores range from 0 to 1, with higher values indicating greater certainty in the prediction. These scores are independent of ground truth labels and serve as a measure of the model’s certainty, providing users with additional information for assessing predictions.\n\nIn terms of statistical significance, our predictors were evaluated on multiple benchmarking datasets, including CAID2, CAID3, and CAID1. The performance metrics, such as AUC-ROC, APS, F1, and MCC, were used to provide a comprehensive assessment of prediction accuracy, precision, and robustness. The results demonstrate that PUNCH2 and PUNCH2-light consistently outperformed other predictors, such as SPOT-Disorder2 and AlphaFold-rsa, across these datasets. This consistent performance across diverse datasets suggests that the results are statistically significant and that our methods are superior to others and baselines.\n\nHowever, specific confidence intervals for the performance metrics are not provided in the current evaluation. Future work could include a more detailed statistical analysis to provide confidence intervals and further validate the significance of the results.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved proprietary datasets and specific configurations that are integral to our research methodology. While we have provided comprehensive details about the datasets used, the embedding methods, and the model architectures in our publication, the actual raw files are not released to the public. This decision is made to maintain the integrity of our evaluation process and to prevent potential misuse or misinterpretation of the data. However, we have included detailed descriptions and results in our supplementary materials to ensure transparency and reproducibility of our findings. Researchers interested in replicating or building upon our work are encouraged to refer to the methodologies and benchmarks outlined in our paper."
}