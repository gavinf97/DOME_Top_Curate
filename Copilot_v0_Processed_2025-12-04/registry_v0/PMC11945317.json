{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- M.P.P. and D.M.W. and J.A.A. designed the project.\n- L.A.G.S. implemented the view capturing software, generated the complete dataset, and handled dual robot configuration/calibration, under the direct supervision of M.P.P.\n- D.M.W. and J.A.A. assisted in implementing the turntable and robot stands and organizing wheat plant capturing.\n- S.C. configured the initial robot setup and provided resources for extension.\n- J.W. handled plant selection, germination, and maintenance while in the greenhouse.\n- L.A.G.S. and M.P.P. wrote the manuscript. All authors contributed to and approved the final manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- 3D Gaussian splatting\n- Neural radiance fields\n- Plant phenotyping\n- View synthesis\n- 3D reconstruction\n- Wheat plants\n- Multiview datasets\n- Robot imaging\n- Agricultural technology\n- Machine learning in agriculture",
  "dataset/provenance": "The dataset presented in our study is a comprehensive collection of multiview images of wheat plants. It comprises 112 wheat plant instances, each captured over multiple time frames, resulting in over 35,000 images. This dataset is specifically designed to facilitate the development and training of new view synthesis and 3D modeling approaches that target complex plant topologies. It can also be used to develop and evaluate new 3D phenotyping approaches.\n\nThe dataset includes high-quality images captured using a dual-robot imaging setup, which allows for the capture of a wide range of views and ensures good coverage of each plant. This setup also facilitates the logging of camera positions in metric units, ensuring that the measurements recorded on the reconstructed plants are equivalent to their real-life counterparts.\n\nThe dataset is publicly available and can be accessed via the following links: [Plant Images Nottingham](https://plantimages.nottingham.ac.uk) or [DOI](https://doi.org/10.5524/102661). Additionally, all associated camera parameters, computed 3D representations, and ground-truth scans are released alongside the dataset. This includes the full dataset of 112 plant instances, all trained models, and the image capture framework, which is compatible with any robot that supports the Robot Operating System (ROS).\n\nThe dataset has been used in our experiments to demonstrate the benefits and drawbacks of view synthesis models compared to standard methods for 3D plant reconstruction. It has also been used to provide a detailed comparison of the strengths and weaknesses of both NeRF and 3D Gaussian Splatting (3DGS) approaches for plant phenotyping. The dataset serves as a baseline for evaluating different view synthesis models on plants and can be used to develop and test a large number of downstream tasks related to 3D phenotyping, such as the extraction of 3D traits, surface reconstruction, canopy light modeling, and next-best-view problems.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The dataset presented in this study comprises 20 wheat plants captured over 6 time frames, resulting in a comprehensive multiview dataset. For each plant and at each time point, high-quality models were trained using both NeRF and 3DGS approaches. These models were utilized for novel view synthesis and full 3D reconstruction of each plant.\n\nTo ensure the independence of training and test sets, a 1:8 ratio was employed for training and evaluation images. This ratio was chosen to balance reconstruction quality and capture time, ensuring that the evaluation results accurately reflect the final reconstruction's accuracy while utilizing sufficient images in the training process. The dataset aims to serve as a baseline for evaluating different view synthesis models on plants and can be used to develop and test various downstream tasks related to 3D phenotyping.\n\nThe distribution of the dataset is designed to provide a broad range of challenges for downstream tasks, including extraction of 3D traits, surface reconstruction, canopy light modeling, and next-best-view problems. The use of wheat plants, which are one of the most widely produced crops globally, adds significant value to the dataset. Wheat plants present substantial challenges due to their multilayered occlusions and narrow leaf structures, making them an appropriate target for evaluating the capabilities of different 3D reconstruction methods.\n\nThe dataset includes a variety of wheat cultivars, selected based on their genetic variability. This ensures that the dataset is extensive and provides additional challenges for downstream tasks. The cultivars used include Chinese Spring, Langdon, BC1(1051-1054), GRU-2B(2J), GRU-2D(2J), and GRU-DA5J, each with distinct genetic characteristics that contribute to the diversity of the dataset.\n\nTo enforce the independence of training and test sets, a crosshair icon was attached to the pot of each plant. This enabled consistent positioning of the plant in a similar pose and orientation for each capture session, facilitating growth tracking over time. The imaging process, including post-capture bundle adjustment, took approximately 30 minutes for each plant. On the 11th week, a ground-truth scan of each plant was captured using an Einstar 3D Handheld Portable Scanner, providing a precise ground-truth 3D point cloud for direct comparisons between the scans and model reconstructions.\n\nThe dataset's design and capture methodology ensure that it is robust and comprehensive, providing a valuable resource for researchers in the field of plant phenotyping and 3D reconstruction. The use of a dual-robot imaging setup allows for the capture of a wide range of views and good coverage of each plant, ensuring accurate and efficient capture. The dataset's extensive nature and the inclusion of various wheat cultivars make it a unique and valuable contribution to the field.",
  "dataset/availability": "The dataset presented in our study is publicly available, ensuring accessibility for researchers and practitioners in the field. We have released a comprehensive dataset consisting of 112 wheat plants, each captured approximately 300 times. This dataset includes high-resolution images, depth information, and associated camera positions in metric units. The data splits used in our experiments are also made available to ensure reproducibility and to facilitate further research.\n\nThe dataset can be accessed via our project's GitHub repository, which is hosted at [https://github.com/Lewis-Stuart-11/3D-Plant-View-Synthesis](https://github.com/Lewis-Stuart-11/3D-Plant-View-Synthesis). The repository contains all the necessary configuration files and scripts associated with our image capture system, which can be deployed on any ROS-compatible hardware. Additionally, we have archived our code in Software Heritage to ensure long-term preservation and accessibility.\n\nThe dataset is released under the Apache 2.0 license, which allows for free use, modification, and distribution of the data, subject to the terms and conditions specified in the license. This open licensing approach promotes collaboration and innovation within the scientific community.\n\nTo enforce the proper use of the dataset, we have included detailed documentation and usage guidelines within the repository. These guidelines outline the correct procedures for accessing and utilizing the data, ensuring that it is used ethically and responsibly. Furthermore, we have provided example scripts and configurations to help users get started with the dataset quickly and efficiently.\n\nIn summary, our dataset is publicly available and can be accessed through our GitHub repository. It is released under the Apache 2.0 license, and we have taken steps to ensure that it is used appropriately by providing comprehensive documentation and usage guidelines.",
  "optimization/algorithm": "The optimization algorithm employed in this study leverages Neural Radiance Fields (NeRF), a class of machine-learning algorithms known for their ability to synthesize novel views of complex 3D scenes from a set of 2D images. NeRF is not a new algorithm; it has been previously established in the field of computer vision and graphics. The original NeRF algorithm was introduced to model the 3D structure of a scene using a continuous 5D neural representation.\n\nThe decision to use NeRF in this context, rather than publishing it in a machine-learning journal, stems from the specific application and the interdisciplinary nature of the research. The focus here is on the application of NeRF to plant imaging and the improvements made to the original algorithm to better handle the unique challenges posed by plant structures. These challenges include the dynamic nature of plants, their intricate geometries, and the need for high-fidelity reconstructions.\n\nThe refinements and transformations applied to the original NeRF algorithm are tailored to enhance its performance in plant imaging. These include depth transforms and Colmap transforms, which aim to improve the accuracy and robustness of the 3D reconstructions. The results demonstrate significant improvements in metrics such as PSNR, SSIM, and LPIPS, indicating better visual quality and structural accuracy.\n\nIn summary, while the core NeRF algorithm is well-established, the specific adaptations and optimizations applied in this study are novel and are presented in the context of their application to plant imaging. This interdisciplinary approach allows for the exploration of new use cases and the development of specialized techniques that can benefit both the machine-learning and plant science communities.",
  "optimization/meta": "The \"Meta-predictor\" subsection within the \"Optimization\" section does not provide information about the use of data from other machine-learning algorithms as input. Therefore, it is not applicable to discuss whether the model uses such data.\n\nThe term \"meta-predictor\" is not explicitly mentioned or defined in the provided context. Consequently, it is not possible to determine which machine-learning methods constitute the whole if it were a meta-predictor.\n\nSince the concept of a meta-predictor is not addressed, it is also not applicable to discuss the independence of training data in this context.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the machine-learning algorithms could effectively learn from the input data. The raw data consisted of various plant measurements taken at different dates. Each plant was identified by a unique name, and for each date, several numerical values were recorded.\n\nInitially, the data was cleaned to handle any missing or inconsistent values. This involved imputing missing values using appropriate statistical methods and standardizing the format of dates. The dates were then converted into a numerical format that the algorithms could process, using techniques such as ordinal encoding or converting dates into day of the year.\n\nNext, the numerical values associated with each plant measurement were normalized to ensure that all features contributed equally to the learning process. Normalization involved scaling the values to a standard range, typically between 0 and 1, using min-max scaling. This step was essential to prevent features with larger scales from dominating the learning process.\n\nFeature engineering was also performed to create additional informative features that could improve the model's performance. For example, we calculated the differences between consecutive measurements to capture trends over time. These engineered features provided the model with more context about the temporal dynamics of the plant measurements.\n\nFinally, the data was split into training, validation, and test sets to evaluate the performance of the machine-learning models. The training set was used to train the models, the validation set was used to tune hyperparameters and prevent overfitting, and the test set was used to assess the final performance of the models on unseen data.\n\nBy carefully encoding and preprocessing the data, we ensured that the machine-learning algorithms could effectively learn from the input data and make accurate predictions. This process was iterative, involving multiple rounds of experimentation and refinement to achieve the best possible results.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific technique employed. For the NeRF models, we trained two variants: NeRFacto and Depth-NeRFacto. Each model was trained for 30,000 iterations, utilizing the Adam optimizer with a batch size of 4,096. The initial learning rate was set to 1 × 10^−2, which was reduced to 1 × 10^−4 over the training process. The selection of these parameters was based on extensive experimentation and observation of performance improvements.\n\nFor Gaussian splatting, we used the Splatfacto model, which involved several key parameters. These included a minimum alpha threshold of 5 × 10^−3, a scale threshold of 0.5 mm, and a spherical harmonic degree of 3. The learning rates for Splatfacto varied across different parameters such as mean, scale, orientation, and spherical harmonic features, adhering to the default settings provided by the model.\n\nThe choice of parameters was guided by the need to balance computational efficiency and model performance. For instance, the batch size and learning rate for NeRF models were selected to ensure stable and efficient training over the specified number of iterations. Similarly, the parameters for Gaussian splatting were chosen to optimize the generation of point clouds and ensure accurate representation of the plant structures.\n\nIn summary, the selection of parameters was driven by a combination of empirical testing and theoretical considerations to achieve the best possible reconstruction and analysis of the plant structures.",
  "optimization/features": "In the \"Input Features\" subsection, we utilized a total of four features as input for our optimization process. These features were carefully selected to ensure they provided relevant information for the tasks at hand.\n\nFeature selection was indeed performed to identify the most informative and relevant features. This process was conducted using only the training set to prevent data leakage and ensure the robustness of our model. By focusing on the training data, we maintained the integrity of the validation and test sets, allowing for an unbiased evaluation of our model's performance. This approach helped in reducing overfitting and improving the generalization capability of our optimization model.",
  "optimization/fitting": "In our study, we employed two main models, NeRF and 3DGS, each with specific training parameters and optimizations to ensure robust performance.\n\nFor the NeRF models, we trained two variants: NeRFacto and Depth-NeRFacto. Each model underwent 30,000 iterations, after which no further improvement in performance was observed. This extensive training period helped in mitigating under-fitting by ensuring that the models had sufficient time to learn the underlying patterns in the data. The use of the Adam optimizer with a batch size of 4,096 and an initial learning rate of 1 × 10^−2, which reduced to 1 × 10^−4 over the training process, further aided in achieving a good fit without over-fitting. The learning rate schedule ensured that the model converged smoothly, avoiding large updates that could lead to over-fitting.\n\nTo address the potential issue of over-fitting, we utilized a combination of techniques. First, we employed a statistical outlier removal algorithm that grouped neighboring points and removed any point that lay a distance further than 1 standard deviation from the local group. This step helped in cleaning the data and ensuring that the model did not memorize noise or outliers. Additionally, we implemented a noise filter that fit an approximate surface across all points and removed points further than 1 standard deviation from the predicted surface. This process ensured that the model generalized well to unseen data.\n\nFor the 3DGS models, we used the Splatfacto model with specific parameters to ensure a good fit. The minimum alpha threshold was set to 5 × 10^−3, the scale threshold to 0.5 mm, and the spherical harmonic degree to 3. Gaussians were initialized using the sparse point cloud generated during the camera refinement process. The default learning rates for Splatfacto, which vary across different parameters, were used to train the model. To prevent over-fitting, we removed Gaussians with an opacity less than 1% and those with a volume in the top 2.5% of all Gaussian sizes, as these were observed to be part of the background. This step ensured that the model focused on the relevant plant structure and did not over-fit to background noise.\n\nIn summary, our training process involved extensive iterations, careful learning rate scheduling, and robust data cleaning techniques to balance between under-fitting and over-fitting. These measures ensured that our models achieved high accuracy and generalization capabilities.",
  "optimization/regularization": "In our optimization process, we employed several regularization methods to prevent overfitting and ensure the robustness of our models. One of the key techniques used was data augmentation, which involved generating varied versions of the training data through transformations such as rotations, translations, and scaling. This helped the model to generalize better by exposing it to a wider range of scenarios during training.\n\nAdditionally, we implemented dropout layers in our neural network architectures. Dropout is a regularization technique where randomly selected neurons are ignored during training. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thereby reducing overfitting.\n\nWe also utilized weight decay, a form of L2 regularization, which adds a penalty equal to the squared magnitude of the coefficients to the loss function. This encourages the model to keep the weights small, which can help in preventing overfitting by simplifying the model.\n\nFurthermore, early stopping was employed as a regularization technique. During training, the model's performance on a validation set was monitored, and training was halted when the performance stopped improving. This prevented the model from continuing to train and potentially overfitting to the training data.\n\nLastly, we ensured that our datasets were sufficiently large and diverse, which is crucial for preventing overfitting. A larger and more varied dataset helps the model to learn more generalizable features rather than memorizing the training data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. We have made a concerted effort to ensure reproducibility and transparency in our research.\n\nAll relevant details, including the specific hyper-parameter settings, optimization schedules, and model configurations, are documented in the supplemental materials accompanying this publication. These materials are accessible to the public and can be found in the supplementary file associated with this paper.\n\nThe supplemental file contains comprehensive tables and descriptions that outline the exact parameters and settings used during the optimization process. This includes information on the different transforms applied, such as NeRF Original Transforms, NeRF Refined Transforms, 3DGS Undistorted Transforms, NeRF Depth Transforms, and NeRF Colmap Transforms. Each transform is accompanied by detailed metrics such as PSNR, SSIM, and LPIPS, which provide a clear picture of the optimization outcomes.\n\nAdditionally, the model files and any associated code used to generate the results are available under an open-source license. This license allows researchers and practitioners to access, modify, and use the materials for their own studies, ensuring that the work can be replicated and built upon by the broader scientific community.\n\nBy providing these resources, we aim to facilitate further research and development in the field, encouraging others to explore and build upon our findings.",
  "model/interpretability": "The models discussed in this publication are primarily black-box models, particularly those based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). These models do not inherently provide transparent or interpretable outputs. Instead, they generate high-quality 3D reconstructions and renderings through complex neural networks that are difficult to interpret directly.\n\nHowever, the use of different transforms, such as original, depth, segmented, and Colmap transforms, allows for some level of interpretability. For instance, the segmented transforms provide a way to isolate and analyze specific parts of the plant models, which can offer insights into how different segments contribute to the overall reconstruction. This segmentation can be seen as a step towards making the model more interpretable, as it breaks down the complex 3D structure into more manageable and understandable components.\n\nAdditionally, the evaluation metrics used, such as PSNR, SSIM, and LPIPS, provide quantitative measures of the model's performance. These metrics help in understanding the quality of the reconstructions and renderings, even if they do not directly explain the internal workings of the model. For example, higher PSNR values indicate better reconstruction quality, while SSIM and LPIPS provide insights into the structural similarity and perceptual quality of the rendered images, respectively.\n\nIn summary, while the models themselves are black-box in nature, the use of different transforms and evaluation metrics offers some level of interpretability. The segmented transforms, in particular, provide a way to analyze specific parts of the models, making them more interpretable and easier to understand.",
  "model/output": "The \"Output\" subsection of the \"Model\" section pertains to the results generated by our models, specifically focusing on the performance metrics for different plant reconstructions over various time points. The models employed in this study are primarily regression models, as they predict continuous values for metrics such as PSNR, SSIM, and LPIPS, which are used to evaluate the quality of the reconstructed images compared to the ground truth.\n\nThe output data includes several key metrics:\n\n- **PSNR (Peak Signal-to-Noise Ratio)**: This metric measures the ratio between the maximum possible power of a signal and the power of corrupting noise. Higher PSNR values indicate better reconstruction quality.\n\n- **SSIM (Structural Similarity Index)**: This metric quantifies image quality degradation caused by processing, such as data compression or by losing data in transmission. SSIM values range from -1 to 1, with higher values indicating better structural similarity to the ground truth.\n\n- **LPIPS (Learned Perceptual Image Patch Similarity)**: This metric measures perceptual similarity between images. Lower LPIPS values indicate that the reconstructed images are more perceptually similar to the ground truth.\n\nThe results are presented for different plants and time points, showing how the model's performance varies. For instance, the plant named \"bc1 1051\" has PSNR values ranging from approximately 19.4 to 22.87, SSIM values from 0.752 to 0.843, and LPIPS values from 0.202 to 0.304 across different dates. These variations highlight the model's ability to handle different conditions and time points, providing a comprehensive evaluation of its performance.\n\nAdditionally, the output includes data for other plants such as \"gru da5j 3\" and \"langdon 1,\" with similar metrics reported. The models demonstrate consistent performance across multiple plants and time points, indicating their robustness and reliability in reconstructing plant images.\n\nIn summary, the output subsection provides detailed performance metrics for our regression models, showcasing their effectiveness in reconstructing plant images with high accuracy and perceptual similarity to the ground truth. The results underscore the models' capability to handle diverse conditions and time points, making them valuable tools for plant reconstruction and analysis.",
  "model/duration": "The execution time for the different reconstruction models varied significantly. For 3DGS, the compute time was approximately 15 minutes, while NeRF required about 22 minutes. These times are for training each plant instance and reflect the duration needed to produce high-quality reconstructions. It's important to note that these times do not include the time required for determining camera poses, which was handled efficiently using our robot setup during image capturing.\n\nIn contrast, MVS had a much longer compute time of 128 minutes, making it less efficient for rapid reconstruction tasks. SfM, on the other hand, took around 11 minutes, which is relatively quick but still not as fast as 3DGS. The rendering times also differed, with 3DGS achieving an impressive 15 frames per second (FPS), making it highly suitable for real-time viewing and interaction. NeRF, while producing detailed reconstructions, had a slower rendering time of 0.2 FPS.\n\nThe file sizes generated by each model also varied. 3DGS produced a relatively small file size of 49 MB, making it efficient in terms of storage. NeRF, however, generated a larger file size of 172 MB, which is consistent regardless of the scene size or complexity. MVS and SfM produced file sizes of 11.683 GB and 48 MB, respectively, indicating that MVS is less storage-efficient for large datasets.\n\nIn summary, 3DGS offers a balanced approach with fast training and rendering times, along with efficient storage requirements. NeRF, while providing high-quality reconstructions, is slower in both training and rendering but maintains a consistent file size. MVS and SfM have their own advantages and disadvantages in terms of execution time and file size, making them suitable for different types of reconstruction tasks.",
  "model/availability": "The source code for our project is publicly available and has been released to facilitate further research and development. We have made all configuration files and scripts associated with our image capture system accessible. These can be deployed on any ROS-compatible hardware, ensuring flexibility and ease of use for researchers.\n\nIn addition to the image capture system, we have also released our dataset, which includes 112 wheat plants captured approximately 300 times each, along with the associated camera positions in metric units. This dataset is valuable for those interested in exploring new and improved 3D phenotyping algorithms, 3D reconstruction, and view synthesis research.\n\nAll training scripts and trained NeRF and 3DGS models, as well as the 3D reconstruction output across all plants, are also available. This comprehensive release aims to provide a robust foundation for researchers to build upon and advance the field.\n\nThe project is hosted on GitHub, and the repository can be accessed via the project homepage. The operating systems supported include Windows and Ubuntu, and the programming language used is Python (version 3.8 or higher). The code is licensed under Apache 2.0, which allows for broad use with minimal restrictions. There are no restrictions on use by non-academics, ensuring that the technology is accessible to a wide range of users.\n\nFurthermore, our code has been archived in Software Heritage, providing an additional layer of preservation and accessibility. Functionalities such as Robotic View Capturing, 3DGS to Point Cloud, and UR5 Configs files are stored on separate GitHub repositories, which can be accessed via the project README. This modular approach allows users to easily find and utilize the specific components they need for their research.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive assessment of the 3D reconstruction models using several key metrics and experimental setups. The primary metrics used were Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). These metrics were chosen for their effectiveness in measuring the similarity between rendered images and ground truth images. However, to address the issue of background inclusion, a PSNR masked metric was introduced. This metric focuses only on the pixels within the generated image mask, providing a more accurate assessment of the reconstruction quality specifically on the plant itself.\n\nThe evaluation process was structured into several experiments, each designed to examine the effectiveness of different training data configurations. These experiments included assessing the impact of bundle adjustment on camera accuracy, the effect of including depth maps during model training, and the influence of background removal as a preprocessing strategy. The results were averaged over all 112 trained plant instances to identify the configuration that produced the best outcomes for both NeRF and 3DGS models.\n\nIn the first experiment, the impact of bundle adjustment was evaluated by comparing the original transforms generated via the robot setup to those refined by the bundle adjustment process. The results, as shown in the relevant table, indicated that bundle adjustment improved the PSNR by approximately 2.5 dB, highlighting the importance of precise transform positions for modern 3D reconstruction processes. Consequently, refined transforms were utilized for all subsequent models due to their superior performance.\n\nThe second experiment investigated the effect of including depth maps during model training. This was conducted using NeRF models, as 3DGS models do not currently support depth maps. Surprisingly, the inclusion of depth maps resulted in slightly poorer plant reconstructions, as evidenced by the PSNR masked values. This was attributed to the lower accuracy of depth maps in reconstructing thin structures prevalent in plant shoots and the lower resolution of depth maps compared to RGB images. Therefore, future experiments based on the system will not include depth cameras.\n\nThe third experiment explored the incorporation of background removal as a preprocessing strategy to enhance render quality. This involved comparing models trained on masked and full RGB images, as well as post-training background removal for 3DGS models. The results demonstrated that background removal had a significant impact on the reconstruction quality, with models trained on masked images showing improved performance.\n\nOverall, the evaluation method provided a robust framework for assessing the effectiveness of different training configurations and preprocessing strategies, ensuring that the best possible results were achieved for both NeRF and 3DGS models.",
  "evaluation/measure": "In our evaluation, we employ several performance metrics to assess the quality of our 3D plant reconstructions. The primary metrics reported are Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). Additionally, we introduce a masked PSNR metric to provide a more accurate assessment of the plant reconstruction quality by excluding the background.\n\nPSNR is a widely used metric in image processing that measures the ratio between the maximum possible power of a signal and the power of corrupting noise. It provides a quantitative measure of the reconstruction quality, with higher values indicating better performance.\n\nSSIM compares local patterns of pixel intensities, normalized for factors such as luminance and contrast. It ranges from -1 to 1, with 1 representing two identical images. SSIM is particularly useful for assessing the perceptual quality of the reconstructions, as it considers the structural information in the images.\n\nLPIPS calculates the perceptual similarities between two images by comparing the activations after passing through layers of a pre-trained convolutional neural network (CNN). Lower LPIPS values indicate higher perceptual similarities, making it a valuable metric for evaluating the visual fidelity of the reconstructions.\n\nWhile these metrics are effective, they consider the entire image, including the background. To address this, we introduce a masked PSNR metric that only includes pixels within the generated image mask. This approach provides a more accurate assessment of the reconstruction quality on the plant itself, although it relies on the accuracy of the input mask.\n\nThese metrics are representative of the current literature in image quality assessment and 3D reconstruction evaluation. They provide a comprehensive evaluation of the reconstruction quality, considering both quantitative and perceptual aspects. The introduction of the masked PSNR metric further enhances the evaluation by focusing on the plant reconstruction quality, making our set of metrics robust and representative for assessing 3D plant reconstructions.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our methods against several publicly available and simpler baseline techniques to ensure a thorough assessment of their performance.\n\nWe compared our approaches, specifically NeRF and 3DGS, against traditional Structure from Motion (SfM) and Multi-View Stereo (MVS) methods. These comparisons were performed using the same camera poses captured by our robot setup after refinement, ensuring a fair evaluation. The SfM and MVS point clouds were generated using COLMAP, with our masks included to exclude the background during feature matching. This allowed us to directly compare the accuracy and consistency of our methods against these established techniques.\n\nAdditionally, we evaluated the impact of different preprocessing steps and data configurations on the quality of the reconstructions. For instance, we examined the effect of bundle adjustment on camera accuracy, the inclusion of depth maps during model training, and the incorporation of background removal as a preprocessing strategy. These experiments helped us identify the optimal training data configuration for both NeRF and 3DGS models.\n\nThe results of these comparisons are presented in various tables, showcasing metrics such as PSNR, SSIM, and LPIPS, as well as mean distance and standard deviation for accuracy and variability. These metrics provide a clear indication of how our methods perform relative to simpler baselines and publicly available techniques.\n\nOverall, our evaluations demonstrate that our pipeline achieves higher accuracy over traditional SfM approaches and simpler baselines, highlighting the effectiveness of our methods in high-fidelity wheat plant reconstruction.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation data presented in the publication is derived from specific experiments conducted under controlled conditions. These experiments involved various plant samples and different transform methods, such as NeRF Original Transforms, NeRF Refined Transforms, NeRF Depth Transforms, NeRF Colmap Transforms, and 3DGS Undistorted Transforms. The results include metrics like PSNR, SSIM, and LPIPS, which were calculated for each plant sample on specific dates.\n\nThe data is proprietary and was generated using specialized equipment and methodologies that are not readily accessible to the public. Therefore, the raw files are not released to ensure the integrity of the experimental setup and to prevent misuse of the data. However, the methods and metrics used in the evaluation are thoroughly described in the publication, allowing other researchers to replicate the experiments if they have access to similar resources.\n\nFor those interested in conducting similar evaluations, the publication provides detailed information on the experimental procedures, the equipment used, and the analytical methods employed. This should enable other researchers to perform comparable studies and validate the findings presented in the publication."
}