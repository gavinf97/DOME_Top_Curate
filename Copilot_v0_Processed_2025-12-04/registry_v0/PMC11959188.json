{
  "publication/title": "Deepdefense",
  "publication/authors": "The authors who contributed to this article are:\n\n- Sven Hauns, who developed the software and wrote the initial draft of the manuscript.\n- O. S. A., who conceived the study.\n- R. B., who oversaw the project.\n- All authors reviewed, contributed to, and approved the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2024",
  "publication/doi": "10.5524/102550",
  "publication/tags": "- Deepdefense\n- Immune system\n- Prokaryotic genomes\n- Machine learning\n- Protein classification\n- Genome scanning\n- Model calibration\n- Neural networks\n- CRISPR-Cas systems\n- Bioinformatics\n- Archaea\n- Bacteria\n- Phage\n- Plasmids\n- Protein cassettes",
  "dataset/provenance": "Our dataset comprises 21,196 unique validated samples. This dataset was used to ensure ample training data for subclasses with limited samples. We employed a stratified 10-fold cross-validation split, allocating 10% of the dataset for testing in each cross-validation step and another 10% of the remaining dataset as a validation set for early stopping. This resulted in 17,169 samples for training, 1,908 for validation, and 2,119 for testing in each cross-validation split.\n\nAdditionally, we used BLAST to create datasets with varying degrees of sequence homology (95%, 90%, 80%, 70%, 60%, 50%, 40%, and 30%). We also reported our results on these sets.\n\nThe dataset was imbalanced for types, consisting of 1,263 Durantia samples, 3,723 Gajiba samples, 1,529 Hachiman samples, 6,882 Wadjet samples, 637 Lamassu samples, 2,807 Septu samples, 647 Shedu samples, 1,097 Thoeris samples, 745 Kiwa samples, and 1,866 Zorya samples. We weighted the samples during training according to their distribution in the dataset to prevent overfitting to majority classes.\n\nWe also used a dataset with 21,196 samples from bacteria and archaea that have an unrelated function to the immune system, ensuring a low similarity to the known defense system by sequence homology and HMM-based search. In addition to cross-validation, we used 10% of the data for an independent test dataset with a maximal sequence homology of 95.\n\nThe data used for training are publicly available as part of the publication. The used HMM models are part of the publication. Additionally, data were taken from the PADLOC website. The trained model, as well as the additionally created training data, can be found in our GitHub repository. Snapshots of our code and other data further supporting this work are openly available in the GigaScience repository, GigaDB.",
  "dataset/splits": "The dataset used in this study comprises 21,196 unique validated samples. To ensure robust training and validation, a stratified 10-fold cross-validation split was employed. This method allocates 10% of the dataset for testing in each cross-validation step and another 10% of the remaining dataset as a validation set for early stopping. This results in 17,169 samples for training, 1,908 for validation, and 2,119 for testing in each cross-validation split. The stratified approach ensures that each test set contains a representative number of samples for each class.\n\nAdditionally, 10% of the data was set aside for an independent test dataset with a maximal sequence homology of 95. This independent test set is used to evaluate the model's performance on unseen data, providing an additional layer of validation.\n\nThe dataset is imbalanced across different types, with varying numbers of samples for each class. For instance, there are 1,263 Durantia samples, 3,723 Gajiba samples, and 6,882 Wadjet samples, among others. To address this imbalance, samples were weighted during training according to their distribution in the dataset. This weighting helps prevent overfitting to majority classes and ensures that the model performs well across all classes.\n\nThe dataset also includes samples from bacteria and archaea that have functions unrelated to the immune system, ensuring a low similarity to known defense systems by sequence homology and HMM-based search. This diverse and balanced approach to dataset splitting and weighting is crucial for developing a reliable and accurate model.",
  "dataset/redundancy": "The dataset used in our study consists of 21,196 unique validated samples. To ensure robust training and evaluation, we employed a stratified 10-fold cross-validation split. This method allocates 10% of the dataset for testing in each cross-validation step and another 10% of the remaining dataset as a validation set for early stopping. This approach ensures that a representative number of samples for each class are included in each test set. Consequently, each cross-validation split results in 17,169 samples for training, 1,908 for validation, and 2,119 for testing.\n\nTo further evaluate the model's performance, we created datasets with varying degrees of sequence homology (95%, 90%, 80%, 70%, 60%, 50%, 40%, and 30%) using BLAST. Additionally, we reported our results on these sets. This strategy helps in assessing the model's ability to generalize across different levels of sequence similarity.\n\nThe dataset is imbalanced for different types, with the number of samples ranging from 637 for Lamassu to 6,882 for Wadjet. To address this imbalance, we weighted the samples during training according to their distribution in the dataset. This weighting helps prevent overfitting to majority classes by dividing the total number of samples by the number of samples in a class multiplied by the number of classes.\n\nAdditionally, we used a dataset with 21,196 samples from bacteria and archaea that have an unrelated function to the immune system. This dataset ensures a low similarity to the known defense system by sequence homology and HMM-based search. We also used 10% of the data for an independent test dataset with a maximal sequence homology of 95. This independent test set helps in evaluating the model's performance on unseen data.\n\nIn summary, our dataset splitting strategy ensures that the training and test sets are independent and representative of the overall distribution. The use of stratified cross-validation and varying degrees of sequence homology helps in robustly evaluating the model's performance and generalizability.",
  "dataset/availability": "The data utilized for training our models is publicly accessible as part of a previous publication. Additionally, the hidden Markov models (HMMs) employed in our study are also available through another publication. Furthermore, supplementary data were obtained from the PADLOC website. To ensure transparency and reproducibility, the trained model and the additional training data generated during our research are available in our GitHub repository. For those interested in accessing snapshots of our code and other supporting data, these are openly available in the GigaScience repository, GigaDB. This comprehensive approach to data sharing ensures that all necessary resources are accessible to the scientific community, facilitating further research and validation of our findings.",
  "optimization/algorithm": "The optimization algorithm employed in our study is Bayesian Optimization with Hyperband (BOHB). This method is not new; it has been previously established and is used to optimize hyperparameters efficiently. BOHB combines the strengths of Bayesian optimization and the Hyperband algorithm, allowing for the exploration of a wide range of hyperparameter configurations while focusing computational resources on the most promising ones.\n\nThe choice to use BOHB in our work is driven by its effectiveness in handling complex optimization problems, particularly in the context of deep learning architectures. It enables us to systematically search through a large space of possible architectures and hyperparameters, ensuring that we identify the most optimal configurations for our specific task of classifying immune system proteins.\n\nGiven that BOHB is a well-known and widely used optimization technique, it was not necessary to publish it in a machine-learning journal. Instead, our focus is on applying this established method to a novel biological problem, demonstrating its utility in the context of immune system classification. This approach allows us to leverage existing optimization techniques to advance our understanding of immune system proteins, rather than developing a new optimization algorithm from scratch.",
  "optimization/meta": "The optimization process involves the use of an ensemble of models, which can be considered a form of meta-predictor. This ensemble is built by randomly initializing each network's parameters. The final prediction is made by averaging the outputs of all ensemble members and selecting the class with the highest confidence. The class certainty is determined by the lowest confidence that agrees with the determined class assignment, providing reliable uncertainty estimations.\n\nThe ensemble consists of multiple neural networks, each optimized using Bayesian Optimization with Hyperband (BOHB). The baseline architectures are designed to be relatively shallow to avoid overconfidence in classification. The final architecture uses a 1D-convolutional layer followed by two bidirectional GRU layers and three linear output layers. Additional information is processed by a second head and concatenated with the output of the GRU module before the final classification.\n\nThe training data is split into training, testing, and validation sets, with 10% of the data used for testing and 10% for validation. This split ensures that the training data is independent for each model in the ensemble. The proteins are encoded using a one-hot encoding, and the models are trained using the Adam optimizer with a learning rate of 0.001 for 150 epochs. Early stopping is employed after a third of the epochs if the validation loss does not improve for 20 epochs to prevent overfitting.\n\nThe use of an ensemble improves the overall performance compared to using a single model. The ensemble approach allows for the use of smaller models while achieving good accuracy, as observed by Guo et al. The final architecture has shown promising results in various classification tasks. The ensemble method also helps in capturing different modes of the underlying solution space, which stochastic models might miss.",
  "optimization/encoding": "The data encoding process involved transforming proteins into a one-hot encoded format. This method ensures that each protein is represented as a binary vector, where each position corresponds to a specific amino acid, and the value indicates its presence or absence. Additionally, supplementary information about the proteins, such as protein length, molecular weight, instability index, isoelectric point, charged residues, extinction coefficient, average hydropathy, and fractions of specific amino acids (A, C, D, E, W, Y), was generated and included in the model. This additional information was concatenated with the output of the GRU module to enhance the model's predictive capabilities. The dataset used for training, validation, and testing was split in a stratified manner, with 10% allocated for testing and another 10% for validation to facilitate early stopping. This approach ensures that each class is adequately represented in the training, validation, and testing sets, maintaining the integrity of the model's performance evaluation. The dataset comprised 21,196 unique validated samples, with a focus on ensuring ample training data for subclasses with limited samples. The proteins were encoded in a one-hot vector, and the Adam optimizer was used with a learning rate of 0.001. Training was conducted for 150 epochs, with early stopping implemented after a third of the epochs if the validation loss did not improve for 20 consecutive epochs. This strategy helps prevent overfitting and ensures that the model generalizes well to unseen data.",
  "optimization/parameters": "In our optimization process, we utilized Bayesian Optimization with Hyperband (BOHB) to fine-tune various architectural parameters. For baseline architectures, we optimized kernel sizes, stride, number of channels, dropout rates, and output dimensions of recursive units. When employing architectures with a second head, we also optimized the number of nodes used to process additional data. Specifically, for transformer-based architectures, we optimized the number of heads for the attention layer, dropout rates, the number of linear layers, and embedding dimensions.\n\nThe optimization process involved creating 75 unique hyperparameter configurations across 15 runs for each of the 8 baseline architectures, resulting in a total of 100 runs. This extensive search allowed us to identify the most effective parameters for our models. The final architecture, which demonstrated promising results, includes a 1D-convolutional layer followed by two bidirectional GRU layers and three linear output layers. This architecture was further refined manually on full fidelity after initial optimization.\n\nThe selection of parameters was driven by the goal of achieving high accuracy and reliability in protein classification tasks. The use of ensembles, where each network's parameters were randomly initialized, helped in capturing different modes of the solution space and improving overall model performance. Additionally, the incorporation of additional protein information, such as molecular weight and instability index, enhanced the model's ability to make accurate predictions. The final architecture was chosen based on its performance in validation tests and its ability to generalize well to unseen data.",
  "optimization/features": "In our study, we utilized a total of 12 distinct features to characterize the proteins used in our analysis. These features include various attributes such as protein length and instability index. The complete list of these features can be found in the supplementary materials.\n\nFeature selection was not explicitly performed as a separate step in our methodology. Instead, we relied on domain knowledge to identify relevant features that would be informative for our classification tasks. This approach ensured that the features used were biologically meaningful and relevant to the problem at hand.\n\nGiven that feature selection was not conducted, the issue of data leakage from the training set did not arise. All features were determined a priori based on existing knowledge and were consistently applied across all datasets, including training, validation, and testing sets. This method helped maintain the integrity of our model evaluation and ensured that the performance metrics were reliable and generalizable.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The final architecture, which includes a 1D-convolutional layer, two bidirectional GRU layers, and three linear output layers, was optimized using Bayesian Optimization with Hyperband (BOHB). This process involved creating 75 unique architectures across 15 runs for a smaller fidelity, followed by manual tuning of the best architecture on full fidelity.\n\nTo mitigate overfitting, we implemented early stopping. Specifically, after one-third of the training epochs had passed, we monitored the validation loss. If the validation loss did not improve for 20 consecutive epochs, training was halted. This approach ensured that the model did not overfit to the training data. Additionally, we used a MultiStepLearningRateScheduler to adapt the learning rate, reducing it by a factor of 0.9 after specific intervals. This helped in fine-tuning the model and preventing it from converging too quickly to a suboptimal solution.\n\nWe also utilized dropout layers within our architecture to introduce regularization, which further helped in preventing overfitting. The dropout rate was optimized during the BOHB process, ensuring that it was appropriately tuned for our specific dataset.\n\nTo address underfitting, we ensured that our models were sufficiently complex by including multiple layers and units. The use of bidirectional GRU layers allowed the model to capture both past and future context, which is crucial for sequence data like proteins. Moreover, the inclusion of additional information processed by a second head and concatenated with the GRU output enriched the feature space, enabling the model to learn more complex patterns.\n\nThe ensemble approach, where multiple networks were trained with randomly initialized parameters, also helped in improving the model's performance and reducing the risk of underfitting. By averaging the predictions of these networks, we achieved more robust and accurate classifications.\n\nIn summary, our optimization process, combined with regularization techniques and ensemble methods, effectively addressed both overfitting and underfitting, leading to a well-generalized model.",
  "optimization/regularization": "In our optimization process, we implemented several techniques to prevent overfitting and ensure robust model performance. One key strategy was the use of early stopping. We trained our models for 150 epochs but monitored the validation loss. If the validation loss did not improve for 20 consecutive epochs after the first third of the training process, we halted the training to prevent the model from overfitting to the training data.\n\nAdditionally, we employed dropout layers within our neural network architectures. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not rely too heavily on any single neuron.\n\nWe also utilized a MultiStepLearningRateScheduler to adjust the learning rate during training. The learning rate was reduced by a factor of 0.9 after specific epochs (3, 12, and then every 10 epochs up to 100). This adaptive learning rate strategy helps in fine-tuning the model by allowing larger updates initially and smaller updates as training progresses, which can aid in convergence and prevent overfitting.\n\nFurthermore, we explored various regularization methods to improve calibration and uncertainty estimation. These included temperature scaling, matrix scaling, vector scaling, label smoothing, and penalizing confident output distributions. Each of these methods was tested both in single and ensemble settings to evaluate their impact on model performance and calibration. The use of ensembles, where multiple models are combined, also helped in reducing overfitting by averaging out the errors of individual models.\n\nOverall, these techniques collectively contributed to building a more generalized and robust model, capable of handling the complexity of the data without overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are publicly available. The source code for the project, named Deepdefense, can be accessed on GitHub at https://github.com/Sv enHauns/Deepdefense. This repository includes the trained model and the additional training data created during our research. The code and data are openly available under the MIT license, ensuring that users can freely access, modify, and distribute the materials.\n\nAdditionally, snapshots of our code and other supporting data are available in the GigaScience repository, GigaDB. This repository provides a comprehensive archive of the materials used in our study, ensuring reproducibility and transparency. The data used for training are also publicly available as part of the publication, along with the HMM models and additional data from the PADLOC website. These resources collectively support the reproducibility of our findings and allow other researchers to build upon our work.\n\nThe project is registered in the bio.tools database under the name Deepdefense, with the RRID: SCR_025346. This registration further ensures the accessibility and recognition of our tools within the scientific community. The availability of these resources underscores our commitment to open science and collaborative research.",
  "model/interpretability": "The model we developed, Deepdefense, is not entirely a black-box model. While it leverages complex neural network architectures, including convolutional layers and bidirectional GRU units, we have implemented several strategies to enhance its interpretability.\n\nOne key aspect of interpretability in our model is the use of ensembles. By randomly initializing each network’s parameters and predicting the class with the highest confidence in the averaged output of all ensemble members, we can provide more reliable uncertainty estimations. This approach allows us to understand the model's confidence in its predictions, which is crucial for interpreting its outputs.\n\nAdditionally, the model's architecture includes a second head that processes additional information provided by the sequence. This additional information is concatenated with the output of the GRU module before the final classification. This design choice makes it possible to trace back how different features contribute to the final prediction, adding a layer of transparency to the model.\n\nWe also employ early stopping based on validation loss to prevent overfitting, which helps in understanding the model's learning process and ensures that it generalizes well to unseen data. The use of a MultiStepLearningRateScheduler further aids in interpreting the training dynamics by adapting the learning rate at specific epochs, which can be observed and analyzed.\n\nMoreover, the model's performance is evaluated using various metrics such as accuracy, ROC-AUC, and AUPRC across different sequence homology levels. This comprehensive evaluation provides insights into the model's strengths and weaknesses, making it easier to interpret its behavior under different conditions.\n\nIn summary, while Deepdefense utilizes advanced neural network techniques, our implementation of ensembles, additional information processing, early stopping, and detailed performance evaluation contributes to its interpretability, making it more transparent than a typical black-box model.",
  "model/output": "The model is designed for multiclass classification tasks, specifically for classifying proteins. It employs a probabilistic classifier that outputs a probability vector for each input instance, indicating the likelihood of the instance belonging to each of the k classes. The final output is determined by selecting the class with the highest predicted probability, provided that this probability exceeds a certain threshold. If none of the probabilities meet the threshold, the model rejects the classification, indicating uncertainty. This approach allows the model to handle cases where it is not confident in its predictions, thereby improving the reliability of the classification results. The model's architecture includes a convolutional layer, bidirectional GRU layers, and linear output layers, which work together to process the input data and produce the final classification. Additionally, the model incorporates mechanisms for handling additional information and employs ensemble methods to enhance prediction quality and uncertainty estimation.",
  "model/duration": "The execution time for our model involved several stages, each contributing to the overall duration. Initially, we optimized 8 baseline architectures using Bayesian Optimization with Hyperband (BOHB) over 15 iterations, creating 75 unique hyperparameter configurations. This process was conducted at a smaller fidelity to efficiently explore the hyperparameter space. Following this, we manually fine-tuned the best-performing architecture on the complete fidelity, which involved training the model for 150 epochs. To prevent overfitting, we employed early stopping after a third of the epochs if the validation loss did not improve for 20 consecutive epochs. Additionally, we used a MultiStepLearningRateScheduler to adapt the learning rate, further refining the training process. The model was trained using the Adam optimizer with a learning rate of 0.001. The entire optimization and training process was computationally intensive, leveraging high-performance computing resources to manage the extensive calculations required.",
  "model/availability": "The source code for the software developed in this study is publicly available. The project is named Deepdefense and can be accessed via its GitHub repository. The repository includes the trained model and additional training data created during the study. For those interested in using the software, it is important to note that it is platform-independent and programmed in Python. Additional requirements for running the software can be found in the public environment file provided in the repository.\n\nThe software is released under the MIT license, which allows for free use, modification, and distribution, subject to the terms of the license. This ensures that users have the flexibility to integrate the software into their own projects or modify it to suit their specific needs.\n\nFurthermore, the software is registered in the bio.tools database, with the identifier SCR_025346. This registration provides additional metadata and context about the software, making it easier for researchers to discover and use. Snapshots of the code and other supporting data are also available in the GigaScience repository, GigaDB, ensuring long-term accessibility and reproducibility of the research.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and accuracy. We employed a stratified 10-fold cross-validation approach, where 10% of the dataset was allocated for testing and another 10% for validation in each fold. This method ensured that each class was represented in every test set, providing a comprehensive evaluation across all data subsets.\n\nAdditionally, we created datasets with varying degrees of sequence homology (ranging from 95% to 30%) using BLAST. This allowed us to assess the model's performance under different conditions of sequence similarity. The results on these datasets demonstrated the model's ability to maintain high accuracy and reliability even as sequence homology decreased.\n\nWe also conducted experiments using an independent test dataset, which comprised 10% of the data with a maximal sequence homology of 95%. This independent evaluation further validated the model's performance and generalizability.\n\nTo benchmark the uncertainty estimations, we trained models on the Zorya class using various calibration methods. We then determined the mean distance of predictions between the test set and a set of unrelated proteins. This benchmarking process helped us identify the most effective calibration techniques for improving the model's uncertainty estimations.\n\nFurthermore, we compared our approach with a recently published method based on hidden Markov model (HMM) homology search. The results showed that our method, Deepdefense, could identify more immune systems in prokaryotic genomes, demonstrating its superior performance.\n\nOverall, the evaluation process involved a combination of cross-validation, independent dataset testing, and comparative analysis with existing methods. This comprehensive approach ensured that our method was thoroughly evaluated and validated for its intended applications.",
  "evaluation/measure": "In the evaluation of our model, we report several key performance metrics to ensure a comprehensive assessment of its capabilities. The primary metrics we focus on include accuracy, the area under the receiver operating characteristic curve (ROC-AUC), and the weighted area under the precision-recall curve (AUPRC). These metrics are widely recognized in the literature and provide a robust evaluation of model performance, especially in imbalanced datasets.\n\nAccuracy is reported as the proportion of correctly classified instances out of the total instances. This metric gives a general sense of the model's performance across all classes. However, since our dataset is imbalanced, we also report ROC-AUC, which measures the model's ability to distinguish between classes across all threshold levels. This is particularly useful for evaluating the performance of the model on minority classes.\n\nAdditionally, we report the weighted AUPRC, which takes into account the precision and recall of the model. Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positives. The weighted AUPRC provides a balanced view of these two metrics, especially important in scenarios where the class distribution is skewed.\n\nWe evaluate our model using cross-validation techniques, including 5-fold and 10-fold cross-validation, to ensure that the performance metrics are reliable and not dependent on a specific train-test split. This approach helps in assessing the model's generalizability and robustness.\n\nFurthermore, we test the model across diverse sequence homology levels, ranging from 95% to 30%. This allows us to understand how the model performs under varying degrees of sequence similarity, which is crucial for its application in genome-wide scans. The reported metrics at each homology level provide insights into the model's performance in different scenarios.\n\nIn summary, the reported performance metrics—accuracy, ROC-AUC, and weighted AUPRC—are representative of the state-of-the-art evaluation practices in the field. These metrics, combined with cross-validation and homology-level testing, ensure a thorough and reliable evaluation of our model's performance.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our approach against publicly available methods and simpler baselines to ensure its robustness and effectiveness. We compared our tool, Deepdefense, with a recently published approach based on the hidden Markov model (HMM) homology search. This comparison demonstrated that Deepdefense can identify more immune systems in prokaryotic genomes, highlighting its superior performance in this context.\n\nAdditionally, we explored various techniques designed to improve model calibration, which is crucial for reliably rejecting unrelated proteins when scanning entire genomes. These techniques included temperature scaling, matrix scaling, and vector scaling, among others. We found that temperature scaling combined with the DOC method provided the best results, achieving a high mean distance between related and unrelated proteins. This combination was chosen for its stable accuracies and high mean distance, which is essential for the main task of rejecting unrelated proteins in genome-wide searches.\n\nWe also compared the performance of single neural networks versus ensembles. Ensembles consistently outperformed single networks, both in terms of distance measurement and accuracy. This finding underscores the importance of using ensemble methods to enhance the reliability and accuracy of our predictions.\n\nFurthermore, we assessed the overall effectiveness of different scaling approaches and calibration methods. Our findings revealed that temperature scaling had the most significant influence on improving uncertainty estimations without adversely affecting overall accuracy. This insight guided our choice of calibration methods for the subsequent analysis.\n\nIn summary, our evaluation involved a comprehensive comparison with publicly available methods and simpler baselines, demonstrating the superiority of our approach in identifying immune systems and rejecting unrelated proteins. The use of ensemble methods and temperature scaling further enhanced the reliability and accuracy of our predictions.",
  "evaluation/confidence": "In the evaluation of our method, we employed cross-validation techniques to ensure the robustness of our results. Specifically, we used 5-fold and 10-fold cross-validation (CV) to assess the performance of our models. These methods help in understanding the variability and reliability of our performance metrics.\n\nFor the 5-fold CV, we achieved an accuracy of 0.96 and an excellent ROC-AUC of 0.99. When using a maximal sequence homology of 95, the accuracy was 0.91 with a ROC-AUC of 0.96. In the 10-fold CV, we observed an average accuracy of 0.96, an average ROC-AUC of 0.995, and a weighted AUPRC of 0.979. These metrics provide a comprehensive view of our model's performance across different folds, indicating consistent and reliable results.\n\nWe also tested our model across various sequence homology levels. At a homology of 95, we maintained high performance with an accuracy of 0.95, ROC-AUC of 0.992, and weighted AUPRC of 0.973. Even at lower homology levels, such as 80 and 70, our model showed good performance, although there was a slight decrease in accuracy and other metrics as homology decreased.\n\nTo further evaluate the confidence in our results, we conducted a benchmark analysis. We trained models on the Zorya class and determined the mean distance of predictions between the test set and a set of unrelated proteins. This benchmark helped us assess the model's ability to distinguish between related and unrelated proteins, which is crucial for our main task of rejecting unrelated proteins in a genome-wide search.\n\nThe benchmark results, presented in tables, show the average distance and accuracy for different methods, including softmax, DOC, label smoothing, and penalizing confidence. The use of ensembles generally outperformed single networks, both in terms of distance and accuracy. Temperature scaling, in particular, showed a significant influence on improving the mean distance without adversely affecting overall accuracy.\n\nIn summary, our evaluation includes robust cross-validation techniques and benchmark analyses, providing confidence intervals and statistical significance for our performance metrics. The consistent high performance across different homology levels and the superior results from ensemble methods support the claim that our approach is effective and reliable.",
  "evaluation/availability": "The raw evaluation files used in our study are publicly available. The data utilized for training our models can be accessed as part of a previous publication. Additionally, the hidden Markov models (HMMs) employed in our research are also available through another publication. Furthermore, supplementary data were obtained from the PADLOC website. All trained models and additional training data generated during our work are accessible via our GitHub repository. For those interested in more comprehensive access, snapshots of our code and other supporting data are openly available in the GigaScience repository, GigaDB. The source code for our project, Deepdefense, is publicly available under the MIT license and can be found on GitHub. This ensures that all necessary resources for replicating our study are accessible to the research community."
}