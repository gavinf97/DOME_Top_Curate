{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- **A.V.-B.** (Conceptualization, Methodology, Investigation, Resources, Data curation, Writing—original draft preparation, Writing—review and editing, Visualization, Supervision, Project administration, Funding acquisition)\n- **L.A.K.-S.** (Conceptualization, Resources, Supervision, Project administration, Funding acquisition)\n- **S.L.** (Conceptualization, Methodology, Validation, Formal analysis, Investigation, Data curation, Writing—original draft preparation, Writing—review and editing, Visualization, Supervision)\n- **M.S.** (Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing—original draft preparation, Writing—review and editing, Visualization)\n- **K.L.** (Software, Validation, Investigation, Data curation)\n- **S.M.** (Methodology, Validation, Writing—review and editing)\n- **W.C.** (Validation, Formal analysis, Investigation, Data curation, Writing—review and editing, Visualization)\n\nAll authors have read and agreed to the published version of the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not applicable",
  "publication/tags": "- Tumor spheroids\n- Deep learning\n- Image segmentation\n- U-Net\n- Multicellular tumor spheroids\n- Radiotherapy\n- Microscopy\n- Spheroid segmentation\n- Machine learning\n- Cancer research",
  "dataset/provenance": "The dataset used in this work is composed of training, validation, and test data, which are publicly available. This includes data on clean spheroids and an extended dataset. The code to train and evaluate the model is also provided. Additionally, all networks optimized for different hyperparameters are available in the GigaScience repository, GigaDB, in the .pth format. The DOME-ML annotations are accessible in the DOME registry.\n\nThe dataset includes a variety of images from different sources and conditions. For instance, some datasets include images with extensive debris, which are crucial for the performance of the automatic segmentation. The datasets are annotated and compiled to support future improvements in spheroid segmentation. They facilitate the development of more individualized and customized models, such as those using the nnU-Net tool.\n\nThe dataset notation stands for different microscopy types and magnifications, such as brightfield/fluorescence microscopy, specific microscope models, and magnification levels. This notation helps in understanding the diversity and specificity of the images included in the dataset.\n\nThe dataset has been validated on a wide variety of published test datasets from previous deep-learning models. The trained U-Net performs well on roughly half of the datasets or 38% of the images, with an average Intersection over Union (IoU) above 0.8. However, some images with ambiguous ground truth or semi-transparent spheroids pose challenges. Classical segmentation techniques work sufficiently well for these problematic images, making deep-learning approaches unnecessary in these cases.\n\nThe dataset includes images from various sources, such as BO10S, BN10S, BN2S, BL5S, FN2S, FL5C, FL5S, and AnaSP. Each dataset has a specific number of images and comments on their characteristics, such as ambiguous ground truth or semi-transparent spheroids. The performance of the deep-learning model and classical segmentation techniques is reported for each dataset, providing a comprehensive evaluation of the dataset's quality and applicability.",
  "dataset/splits": "The dataset used in this work includes several splits for training, validation, and testing purposes. The primary splits are for training, validation, and testing, but additional splits are also mentioned for specific purposes.\n\nThe training data consists of a wide variety of images, including those with extensive debris, which are crucial for the performance of the automatic segmentation. The validation data is used to tune the model's hyperparameters and assess its performance during training. The test data includes a diverse set of published datasets from previous deep-learning models, allowing for a thorough evaluation of the trained U-Net's performance.\n\nIn the test data, there are multiple datasets with varying numbers of images. For instance, the dataset labeled BO10S contains 66 images, while BN10S has 21 images. Other datasets like BN2S and BL5S contain 154 and 50 images, respectively. The distribution of data points in each dataset varies, with some datasets having a higher proportion of images with ambiguous ground truth or semi-transparent spheroids.\n\nAdditionally, the dataset includes an extended validation set used for illustrating the performance of the U-Net on larger, better-discriminated spheroids at intermediate levels of debris. This extended set is part of the overall validation process to ensure the model's robustness and generalizability.\n\nThe datasets are annotated and compiled to support future improvements in spheroid segmentation. They are publicly provided to facilitate the development of more customized models and to incorporate the automatic segmentation into existing or future tools for spheroid analysis or medical image analysis. The datasets are available in the GigaScience repository, along with the code to train and evaluate the model.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used for training, validation, and testing in this work are publicly available. This includes data on clean spheroids and an extended dataset, which encompasses images with extensive debris. Alongside the datasets, the code necessary to train and evaluate the model is also provided. These resources are accessible to support future improvements in spheroid segmentation.\n\nThe datasets and associated code can be found in the GigaScience repository, specifically in GigaDB. This repository includes all networks optimized for various hyperparameters, formatted in the .pth file format. Additionally, the DOME-ML annotations are available in the DOME registry.\n\nThe data is released under the GNU GPL license, ensuring that users have the freedom to use, modify, and distribute the software and datasets, provided they adhere to the terms of the license. This approach promotes transparency and encourages further development and validation of spheroid segmentation models.\n\nTo ensure the integrity and reproducibility of the results, the datasets and code are archived in Software Heritage. This archiving process helps to preserve the exact versions of the datasets and code used in the study, making it easier for other researchers to replicate the findings and build upon them.\n\nThe availability of these resources is crucial for advancing the field of spheroid segmentation. By providing access to the datasets and code, we aim to facilitate the development of more individualized and customized models. This includes the use of tools like the recently introduced nnU-Net, which can help improve the accuracy and robustness of spheroid segmentation in various applications.",
  "optimization/algorithm": "The optimization algorithm used in our study falls under the class of adaptive learning rate optimizers, specifically focusing on stochastic optimization methods. We employed two main optimizers: Adam and a combination of RAdam and Lookahead.\n\nAdam, which stands for Adaptive Moment Estimation, is a widely recognized and commonly used optimizer in the machine learning community. It combines the advantages of two other extensions of stochastic gradient descent. Specifically, Adam uses adaptive learning rates for each parameter, which helps in handling sparse gradients on noisy problems. This makes it a robust choice for a variety of deep learning tasks.\n\nThe combination of RAdam and Lookahead represents a more modern approach to optimization. RAdam, or Rectified Adam, addresses some of the issues with the initial warm-up phase in Adam, making it more stable and effective, especially in the early stages of training. Lookahead, on the other hand, is a technique that aims to improve the optimization process by taking a step forward and then looking ahead to adjust the parameters more effectively. This combination is designed to enhance the convergence speed and stability of the training process.\n\nThese optimizers were chosen for their proven effectiveness in various deep learning applications. While they are not new algorithms, their combination and specific implementations in our study are tailored to the unique challenges of our image segmentation task. The decision to use these optimizers was driven by their ability to handle the complexities of our dataset and the need for efficient and accurate training.\n\nThe choice of optimizers was validated through extensive experimentation and comparison with other methods. The results demonstrated that these optimizers provided the best performance in terms of the evaluation metrics used, such as Jaccard Coefficient of Distance (JCD), Relative Diameter Deviation (RDD), and Relative Circularity Deviation (RCD). This validation process ensured that the selected optimizers were well-suited for our specific application.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. We began by transforming the original training images to double the dataset size through data augmentation. This process involved applying one of three random transformations to each image: vertical flip, horizontal flip, or a 180-degree rotation. These transformations were chosen because they are valid for spheroids, which do not have a specific orientation within an image. By conserving the original rectangular resolution, we avoided the need for interpolating pixels on the image borders, which could introduce artifacts and complicate the training process.\n\nFor the input resolution, we initially set it to half of the original image size. This decision was based on the observation that reducing the image size by half led to the highest accuracy in our models. The optimization process involved evaluating different image sizes, and it was determined that halving the original size provided the best performance.\n\nAdditionally, we employed transfer learning to leverage pre-trained models, which significantly improved the performance of our fully convolutional networks (FCNs). Transfer learning allowed us to start with a model that had already learned relevant features from a large dataset, thereby accelerating the training process and enhancing the model's ability to generalize to new data.\n\nThe loss function used for quantifying the FCN’s error was optimized by comparing distribution-based loss and region-based loss. We selected the Dice loss for region-based evaluation and the Cross-Entropy loss as the distribution-based loss function. Furthermore, we tested the Focal loss, which is particularly suitable for imbalanced class scenarios.\n\nTwo different optimizers were evaluated for minimizing the loss function: Adam, a widely used optimizer, and a combination of RAdam and Lookahead, which are more modern optimizers. The initial parameters for the backbone optimization included transfer learning, no data augmentation, the half-sized input resolution, Cross-Entropy loss function, and the Adam optimizer. These parameters were then individually optimized in consecutive order to achieve the best performance.\n\nIn summary, our data encoding and preprocessing involved data augmentation, transfer learning, and careful selection of input resolution and loss functions. These steps were essential in enhancing the performance and reliability of our machine-learning models for image segmentation.",
  "optimization/parameters": "In our study, several key parameters were optimized to enhance the performance of our fully convolutional network (FCN) models for tumor spheroid segmentation. The primary parameters included the choice of backbone architecture, loss function, optimizer, input resolution, and the use of data augmentation and transfer learning.\n\nThe backbone architectures evaluated were ResNet 34 and VGG 19-bn for the U-Net, and W48 for the HRNet. These were selected based on their performance in preliminary tests and existing literature. For the U-Net, ResNet 34 was found to be optimal due to its superior performance across various metrics and lower computational requirements compared to VGG 19-bn. For the HRNet, the W48 backbone was preferred as it consistently showed the best performance with the lowest standard deviations.\n\nThree loss functions were tested: Cross-Entropy, Focal, and Dice. Cross-Entropy and Focal loss were found to be the most effective for the HRNet, while Dice loss was optimal for the U-Net. The choice of loss function was crucial in balancing the trade-offs between distribution-based and region-based errors.\n\nTwo optimizers were evaluated: Adam and a combination of RAdam and Lookahead. For the U-Net, the combination of RAdam and Lookahead yielded the highest accuracy. In contrast, the HRNet performed best with the Adam optimizer. The selection of the optimizer was based on its ability to minimize the loss function efficiently.\n\nData augmentation and transfer learning were both employed to improve model performance. Data augmentation involved transforming the original training images through vertical flips, horizontal flips, or 180-degree rotations, effectively doubling the dataset size. This technique helped in improving performance and avoiding overfitting. Transfer learning, which involves initializing the model with pre-trained weights, significantly enhanced accuracy, particularly for the U-Net.\n\nThe input resolution was another critical parameter. Using images at half the original resolution (650 × 515) resulted in the best performance. Higher resolutions reduced accuracy due to increased detail complexity, while lower resolutions led to a loss of relevant information. This resolution was chosen as it provided a good balance between detail and generalization.\n\nIn summary, the selection of these parameters was driven by a combination of empirical testing and theoretical considerations. The optimal configuration for the U-Net included the ResNet 34 backbone, Dice loss, RAdam & Lookahead optimizer, half-resolution input, and the use of both data augmentation and transfer learning. For the HRNet, the optimal setup featured the W48 backbone, Cross-Entropy loss, Adam optimizer, half-resolution input, and the same data augmentation and transfer learning strategies.",
  "optimization/features": "Not applicable.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues during the training of our models.\n\nTo mitigate overfitting, we utilized data augmentation techniques. This involved transforming the original training images through vertical flips, horizontal flips, or 180-degree rotations. By doubling the size of the training dataset, we enhanced the model's ability to generalize to new, unseen data. Additionally, we implemented transfer learning, which leveraged pre-trained models to provide a robust starting point for our training process. This approach helped in reducing the risk of overfitting by ensuring that the model had a solid foundation of learned features from a large, diverse dataset.\n\nTo address underfitting, we carefully selected and optimized various hyperparameters. We experimented with different backbones, such as ResNet 34 and VGG 19-bn for U-Net, and W48 for HRNet, to ensure that the model architecture was complex enough to capture the necessary patterns in the data. Furthermore, we evaluated different loss functions, including Cross-Entropy, Focal, and Dice loss, to find the most suitable one for our specific task. The optimization process involved comparing the performance of these loss functions and selecting the one that yielded the best results.\n\nWe also tested different optimizers, such as Adam and a combination of RAdam and Lookahead, to ensure efficient convergence during training. The combination of these strategies helped in achieving a balance between model complexity and generalization, thereby avoiding both overfitting and underfitting. The final hyperparameter configurations for the U-Net and HRNet models were determined through extensive experimentation and validation, ensuring that the models performed optimally on the test dataset.",
  "optimization/regularization": "In our study, we employed data augmentation as a regularization method to prevent overfitting. This technique involves generating additional training data by applying transformations to the original dataset. Specifically, each original training image was transformed once, effectively doubling the size of the training dataset. The transformations applied included vertical flips, horizontal flips, and rotations by 180 degrees. These transformations were chosen because they are valid for our dataset, as the spheroids do not have a specific orientation within an image. By increasing the diversity of the training data, data augmentation helps improve the model's performance and robustness, making it less likely to overfit to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are thoroughly documented within the publication. Specifically, the optimal configurations for both the U-Net and HRNet models are detailed, including backbones, optimizers, loss functions, resize factors, and whether transfer learning and data augmentation were employed.\n\nFor the U-Net model, the optimal configuration includes a ResNet 34 backbone, the RAdam optimizer combined with Lookahead, Dice loss, a resize factor of 1/2, transfer learning, and data augmentation. This setup was found to yield the highest accuracy.\n\nThe HRNet model, on the other hand, achieved the highest accuracy with a W48 backbone, the Adam optimizer, Cross-Entropy loss, a resize factor of 1/2, transfer learning, and data augmentation.\n\nThese configurations were evaluated using several metrics, including the Jaccard coefficient of difference (JCD), relative diameter deviation (RDD), relative circularity deviation (RCD), ambiguous spheroid fraction (ASF), and invalid spheroid fraction (ISF). The values of these metrics for different configurations are provided in the supplementary figures, which highlight the optimal settings for each model.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly mentioned as being publicly available. However, the detailed descriptions of the configurations and optimization processes should enable replication of the results by other researchers.\n\nThe publication does not specify the licensing terms under which these configurations and optimization schedules are made available. It is assumed that standard academic practices apply, allowing for use in research and educational contexts with proper citation. For specific licensing details, readers are encouraged to contact the authors or the publishing entity.",
  "model/interpretability": "The model presented in this work is a deep learning-based U-Net architecture, which is generally considered a black-box model. This means that the internal workings of the model are not easily interpretable, and it is challenging to understand how the model makes its predictions. The U-Net architecture is designed to learn complex patterns and features directly from the input data, which makes it highly effective for tasks like image segmentation but also makes it difficult to interpret.\n\nHowever, there are some aspects of the model that can be considered transparent. For instance, the model's performance can be evaluated using various metrics, such as the Intersection over Union (IoU) or the Jaccard Distance (JCD). These metrics provide a quantitative measure of the model's accuracy and can be used to compare the performance of different models or configurations. Additionally, the model's predictions can be visualized and compared to the ground truth, which can provide some insights into the model's behavior.\n\nIn this work, the U-Net model was trained and validated on a variety of datasets, and its performance was evaluated using several metrics. The results showed that the model performs well on roughly half of the datasets or 38% of the images, with an average IoU above 0.8. However, the model struggled with two types of images: those with ambiguous ground truth and those where the spheroids appear semi-transparent. For these challenging cases, classical segmentation techniques were found to work sufficiently well, making the use of deep-learning approaches unnecessary.\n\nThe model's predictions can be visualized and compared to the ground truth, which can provide some insights into the model's behavior. For example, the model's predictions can be overlaid on the input images, and the areas of agreement and disagreement can be highlighted. This can help to identify the types of errors that the model makes and to understand the limitations of the model.\n\nIn summary, while the U-Net model presented in this work is a black-box model, there are some aspects of the model that can be considered transparent. The model's performance can be evaluated using various metrics, and the model's predictions can be visualized and compared to the ground truth. This can provide some insights into the model's behavior and help to identify the types of errors that the model makes.",
  "model/output": "The model discussed in this publication is designed for image segmentation, specifically for tumor spheroids. It is a classification task at the pixel level, where each pixel is classified as either inside or outside the spheroid. The output of the fully convolutional network (FCN) model is a probability heatmap, where each pixel takes a probability value between 0 (black) and 1 (white), indicating the likelihood of the pixel belonging to the spheroid. A threshold of 0.5 is typically applied to this heatmap to classify pixels into two categories: those inside the spheroid (white) and those outside (black). This binary classification is then used to extract the contour of the spheroid, represented as a polygonal chain.\n\nThe model's performance is evaluated using various metrics, including the Jaccard distance coefficient (JCD), regional distance coefficient (RDD), and regional contour distance (RCD), among others. These metrics help quantify the accuracy and reliability of the segmentation. The model's output is further refined through post-processing steps to transform the probability heatmap into a precise spheroid contour.\n\nThe segmentation process is crucial for developing an automatic classification system for spheroid image time series, which can support machine learning methods to forecast tumor spheroid fate early. The model's availability is ensured through a minimal tool with a graphical user interface, compiled using the ONNX Runtime for production use. This tool includes a README file with detailed requirements and usage instructions, as well as example images for testing. The project is open-source and licensed under the GNU GPL, with additional information available on the project's homepage and archived in Software Heritage.",
  "model/duration": "The execution time of our models varies depending on the hardware used. On a CPU (Intel Core i7-4770), both the U-Net and HRNet models take approximately 1.8 seconds to segment a single image. When utilizing a GPU (NVIDIA GeForce RTX 3080) for serial segmentation, the U-Net model requires only 0.03 seconds per image, while the HRNet model takes 0.08 seconds. Despite the HRNet model having a larger size (251 MB compared to the U-Net's 158 MB), the computation times for both models are quite comparable. The U-Net was ultimately chosen for automatic segmentation due to its slightly higher accuracy in our specific setting, even though a recently published deep learning approach suggested that HRNet achieved the highest accuracy.",
  "model/availability": "The source code for the project, named SpheroidSegDeDeb, is publicly available. It can be accessed via the project's homepage. The project is platform-independent and requires Python version 3.8.10 or higher. Additionally, users need to download the model from a specified URL. The requirements and usage instructions are documented in the provided README file, which also includes example images for testing purposes. The software is licensed under the GNU GPL. It is also archived in Software Heritage and has a BioTool ID and a SciCrunch ID for easy reference. The ONNX Runtime is used to compile the model and facilitate its deployment in production environments.",
  "evaluation/method": "The evaluation of the models involved several key metrics to assess segmentation performance. These metrics included the Jaccard coefficient distance (JCD), relative diameter deviation (RDD), relative circularity deviation (RCD), invalid spheroid fraction (ISF), and ambiguous spheroid fraction (ASF). These metrics were computed for individual images to report not only their mean values but also their deviations, providing insights into the reliability of the models across the test dataset.\n\nThe models were trained on a GPU, specifically the NVIDIA GeForce RTX 3080, with a mini-batch size of 2. For higher resolution images or when comparing specific backbones, online learning was used instead of mini-batches to prevent memory overflow. This approach reduced the batch size to 1, ensuring that the models could handle the increased computational demands.\n\nThe evaluation metrics were used to optimize hyperparameters, ensuring that the models achieved the best possible performance. The models were tested on a variety of published test datasets from previous deep-learning models. The trained U-Net performed well on approximately half of the datasets, with an average Intersection over Union (IoU) above 0.8, sometimes surpassing the original models corresponding to the datasets.\n\nHowever, two types of images posed challenges: those with ambiguous ground truth and those where spheroids appeared semi-transparent. For images with ambiguous ground truth, the U-Net often segmented reasonably, but classical segmentation techniques worked sufficiently well, making deep-learning approaches unnecessary in these cases. Similarly, for semi-transparent spheroids, classical methods like Otsu thresholding, sometimes combined with image erosion, performed adequately.\n\nThe evaluation also included a comparison with classical segmentation techniques from the original publications and simple Otsu thresholding. This comparison highlighted the strengths and limitations of the deep-learning models, providing a comprehensive assessment of their performance.",
  "evaluation/measure": "The evaluation of the segmentation models employs several metrics to assess accuracy comprehensively. The primary metric is the Jaccard Class Difference (JCD), which measures the relative difference between the predicted and target pixel sets. JCD values range from 0 (perfect overlap) to 1 (no intersection), indicating the relative area error of the segmentation. A JCD below 0.2 is considered justifiable and aligns with deviations observed between segmentations by different humans.\n\nIn addition to JCD, other metrics include the Relative Diameter Deviation (RDD) and Relative Circularity Deviation (RCD), which assess the error in the estimated diameter and circularity of the segmented spheroids, respectively. These metrics help evaluate the validity of the spherical shape assumption used in volume estimation.\n\nThe Ambiguous Spheroid Fraction (ASF) denotes the fraction of cases where the spheroid is correctly segmented, but additional nonexistent spheroids are detected. The Invalid Spheroid Fraction (ISF) indicates cases where the spheroid is found in the wrong place or not found at all.\n\nThese metrics are computed for individual images to report both mean values and deviations, providing insights into the reliability of the segmentation across the test dataset. The use of these metrics is in line with common practices in the field and ensures a thorough evaluation of the models' performance.",
  "evaluation/comparison": "In the evaluation of our trained U-Net model, a comprehensive comparison was conducted against both publicly available deep-learning models and simpler classical segmentation techniques. The U-Net was tested on a wide variety of published test datasets, demonstrating strong performance on approximately half of the datasets, achieving an average Intersection over Union (IoU) above 0.8. This performance sometimes surpassed the original models corresponding to these datasets.\n\nHowever, certain types of images posed challenges for the U-Net. Specifically, images with ambiguous ground truth (comprising 20% of the images) and those where spheroids appeared semi-transparent (42% of the images) were problematic. For these challenging cases, classical segmentation techniques, such as the original publication's method and simple Otsu thresholding, often performed sufficiently well, making deep-learning approaches less necessary in these scenarios.\n\nThe evaluation metrics for different backbones of the U-Net were also assessed, with ResNet 34 achieving the best results and being selected as the backbone. Additionally, the impact of different extensions to the training dataset, including transfer learning and data augmentation, was evaluated. The highest accuracy for the U-Net was achieved when both transfer learning and data augmentation were employed.\n\nIn summary, while the U-Net showed robust performance on many datasets, simpler baselines were found to be effective for certain image types, highlighting the importance of considering multiple approaches in segmentation tasks.",
  "evaluation/confidence": "The evaluation of the segmentation models employed several metrics to assess accuracy, including the Jaccard distance (JCD), relative diameter deviation (RDD), relative circularity deviation (RCD), ambiguous spheroid fraction (ASF), and invalid spheroid fraction (ISF). These metrics were computed for individual images to report not only their mean values but also their deviations, providing insights into the reliability of the results across the test dataset.\n\nConfidence intervals are provided for each metric, indicating the variability and reliability of the measurements. For instance, the JCD values are presented with their corresponding standard deviations, allowing for an assessment of the consistency of the segmentation performance. This approach ensures that the reported metrics are not just point estimates but are accompanied by a measure of their uncertainty.\n\nStatistical significance is considered in the evaluation process. The use of confidence intervals and the reporting of deviations help in understanding whether the differences in performance between models are meaningful. For example, the comparison of different backbones for the U-Net and HRNet models includes bolded values that highlight the optimum performance in each column, suggesting that these differences are statistically significant.\n\nThe evaluation also includes a discussion on the types of images that pose challenges to the models, such as those with ambiguous ground truth or semi-transparent spheroids. Classical segmentation techniques are noted to perform sufficiently well on these problematic images, indicating that while deep-learning approaches show promise, they may not always be necessary or superior in all scenarios.\n\nOverall, the evaluation metrics and the accompanying statistical measures provide a comprehensive assessment of the models' performance, ensuring that the claims of superiority are backed by reliable and significant data.",
  "evaluation/availability": "The raw evaluation files used for this work are publicly available. This includes data on clean spheroids and the extended dataset. Additionally, the code to train and evaluate the model is also accessible. All supporting data, including networks optimized for different hyperparameters, are available in the GigaScience repository, specifically in GigaDB. These networks are provided in the .pth format. Furthermore, DOME-ML annotations are available in the DOME registry. The data and code can be accessed to facilitate reproducibility and further research."
}