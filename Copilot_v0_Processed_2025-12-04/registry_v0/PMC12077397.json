{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are Giuseppe Gallitto, Robert Englert, Balint Kincses, Raviteja Kotikalapudi, Jialin Li, Kevin Hoffschlag, Ulrike Bingel, and Tamas Spisak. Unfortunately, the specific contributions of each author to the paper are not detailed.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "10.1148/ryai.210064",
  "publication/tags": "- Machine Learning\n- Neuroimaging\n- Brain Connectivity\n- Predictive Modeling\n- External Validation\n- Data Splitting\n- Functional MRI\n- Brain Development\n- Statistical Power\n- Cross-Validation\n- Brain-Behavior Associations\n- Reproducibility\n- Clinical Prediction Models\n- Sample Size Determination\n- Adaptive Sample Splitting",
  "dataset/provenance": "The datasets used in our study are publicly available and have been utilized in previous research, ensuring their reliability and relevance. We employed four distinct datasets: the Autism Brain Imaging Data Exchange (ABIDE), the Human Connectome Project (HCP), the Information eXtraction from Images (IXI), and the Breast Cancer Wisconsin (BCW) dataset.\n\nThe ABIDE dataset comprises resting-state data from 866 participants, including individuals with autism spectrum disorder and neurotypical controls. This dataset has been widely used in the community for studying brain connectivity and autism. The preprocessed regional time-series data were obtained from a previous study, which utilized the C-PAC pipeline for preprocessing.\n\nThe HCP dataset includes imaging and behavioral data from approximately 1,200 healthy subjects. We used preprocessed resting-state functional magnetic resonance imaging (fMRI) connectivity data from 999 participants. This dataset is renowned for its high-quality data and has been extensively used in neuroimaging research to build predictive models for various cognitive and behavioral outcomes.\n\nThe IXI dataset, published by the Neuroimage Analysis Center at Imperial College London, consists of approximately 600 structural MRI images from a diverse population of healthy individuals. The dataset includes high-resolution brain images from three different MRI scanners, making it a valuable resource for studying brain development and aging.\n\nThe BCW dataset, available on Kaggle, contains diagnostic data for breast cancer. This dataset has been used in numerous machine learning studies for classification tasks related to breast cancer diagnosis.\n\nAll these datasets are publicly available for download from their respective repositories, ensuring transparency and reproducibility in our research. The ABIDE raw data can be accessed via the 1000 Functional Connectomes Project, while the preprocessed dataset is available on OSF. The HCP raw and preprocessed data are available via ConnectomeDB and the Human Connectome Project website, respectively. The BCW preprocessed dataset is available on Kaggle, and the IXI raw and preprocessed datasets can be accessed via the Biomedical Image Analysis Group and Zenodo, respectively.",
  "dataset/splits": "In our study, we employed various data splitting strategies to evaluate the performance of our models. The primary splits we focused on were the commonly used Pareto split (80% training, 20% testing), a 90% training and 10% testing split, and a half-split (50% training, 50% testing). Additionally, we explored alternative splits such as 75% training and 25% testing, as well as 70% training and 30% testing.\n\nThe Pareto split is widely recognized for its balance between training and testing data, providing a robust evaluation of model performance. The 90% training split, while offering slightly higher performance in some cases, often resulted in inconclusive external validation due to the small size of the testing set, especially with lower sample sizes. The half-split, although it trains on fewer data points, benefits from a larger testing set, which can lead to more statistically significant results.\n\nThe 75% training split demonstrated performance comparable to the Pareto and adaptive splitting techniques but struggled with statistical significance at smaller sample sizes. Conversely, the 70% training split exhibited good statistical significance but at the cost of lower overall performance, similar to the trend observed with the half-split strategy.\n\nIn summary, we utilized multiple data splitting strategies to ensure a comprehensive evaluation of our models' performance. The choice of split ratio depended on the specific requirements of the analysis, with adaptive splitting and half-splitting providing sufficient statistical power for external validation in most cases.",
  "dataset/redundancy": "In our study, we employed a robust adaptive splitting procedure to ensure the independence of model discovery and external validation phases. This approach addresses common issues such as overfitting and cross-validation failure, thereby enhancing the reliability and reproducibility of our results.\n\nThe datasets were split using an adaptive splitting method, which determines the optimal ratio of data to be used for model discovery and external validation. This method ensures that the training and test sets are independent, mitigating the risk of data leakage and ensuring that the model's performance is genuinely evaluated on unseen data.\n\nTo enforce the independence of the training and test sets, we registered our models and utilized the adaptive splitting procedure. This procedure dynamically adjusts the split ratio based on the data characteristics, ensuring that the model is trained on a representative subset of the data while being validated on a completely independent subset.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets. By using registered models and adaptive splitting, we ensure that our datasets are split in a manner that maintains the integrity of the data distribution, avoiding issues such as class imbalance or overfitting to specific subsets of the data. This approach provides a more robust and generalizable evaluation of our models, aligning with best practices in the field of machine learning and neuroimaging.",
  "dataset/availability": "The data utilized in our study are publicly available and can be accessed through various repositories. The Autism Brain Imaging Data Exchange (ABIDE) raw data is available via the 1000 Functional Connectomes Project. The preprocessed dataset for ABIDE is accessible through OSF. The Human Connectome Project (HCP) raw data can be found on ConnectomeDB, while the preprocessed data is available via the Human Connectome Project website. The Breast Cancer Wisconsin (BCW) preprocessed dataset is hosted on Kaggle. For the IXI dataset, both raw and preprocessed data are available via the Biomedical Image Analysis Group at Imperial College London and Zenodo, respectively.\n\nThe data splits used in our analyses are also publicly available. The Python implementation of the AdaptiveSplit package, which includes the code for generating these splits, is available on GitHub. Additionally, the specific Python scripts and data used for our analyses are accessible in the GitHub repository. Archival copies of the code are maintained in Software Heritage to ensure long-term accessibility and reproducibility.\n\nThe availability of these datasets and the associated code ensures that our findings can be independently verified and built upon by other researchers. All data and code are released under licenses that permit unrestricted reuse, distribution, and reproduction, provided that the original work is properly cited. This approach promotes transparency and facilitates the replication of our results, contributing to the broader scientific community's efforts in advancing research in this field.",
  "optimization/algorithm": "The optimization algorithm discussed in this work is not a novel machine-learning algorithm but rather a novel approach to adaptive sample splitting for predictive modeling studies. It focuses on optimizing the trade-off between model discovery and external validation phases.\n\nThe core of this approach is a stopping rule that adaptively determines the optimal splitting strategy during data acquisition. This rule balances model performance and the statistical power of the external validation. It involves continuous model fitting and hyperparameter tuning throughout the discovery phase, evaluating a stopping criterion to decide when to transition to the external validation phase.\n\nThe stopping rule is formalized as a function that takes into account customizable parameters, the data acquired so far, and the machine-learning model to be trained. It ensures that the discovery phase ends only when the desired compromise between model performance and validation power is achieved.\n\nThis adaptive splitting approach is implemented in a Python package called \"AdaptiveSplit.\" The package provides a concrete, customizable implementation for the splitting rule, allowing researchers to apply it to their predictive modeling studies. The algorithm leverages learning curve analysis to estimate current predictive performance and the expected gain from adding new data, ensuring reliable and stable performance estimates.\n\nThe adaptive splitting strategy is evaluated on data involving more than 3,000 participants from four different datasets, demonstrating its effectiveness in identifying the optimal time to stop model discovery and maximize predictive performance without compromising the external validation phase.",
  "optimization/meta": "The model discussed in this publication does not function as a meta-predictor. It does not utilize data from other machine-learning algorithms as input. Instead, it focuses on optimizing the trade-off between model discovery and external validation through an adaptive sample splitting approach. This approach ensures that the model's predictive performance is maximized without risking a low-powered, inconclusive external validation.\n\nThe adaptive splitting procedure starts with fixing a stopping rule. During the discovery phase, candidate models are trained, and the splitting rule is repeatedly evaluated as data acquisition proceeds. When the splitting rule activates, the model is finalized and publicly deposited or registered. This process ensures transparency and a clear separation of model discovery and external validation.\n\nThe independence of the training data is maintained through the adaptive sample splitting procedure. The model is trained on a discovery dataset and then validated on a separate, independent external dataset. This ensures that the external validation is robust and that the model's performance is generalizable to new, unseen data. The proposed design and splitting approach contribute to addressing issues of replicability, effect size inflation, and generalizability in predictive modeling studies.",
  "optimization/encoding": "In our study, data encoding and preprocessing varied depending on the dataset used. For the Autism Brain Imaging Data Exchange (ABIDE) dataset, we utilized preprocessed resting-state data from 866 participants. The regional time-series data were obtained through the Pre-processed Connectome Project, processed using the C-PAC pipeline without global signal regression. We computed tangent correlation across the time series of 122 brain regions, derived from a multilevel bootstrap analysis of stable clusters, to generate functional connectivity estimates as features for our predictive model.\n\nFor the Human Connectome Project (HCP) dataset, which includes imaging and behavioral data from approximately 1,200 healthy subjects, we used preprocessed resting-state functional MRI connectivity data. These data consisted of partial correlations of mean regional time series from 100 brain parcels, derived via independent component analysis. The minimal preprocessing pipelines for structural, functional, and diffusion MRI were employed, focusing on spatial artifact/distortion removal, surface generation, cross-modal registration, and alignment to standard space.\n\nThe Information eXtraction from Images (IXI) dataset, comprising approximately 600 structural MRI images from a diverse population, underwent structural preprocessing using FreeSurfer software. This involved motion correction, skull stripping, removal of the cerebellum and brain stem, intensity correction, segmentation, tessellation, smoothing, and topology correction. Cortical volume measurements were obtained using the Desikan–Killiany brain atlas, resulting in 68 regional volume measures.\n\nThe Breast Cancer Wisconsin (BCW) dataset contains diagnostic features computed from digitized images of fine-needle aspirates of breast masses. The procedure involved extracting cells from suspicious breast tissue areas, smearing them onto glass slides, staining to highlight cellular structures, and scanning to create digital images. Specialized software analyzed these images to extract 30 features quantifying various morphological characteristics of the cell nuclei, such as size, shape, and texture. These features were used to create a predictive model for breast cancer diagnosis.\n\nIn summary, each dataset underwent specific preprocessing steps tailored to its characteristics, ensuring that the data were appropriately encoded and prepared for the machine-learning algorithms. This approach allowed us to maximize the predictive performance and generalizability of our models across different types of data.",
  "optimization/parameters": "In our study, the number of features (p) used in the model is fixed and determined by the dataset being analyzed. These features are the predictors used to make predictions about the target variable. The selection of these features is not part of the optimization process but rather a characteristic of the dataset itself. For instance, in the breast cancer diagnosis example, the features might include various clinical and imaging variables that are known to be relevant for predicting whether a tumor is malignant or benign.\n\nThe adaptive splitting design we propose does not involve selecting or optimizing the number of features. Instead, it focuses on determining the optimal split between the discovery and external validation samples to balance model performance and validation power. The model's architecture and hyperparameters, such as the regularization parameter, are tuned during the discovery phase using cross-validation. However, the set of features used is predetermined based on domain knowledge and the specific dataset.\n\nIn summary, the number of features (p) is a fixed characteristic of the dataset and is not a parameter that is optimized within our adaptive splitting framework. The optimization process primarily deals with the sample size split and hyperparameter tuning to achieve the best model performance and validation power.",
  "optimization/features": "In our study, the number of input features varies depending on the dataset used. For instance, the BCW dataset utilizes 30 different features extracted from digitized images of fine-needle aspirates of breast masses. These features quantify various morphological characteristics of the cell nuclei, such as size, shape, and texture.\n\nFeature selection was not explicitly mentioned as a separate step in our methodology. However, the models employed, such as Ridge regression for regression tasks and L2-regularized logistic regression for classification tasks, inherently perform feature selection through their regularization mechanisms. These methods penalize large coefficients, effectively selecting the most relevant features during the model training process.\n\nThe feature processing workflow was frozen and publicly deposited before data acquisition, ensuring that all feature extraction and preprocessing steps were predefined and not influenced by the training data. This approach guarantees that the feature selection process, as part of the model training, was conducted using the training set only, maintaining the integrity of the external validation process.",
  "optimization/fitting": "The fitting method employed in our study is designed to balance model performance and statistical power through an adaptive splitting strategy. This approach involves continuous model fitting and hyperparameter tuning throughout the discovery phase, with evaluations conducted after the addition of new participants. The stopping rule determines when the desired compromise between model performance and validation power has been achieved, marking the end of the discovery phase and the start of the external validation phase.\n\nTo address the potential issue of overfitting, especially given the complexity of the models used, several measures were implemented. Internal validation approaches, such as cross-validation, were utilized to provide unbiased evaluations of predictive performance. Additionally, the adaptive splitting procedure ensures that the model is not overly sensitive to the training data by continuously assessing the learning curve and power curve. This helps in identifying when the model's performance plateaus, indicating that further addition of data may not significantly improve the model.\n\nInformation leakage, a common cause of overfitting, was mitigated by ensuring that all preprocessing steps, such as feature standardization, were compliant with the cross-validation process. This prevents any information from the test dataset from influencing the training process.\n\nUnderfitting was addressed by ensuring that the model had sufficient complexity to capture the underlying patterns in the data. The adaptive splitting strategy allows for the model to be trained on an optimal number of participants, balancing the need for a large enough training set to capture complex relationships without overfitting.\n\nThe stopping rule includes hard sample size thresholds to prevent early stopping, ensuring that both the discovery and validation samples are of sufficient size. This helps in obtaining reliable performance and power estimates, further mitigating the risks of both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive models. One of the key methods used was regularization. For regression tasks, we utilized Ridge regression, which adds a penalty equal to the square of the magnitude of coefficients to the loss function. This helps to shrink the coefficients of less important features, reducing the model's complexity and preventing overfitting. The regularization parameter, alpha, was fine-tuned using cross-validation to find the optimal balance between bias and variance.\n\nFor classification tasks, we applied L2-regularized logistic regression. Similar to Ridge regression, this method adds a penalty term to the loss function, encouraging smaller coefficients and thus reducing the risk of overfitting. The alpha parameter was also tuned through cross-validation to optimize model performance.\n\nAdditionally, we implemented cross-validation within the model discovery phase. This involved repeatedly holding out parts of the discovery dataset for testing purposes, providing an unbiased evaluation of predictive performance. We also addressed potential issues like information leakage by ensuring that feature preprocessing steps were compliant with the cross-validation procedure.\n\nOur adaptive splitting approach further helped in optimizing the trade-off between model discovery and external validation. By continuously evaluating a stopping rule based on predictive performance and validation power, we determined the optimal point to stop data acquisition for training, thereby minimizing the risk of overfitting and ensuring reliable external validation.",
  "optimization/config": "The source code and requirements for the project are available. The project, named AdaptiveSplit, can be accessed via its GitHub repository. It is platform-independent and requires Python 3.9 or higher. The code is licensed under the GNU General Public License, version 3.0. Additionally, archival copies of the code repositories are available via Software Heritage. This ensures that the hyper-parameter configurations, optimization schedule, model files, and optimization parameters are transparent and accessible to the public. The availability of these resources supports reproducibility and allows other researchers to build upon the work.",
  "model/interpretability": "The interpretability of our predictive models is a critical aspect that we have addressed to ensure transparency and trustworthiness. Our models are not entirely black-box; instead, we have implemented several strategies to enhance their interpretability.\n\nFirstly, we have utilized linear regression-based models in some of our analyses, which are inherently interpretable. These models provide clear coefficients that indicate the direction and magnitude of the relationship between each feature and the outcome variable. For instance, in our study involving brain data to predict behavioral characteristics, the linear models allowed us to identify specific brain regions that significantly contributed to the predictions.\n\nAdditionally, for more complex models like deep neural networks, we have employed techniques such as feature importance analysis and partial dependence plots. These methods help in understanding which features (e.g., specific brain regions or demographic variables) are most influential in the model's predictions. For example, in our age prediction model using gray matter probability images, we generated partial dependence plots to visualize how changes in specific brain regions affect the predicted age.\n\nFurthermore, we have ensured that all feature preprocessing steps and model weights are publicly disclosed upon model registration. This transparency allows other researchers to scrutinize and replicate our models, thereby enhancing interpretability and reproducibility. The adaptive splitting procedure we introduced also contributes to interpretability by clearly separating the model discovery and external validation phases, ensuring that the final model's performance is evaluated on independent data.\n\nIn summary, while some of our models may involve complex algorithms, we have taken steps to make them interpretable through the use of linear models, feature importance analysis, and transparent reporting practices. This approach ensures that our models are not only powerful but also understandable and trustworthy.",
  "model/output": "The model's output varies depending on the dataset and the specific task it is trained for. For the BCW dataset, the model is used for classification to predict diagnosis. The performance is evaluated using a confusion matrix, and the predictive performance is measured by accuracy. Learning and power curves are also provided to assess the model's validity.\n\nFor the IXI dataset, the model is used for regression to predict age. The performance is evaluated by comparing the true age against the predicted age, with Pearson's correlation serving as the measure of predictive performance. Similarly, learning and power curves are used to evaluate the model's validity.\n\nAdditionally, the model has been applied to the HCP dataset for regression to predict fluid intelligence. The performance is visualized using scatterplots and learning curves.\n\nThe AdaptiveSplit package was used to generate learning and power curves for the BCW and IXI datasets, providing a comprehensive view of the model's performance across different sample sizes. The package helps in determining the optimal ratio of data to be used for model discovery and external validation, ensuring robust and flexible predictive modeling.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the AdaptiveSplit package is publicly available. It is implemented in Python and can be accessed via GitHub at the repository https://github.com/pni-lab/adaptivesplit. The package is designed to be platform-independent, ensuring compatibility across various operating systems.\n\nIn addition to the GitHub repository, archival copies of the code are available through Software Heritage. This ensures long-term preservation and accessibility of the codebase.\n\nThe software is licensed under the GNU General Public License, version 3, which allows for free use, modification, and distribution of the code, provided that the original work is properly cited. This license promotes open collaboration and transparency in the scientific community.\n\nThe AdaptiveSplit package includes all necessary requirements and dependencies, with Python 3.9 or higher being the minimum required version. This ensures that users can easily set up and run the software in their environments.\n\nFor those who prefer not to use the source code directly, the package can be utilized through various methods such as executable files, web servers, virtual machines, or container instances. These options provide flexibility in how the software can be integrated into different workflows and research projects.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive analysis using four large, openly available datasets. These datasets encompassed both classification and regression tasks, varying in the number of participants, predictive features, achievable predictive effect size, and data homogeneity.\n\nWe contrasted the proposed adaptive splitting method with fixed training and validation sample sizes, specifically using 50%, 60%, or 90% of the total sample size for discovery and the remainder for external validation. To simulate various sample size budgets, we performed random sampling without replacement. For each total sample size, we incremented the active sample size starting from 10% of the total, increasing by 5% increments. At each step, we evaluated the stopping rule using AdaptiveSplit, fitting either a Ridge model for regression tasks or an L2-regularized logistic regression for classification tasks. Model fitting included cross-validated fine-tuning of the regularization parameter, resulting in nested cross-validation estimates of prediction performance and validation power. Robust estimates and confidence intervals were obtained through bootstrapping.\n\nThis procedure was repeated until the stopping rule was triggered, determining the final discovery sample size. We then trained the models on the discovery sample and obtained predictions for the external validation sample. This entire process was repeated 100 times for each simulated sample size budget in each dataset to estimate confidence intervals for the model’s performance in external validation and its statistical significance.\n\nThe adaptive splitting procedure was performed with a target power of 0.8, alpha = 0.05, tmin = n_total / 3, v_min = 12, and s_min = 0. P-values were calculated using a permutation test with 5,000 permutations. The results confirmed that the adaptive splitting approach successfully identified the optimal time to stop data acquisition for training, maintaining a balance between predictive performance and external validation power across various sample size budgets.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our models across different datasets. The primary metrics reported include the variance explained in cognitive abilities, age prediction, and classification accuracy for diagnostic tasks.\n\nFor the HCP dataset, we measured the variance explained in cognitive abilities using functional brain connectivity, achieving a notable 13%. In the IXI dataset, structural MRI data, specifically gray matter probability maps, explained 48% of the variance in age. For diagnostic tasks, we reported classification accuracy, with 65.5% for autism diagnosis using functional brain connectivity in the ABIDE dataset and an impressive 92% for breast cancer diagnosis in the BCW dataset.\n\nThese metrics are representative of common performance measures in the literature, ensuring that our results are comparable to other studies in the field. We also evaluated the statistical significance of our models' predictive performance, ensuring that our findings are robust and reliable. Additionally, we considered the shape of the learning curve across different sample sizes, providing a comprehensive view of model performance under various conditions.\n\nThe use of these metrics allows for a thorough evaluation of our models' performance, demonstrating their effectiveness in different scenarios and ensuring that our results are meaningful and applicable to real-world research.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our proposed adaptive splitting method against several commonly used fixed splitting strategies. These included the Pareto split (80%:20%), half-split (50%:50%), 90%:10% split, and additional splits such as 75%:25% and 70%:30%, which are frequently encountered in the literature.\n\nWe applied these methods to four large, openly available datasets, each representing different types of tasks and data characteristics. The datasets included functional brain connectivity data from the HCP dataset, structural MRI data from the IXI dataset, functional brain connectivity data for autism diagnosis from the ABIDE dataset, and imaging data for breast cancer diagnosis from the BCW dataset.\n\nOur analysis focused on evaluating the predictive performance and statistical power of each splitting strategy across various sample size budgets. We simulated different sample sizes and incrementally increased the active sample size to assess when each method's stopping rule would indicate sufficient data had been collected for training.\n\nThe adaptive splitting method demonstrated robust performance, providing external validation results comparable to the Pareto split in most cases. It also showed better statistical power compared to the 90%:10% split, which often resulted in inconclusive validation due to insufficient sample sizes for external validation.\n\nAdditionally, we compared our method to simpler baselines like the half-split, which, despite using fewer samples for training, managed to maintain statistical power due to a larger validation sample. The 75%:25% split performed similarly to the Pareto and adaptive splits but struggled with smaller sample sizes. The 70%:30% split, while statistically significant, showed lower overall performance.\n\nOverall, our adaptive splitting method proved effective in balancing predictive performance and statistical power, making it a reliable choice for various research scenarios, especially when the expected predictive performance is low.",
  "evaluation/confidence": "In the evaluation of our models, we have considered various performance metrics and their statistical significance. For instance, we have examined the relationship between discovery scores, external validation scores, and sample size across different datasets. This analysis helps illustrate how model performance on the discovery set compares with external validation as the sample size increases.\n\nWe have also explored different data splits, such as 75-25% and 70-30%, in comparison to the adaptive-split method. The 75-25% split shows performance comparable to other methods but with lower statistical significance at smaller sample sizes. On the other hand, the 70-30% split is robust but comes at the cost of lower performance.\n\nOur evaluation includes learning curves and power curves, which provide insights into the predictive performance and statistical power of the models. These curves help us understand the model's validity and its ability to generalize to new data.\n\nAdditionally, we have used nested cross-validation to ensure that our performance metrics are reliable and not overly optimistic. This method helps in obtaining a more accurate estimate of the model's performance by reducing overfitting.\n\nIn summary, our evaluation process includes a thorough examination of performance metrics, statistical significance, and the use of robust validation techniques to ensure the reliability and generalizability of our results.",
  "evaluation/availability": "The raw and preprocessed data used in our study are publicly available for download from their respective repositories. For the Autism Brain Imaging Data Exchange (ABIDE) dataset, the raw data can be accessed via the 1000 Functional Connectomes Project, while the preprocessed dataset is available on OSF. The Human Connectome Project (HCP) data, both raw and preprocessed, can be found on ConnectomeDB and the Human Connectome Project website. The Breast Cancer Wisconsin (BCW) preprocessed dataset is hosted on Kaggle. For the Information eXtraction from Images (IXI) dataset, raw data is available through the Biomedical Image Analysis Group, and the preprocessed dataset can be accessed via Zenodo.\n\nAdditionally, the Python implementation of the AdaptiveSplit package, which was used in our evaluation, is publicly available on GitHub. The Python scripts and data used for the analyses presented in this manuscript can also be accessed in the GitHub repository, with archival copies of the code available in Software Heritage. The Dome-ML annotations are available via the DOME registry. All these resources are accessible under licenses that permit unrestricted reuse, distribution, and reproduction, provided the original work is properly cited."
}