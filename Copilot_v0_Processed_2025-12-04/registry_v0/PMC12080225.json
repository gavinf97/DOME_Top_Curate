{
  "publication/title": "HVSeeker: a deep learning-based method for identification of host and viral DNA sequences",
  "publication/authors": "The authors who contributed to this article are:\n\n- **A. A.-N.** and **S. H.** developed the software.\n- **A. A.-N.**, **S. H.**, and **O. S. A.** wrote the initial draft of the manuscript.\n- **O. S. A.** conceived the study and analyses.\n- **O. S. A.** and **V. D. T.** performed the data acquisition.\n- **O. S. A.** and **R. B.** oversaw the project.\n- All authors reviewed, contributed to, and approved the manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Deep learning\n- DNA sequences\n- Host-virus identification\n- Machine learning\n- Sequence analysis\n- Bioinformatics\n- Viral metagenomics\n- Bacteriophage prediction\n- Model benchmarking\n- Preprocessing techniques",
  "dataset/provenance": "The dataset used in this study was gathered from well-known bioinformatics databases, specifically the National Center for Biotechnology Information (NCBI) and the Integrated Microbial Genomes & Microbiomes—Viruses (IMGVR). The data consists of bacterial and phage DNA sequences, with each file containing a complete genome of varying lengths. The dataset includes metadata such as a unique ID for each sequence and its length.\n\nThe full dataset comprises 536 bacterial sequences and 2,687 phage sequences. These sequences were encapsulated within FASTA files and varied in both biological origins and lengths. The dataset was preprocessed to extract proteins from the diverse DNA sequences, which correspond to multiple classes. The input data for the subsequent training models constituted both DNA and protein sequences.\n\nThe dataset was split into training, validation, and testing sets. The training set consisted of 452,608 sequences, which is 80% of the total dataset. The validation set included 56,576 sequences, making up 10% of the dataset, and the testing set also contained 56,576 sequences, which is the remaining 10%.\n\nThe dataset was further processed to ensure consistency in data length. DNA sequences were adjusted to a uniform standard of 1,000 base pairs (bp). Sequences shorter than this length were handled using three distinct strategies: padding, contigs assembly, and a sliding-window process. Padding involved cycling through the sequence until it reached the required length. The contigs assembly method combined multiple shorter sequences to create a longer sequence, which was then split into subsequences of 1,000 bp. The sliding-window process used a 1,000 bp window to slide over the input DNA sequence in 100 bp steps.\n\nAfter preprocessing, the dataset increased to 565,760 DNA sequences. The model training process employed the holdout method, with 80% of the data used for training and the remaining 20% equally split between validation and testing. To prevent overfitting, an early-stopping technique was used, which terminates the training when the performance on the validation set degrades.",
  "dataset/splits": "The dataset used in our study comprises a total of 565,760 DNA sequences. These sequences are divided into three main splits: the training set, the validation set, and the testing set.\n\nThe training set consists of 452,608 sequences, which accounts for 80% of the entire dataset. This split is used to train the model, allowing it to learn patterns and features from the data.\n\nThe validation set contains 56,576 sequences, making up 10% of the dataset. This split is utilized to fine-tune the model parameters and prevent overfitting during the training process.\n\nThe testing set also includes 56,576 sequences, which is another 10% of the dataset. This split is reserved for the final evaluation of the model's performance, ensuring that the model's effectiveness is assessed on unseen data.\n\nIn summary, the dataset is split into three parts: a training set with 452,608 sequences, a validation set with 56,576 sequences, and a testing set with 56,576 sequences. This distribution allows for comprehensive training, validation, and testing of the model.",
  "dataset/redundancy": "The datasets were split into training, validation, and testing sets to ensure independent evaluation and prevent data leakage. The training set comprised 80% of the total sequences, while the validation and testing sets each contained 10% of the sequences. This split was maintained consistently across different datasets, including those for DNA and protein sequences.\n\nTo enforce independence between the training and test sets, techniques such as undersampling were employed to balance the classes and prevent bias. Additionally, for protein sequence datasets, BLAST was used to limit sequence homology, ensuring that the training and test sets did not contain overly similar sequences. This approach helped in capturing a broad representation of protein diversity and maintained the independence of the datasets.\n\nThe distribution of the datasets in this study is designed to be more comprehensive compared to previously published machine learning datasets in the field. By ensuring a balanced and independent split, the models were trained and evaluated on diverse and representative data, which is crucial for generalizable and reliable predictions. This methodology aims to address the challenges of imbalanced datasets and ensure that the models perform well across various sequence types and lengths.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages deep learning techniques, specifically Long Short-Term Memory (LSTM) networks for the DNA-based model and ProteinBERT for the protein-based model. LSTMs are a type of recurrent neural network (RNN) designed to handle sequential data, making them well-suited for DNA sequences where each nucleotide is contextually related to its neighbors. The DNA-based model, HVSeeker-DNA, consists of three bidirectional LSTM units followed by fully connected layers and a softmax activation function for final predictions. This architecture allows the model to capture complex patterns in the DNA sequences effectively.\n\nFor the protein-based model, HVSeeker-Protein, we utilized ProteinBERT, a pre-trained model on a vast dataset of protein sequences. ProteinBERT provides robust embeddings that accurately represent input proteins, enabling precise classification. The model's architecture includes a transformer with multiple attention heads and layers, fine-tuned using a Bayesian optimizer to adjust parameters such as the learning rate, number of training epochs, and learning rate decay factor. This fine-tuning process ensures optimal performance and generalization across different datasets.\n\nThe choice of these specific algorithms was driven by their proven effectiveness in handling sequential and biological data. While these algorithms are not entirely new, their application in the context of bacteriophage identification and host prediction is innovative. The focus of our publication is on the biological and computational biology aspects of the problem, rather than the development of new machine-learning algorithms. Therefore, it is more appropriate to publish these findings in a computational biology or bioinformatics journal, where the biological significance and practical applications of the models can be fully appreciated.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it is a deep learning-based method called HVSeeker, designed specifically for the identification of host and viral DNA sequences in metagenomic data.\n\nHVSeeker employs a robust architecture that includes three bidirectional LSTM layers, followed by a linear layer with dropout and an ELU activation function. The model processes one-hot encoded DNA data, which is prepared using various preprocessing methods such as padding, contig assembly, and sliding windows.\n\nThe optimization process involves fine-tuning parameters like the learning rate, number of training epochs, and learning rate decay factor. The effectiveness of these optimizations is evaluated using five-fold cross-validation over five runs, ensuring that sequence homology between training and test sets does not exceed 0.95. This rigorous testing method guarantees a fair and reliable assessment of the model's performance.\n\nThe model's performance is evaluated using key metrics such as accuracy, precision, recall, and F1 score. These metrics provide a comprehensive view of the model's predictive strength. The experiments conducted to optimize sequence length and preprocessing techniques demonstrate the model's ability to adapt to various conditions and maintain high accuracy and robustness.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for preparing the genomic sequence data for the machine-learning algorithm. Initially, DNA sequences were standardized to a uniform length of 1,000 base pairs (bp). Sequences shorter than this length were handled using three strategies: padding, contigs assembly, and a sliding-window process. Padding involved repeating the sequence until the required length was attained. The contigs assembly approach combined multiple shorter sequences to create a longer sequence, which was then split into 1,000 bp subsequences, with any remaining segments padded. The sliding-window process involved selecting the first 1,000 bp of each sequence and then moving the window by 100 bp increments until the end of the DNA sequence.\n\nAfter standardizing the sequence lengths, exact duplicate sequences were removed to ensure data uniqueness. The nucleotide sequences, originally in the form of adenine, cytosine, guanine, and thymine (ACGT), were then transformed into a binary matrix using one-hot encoding. This conversion facilitated easier processing and interpretation of the genetic data by the computational model.\n\nTo address class imbalance in the dataset, an undersampling approach was employed. This procedure ensured that the model did not exhibit bias towards any particular class, leading to more reliable and generalizable predictions. Following these preprocessing steps, the dataset was increased to 565,760 DNA sequences.\n\nFor the model training process, the holdout method was used, which is recommended for larger datasets. The data was allocated with 80% for the training set, and the remaining 20% was equally split between validation and testing, at 10% each. To prevent overfitting, an early-stopping technique was implemented, terminating the training when the performance on the validation set degraded.\n\nThe final dataset comprised 98,720 unique phage sequences and 122,366 unique bacterial sequences. To ensure a broad representation of protein diversity, BLAST was used to limit sequence homology between sets. This approach allowed for the effective capture of the wide range of diversity found in bacterial and phage proteins. For testing purposes, the data was divided into 80% training data and 20% test data, and this process was repeated five times.",
  "optimization/parameters": "In our study, the optimization process involved fine-tuning several key parameters to enhance the model's performance. Specifically, we focused on optimizing the learning rate, the number of training epochs, and the learning rate decay factor. These parameters were fine-tuned using a Bayesian optimizer, which employed an expected improvement acquisition function. The optimization process was conducted over 25 evaluations on 1,000 proteins sampled from the training set. This method allowed us to systematically explore the parameter space and identify the optimal settings for each parameter.\n\nThe selection of these parameters was guided by their critical role in the training dynamics of the model. The learning rate determines the step size at each iteration while moving toward a minimum of the loss function. An appropriately chosen learning rate ensures that the model converges efficiently without overshooting the optimal solution. The number of training epochs dictates how many times the entire training dataset is passed through the model. Too few epochs may result in underfitting, while too many can lead to overfitting. The learning rate decay factor adjusts the learning rate over time, helping to fine-tune the model as training progresses.\n\nTo ensure the robustness of our optimization process, we conducted five-fold cross-validation for five runs. This approach involved dividing the data into five subsets, using four subsets for training and one for validation in each fold. This method helped us to rigorously test the model across different subsets of the data, ensuring that our assessment of its performance was both fair and reliable. Additionally, we used BLAST to ensure that sequence homology between the training and test sets did not exceed 0.95, further enhancing the reliability of our evaluations.",
  "optimization/features": "The input features for our models are derived from DNA sequences, which are transformed into a binary matrix using one-hot encoding. This process results in a matrix of dimensions 6 × 1,000, where each nucleotide is represented by a unique binary vector. Therefore, the number of features (f) used as input is 6,000.\n\nFeature selection was not explicitly performed in the traditional sense, as the one-hot encoding process inherently selects all possible nucleotide representations. The choice of sequence length (1,000 base pairs) was determined through experimentation and optimization, ensuring that the model captures sufficient informational content without introducing excessive noise.\n\nThe standardization of DNA sequence lengths to 1,000 base pairs was a crucial preprocessing step. This length was chosen based on extensive testing, which showed that it provided the best balance between capturing relevant sequence information and avoiding overfitting. Shorter sequences were handled using padding, assembly of contigs, or a sliding-window process, ensuring consistent input lengths for the model. Duplicate sequences were removed to avoid bias, and class balance in the dataset was achieved through undersampling. These steps collectively ensured that the input features were robust and reliable for training the models.",
  "optimization/fitting": "In our study, we carefully addressed the issues of overfitting and underfitting to ensure the robustness of our model. To mitigate overfitting, we employed several strategies. First, we used five-fold cross-validation for the optimization process over five runs. This method allowed us to rigorously test the model across different subsets of the data, ensuring that our assessment of its performance was fair and reliable. Additionally, we ensured that sequence homology between the training and test sets did not exceed 0.95 by using BLAST. This step was crucial in preventing the model from memorizing specific sequences and generalizing better to unseen data.\n\nWe also experimented with different sequence lengths to find the optimal balance. Sequences of 2,000 base pairs (bp) showed signs of overfitting, as the model consistently predicted the bacteria class, indicating an inability to generalize. In contrast, sequences of 1,000 bp demonstrated the best performance in terms of the F1 score, suggesting that this length provided a good balance between capturing sufficient information and avoiding overfitting.\n\nTo address underfitting, we compared our model with existing tools and found that our approach outperformed others in terms of precision, recall, accuracy, and F1 score. For instance, Rnn-VirSeeker struggled with underfitting, achieving only about 50% accuracy on the training data. This underperformance was likely due to the label-encoding technique used, which can skew data analysis and interpretation. Our model, however, achieved validation accuracies exceeding 80%, indicating a robust architecture capable of adapting to various preprocessing strategies.\n\nFurthermore, we conducted experiments using different preprocessing techniques, such as padding, assembly of contigs, and sliding windows. The padding method slightly outperformed the others in terms of validation accuracy, likely due to its tendency to duplicate nucleotides, providing more consistent training data. The assembly method, which might combine sequences from varied origins, introduced higher variability and confusion, potentially leading to underfitting. Despite these differences, all three methods achieved similar scores across all metrics, highlighting the robustness of our model's architecture and preprocessing steps.\n\nIn summary, we employed cross-validation, controlled sequence homology, and optimized sequence lengths to address overfitting. We also ensured that our model did not underfit by comparing it with existing tools and using effective preprocessing techniques. These measures collectively contributed to the model's ability to generalize well to unseen data.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of early stopping during the training process. This technique monitors the model's performance on a validation set and halts training when the performance starts to degrade, thereby preventing the model from becoming too specialized to the training data.\n\nAdditionally, we employed dropout regularization in our HVSeeker-DNA model. Specifically, we used a dropout rate of 0.2 in the fully connected layer following the LSTM units. Dropout randomly sets a fraction of the input units to zero during training, which helps to prevent the model from relying too heavily on any single feature and promotes generalization.\n\nAnother crucial step in our preprocessing pipeline was the removal of duplicate sequences. This ensured that the model did not memorize specific sequences but rather learned to generalize from the data.\n\nFurthermore, we utilized five-fold cross-validation to rigorously test the model across different subsets of the data. This method helps to ensure that the model's performance is consistent and not just a result of overfitting to a particular subset of the data.\n\nTo maintain class balance, we employed undersampling. This technique ensures that the model does not exhibit bias towards any particular class, leading to more reliable and generalizable predictions.\n\nLastly, we standardized the DNA sequence lengths to 1,000 base pairs and handled shorter sequences using padding, assembly of shorter sequences, or a sliding-window process. These preprocessing steps helped to create a consistent input format for the model, reducing the risk of overfitting to varying sequence lengths.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are thoroughly documented within the publication. These details include the learning rate, number of training epochs, and learning rate decay factor, which were fine-tuned to optimize model performance. The specific configurations and schedules are outlined in the results and discussion sections, where we describe the experiments conducted to determine the best sequence lengths and preprocessing techniques.\n\nThe model files, including the trained models for both HVSeeker-DNA and HVSeeker-Protein, are available for download. These files can be accessed through the supplementary materials provided with the publication. The models are shared under an open-source license, allowing researchers to use, modify, and distribute them for further studies or applications.\n\nAdditionally, the optimization parameters and the methods used for evaluation, such as five-fold cross-validation and BLAST for sequence homology, are detailed in the methodology section. This ensures transparency and reproducibility of the results. The dataset used for training, validation, and testing is also described, including the steps for preparing and formatting the data.\n\nFor those interested in replicating our experiments or building upon our work, all necessary information is provided, including scripts and code snippets used in the training and evaluation processes. This comprehensive documentation aims to facilitate further research and development in the field of bacteriophage classification.",
  "model/interpretability": "The model HVSeeker, while leveraging deep learning techniques, is not entirely a black-box system. Its architecture and preprocessing steps provide some level of interpretability. The DNA data is initially encoded using principles such as padding, contigs-assembly, and sliding windows, which are straightforward and understandable preprocessing techniques. These methods ensure that the input data is consistent and suitable for the model's requirements.\n\nThe core of the model consists of three bidirectional LSTM layers, which are followed by a linear layer with a moderate amount of dropout and an ELU activation function. The use of bidirectional LSTMs allows the model to capture both past and future context in the sequences, which can be crucial for understanding the patterns in DNA data. The dropout layer helps in regularizing the model, preventing overfitting, and making the model more robust.\n\nThe output layer provides predictions based on the processed data. The model's performance is evaluated using metrics such as precision, recall, accuracy, and F1 score, which are standard and interpretable metrics in the field of machine learning. These metrics give a clear view of the model's predictive strength and its ability to generalize to unseen data.\n\nAdditionally, the model's performance was assessed across different sequence lengths and preprocessing techniques. The findings indicate that the model performs best with a sequence length of 1,000 base pairs, and the padding method slightly outperformed others in terms of validation accuracy. These insights provide a clearer understanding of how different factors influence the model's performance.\n\nOverall, while the deep learning components of HVSeeker may not be fully transparent, the preprocessing steps, architecture choices, and evaluation metrics provide a level of interpretability. This allows researchers to understand how the model processes data and makes predictions, making it more than just a black-box system.",
  "model/output": "The model is a classification model designed to identify host and viral DNA sequences. Specifically, it classifies sequences as either bacteriophage or bacterial. The output of the model is a prediction indicating the class of the input DNA sequence. This is achieved through a softmax activation function in the final layer, which provides probabilities for each class, allowing for the classification of the sequence.\n\nThe model employs bidirectional LSTM layers followed by fully connected layers to process the input DNA sequences, which are one-hot encoded. The architecture is tailored to handle sequential data, making it suitable for DNA sequence analysis. The final output is a classification result, which is evaluated using metrics such as precision, recall, accuracy, and F1 score to assess the model's performance.\n\nThe model's output is formatted to be compatible with various preprocessing techniques, including padding, assembly of contigs, and sliding windows. These techniques ensure that the input sequences are consistent in length and format, which is crucial for the model's accuracy. The output is also designed to be easily interpretable, providing clear indications of the model's predictions and their confidence levels.\n\nThe model's performance is rigorously tested using cross-validation and benchmarking against existing tools. This ensures that the model generalizes well to unseen data and maintains high accuracy across different sequence lengths and preprocessing methods. The output of the model is saved and can be further analyzed or used in downstream applications, such as identifying viral sequences in genomic data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for HVSeeker is publicly available. It can be accessed through the Software Heritage archive, which provides a permanent and reliable way to preserve and share software. The specific archive entry for HVSeeker can be found at the following URL: https://archive.softwareheritage.org/swh:1:snp:02520873e4566c63c913d962d9fd01d81410f348. Additionally, the source code is hosted on GitHub, where it can be accessed and downloaded by researchers and developers interested in using or contributing to the project. The GitHub repository is located at https://github.com/bulatef/HVSeeker.\n\nThe software is distributed under the terms of the Creative Commons Attribution License, which allows for unrestricted reuse, distribution, and reproduction in any medium, provided that the original work is properly cited. This license ensures that the software can be freely used and adapted by the scientific community, fostering collaboration and innovation.\n\nFor those who prefer not to run the software locally, HVSeeker can also be accessed through a web server. This server provides a user-friendly interface for running the algorithm without the need for local installation or configuration. The web server is part of the DOME-ML (Data and Opinion Mining for Evidence-based Medicine and Life sciences) initiative, which aims to make scientific software more accessible and interoperable. The specific entry for HVSeeker in the DOME-ML registry can be found at https://registry.dome-ml.org/review/igr5x3a1vs.\n\nAdditionally, sample data for both DNA and protein sequences is available on the GitHub repository. This data can be used to test and validate the software, ensuring that it performs as expected in different scenarios. The sample data is organized into two main directories: HVSeeker-DNA/Sample_Data and HVSeeker-Protein/Sample_Data. These directories contain a variety of sequences that can be used to evaluate the performance of the algorithm.",
  "evaluation/method": "The evaluation of our method involved a rigorous process to ensure its effectiveness and reliability. We employed five-fold cross-validation for the optimization process, conducting five runs to fine-tune parameters such as the learning rate, number of training epochs, and learning rate decay factor. This approach allowed us to optimize the model’s performance across different subsets of the data.\n\nTo ensure that sequence homology between the training and test sets did not exceed 0.95, we used BLAST. This step was crucial for rigorously testing the model's ability to generalize to unseen data, ensuring that our assessment of its performance was fair and reliable.\n\nFor evaluating the effectiveness of the proposed models, we focused on four key metrics: accuracy, precision, recall, and F1 score. Accuracy was calculated as the ratio of correctly predicted instances to the total number of predictions. These metrics provided a comprehensive view of the model's performance, highlighting its strengths and areas for improvement.\n\nIn addition to cross-validation, we compared our method with existing bacteriophage classification approaches, including Seeker, Rnn-VirSeeker, DeepVirFinder, and PPR-Meta. Each algorithm was trained using the same dataset to ensure a fair comparison. The results demonstrated that our method consistently outperformed the alternatives across most evaluated metrics, particularly in terms of recall and F1 score. This superior performance can be attributed to the balanced dataset used for training and the robustness of our preprocessing and model architecture.\n\nFurthermore, we conducted experiments to test the model's ability to generalize to data with varying sequence lengths and homology conditions. The results indicated that our method achieved high performance across different sequence lengths, with the best F1 score observed at 1,000 base pairs and the best recall at 1,500 base pairs. However, excessively large sequence lengths led to overfitting, while overly small sequences underperformed.\n\nOverall, the evaluation process confirmed the effectiveness and reliability of our method, showcasing its ability to accurately process sequences from varied environments and generalize to unseen data.",
  "evaluation/measure": "In our evaluation, we focused on four key metrics to assess the effectiveness of the proposed models: accuracy, precision, recall, and F1 score. These metrics are widely recognized and used in the literature for evaluating the performance of sequence identification methods.\n\nAccuracy is the ratio of correctly predicted instances to the total number of predictions. It provides a general measure of how often the model is correct. Precision, on the other hand, measures the proportion of true positive predictions among all positive predictions made by the model. It is crucial for understanding the model's ability to avoid false positives. Recall, also known as sensitivity, measures the proportion of true positive predictions among all actual positives in the dataset. It is important for understanding the model's ability to identify all relevant instances. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nThese metrics are representative of the standards used in the field. They allow for a comprehensive evaluation of the model's performance, ensuring that we consider both the correctness of positive predictions and the ability to identify all relevant instances. By reporting these metrics, we aim to provide a clear and comparable assessment of our models' effectiveness.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed method, HVSeeker, against several publicly available tools using benchmark datasets. The tools compared included Seeker, Rnn-VirSeeker, DeepVirFinder, and PPR-Meta. All algorithms were trained using the same dataset to ensure a fair comparison.\n\nThe evaluation metrics used were precision, recall, accuracy, and F1 score. HVSeeker consistently outperformed the other methods across most metrics. For instance, HVSeeker achieved an F1 score of 76.73%, significantly higher than Seeker's 57.79%, Rnn-VirSeeker's 0%, DeepVirFinder's 41.66%, and PPR-Meta's 44%. This superior performance was evident in both the overall benchmark dataset and in datasets with varying degrees of homology, demonstrating HVSeeker's robustness and generalizability.\n\nAdditionally, we compared HVSeeker against simpler baselines, such as Hidden Markov Models (HMMs), to assess its effectiveness. The results showed that HVSeeker's performance was markedly better, indicating that our method's complexity is justified by its superior accuracy and reliability.\n\nThe comparison also included an analysis of different preprocessing methods, such as padding, contigs assembly, and sliding window approaches. HVSeeker performed well across all preprocessing strategies, further highlighting its versatility and effectiveness in various scenarios.",
  "evaluation/confidence": "The evaluation of our method's performance was conducted rigorously to ensure the reliability of our results. We employed five-fold cross-validation for the optimization process, which involved five runs. This approach helps to mitigate the risk of overfitting and provides a more robust estimate of the model's performance.\n\nTo ensure that our model's performance was not merely a result of memorizing the training data, we used BLAST to guarantee that sequence homology between the training and test sets did not exceed 95%. This method ensures that our model's performance is evaluated on data that is sufficiently different from the training set, providing a fair and reliable assessment.\n\nWe focused on four key metrics: accuracy, precision, recall, and F1 score. These metrics were chosen because they provide a comprehensive view of the model's performance across different aspects of classification. Accuracy measures the overall correctness of the predictions, precision indicates the correctness of positive predictions, recall measures the ability to identify all relevant instances, and the F1 score provides a balance between precision and recall.\n\nThe results of our evaluation show that our method consistently outperforms other tools across all measured metrics. For instance, our method achieved an F1 score of 0.767, which is significantly higher than that of other methods like Seeker, DeepVirFinder, and PPR-Meta. This indicates that our method is not only accurate but also reliable in identifying relevant sequences while minimizing false positives.\n\nHowever, it is important to note that while our method shows superior performance, the specific confidence intervals for the performance metrics are not provided in this summary. Additionally, the statistical significance of the results compared to other methods is not explicitly stated. Therefore, while the results are promising, further detailed statistical analysis would be necessary to claim definitively that our method is superior to others and baselines.",
  "evaluation/availability": "Not enough information is available."
}