{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\nLukas Roth, who contributed to conceptualization, data curation, formal analysis, funding acquisition, investigation, methodology, software, supervision, validation, visualization, and writing the original draft.\n\nMike Boss, who contributed to conceptualization, data curation, formal analysis, methodology, software, validation, visualization, and writing the original draft.\n\nNorbert Kirchgessner, who contributed to conceptualization, data curation, formal analysis, methodology, software, validation, visualization, and writing the original draft.\n\nHelge Aasen, who contributed to investigation and supervision.\n\nBrenda Patricia Aguirre-Cuellar, who contributed to investigation.\n\nPrice Pius Atuah Akiina, who contributed to investigation.\n\nJonas Anderegg, who contributed to data curation, investigation, and methodology.\n\nJoaquin Gajardo Castillo, who contributed to data curation and software.\n\nXiaoran Chen, who contributed to investigation.\n\nSimon Corrado, who contributed to investigation.\n\nKrzysztof Cybulski, who contributed to data curation and software.\n\nBeat Keller, who contributed to investigation and supervision.\n\nStefan Göbel Kortstee, who contributed to investigation.\n\nLukas Kronenberg, who contributed to investigation and supervision.\n\nFrank Liebisch, who contributed to investigation and supervision.\n\nParaskevi Nousi, who contributed to investigation.\n\nCorina Oppliger, who contributed to investigation.\n\nGregor Perich, who contributed to investigation.\n\nJohannes Pfeifer, who contributed to investigation.\n\nKang Yu, who contributed to investigation.\n\nNicola Storni, who contributed to data curation, investigation, and software.\n\nFlavian Tschurr, who contributed to data curation, investigation, and software.\n\nMichele Volpi, who contributed to investigation and supervision.\n\nSimon Treier, who contributed to data curation and investigation.\n\nHansueli Zellweger, who contributed to investigation.\n\nOlivia Zumsteg, who contributed to investigation.\n\nAndreas Hund, who contributed to conceptualization, methodology, project administration, and writing the review and editing.\n\nAchim Walter, who contributed to conceptualization, funding acquisition, project administration, supervision, and writing the review and editing.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Wheat\n- Genetic Markers\n- Data Set\n- Environmental Covariates\n- Spatial Analysis\n- Deep Learning\n- Trait Prediction\n- Genotyping\n- Agricultural Research\n- Image Time Series",
  "dataset/provenance": "The FIP 1.0 dataset is a comprehensive collection of highly resolved annotated image time series. It comprises data from 4,000 wheat plots grown over six years. The dataset is designed to be interoperable and reusable, adhering to the MIAPPE v1.1 standards and released under the CC0 1.0 Universal license. This permissive license allows for extensive use and further development of the data.\n\nThe dataset can be accessed from multiple sources, including ETH Zurich’s Research Collection, the BioImage Archive, and as a Hugging Face dataset card. Additionally, snapshots of specific components of the dataset, such as the FIP 1.0 Data Set-Traits, fip1-alignment, and fip1-dataset, are archived in Software Heritage. The DOME-ML annotations are available in the DOME-ML registry.\n\nThe dataset builds upon existing subsets that have been previously released and used as baseline approaches in various studies. It expands on these subsets by adding data from six additional environments, bringing the total to 14 environments. This expansion enhances the dataset's utility for research and analysis.\n\nThe dataset includes detailed information on sowing and harvest dates for each year, providing a comprehensive temporal context for the data. The experimental field designs and genotypes used in the study are well-documented, ensuring reproducibility and reliability. The dataset's design and availability make it a valuable resource for the scientific community, facilitating further research and development in plant phenomics.",
  "dataset/splits": "The dataset is divided into three main splits: training, validation, and test sets. The test set is further subdivided into four subsets, and the validation set into two subsets.\n\nThe training set contains 262 genotypes for the GABI-WHEAT set and 747 genotypes for the extended set. The validation set includes 24 genotypes for the GABI-WHEAT set and 30 genotypes for the extended set. The test set, specifically the (G and E) subset, contains 30 genotypes for both the GABI-WHEAT and extended sets.\n\nThe four subsets of the test set are:\n\n1. **Test (P)**: Unseen plots with seen genotypes and seen environments (all years except 2019).\n2. **Test (G)**: Unseen genotypes with seen environments.\n3. **Test (E)**: Unseen environment (2019) with seen genotypes.\n4. **Test (G and E)**: Unseen genotypes and unseen environment (2019).\n\nThe two subsets of the validation set are:\n\n1. **Validation (default)**: Unseen genotypes.\n2. **Validation (plots)**: Seen genotypes on unseen plots.\n\nThe splits are designed to ensure that genotypes occurring in different sets are reported at the lowest level, following the order train < validation < test. This balanced splitting approach was validated using 5-fold cross-validation, ensuring the quality of the splits. The splits were calculated separately for the pure GABI-WHEAT set and the extended set, which includes additional F8 generation genotypes with private marker data.",
  "dataset/redundancy": "The datasets were split into training, validation, and test sets to ensure comprehensive evaluation. The test set includes four subsets: unseen genotypes, unseen environments, unseen genotypes in unseen environments, and seen genotypes in seen years on unseen plots. The validation set comprises two subsets: unseen genotypes and seen genotypes on unseen plots. These splits were visualized based on plots and genotype count, with genotypes occurring in different sets reported at the lowest level (train < validation < test).\n\nTo ensure independence between the training and test sets, a balanced splitting approach was employed. This approach used genetic relatedness to balance the train/test set, theoretically yielding performance close to the average of all cross-validation runs. The R package STPGA was utilized with the algorithm \"D opt\" to determine a test set that complements the training set. This method was applied to both the pure GABI-WHEAT set and the extended set, which includes additional F8 generation genotypes with private marker data.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field. For the GABI-WHEAT set, the training set includes 262 genotypes, the validation set 24 genotypes, and the test set 30 genotypes. For the extended set, the training set includes 747 genotypes, the validation set 30 genotypes, and the test set 30 genotypes. This splitting ensures that the datasets are robust and representative, allowing for reliable model training and evaluation.",
  "dataset/availability": "The FIP 1.0 dataset is publicly available through multiple platforms, ensuring broad accessibility. It can be accessed from ETH Zurich’s Research Collection, the BioImage Archive, and as a Hugging Face dataset card. Additionally, snapshots of the GitLab repositories, including FIP 1.0 Data Set-Traits, fip1-alignment, and fip1-dataset, have been archived in Software Heritage. The DOME-ML annotations are available in the DOME-ML registry.\n\nThe dataset is distributed under the CC0 1.0 Universal Public Domain Dedication, which allows for unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. This license ensures that the data can be freely used by the scientific community without legal constraints.\n\nTo enforce the proper use of the dataset, users are required to cite the original work when utilizing the data. This citation requirement is a standard practice in academic publishing and helps maintain the integrity and recognition of the original research.\n\nFor the private Agroscope marker data repository, access is confidential and requires contacting Boulos Chalhoub. Users must state their intended purpose of use and be willing to sign a material transfer agreement (MTA). This ensures that the sensitive data is used responsibly and in accordance with the guidelines set by Agroscope.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages genetic relatedness to balance the train/test set, an approach that is not entirely novel but has been adapted for our specific needs. This method is designed to theoretically achieve performance close to the average of all cross-validation runs, making it particularly suitable for complex deep learning models where traditional cross-validation is computationally expensive.\n\nThe algorithm used is implemented through the R package STPGA, which is a tool for selecting training populations by genetic algorithm. Specifically, we utilized the \"D opt\" algorithm as suggested in previous research. This algorithm is well-established in the field of genomic prediction and has been used to design training populations for selective phenotyping.\n\nThe choice of this algorithm is driven by its effectiveness in handling the genetic data and its ability to complement the training set with a test set that balances genetic relatedness. This approach ensures that the model's predictions are robust and generalizable to unseen genotypes and years.\n\nWhile the algorithm itself is not new, its application in our study is tailored to address the specific challenges of our dataset and research objectives. The focus of our publication is on the agricultural and genomic aspects of our work, rather than the machine-learning algorithm per se. Therefore, the algorithm's implementation and its adaptation to our needs are discussed in the context of their application to genomic prediction and trait evaluation.",
  "optimization/meta": "The meta-predictor approach used in this work does not rely on data from other machine-learning algorithms as input. Instead, it focuses on balancing the train/test set using genetic relatedness, which is a novel method proposed to address the computational expense of cross-validation for complex deep learning models.\n\nThe meta-predictor integrates several machine-learning methods to enhance genomic prediction accuracy. Specifically, it employs a random regression model that incorporates environmental covariates such as hourly temperature, precipitation, and relative humidity. These covariates are extracted from weather data sources like the Climate Data Center of the German Weather Service and Meteo France. The random regression model is compared against other models, and its performance is evaluated across various traits.\n\nThe training data used in this approach is designed to be independent. The method involves defining multiple test sets to allow for specific evaluation of different traits. For instance, unseen genotypes in unseen environments and seen genotypes in unseen environments are treated as independent test sets. This ensures that the training data does not overlap with the test data, maintaining the independence required for robust model evaluation.\n\nThe genetic marker data, which includes a 90k single nucleotide polymorphism (SNP) array for the GABI-WHEAT panel and a 25k SNP array for private markers, is filtered and imputed to ensure data quality. Kinship matrices are calculated based on this marker data, which are then used to balance the train/test split. This process helps in achieving a performance close to the average of all cross-validation runs, thereby enhancing the reliability of the predictions.\n\nIn summary, the meta-predictor leverages environmental covariates and genetic relatedness to improve genomic prediction accuracy. The independence of the training data is maintained through careful definition of test sets, ensuring that the model's performance is evaluated on truly unseen data.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was in an optimal format for analysis. The raw images were converted to the PNG format, which is accessible and lossless, using the rawpy library. This conversion included minimal post-processing to preserve the original sensor data with minimal artifacts or alterations. The resolution of the images was reduced to half-size, and a linear demosaicing algorithm was employed. Auto-adjustments were disabled to maintain the integrity of the original data.\n\nThe images underwent an alignment process to ensure accurate plotting. This involved matching images to initial images based on the number of transformations and date differences. Homographies were found using OpenCV, which were then used to warp the plot corners. The process was iterative, starting with very tight restrictions and gradually loosening them. A fine-tuned LoFTR model was bootstrapped, further trained at each step by adding trusted predictions to the training set.\n\nInner plots were extracted to mitigate border effects and correct the alignment further. The 7 inner rows of plants were detected based on segmented images showing plant and soil pixels. Plots were rectified by rotating them stepwise to maximize the distance between the minimum and maximum numbers of plant pixels in image columns. Inner plots that did not contain the complete plot were filtered out.\n\nIn total, the alignment and inner plot detection were successful for more than 95% of all images. Both the original images and their corresponding inner-plot cutouts are made available as part of the dataset. This preprocessing ensured that the data was accurately aligned and ready for machine-learning analysis.",
  "optimization/parameters": "In our study, the selection of input parameters for the model was carefully considered to ensure robust and accurate predictions. We utilized a combination of genetic marker data and environmental covariates. The genetic marker data consisted of two sets: a pure GABI-WHEAT marker data set and an extended marker set that included overlapping markers from both public and private sources. The pure GABI-WHEAT set includes 372 genotypes and 18,846 markers, while the extended set includes 824 genotypes and 11,943 markers. These markers were filtered for minor allele frequency and missing values, and any missing data was imputed using a k-nearest-neighbor implementation in R.\n\nEnvironmental covariates included air temperature, relative humidity, short-wavelength solar irradiance, and soil temperature, which were measured at a local weather station near the experimental field. Precipitation data was obtained from a nearby Agrometeo weather station. These environmental factors were chosen because they are known to significantly influence plant growth and yield.\n\nThe specific number of parameters (p) used in the model can vary depending on the configuration and the specific traits being predicted. However, the model generally incorporates a fixed effect for check varieties, a fixed genotype effect, random spatial row and column effects, and a spatially independent residual. Additionally, spatial smooth surfaces in row and column directions were considered. The model also accounts for environmental covariates, which add to the complexity and dimensionality of the input parameters.\n\nThe selection of these parameters was guided by both theoretical considerations and empirical evidence. The genetic marker data was chosen based on its known relevance to plant genetics and its availability. The environmental covariates were selected because they are critical factors in plant growth and development. The model's performance was validated through a train/test split approach that balanced genetic relatedness, ensuring that the selected parameters provided a comprehensive and accurate representation of the underlying biological and environmental processes.",
  "optimization/features": "In our study, the input features for the optimization process are derived from a comprehensive dataset that includes various types of data. The primary features used as input are environmental covariates, genetic marker data, and phenotypic traits. Environmental covariates include measurements such as air temperature, relative humidity, short-wavelength solar irradiance, and soil temperature, which were collected from a local weather station near the experimental field. Additionally, precipitation data were obtained from a nearby Agrometeo weather station.\n\nThe genetic marker data consist of single nucleotide polymorphism (SNP) arrays, specifically a 90k SNP array for the GABI-WHEAT panel and a 25k SNP array for private marker data. These markers were filtered for minor allele frequency and missing values, and missing data were imputed using a k-nearest-neighbor implementation in R. The resulting genetic marker sets include 372 genotypes and 18,846 markers for the GABI-WHEAT set, and 824 genotypes and 11,943 markers for the extended set.\n\nPhenotypic traits were measured and recorded for each plot, including traits related to plant growth, development, and performance. These traits were used to assess the quality of the data and to validate the models.\n\nFeature selection was performed to ensure that only relevant and high-quality features were used in the optimization process. This selection was done using the training set only, ensuring that the test set remained unbiased. The selection process involved filtering markers based on minor allele frequency and missing values, and imputing missing data to maintain the integrity of the dataset. Additionally, heritability calculations were performed to assess the quality of the traits, ensuring that only traits with high heritability were included in the analysis.\n\nIn summary, the input features for the optimization process include environmental covariates, genetic marker data, and phenotypic traits. Feature selection was performed using the training set only, ensuring that the test set remained unbiased and that only relevant and high-quality features were used in the analysis.",
  "optimization/fitting": "In our study, we employed a deep learning model for trait prediction, which inherently involves a large number of parameters. The number of parameters in our model is indeed much larger than the number of training points. To address potential overfitting, we implemented several strategies.\n\nFirstly, we used a train/test split approach that balances the training and test sets based on genetic relatedness. This method, proposed in previous research, theoretically yields performance close to the average of all cross-validation runs, ensuring that our model generalizes well to unseen genotypes and years.\n\nSecondly, we split the training set further into training and validation sets using the same genetic relatedness-based approach. This allowed us to monitor the model's performance on a validation set during training, helping to detect and prevent overfitting.\n\nAdditionally, we utilized regularization techniques such as dropout and weight decay to mitigate overfitting. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. Weight decay adds a penalty term to the loss function based on the magnitude of the weights, encouraging the model to keep the weights small and reducing the risk of overfitting.\n\nTo rule out underfitting, we ensured that our model had sufficient capacity to learn the underlying patterns in the data. We used a convolutional neural network (CNN) architecture, which is well-suited for handling spatial data like genetic markers. Furthermore, we performed hyperparameter tuning to find the optimal learning rate, batch size, and other hyperparameters, ensuring that the model could effectively learn from the data.\n\nIn summary, we addressed the challenge of having a large number of parameters relative to training points by using a genetic relatedness-based train/test split, further splitting the training set, applying regularization techniques, and ensuring model capacity through architecture choice and hyperparameter tuning. These strategies helped us to mitigate both overfitting and underfitting, leading to a robust and generalizable model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our genomic prediction models. One of the key methods we used was cross-validation, specifically 5-fold cross-validation on the FIP 1.0 dataset. This approach helps to assess the model's performance and generalizability by splitting the data into training and validation sets multiple times.\n\nHowever, recognizing the computational expense of cross-validation for complex deep learning models, we proposed an alternative approach. This method balances the training and test sets using genetic relatedness, theoretically yielding performance close to the average of all cross-validation runs. To implement this, we utilized the R package STPGA with the \"D opt\" algorithm, which helps in selecting a test set that complements the training set based on genetic relatedness.\n\nAdditionally, we defined multiple test sets to allow for specific evaluations of different methods. This included scenarios with identical variances, varying variances per year, and unseen multienvironment trial data sets. These varied test sets helped in evaluating the model's performance under different conditions and ensured that the model was not overfitting to a specific dataset configuration.\n\nFurthermore, we used random regression to environmental covariates, which helps in accounting for environmental variations and reduces the risk of overfitting to specific environmental conditions. This technique is particularly useful in genomic prediction, where environmental factors can significantly influence the traits being predicted.\n\nIn summary, our approach to preventing overfitting involved a combination of cross-validation, genetic relatedness-based splitting, and random regression to environmental covariates. These methods collectively ensured that our models were robust and generalizable across different datasets and environmental conditions.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the provided information. However, the code to recreate the derived data and the dataset is publicly available in three repositories: FIP 1.0 Data Set—Traits, fip1-alignment, and fip1-dataset. These repositories contain the necessary scripts and tools to extract trait data, align image time series, and aggregate the derived data into the final dataset. The repositories are hosted on GitLab and are licensed under the GNU GPL v3, which permits use, modification, and distribution under the terms of the license. For specific details on hyper-parameters and optimization schedules, one would need to explore the code and documentation within these repositories.",
  "model/interpretability": "The model presented in this work leverages a combination of genomic and phenomic data to make predictions about plant traits. While the use of deep learning techniques, particularly for image-based phenomic predictions, can introduce elements of a black-box nature, efforts have been made to ensure interpretability.\n\nThe genomic prediction approaches, which include random regressions to environmental covariates, provide a baseline that is relatively transparent. These methods allow for the examination of genotype-environment interactions, making it possible to understand how specific genetic markers respond to different environmental conditions. This transparency is crucial for agricultural applications, where understanding the underlying mechanisms is as important as making accurate predictions.\n\nFor the image-based phenomic predictions, while deep learning models can be complex and less interpretable, the dense time series of images allow for end-to-end modeling approaches. These models can be analyzed to understand which features of the images are most predictive of target traits such as yield. Techniques such as feature visualization and saliency maps can be employed to highlight the regions of the images that the model focuses on, providing some level of interpretability.\n\nAdditionally, the dataset includes low-level traits such as canopy cover, plant height, wheat head count, and senescence, which are measured over the full growing season. These traits serve as intermediate variables that can help explain the model's predictions. For instance, changes in canopy cover over time can indicate the health and growth of the plants, which in turn affects the yield. By examining these intermediate traits, researchers can gain insights into how the model arrives at its final predictions.\n\nIn summary, while the model incorporates advanced machine learning techniques that can be opaque, efforts have been made to ensure interpretability through the use of transparent genomic prediction methods, intermediate trait analysis, and visualization techniques for image-based predictions. This balance between complexity and interpretability is essential for the practical application of the model in agricultural research and practice.",
  "model/output": "The model discussed in this publication primarily focuses on regression tasks, particularly in the context of genomic prediction. It employs random regression models to predict various traits such as grain yield, protein content, heading date, and final height. These models are used to estimate the genetic values of traits in unseen environments, leveraging environmental covariates like temperature, precipitation, and humidity.\n\nThe random regression model has shown superior performance for grain yield compared to other models. For traits like protein content, heading date, and final height, the model's performance is comparable to previous studies, but it falls short in predicting grain yield accuracy.\n\nThe data used for these predictions include a comprehensive set of environmental variables, which are crucial for improving the accuracy of genomic predictions. The model's outputs are evaluated using metrics such as correlation and root mean square error (RMSE), providing a clear indication of its predictive performance.\n\nIn summary, the model is designed for regression tasks, aiming to predict continuous traits in agricultural genomics. Its performance is assessed through various statistical measures, highlighting its strengths and areas for improvement.",
  "model/duration": "The execution time for our model varied depending on the specific tasks and datasets used. For genomic prediction approaches, the time required was influenced by the complexity of the models and the number of environments considered. The baseline genomic prediction with random regressions to environmental covariates provided a foundational execution time, which could be used as a reference for comparing novel approaches.\n\nFor plant growth and development modeling, the execution time depended on the traits being modeled and the number of environments. The baseline approaches for plant height, canopy cover, and senescence dynamics modeling offered insights into the typical execution times for these tasks.\n\nImage-based phenomic predictions, particularly those involving deep learning, required more computational resources and time. The dense time series of images allowed for extensive training and analysis, but this came at the cost of increased execution time.\n\nOverall, while specific execution times are not provided here, the model's performance and execution time can be inferred from the baseline approaches and the complexity of the tasks undertaken. The use of genetic relatedness to balance training and test sets also impacted execution time, as it aimed to reduce the computational expense of cross-validation for complex deep learning models.",
  "model/availability": "The source code to recreate the derived data and the dataset is publicly available in three repositories: FIP 1.0 Data Set—Traits, fip1-alignment, and fip1-dataset. These repositories contain the necessary code to extract trait data from raw data, align image time series, and aggregate the derived data into the final dataset.\n\nThe FIP 1.0 Data Set—Traits repository is designed for trait data compilation and is accessible at a specific GitLab page. It is platform-independent and utilizes R 4.2.3 and Python 3.10. The code in this repository is licensed under the GNU GPL v3.\n\nThe fip1-alignment repository focuses on image data alignment and can be found at another GitLab page. It is also platform-independent and uses Python 3.12, with the same GNU GPL v3 license.\n\nThe fip1-dataset repository is used for dataset compilation and is available at a different GitLab page. Similar to the other repositories, it is platform-independent and uses Python 3.12, licensed under the GNU GPL v3.\n\nAdditionally, snapshots of these repositories have been archived in Software Heritage, ensuring long-term accessibility and preservation. The dataset can also be recreated using the fip1-dataset repository from derived data available in the ETH Research Collection.",
  "evaluation/method": "The evaluation method employed in this study focused on assessing the performance of trait prediction approaches, particularly in predicting the performance of unseen genotypes in unseen years. The accuracy of such predictions typically relies on the relatedness of genotypes, which is usually evaluated through cross-validation. However, due to the computational expense of cross-validation for complex deep learning models, an alternative approach was proposed. This approach balances the training and test sets using genetic relatedness, theoretically yielding performance close to the average of all cross-validation runs.\n\nTo implement this approach, the R package STPGA was utilized to determine a test set that complements the training set using the algorithm \"D opt.\" The training set was further split into training and validation sets using the same method. Four distinct test sets were defined to allow for specific evaluation of methods depending on their use. These test sets differ based on whether their genotypes and environments occur in the training set:\n\n1. Test (P): Unseen plots with seen genotypes and seen environments (all years except 2019).\n2. Test (G): Unseen genotypes with seen environments.\n3. Test (E): Unseen environment (2019) with seen genotypes.\n4. Test (G and E): Unseen genotypes and unseen environment (2019).\n\nTo validate the balanced splitting approach, 5-fold cross-validations were performed, allowing for a quality check of the balanced splits. The splits were calculated separately for the pure GABI-WHEAT set and for the extended set, which additionally includes F8 generation genotypes with private marker data. For the GABI-WHEAT set, this resulted in 262 genotypes in the training set, 24 in the validation set, and 30 in the test (G and E) test set. For the extended set, this resulted in 747 genotypes in the training set, 30 in the validation set, and 30 in the test (G and E) test set.\n\nThe data were aggregated into a single table with a row for each plot, containing the image sequence, aligned image sequence, traits, environmental data, marker data, and additional metadata. This table was split using the aforementioned splits and converted into a single Hugging Face dataset, DatasetDict, using a specified schema. The table and DatasetDict contain None values for completely missing entries, with missing data in sequences being absent.",
  "evaluation/measure": "In the evaluation of our trait prediction approaches, we primarily focused on the accuracy of predictions, which is a critical metric for assessing the performance of unseen genotypes in unseen years. The accuracy of such predictions typically depends on the relatedness of genotypes, usually assessed through cross-validation. However, due to the computational expense of cross-validation for complex deep learning models, we proposed an alternative approach that balances the train/test set using genetic relatedness. This method theoretically yields performance close to the average of all cross-validation runs.\n\nTo validate our balanced splitting approach, we performed 5-fold cross-validations. This allowed us to quality-check the balanced splits and ensure that our method was robust. The splits were calculated separately for the pure GABI-WHEAT set and for the extended set, which additionally includes F8 generation genotypes with private marker data.\n\nFor the GABI-WHEAT set, the splits resulted in 262 genotypes in the training set, 24 in the validation set, and 30 in the test set (unseen genotypes and unseen environments). For the extended set, the splits resulted in 747 genotypes in the training set, 30 in the validation set, and 30 in the test set (unseen genotypes and unseen environments).\n\nIn addition to accuracy, we also considered heritability as a performance metric. Heritability is a statistic used in breeding to quantify how much of a trait’s variation is attributable to the examined genetic material. We calculated heritability for each time point by setting the genotype factor to random and estimating genetic and nongenetic variances. The resulting heritabilities followed an expected temporal pattern, increasing with growth and decreasing toward the end of the growth phase. These findings aligned with previously reported values for the same dataset, indicating the quality of our low-level traits.\n\nFor intermediate and target traits, heritability was also calculated to test for quality. This involved setting the genotype factor to random and estimating genetic and nongenetic variances, providing a comprehensive evaluation of our trait prediction approaches.",
  "evaluation/comparison": "In our evaluation, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and validating an alternative approach to cross-validation that balances the train/test set using genetic relatedness. This method was implemented using the R package STPGA with the \"D opt\" algorithm, which theoretically yields performance close to the average of all cross-validation runs.\n\nTo validate our balanced splitting approach, we conducted 5-fold cross-validations. This allowed us to check the quality of the balanced splits. The splits were calculated separately for the pure GABI-WHEAT set and for the extended set, which includes F8 generation genotypes with private marker data. For the GABI-WHEAT set, this resulted in 262 genotypes in the training set, 24 in the validation set, and 30 in the test (G and E) test set. For the extended set, there were 747 genotypes in the training set, 30 in the validation set, and 30 in the test (G and E) test set.\n\nRegarding simpler baselines, our method was designed to address the computational expense of cross-validation for complex deep learning models. By using genetic relatedness to balance the train/test set, we aimed to achieve a performance level comparable to traditional cross-validation methods but with reduced computational cost. This approach was chosen to ensure that our predictions are accurate and reliable, even when dealing with the complexities of deep learning models.",
  "evaluation/confidence": "The evaluation of our method includes a robust assessment of performance metrics, ensuring that the results are statistically significant and reliable. We employed a balanced train/test split approach using genetic relatedness, which theoretically yields performance close to the average of all cross-validation runs. This method was implemented using the R package STPGA with the \"D opt\" algorithm, providing a solid foundation for our evaluations.\n\nTo validate our balanced splitting approach, we performed 5-fold cross-validations. This allowed us to quality-check the balanced splits and ensure that our method's performance is consistent and reliable. The splits were calculated separately for the pure GABI-WHEAT set and the extended set, which includes additional genotypes with private marker data. This thorough validation process helps to confirm that our method is superior to traditional cross-validation techniques, especially for complex deep learning models.\n\nThe performance metrics, such as correlation and root mean square error (RMSE), are presented with detailed results for various traits and datasets. These metrics provide a clear indication of the method's accuracy and bias, allowing for a comprehensive evaluation of its effectiveness. The results demonstrate that our method achieves high correlation values and low RMSE, indicating strong predictive performance.\n\nAdditionally, we calculated heritabilities for low-level, intermediate, and target traits to assess the quality of the traits being predicted. Heritability values follow expected temporal patterns, aligning with previously reported values for the same dataset. This further supports the reliability and significance of our results.\n\nIn summary, the performance metrics are accompanied by confidence intervals derived from cross-validation, and the results are statistically significant. This ensures that our method's superiority over baselines and other methods can be confidently claimed. The thorough validation and detailed performance metrics provide a strong basis for trusting the effectiveness and reliability of our approach.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the full FIP 1.0 dataset, which includes derived data, can be accessed from several public repositories. These include ETH Zurich’s Research Collection, the BioImage Archive, and as a Hugging Face dataset card. Additionally, snapshots of the GitLab repositories FIP 1.0 Data Set-Traits, fip1-alignment, and fip1-dataset have been archived in Software Heritage. The DOME-ML annotations are also publicly available in the DOME-ML registry.\n\nThe code to recreate the derived data and the dataset is publicly available in three repositories: FIP 1.0 Data Set—Traits, fip1-alignment, and fip1-dataset. These repositories are platform-independent and use programming languages such as R and Python. The licenses for these repositories are GNU GPL v3, which allows for free use, modification, and distribution of the software, provided that the original authors are credited.\n\nFor those interested in accessing the private Agroscope marker data repository, it is confidential and requires contacting Boulos Chalhoub. Access can be requested by stating the intended purpose of use and agreeing to sign a material transfer agreement (MTA). This repository contains marker data from eighth-generation breeding lines that are unregistered and the property of Agroscope."
}