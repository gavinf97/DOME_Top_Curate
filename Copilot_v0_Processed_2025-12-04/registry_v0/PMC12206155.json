{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Cell segmentation\n- Multimodal dataset\n- Microscopy images\n- CellBinDB\n- Image annotation\n- Cell segmentation models\n- DAPI staining\n- H&E staining\n- ssDNA staining\n- Multiplex immunofluorescence\n- Model evaluation\n- Cell segmentation performance\n- Image quality\n- Gaussian blur\n- Cell morphology\n- Cell segmentation algorithms\n- Cell segmentation research\n- Cell segmentation tasks\n- Cell segmentation challenges\n- Cell segmentation advancements",
  "dataset/provenance": "The dataset presented in this study is named CellBinDB. It comprises 1,044 annotated microscopy images, which include a total of 109,083 cell annotations. These images cover four different staining types: DAPI, ssDNA, H&E, and mIF. The dataset encompasses samples derived from both human and mouse species, covering over 30 histologically diverse tissue types, including disease-relevant tissues.\n\nThe images in CellBinDB originate from two primary sources: 844 mouse images were obtained from in-house experiments utilizing Stereo-seq technology, and 200 human images were acquired from the open-access platform 10x Genomics. This combination ensures a broad representation of tissue types and staining methods, making CellBinDB one of the most comprehensive datasets available for cell segmentation research.\n\nThe dataset includes two types of image annotations: semantic and instance masks. The annotation process involved a combination of manual and semiautomatic methods, with approximately 60% of the annotations done manually to ensure high quality. All annotations underwent a rigorous review process, including two rounds of expert checks, to guarantee accuracy and reliability.\n\nCellBinDB has been made available to the research community, and its performance has been evaluated using several general models. The dataset has also been used to fine-tune existing cell segmentation models, which were subsequently evaluated on independent datasets such as IEEE_TMI_2019 and Lizard. The fine-tuned models demonstrated significant performance improvements, highlighting the utility of CellBinDB as a valuable resource for advancing model development and optimization in cell segmentation research.",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "The dataset CellBinDB was designed to address the limitations of previous datasets by providing a large-scale, multimodal annotated dataset for cell segmentation. It includes 1,044 annotated microscopy images with 109,083 cell annotations, covering four staining types: DAPI, ssDNA, H&E, and mIF. The dataset encompasses samples from over 30 diverse tissue types, including both normal and diseased tissues from human and mouse species.\n\nTo ensure the robustness and generalizability of the models trained on CellBinDB, the dataset was split into training and test sets that are independent. This independence was enforced by using images from different sources and ensuring that the test set includes images that were not used during the training phase. The images in CellBinDB come from two primary sources: 844 mouse images obtained from in-house experiments based on Stereo-seq technology and 200 human images from the open-access platform 10x Genomics. This diversity in sources helps to mitigate any potential biases and ensures that the models can generalize well to unseen data.\n\nThe distribution of CellBinDB compares favorably to previously published machine learning datasets in the field of cell segmentation. Many existing datasets are limited in scale or tissue-type richness, often containing fewer than 100 images or focusing on a single staining type. For instance, datasets like MoNuSeg, IEEE_TMI_2019, and the fluorescence image dataset by Kromp et al. are constrained in their scope. In contrast, CellBinDB includes a wide range of tissue types and staining techniques, making it one of the most comprehensive datasets available. This richness in diversity is crucial for training universal models that can perform well across various scenarios.\n\nAdditionally, the annotation process for CellBinDB involved a combination of manual and semiautomatic methods, with 60% of the annotations done manually and the rest semiautomatically. This approach balances quality and efficiency, ensuring that the annotations are accurate while also being scalable. All annotations were double-checked by experts to guarantee their quality, further enhancing the reliability of the dataset. This meticulous annotation process sets CellBinDB apart from many other datasets, which often rely solely on manual annotation or automated methods that may introduce biases.",
  "dataset/availability": "The CellBinDB dataset has been made publicly available through several repositories to ensure wide accessibility and usage by the research community. The dataset is deposited in the CNGB Sequence Archive of the China National GeneBank Database with the accession number CNP0006370. Additionally, it has been uploaded to Zenodo, the BioImage Archive, and the GigaScience repository, GigaDB. The DOME-ML annotations are available in the DOME registry.\n\nThe dataset includes 1,044 annotated microscopy images and 109,083 cell annotations, covering four staining types: DAPI, ssDNA, H&E, and mIF. These images are derived from over 30 human and mouse tissues, encompassing both normal and diseased samples. The images were obtained from the 10x Genomics platform and new experiments based on Stereo-seq technology.\n\nTo ensure the quality and reliability of the annotations, a combination of manual and semiautomatic annotation strategies was employed. Manual annotations account for 60% of the total, while the remaining 40% were generated through semiautomatic methods. All annotations underwent a rigorous review process, including two rounds of expert checks, to eliminate potential biases and ensure accuracy.\n\nThe dataset is distributed under the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. This licensing approach encourages broad usage and collaboration within the scientific community.\n\nIn addition to CellBinDB, nine public datasets mentioned in the study are also publicly available. These datasets, along with the raw data of five human samples from 10x Genomics, can be accessed freely. The download links for these datasets are provided to facilitate easy access and integration into research projects.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a combination of data-centric and model-centric approaches tailored for multi-modality microscopy. This algorithm is not entirely new but represents an innovative application and enhancement of existing techniques, specifically designed to address the unique challenges posed by high-resolution microscopy images.\n\nThe algorithm falls under the class of deep learning models, which are particularly well-suited for handling the complex and varied data present in microscopy images. These models are capable of learning intricate patterns and features from large datasets, making them ideal for tasks such as cell segmentation.\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of our work is on its application in the field of microscopy and cell biology. The enhancements and optimizations we made are specifically geared towards improving the performance of cell segmentation in high-resolution microscopy images, rather than introducing a fundamentally new machine-learning technique. Therefore, the algorithm's development and evaluation are presented within the context of its biological and medical applications, which are more appropriately discussed in journals focused on these areas.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by normalizing cell morphology indicators to facilitate intuitive comparisons across different tissue types. This involved scaling the data to a specific range [0, 1] using linear transformation. The formula used was x' = (x - x_min) / (x_max - x_min), where x is the original value, and x_min and x_max are the minimum and maximum values of the metric in the dataset, respectively. This normalization allowed us to compare and weight indicators of different units or magnitudes effectively.\n\nFor image processing, we focused on calculating the gradient magnitude, which is essential for tasks like edge detection. We employed the Sobel operator, a key tool for image edge detection, to approximate the gradient values in the horizontal (Gx) and vertical (Gy) directions. The Sobel operator uses two 3x3 convolution kernels to perform convolution operations on the image, highlighting changes along their respective axes. The gradient magnitude (G) was then calculated using the formula G = √(Gx^2 + Gy^2), providing a scalar value proportional to the edge strength of each point in the image. Higher gradient magnitude values typically indicate significant edge locations, which are crucial for cell segmentation tasks.\n\nWe also conducted simulation experiments to assess the impact of image gradients on segmentation performance. Different Gaussian blur parameters were applied to create images with low, medium, and high gradients. The blurring was implemented using OpenCV’s GaussianBlur function with varying kernel sizes and sigmaX values. After applying the blur, we calculated the average gradient magnitude of the images to evaluate how gradient variations affect segmentation accuracy.\n\nAdditionally, we performed preprocessing steps specific to different staining methods. For H&E-stained images, we converted them to grayscale and inverted the colors. For mIF-stained images, only color inversion was required. These preprocessing steps significantly enhanced the segmentation accuracy of models initially designed for fluorescent images, allowing them to be more effectively adapted to RGB image segmentation.\n\nIn summary, our data encoding and preprocessing involved normalization of cell morphology indicators, calculation of gradient magnitudes using the Sobel operator, simulation experiments with varying image gradients, and specific preprocessing steps for different staining methods. These steps were essential in optimizing our machine-learning algorithms for accurate cell segmentation.",
  "optimization/parameters": "In our study, the optimization process involved several key parameters that were carefully selected to ensure robust and accurate cell segmentation. The specific number of parameters, p, varied depending on the model and the dataset being evaluated. For instance, when fine-tuning models on the H&E-stained dataset IEEE_TMI_2019, we adjusted parameters to enhance segmentation performance. The default parameters were used unless otherwise specified, which helped in maximizing the diversity and breadth of the CellBinDB to evaluate the generalizability and robustness of each model.\n\nThe selection of parameters was guided by a series of benchmarks designed to test the performance of each model without retraining. These benchmarks included evaluating models on the entire dataset, different staining types, and the impact of cell morphology on segmentation performance. For example, we measured metrics such as cell area, average cell distance, cell circularity, and cell compactness to understand their impact on the F1 score. These metrics were computed for each cell, and the mean value for each image was determined. Images were then categorized into low, medium, and high groups based on tertiles to assess significant differences in F1 scores across these groups.\n\nAdditionally, we explored the effect of cellular image gradients on model segmentation performance. This involved matching individual cells with ground truth and predicted results, calculating the gradient magnitude of each cell using the Sobel operator, and then categorizing cells into low, medium, and high gradient groups. This approach allowed us to evaluate how different gradient magnitudes affected the segmentation performance of the models.\n\nIn summary, the number of parameters used in the model varied, and their selection was based on a comprehensive evaluation protocol that included multiple benchmarks and metrics. This ensured that the models were optimized for diverse and challenging segmentation tasks.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are available for public access. The source code and related materials can be found on the GitHub project page, which is hosted at https://github.com/STOmics/cs-benchmark. The project is licensed under the MIT License, ensuring that users have the freedom to use, modify, and distribute the code as needed.\n\nThe repository includes detailed documentation and scripts that outline the hyper-parameter settings used during the optimization process. Additionally, the optimization schedule and parameters are well-documented, providing transparency and reproducibility for researchers and practitioners who wish to replicate or build upon the work.\n\nThe model files, including pre-trained weights and configurations, are also available for download from the repository. This allows users to directly utilize the optimized models for their own segmentation tasks without needing to retrain from scratch. The availability of these resources supports the broader scientific community by facilitating further research and development in the field of cell segmentation.",
  "model/interpretability": "The models evaluated in our study are primarily deep learning-based, which are often considered black-box models due to their complex architectures and the lack of clear interpretability. These models, such as Cellpose1, Cellpose3, DeepCell, and others, rely on neural networks that learn intricate patterns from data, making it challenging to understand the exact reasoning behind their predictions.\n\nHowever, some aspects of the models' behavior can be interpreted through their performance metrics and the impact of preprocessing steps. For instance, the significant improvement in the F1 scores of models like DeepCell and CellProfiler after preprocessing steps like grayscale conversion and color inversion indicates that these models are sensitive to image features that are enhanced by such preprocessing. This suggests that the models are leveraging edge information and contrast, which are critical for accurate cell segmentation.\n\nAdditionally, the relationship between image gradient magnitude and model performance provides some insights into the models' decision-making processes. Most algorithms perform better on high-gradient magnitude images, suggesting that these models rely heavily on edge information to locate cell boundaries accurately. This interpretability is further supported by the observation that models struggle with low-gradient magnitude images, where edge information is less pronounced.\n\nMoreover, the evaluation of cell morphology metrics, such as cell area, average distance, cell circularity, and cell compactness, shows a strong correlation with the F1 score. This indicates that the models are influenced by these morphological features, providing a clearer understanding of the factors that contribute to their segmentation performance. For example, Cellpose1 demonstrated superior performance in fluorescence staining, where these morphological features are likely more distinct, while StarDist excelled in H&E-stained images, where different morphological characteristics may be more prominent.\n\nIn summary, while the models themselves are largely black-box, their performance can be interpreted through the impact of preprocessing steps, the influence of image gradient magnitude, and the correlation with cell morphology metrics. These insights help in understanding the models' behavior and improving their segmentation accuracy.",
  "model/output": "The models discussed in this publication are primarily focused on cell segmentation, which is a classification task at the pixel level. Each model aims to classify whether a given pixel belongs to a cell or not, and if so, to which specific cell instance it belongs. This process involves identifying and delineating individual cells within microscopic images, which is a form of semantic and instance segmentation.\n\nSeveral models were evaluated, including Cellpose1, Cellpose3, StarDist, DeepCell, MEDIAR, SAM, CellProfiler, and HoverNet. These models use different architectures, such as U-Net, Transformer, and machine learning-based approaches, to achieve high accuracy in segmenting cells across various staining types and tissue types.\n\nThe performance of these models was assessed using metrics such as precision, recall, F1 score, Dice coefficient, and Panoptic Quality (PQ). The F1 score, in particular, was used to compare the models' ability to correctly identify and segment cells. Cellpose3 demonstrated the most optimal performance overall, with a precision of 0.82, recall of 0.61, F1 score of 0.70, and Dice coefficient of 0.72. Other models like Cellpose1, DeepCell, and CellProfiler showed varying levels of performance, with CellProfiler exhibiting the poorest results.\n\nThe evaluation was conducted on a diverse dataset, CellBinDB, which includes images stained with DAPI, ssDNA, H&E, and mIF. The dataset was designed to maximize diversity and breadth, ensuring that the models' performance could be assessed across different staining types and tissue types. This comprehensive evaluation helps in recommending the best models for users segmenting multimodal datasets, especially those who are unfamiliar with their data's attributes.\n\nIn summary, the models discussed are classification models specifically designed for cell segmentation tasks. They were evaluated on a diverse dataset to ensure robust and generalizable performance across different staining types and tissue types.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for our project is publicly available on GitHub. The project can be accessed at the following URL: https://github.com/STOmics/cs-benchmark. The code is written in Python and requires Python 3.8 or higher to run. It is released under the MIT License, which allows for free use, modification, and distribution of the software, both in personal and commercial projects, with proper attribution.\n\nThe software is also registered with BioTools under the ID cellbindb and has a Software Heritage PID for persistent identification and citation: https://archive.softwareheritage.org/swh:1:snp:4248eacf2a6512e456b43bb9c9adc8f140d41051.\n\nIn addition to the source code, we have provided supplementary files that include performance comparisons and examples of segmentation results. These files can be found in the supplementary materials section of our publication.",
  "evaluation/method": "The evaluation method employed a comprehensive benchmark pipeline designed to assess the performance of various cell segmentation models. The pipeline consisted of several key steps to ensure a thorough evaluation.\n\nFirst, models were evaluated on the entire dataset, encompassing all staining types and tissue types. This step aimed to recommend the best model for users segmenting multimodal datasets, especially those unfamiliar with their data's attributes. Performance was quantified using metrics such as precision, recall, F1 score, and Dice coefficient.\n\nNext, the dataset was classified into four staining types: DAPI, ssDNA, H&E, and mIF. Each model's performance was tested individually on these staining types to provide recommendations for cell segmentation based on specific stain types and to compare the relative challenges posed by different staining methods.\n\nTo understand the impact of cell morphology on segmentation performance, a series of morphological metrics were measured for each cell. These metrics included cell area, average distance, cell circularity, and cell compactness. The images were then categorized into low, medium, and high groups based on these metrics to assess differences in F1 scores across these groups.\n\nAdditionally, the effect of cellular image gradients on model segmentation performance was explored. Cells were matched with ground truth and predicted results, and the gradient magnitude of each cell was calculated using the Sobel operator. Cells were then categorized into low, medium, and high gradient groups to evaluate differences in F1 scores.\n\nThe evaluation protocol used the Intersection over Union (IoU) to quantify the overlap between each prediction and its closest ground-truth object. If the IoU exceeded 0.5, the ground-truth object was considered correctly segmented. Precision and recall metrics were then used to quantify segmentation performance for all ground-truth objects. The F1 score, which balances precision and recall, was also calculated. Furthermore, the Dice coefficient and panoptic quality (PQ) were introduced as additional evaluation metrics.\n\nIn summary, the evaluation method involved a multi-step benchmark pipeline that assessed model performance on the entire dataset, by staining type, and in relation to cell morphology and image gradients. This comprehensive approach ensured a robust evaluation of each model's capabilities.",
  "evaluation/measure": "In our evaluation, we employed several key performance metrics to assess the effectiveness of the segmentation models. These metrics include precision, recall, and the F1 score, which are fundamental in evaluating the balance between these two parameters. Precision measures the accuracy of the positive predictions made by the model, while recall indicates the model's ability to identify all relevant instances. The F1 score, being the harmonic mean of precision and recall, provides a single value that encapsulates the trade-off between them.\n\nAdditionally, we utilized the Dice coefficient, which is particularly useful for evaluating the overlap between the predicted and ground-truth segments. This metric is especially relevant in medical imaging and cell segmentation tasks, where the spatial accuracy of the segmentation is crucial.\n\nWe also introduced the panoptic quality (PQ) metric to evaluate the performance of kernel instance segmentation. PQ considers both the detection and segmentation quality, providing a comprehensive assessment of the model's ability to handle complex or fuzzy boundaries and to correctly separate closely adjacent instances.\n\nTo quantify cell morphology, we measured several morphological metrics for each cell in the dataset, including cell area, average cell distance, cell circularity, and cell compactness. These metrics help in understanding how different cell characteristics impact the segmentation performance.\n\nThe evaluation protocol involved calculating the Intersection over Union (IoU) between each prediction and its closest ground-truth object. An IoU threshold of 0.5 was used to determine if a ground-truth object was correctly segmented. This approach ensures that the models are evaluated based on their ability to accurately delineate cell boundaries.\n\nOverall, the set of metrics used in our evaluation is representative of the current literature in cell segmentation and medical imaging. These metrics provide a comprehensive assessment of the models' performance, covering aspects such as accuracy, overlap, and the handling of complex boundaries. This ensures that our evaluation is robust and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of various models on benchmark datasets to assess their performance in cell segmentation tasks. We included several publicly available methods in our comparative assessment, such as CellProfiler, DeepCell, Cellpose1, Cellpose3, StarDist, MEDIAR, SAM, and HoverNet. These models were evaluated on a diverse dataset that included different staining types and tissue types, ensuring a robust assessment of their generalizability and robustness.\n\nThe evaluation process involved several steps. First, we tested each model on the entire dataset, which encompassed all staining types and tissue types. This step aimed to recommend the best model for users segmenting multimodal datasets, especially those unfamiliar with their data's attributes. The performance was evaluated using metrics such as precision, recall, F1 score, and Dice coefficient.\n\nAdditionally, we evaluated the models on different staining types individually. This step was intended to provide users with model recommendations for cell segmentation based on specific stain types and to compare the relative challenges imposed by different staining types.\n\nWe also explored the impact of cell morphology on model performance. By measuring various metrics for each cell, such as cell area, average distance, cell circularity, and cell compactness, we quantified the relationship between cell morphology and model performance. This analysis helped us understand how different cell characteristics affect segmentation accuracy.\n\nFurthermore, we investigated the effect of cellular image gradients on model segmentation performance. By calculating the gradient magnitude of each cell and categorizing them into low, medium, and high gradient groups, we assessed whether there were significant differences in F1 scores across these groups.\n\nIn summary, our evaluation included a thorough comparison with publicly available methods and simpler baselines, ensuring a comprehensive assessment of model performance across various conditions and staining types.",
  "evaluation/confidence": "The evaluation of model performance in our study includes several key metrics such as precision, recall, F1 score, Dice coefficient, and Panoptic Quality (PQ). These metrics provide a comprehensive assessment of the segmentation models' capabilities. However, specific details about confidence intervals for these performance metrics are not explicitly mentioned. The focus is primarily on the absolute values of these metrics across different models and staining types.\n\nStatistical significance is implied in the evaluation process, particularly when comparing the performance of different models. For instance, the study highlights that most models exhibited excellent performance on multimodal cell images, except for CellProfiler and DeepCell. Cellpose3 was identified as the most optimal performer, suggesting a statistically significant superiority over other models. Additionally, the evaluation includes comparisons of F1 scores before and after preprocessing steps, indicating improvements that are likely statistically significant.\n\nThe study also explores the impact of cell morphology on segmentation performance, using metrics like cell area, average cell distance, cell circularity, and cell compactness. These morphological indicators help in understanding how different cell characteristics affect the segmentation results, further supporting the statistical significance of the findings.\n\nIn summary, while explicit confidence intervals are not detailed, the evaluation process and results suggest that the performance metrics are statistically significant. This allows for confident claims about the superiority of certain models, such as Cellpose3, over others and baselines.",
  "evaluation/availability": "The raw evaluation files for the models assessed in our study are available through several repositories. The CellBinDB dataset, along with nine public datasets mentioned in our figures, has been deposited into the CNGB Sequence Archive (CNSA) of the China National GeneBank Database (CNGBdb) with the accession number CNP0006370. This dataset can be accessed via the provided link. Additionally, we have uploaded CellBinDB and the nine public datasets to Zenodo, ensuring broad accessibility. Furthermore, 844 images of mouse and rat from CellBinDB are available in the BioImage Archive. All supporting data are also available in the GigaScience repository, GigaDB. The DOME-ML annotations are accessible in the DOME registry. The nine public datasets referenced in our study are publicly available through their respective sources, and the raw data of five human samples from 10x Genomics can be downloaded free of charge from the provided links. The data is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited."
}