{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- M. C. Barbosa\n- A. R. P. Aschoal\n- L. C. A.\n- J. F. M. S.\n\nThe contributions of each author are as follows:\n\n- Conceptualization: M. C. Barbosa, A. R. P. Aschoal, L. C. A., J. F. M. S.\n- Methodology: M. C. Barbosa, A. R. P. Aschoal, L. C. A., J. F. M. S.\n- Software development: M. C. Barbosa\n- Validation: All authors\n- Revision: All authors\n- Writing: All authors",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Microbiome\n- Data Analysis\n- Soil Health\n- Human Health\n- Taxa Selection\n- Predictive Modeling\n- Bioinformatics\n- Compositional Data Analysis\n- Metagenomics",
  "dataset/provenance": "In our study, we utilized a total of 30 datasets sourced from three distinct groups. The first group, labeled \"Group A: Literature,\" comprises 19 datasets, each corresponding to a unique environmental variable. These datasets were originally stored as Rdata files by the authors and were transformed into count tables for our analysis. The second group, \"Group B: ML Repo,\" serves as a positive control and includes five human gut datasets. These datasets were obtained from a machine learning repository and encompass studies on pediatric Crohn’s disease, infant microbiome, and vaginal health. The third group, \"Group C: Cross-Studies,\" consists of datasets extracted from MGnify and is designed to evaluate the generalization power of our tool, CODARFE. This group includes two sets of datasets: one focusing on pH measurements in arable soil and another on age measurements in humans from various digestive system samples subjected to different external effects. All datasets in Group C were already in count table format and required no additional preprocessing. The experiments were conducted using splits of the same data for training and testing.",
  "dataset/splits": "In our study, we utilized splits of the same data for both training and testing purposes across all experiments. This approach ensured that each dataset was consistently divided into training and testing subsets, allowing for robust evaluation of our models. The specific number of data points in each split varied depending on the dataset and the study, but the general principle of using the same data splits for both training and testing was maintained throughout.\n\nFor the soil cross-studies analysis, we included datasets from two primary studies: Chroňáková et al. and Nguyen et al. These datasets were generated from the V1–V3 region of the 16S rRNA gene. The Chroňáková et al. dataset focused on prokaryotic diversity in soil impacted by outdoor cattle husbandry, collected in the Czech Republic in 2011. The Nguyen et al. dataset investigated bacterial communities in the soil of ginseng fields in South Korea, collected in 2012.\n\nIn addition to these soil studies, we also conducted analyses on human age measurements from four different projects. These projects involved samples collected from various parts of the digestive system and subjected to different external effects, such as antibiotic treatment and HIV. The datasets for these projects are available under specific identifiers.\n\nThe distribution of data points in each split was designed to ensure that the models were trained and tested on representative subsets of the data. This approach helped in assessing the predictive power and generalizability of the models across different studies and conditions. However, the exact number of data points in each split is not specified here, as it varied depending on the dataset and the specific analysis being conducted.\n\nFor further details on the datasets and their splits, readers can refer to the supplementary materials and the data availability section, where the original raw data and additional information are hosted.",
  "dataset/redundancy": "In our study, we employed a rigorous approach to ensure the independence and robustness of our datasets. All experiments were conducted using splits of the same data for training and testing. This method was chosen to maintain consistency and to evaluate the model's performance under controlled conditions.\n\nThe datasets were divided into three main groups: \"Group A: Literature,\" \"Group B: ML Repo,\" and \"Group C: Cross-Studies.\" Each group served a specific purpose in our analysis. For instance, \"Group A\" was used to evaluate the coefficient of correlation and processing time required by different methods, while \"Group B\" acted as a positive control due to preexisting evidence of relationships between taxa and sample metadata variables.\n\nTo enforce the independence of training and test sets, we ensured that the splits were random and that there was no overlap between the training and testing data. This was crucial for assessing the model's generalization capabilities and preventing data leakage, which could otherwise lead to overoptimistic performance estimates.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. We collected a total of 30 datasets from three different sources, each undergoing specific analyses tailored to their group. This comprehensive approach allowed us to cover a wide range of scenarios and environmental variables, providing a robust foundation for our conclusions.\n\nIn summary, the datasets were split randomly and independently, with no overlap between training and testing sets. This methodology ensures that our results are reliable and comparable to other studies in the field.",
  "dataset/availability": "The data and analyses related to our study are publicly available. The CODARFE plots and analyses can be accessed through Zenodo. Additionally, the data and code used for CODARFE analysis are also hosted on Zenodo. This ensures that the methods and results are reproducible and accessible to the scientific community.\n\nFor the original raw data, different groups are hosted in various repositories. Group A's data can be found in a specified location. Group B's data is available on GitHub, with separate datasets for infant age, PCDAI using baseline CD ileum, PCDAI using baseline CD rectum, Nugent score, and vaginal pH. Group C's data is from the MGnify Projects, with specific accession numbers provided for each dataset.\n\nThe machine learning annotations have been deposited in the DOME registry, ensuring that all relevant information is publicly accessible. This approach allows for transparency and verification of the results presented in the study.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is ensemble learning, specifically the random forest algorithm. This algorithm is well-established and widely used in the machine-learning community for its robustness and ability to handle large datasets with high dimensionality.\n\nThe random forest algorithm is not new; it has been extensively studied and applied in various domains. It was introduced by Breiman in 2001 and has since become a standard tool in the machine-learning toolkit. Given its widespread use and the extensive literature already available, publishing it in a machine-learning journal would not add significant new insights to the field.\n\nOur focus was on applying this established algorithm to a specific problem in microbiome-based environmental prediction. We optimized the random forest algorithm by testing various hyperparameters and combinations to achieve the best performance for our particular dataset. This involved evaluating 1,243 different combinations of machine-learning algorithms and hyperparameters, using mean absolute error (MAE) as the primary metric for assessment. The random forest algorithm with specific parameters (\"n_estimators = 160\" and \"criterion = poisson\") was found to yield the best performance for our predictions.\n\nIn summary, while the random forest algorithm itself is not new, our work contributes to the application and optimization of this algorithm in the context of microbiome-based environmental prediction. This approach allows us to leverage the strengths of an established machine-learning technique to address a specific scientific challenge.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input in the traditional sense of a meta-predictor. Instead, it employs a comprehensive process that involves multiple machine-learning algorithms and techniques to select and train the final predictor model.\n\nSeveral machine-learning methods were evaluated, including linear support vector regression, linear stochastic gradient descent, Huber regression, Theil-Sen estimator, random forest, and support vector machine with various kernels. These methods were tested with different hyperparameters and transformations, resulting in numerous combinations. The model with the highest score, based on the sum of normalized metrics, was selected.\n\nThe final predictor model uses a random forest algorithm with specific parameters (\"n_estimators = 160\" and \"criterion = poisson\"), which was chosen for its superior performance in predictions. The training data for the random forest model is independent, as it is trained on 80% of the dataset and evaluated on the remaining 20%, with this process repeated 10 times to calculate the average mean absolute error (MAE) estimations.\n\nThe model's development process ensures that the training data is independent and that the final predictor model is selected based on its performance across multiple metrics. This approach helps to mitigate issues related to data dimensionality and ensures that the selected predictors are most associated with the target variable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. Initially, we removed predictors with low variance, defined as those with variance less than or equal to one-eighth of the mean variance in the dataset. This step helped eliminate uninformative columns.\n\nFor the target variable, we applied a square-root transformation if the coefficient of variation (CV) was 0.2 or higher, which indicated significant data dispersion. This transformation helped mitigate noise and reduce errors in our machine learning models. If the CV was below 0.2 and the target contained negative values, we applied a simple shift to ensure compatibility with the Poisson distribution used in our models.\n\nWe evaluated two compositional data transformations: Hellinger and center log-ratio (CLR). The Hellinger transformation was chosen for its ability to preserve the Euclidean structure of the data, while the CLR transformation was selected for its capability to convert compositional data into a real-valued space, allowing the application of traditional statistical methods without the risk of misleading results.\n\nSeveral regression methods were tested, including linear support vector regression, linear stochastic gradient descent, Huber regression, and the Theil-Sen estimator. These methods were chosen for their ability to provide weights corresponding to the importance of each predictor. Each method was applied to both the Hellinger and CLR transformations, resulting in a comprehensive evaluation of different model hyperparameter transformations (MHTs).\n\nThe data were then subjected to recursive feature elimination (RFE) to train and evaluate the MHTs using four metrics: R-squared adjusted, p-value of the F-test, root mean square error (RMSE), and Bayesian information criterion (BIC). The MHT with the highest score, calculated as the sum of the min-max normalized metrics, was selected for further analysis.\n\nFor the predictor algorithm, we tested various regressors, including linear support vector regression, linear stochastic gradient descent, Huber regression, Theil-Sen estimator, random forest, and support vector machines with different kernels. This resulted in 1,243 combinations, all using the CLR transformation. Each model was trained on 80% of the dataset and evaluated on the remaining 20%, with 10 repetitions to calculate the average mean absolute error (MAE) estimations. The random forest with specific parameters (\"n_estimators = 160\" and \"criterion = poisson\") performed best and was chosen for the CODARFE predictor model.\n\nIn summary, our data encoding and preprocessing involved removing low-variance predictors, transforming the target variable, applying compositional data transformations, and evaluating multiple regression methods. This rigorous approach ensured that our machine-learning models were robust and reliable for microbiome-based environmental predictions.",
  "optimization/parameters": "In our study, we evaluated several algorithms, including linear support vector regression, linear stochastic gradient descent, Huber regression, and the Theil-Sen estimator. Each algorithm was tested with a wide range of hyperparameters and applied to various transformations, resulting in a total of 2,270 combinations. These combinations were termed model hyperparameter transformations (MHTs). Each MHT was evaluated using recursive feature elimination (RFE), which removes 1% of the total predictors with the smallest weights at each iteration, generating up to a hundred trained models.\n\nFor the selection of the predictor algorithm, we tested a new set of regressors: linear support vector regression, linear stochastic gradient descent, Huber regression, Theil-Sen estimator, random forest, and support vector machine with various kernels. This resulted in 1,243 combinations. Only the center-log-ratio (CLR) transformation was used in this phase. Each model was trained on 80% of the dataset and evaluated on 20% with 10 repetitions to calculate the average of the mean absolute error (MAE) estimations. The random forest with specific parameters (\"n_estimators = 160\" and \"criterion = poisson\") resulted in better performance for predictions and was chosen for use in COADRFE.\n\nThe number of parameters (p) used in the model varies depending on the specific MHT and the algorithm used. The selection of p was done through a rigorous evaluation process involving RFE and cross-validation with four metrics: R² adjusted, Bayesian information criterion (BIC), root mean squared error (RMSE), and the P value of the F test. These metrics were normalized using the minmax method and summed to create the model’s score. The MHT with the highest model score was selected, ensuring that the model had the best balance of predictive power and generalization.",
  "optimization/features": "In our study, the number of input features varied throughout the process due to the use of recursive feature elimination (RFE). Initially, a wide range of features was considered, but this number was iteratively reduced based on their importance.\n\nFeature selection was indeed performed using RFE, which is a technique that removes a small percentage of the least important features at each iteration. This process was guided by the weights assigned to each predictor by the regression algorithms used, such as Huber regression.\n\nTo ensure the robustness of our feature selection process, it was conducted using only the training set. This approach helps to prevent data leakage and ensures that the selected features are truly indicative of the underlying patterns in the data, rather than being influenced by the test set. By using the training set exclusively for feature selection, we maintain the integrity of our model evaluation and avoid overfitting.",
  "optimization/fitting": "In our study, we evaluated several regression algorithms, including linear support vector regression, linear stochastic gradient descent, Huber regression, and the Theil-Sen estimator. Each algorithm was tested with a wide range of hyperparameters and applied to various transformations, resulting in 2,270 combinations. To address the potential issue of overfitting, given the high number of parameters relative to training points, we employed cross-validation using the root mean squared error (RMSE) metric. This approach helped to ensure that our models generalized well to unseen data.\n\nTo mitigate overfitting, we used recursive feature elimination (RFE), which iteratively removes the least important predictors. This process not only helps in reducing the dimensionality of the data but also ensures that the model focuses on the most relevant features, thereby reducing the risk of overfitting. Additionally, we utilized regularization techniques, such as setting the \"alpha\" value in Huber regression, to control the complexity of the model and prevent it from fitting the noise in the data.\n\nTo rule out underfitting, we evaluated our models using multiple criteria, including the adjusted R-squared (R² adjusted), Bayesian Information Criterion (BIC), and the p-value of the F-test. These metrics helped us to select models that not only had a good fit to the data but also were statistically significant. The adjusted R-squared penalizes the addition of unnecessary predictors, ensuring that the model is concise and generalizes well. The BIC, which penalizes the number of predictors, further helped in selecting models that balanced complexity and fit.\n\nMoreover, we normalized all metrics using the min-max method and summed them to create a model score. The model with the highest score was selected, ensuring that it performed well across all evaluation criteria. This comprehensive evaluation process helped us to identify the best-performing model while avoiding both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the primary methods used was cross-validation, specifically focusing on the root mean squared error (RMSE) metric. This approach is crucial because overfitting can significantly degrade the performance of a prediction model, making it the most critical metric to monitor.\n\nAdditionally, we utilized regularization techniques inherent in some of the algorithms we evaluated. For instance, Huber regression includes an \"alpha\" parameter that controls the regularization, helping to prevent the model from becoming too complex and overfitting the training data. The specific value of \"alpha\" was set at 0.0003, which was determined through extensive hyperparameter tuning.\n\nAnother regularization method employed was the Bayesian Information Criterion (BIC). BIC not only helps in selecting models with a good fit but also penalizes models with a higher number of predictors, thereby reducing the risk of overfitting. This criterion was particularly useful in differentiating between models that had similar R-squared values but differed in the number of predictors used.\n\nFurthermore, we applied recursive feature elimination (RFE) as part of our model evaluation process. RFE iteratively removes the least important predictors, which helps in reducing the dimensionality of the data and mitigates overfitting by focusing on the most relevant features.\n\nLastly, the use of the adjusted R-squared (R² adjusted) metric also played a role in preventing overfitting. This metric includes a penalty for the number of predictors in the model, encouraging the selection of more parsimonious models that generalize better to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly documented and available for reference. Specifically, we evaluated several algorithms, including linear support vector regression, linear stochastic gradient descent, Huber regression, and the Theil-Sen estimator. Each algorithm was tested with a wide range of hyperparameters and applied to various transformations, resulting in 2,270 combinations. The evaluation criteria included R² adjusted, Bayesian information criterion (BIC), root mean squared error (RMSE), and the P value of the F test. These metrics were normalized using the minmax method and summed to create the model’s score.\n\nThe selected model hyper-parameter transformation (MHT) is the one with the highest model score. In our analysis, Huber regression yielded the highest scores when paired with the centered log-ratio (CLR) transformation. The \"epsilon\" parameter, determining outlier influence, was set to 2, and the \"alpha\" value, controlling regularization, was set at 0.0003.\n\nThe data and code for CODARFE analysis are hosted on Zenodo, ensuring that all necessary files and configurations are accessible to the public. Additionally, the machine learning annotations have been deposited in the DOME registry. These resources are available under open-access licenses, allowing researchers to reproduce our findings and build upon our work.\n\nThe CODARFE plots and analysis are also hosted on Zenodo, providing a comprehensive view of our methodology and results. The original raw data for different groups are available through various repositories, including GitHub and the MGnify Projects, with specific accession numbers provided for each dataset. This ensures transparency and reproducibility in our research.\n\nIn summary, all hyper-parameter configurations, optimization parameters, and model files are reported and made available to the scientific community. The data and code are hosted on Zenodo and the DOME registry, with open-access licenses facilitating widespread use and further research.",
  "model/interpretability": "The model developed in this study is not a black-box model. It is designed to be transparent and interpretable, allowing users to understand the relationships between predictors and the target variable. The model employs several techniques to ensure interpretability.\n\nOne key aspect is the use of regression algorithms that return weights corresponding to the importance of each predictor. Algorithms such as linear support vector regression, linear stochastic gradient descent, Huber regression, and the Theil-Sen estimator were evaluated for their ability to provide these weights. This means that each predictor's contribution to the model's predictions can be quantified and understood.\n\nAdditionally, the model uses recursive feature elimination (RFE) to iteratively remove the least important predictors. This process not only helps in reducing the dimensionality of the data but also ensures that the remaining predictors are the most relevant to the target variable. The RFE process is repeated multiple times, generating up to a hundred trained models, each with a different set of predictors. This iterative approach allows for a thorough evaluation of predictor importance.\n\nThe model's performance is evaluated using several metrics, including the coefficient of determination (R²), the Bayesian information criterion (BIC), root mean squared error (RMSE), and the P-value of the F-test. These metrics are normalized using the minmax method and summed to create a model score. The model with the highest score is selected, ensuring that it balances correlation strength, predictor quantity, and statistical significance.\n\nFurthermore, the model's transparency is enhanced by the use of compositional data analysis (CODA) transformations, such as the centered log-ratio (CLR) transformation. These transformations help in mitigating the influence of non-selected predictors on the selected ones, making the model's predictions more reliable and interpretable.\n\nIn summary, the model is designed to be transparent and interpretable, with clear techniques for evaluating predictor importance and model performance. This ensures that users can understand the relationships between predictors and the target variable, making the model a valuable tool for environmental prediction.",
  "model/output": "The model developed in our study is a regression model. It is specifically designed for compositional data analysis (CODA) and focuses on predicting a continuous target variable based on microbiome data. The model employs various regression techniques, including Huber regression, linear support vector regression, linear stochastic gradient descent, and the Theil-Sen estimator. These algorithms were chosen for their ability to handle outliers and provide robust predictions.\n\nThe model's performance is evaluated using several metrics, including the coefficient of determination (R²), the Bayesian Information Criterion (BIC), the root mean squared error (RMSE), and the p-value of the F-test. These metrics help ensure the model's accuracy, robustness, and statistical significance. The recursive feature elimination (RFE) technique is used to iteratively remove the least important predictors, thereby improving the model's predictive power and reducing overfitting.\n\nThe final selected model is the one that achieves the highest score, which is a sum of the normalized values of the evaluation metrics. This approach ensures that the model is both accurate and efficient in predicting the target variable. The model's output includes the selected predictors that are most associated with the target variable, providing insights into the key factors influencing the predictions.",
  "model/duration": "The execution time of our model, CODARFE, was evaluated and compared against other tools using simulated compositional data. The benchmark was conducted on a Linux Ubuntu 24.04 server with a Xeon E5-2620 v4@2.10 GHz processor and 192 GB of DDR4 RAM. The experiments involved two sets of data: one with a varying number of samples and a fixed number of predictors, and another with a fixed number of samples and a varying number of predictors.\n\nCODARFE demonstrated a linear runtime, which is advantageous for handling large datasets. In terms of computational time regarding the number of samples, CODARFE showed a moderate increase in runtime, but it did not exceed 400 seconds (6 minutes) per database. When considering the number of predictors, CODARFE's execution time grew linearly, making it suitable for datasets with a high number of predictors (e.g., > 1,500). This linear growth is a significant advantage over other tools like Coda4Microbiome and Selbal, which exhibit quadratic time complexity due to data transformation, limiting their usage for large datasets.\n\nIn comparison, CLR-LASSO had the lowest runtime in both scenarios, but it is a simpler approach with challenges in result interpretation. CODARFE's linear runtime and ability to handle large datasets make it a robust choice for microbiome-based environmental predictions. Additionally, CODARFE's performance was consistent across different datasets, ensuring reliable and efficient execution times.",
  "model/availability": "The source code for CODARFE is publicly available. It can be accessed via the project's homepage on GitHub. The software is platform-independent, with the exception of the graphical user interface, which is specifically developed for Windows. It is programmed in Python and requires Python 3.10 or higher to run. The software is licensed under the Apache License Version 2.0, which permits its use, modification, and distribution under certain conditions. Additionally, the software has been registered with SciCrunch.org under the ID SCR_026180 and with BioTools under the ID biotools:codarfe. The Software Heritage Library also includes the software with the PID swh:1:snp:cf926f8bf9f8ab5b14d9a2fb174d6b2d445ac425.",
  "evaluation/method": "The evaluation of CODARFE involved a comprehensive approach using three distinct groups of datasets, each serving different purposes. The first group, labeled \"Group A: Literature,\" consisted of 19 datasets, each corresponding to an environmental variable. These datasets were preprocessed into count tables and used to evaluate the coefficient of correlation and processing time required by different methods. A hold-out prediction evaluation was also conducted to assess the performance of CODARFE in predicting environmental variables.\n\nThe second group, \"Group B: ML Repo,\" acted as a positive control. This group included five human gut datasets obtained from the ML Repo collection. These datasets had preexisting evidence linking one or more taxa to the sample metadata variable of interest. The evaluation involved calculating the number of taxa selected by the tools that were also supported by articles as linked to the target variable. This group included datasets on pediatric Crohn’s disease, infant microbiome, and vaginal health.\n\nThe third group, \"Group C: Cross-Studies,\" was designed to evaluate CODARFE's generalization power and potential limitations. This group consisted of two sets of datasets extracted from MGnify. The first set included two projects with pH measurements in arable soil under the influence of a ginseng field and cattle. The second set involved age measurements in humans from four projects, with samples collected from different parts of the digestive system and subjected to various external effects such as antibiotic treatment and HIV. The evaluation in this group involved training a model on samples from one project and predicting the variable of interest on a different project from the same biome.\n\nAdditionally, the evaluation included assessing the impact of missing predictors and the sequenced region on the model's performance. The results indicated that the sequenced region and the preprocessing pipeline significantly affected the model's accuracy. The evaluation also considered the batch effect, which was avoided by training models on a single dataset without batch mixing. This approach ensured that the models identified bacteria specifically associated with the target variable, such as pH, without being influenced by spurious correlations introduced by batch effects.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the predictive power and accuracy of CODARFE and other tools. The primary metrics used include the Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and the coefficient of determination (R²). These metrics are widely recognized in the literature for evaluating the performance of predictive models.\n\nMAE provides a straightforward measure of the average magnitude of errors in a set of predictions, without considering their direction. It is particularly useful for understanding the average error size. MAPE, on the other hand, expresses the error as a percentage of the actual values, making it easier to interpret the relative accuracy of predictions across different datasets. This metric is crucial for understanding how well the model performs in relation to the scale of the target variables.\n\nR² values were used to gauge the model's generalization capability. This metric indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. Higher R² values signify better model performance in terms of explaining the variability in the data.\n\nAdditionally, we evaluated the tools based on their ability to handle missing taxa through imputation methods. The performance of these methods was assessed by calculating the MAE as the percentage of missing taxa increased. This evaluation is essential for understanding the robustness of the tools in real-world scenarios where data may be incomplete.\n\nThe distribution of predictions was also analyzed, with specific attention to the best, median, and worst-case scenarios in the hold-out validation process. This analysis helps in understanding the consistency and reliability of the predictions across different conditions.\n\nIn summary, the set of metrics reported in our study is representative of standard practices in the field. They provide a comprehensive evaluation of the tools' predictive accuracy, robustness, and generalization capabilities. These metrics are widely used in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the domain.",
  "evaluation/comparison": "A comparison was conducted between CODARFE and four other state-of-the-art tools to evaluate their performance in calculating, extracting, and evaluating associations between the microbiome and environmental variables. These tools represent significant advancements in microbiome analysis, each with its own advantages and limitations.\n\nThe tools compared include BRACoD, Coda4Microbiome, Selbal, and CLR-LASSO. BRACoD employs Bayesian regression models to identify biological markers of diseases. Coda4Microbiome uses hierarchical models based on the Bayesian posterior approach to account for the inherent variability of count data. Selbal identifies coabundance patterns among microbiome species using the balances method. CLR-LASSO uses the centered log-ratio transformation for Euclidean representation of data and penalizes LASSO regression for variable selection.\n\nFor a fair comparison, each tool, including CODARFE, was trained with its default parameters without any fine-tuning. BRACoD was used with its default settings, filtering the input data to keep only the top 300 most prevalent bacteria. Coda4Microbiome and Selbal used the \"nearZeroSum\" function to filter the input data. CLR-LASSO required setting a lambda value to mimic the number of variables selected by CODARFE as closely as possible.\n\nThe datasets used for benchmarking included 19 soil health metrics and 5 human disease datasets. For the soil health metrics, CODARFE achieved the best correlation between generalized and real values for 18 out of 19 measurements. In the human disease datasets, CODARFE outperformed other tools by at least 0.2 in terms of R² for the pediatric Crohn’s disease dataset. However, for vaginal health datasets, BRACoD and Coda4Microbiome outperformed CODARFE, possibly due to the low data variation affecting generalization performance.\n\nAdditionally, a cross-studies prediction simulation was conducted to capture the predictive power fairly. Datasets from several projects with different topics but at least one environmental variable in their metadata were gathered. The CODARFE predictor was trained on one dataset and used to predict the target from other datasets. The predictive power was measured using R² values, error percentages, and the percentage of missing taxa in test datasets that could not be replaced.\n\nIn summary, CODARFE demonstrated strong performance in data generalization and the proportion of correctly selected taxa compared to other tools. The comparison highlighted the strengths and limitations of each tool, providing insights into their applicability for different types of microbiome analysis.",
  "evaluation/confidence": "In our evaluation, we employed several metrics to assess the performance of our method, CODARFE, and compared it with other tools. To ensure the robustness of our findings, we used statistical significance tests and confidence intervals where applicable.\n\nFor instance, we calculated the mean absolute percentage error (MAPE) to express the error as a percentage of each target variable’s mean. This metric provides a clear indication of the prediction accuracy and helps in understanding the relative error across different metrics. We also used the R² adjusted metric, which not only measures the correlation strength with the target variable but also penalizes the inclusion of unnecessary predictors, thus helping to reduce the type I error during the fitting process.\n\nThe Bayesian Information Criterion (BIC) was used to differentiate models with similar R² values but different numbers of predictors. This criterion is crucial for maintaining a low type I error rate and ensuring that the model is not overfitted. Additionally, the root mean squared error (RMSE) was used in cross-validation to further reduce overfitting, which is a critical metric since overfitting can severely impact the predictive power of a model.\n\nWe also considered the P-value of the F-test to ensure the statistical significance of the selected predictors. All these metrics were normalized using the min-max method and summed to create the model’s score. The selected model hyperparameter transformation (MHT) is the one with the highest model score, ensuring that our method is both statistically significant and superior in performance.\n\nIn our simulations, we repeated the hold-out method multiple times to calculate the average MAPE, providing a more reliable estimate of the prediction error. This approach helps in understanding the variability and consistency of our method’s performance across different datasets.\n\nFurthermore, we performed recursive feature elimination (RFE) to iteratively reduce the number of columns by filtering for importance. This process not only helps in mitigating issues related to data dimensionality but also ensures that the selected predictors are statistically significant and contribute meaningfully to the prediction model.\n\nOverall, our evaluation process is designed to provide a comprehensive and statistically robust assessment of CODARFE’s performance, ensuring that our claims of superiority over other methods and baselines are well-founded.",
  "evaluation/availability": "The raw evaluation files for our study are publicly available. The CODARFE plots and analysis can be accessed through Zenodo. Additionally, the data and code used for CODARFE analysis are also hosted on Zenodo. For specific datasets, the original raw data for Group A can be found in the referenced source. Group B data is available on GitHub, including details on infant age, PCDAI using baseline CD ileum and rectum, Nugent score, and vaginal pH. Group C data is sourced from the MGnify Projects, with specific accession numbers provided for each study. The machine learning annotations have been deposited in the DOME registry. All these resources are accessible to the public, ensuring transparency and reproducibility of our findings."
}