{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Artificial Intelligence\n- Machine Learning\n- Deep Learning\n- Convolutional Neural Networks\n- Colorectal Cancer\n- Medical Imaging\n- Polarimetric Imaging\n- Data Augmentation\n- Model Performance\n- Histograms\n- Mueller Matrix\n- Classification Metrics\n- Validation Techniques\n- Early Stopping\n- Learning Rate Scheduling\n- PyTorch\n- Computer-Aided Detection\n- Statistical Methods\n- Image Recognition\n- Model Training",
  "dataset/provenance": "The dataset presented in this study, known as ColoPola, comprises polarimetric images of colorectal cancer tissues. It includes 572 tissue slices, with 284 samples identified as cancerous and 288 as healthy. For each slice, 36 polarimetric images are prepared, resulting in a total of 20,592 images. These images are stored in the TIFF file format and are available for download in six Roshal archive (RAR) files. The dataset is divided into three sets: training, validation, and testing, with a ratio of approximately 65:15:20. The training set contains 365 slices, the validation set contains 92 slices, and the testing set contains 115 slices. Each image has a size of 1,280 × 1,024 pixels and is processed to a size of 224 × 224 × 36 for input into deep learning models. The dataset is designed to facilitate the development and evaluation of machine learning and deep learning models for the detection of colorectal cancer using polarimetric imaging techniques. The dataset is available in the Zenodo repository and additional supporting data are available in the GigaScience repository, GigaDB. The dataset has not been used in previous papers by the community, but it has been used in this study to train and evaluate five deep learning models for the classification of colorectal cancer tissues.",
  "dataset/splits": "The dataset was divided into three distinct sets for the purpose of training, validation, and testing. The splits were made in a ratio of approximately 65:15:20.\n\nThe training set contained 365 slices, with 184 normal slices and 181 cancer slices. The validation set consisted of 92 slices, evenly split with 46 normal and 46 cancerous slices. The testing set included 115 slices, with 58 normal and 57 cancerous slices.\n\nThe samples were uniformly distributed between the normal and cancer classes, with a total of 288 normal samples and 284 cancer samples. These samples were randomly assigned to the training, validation, and testing sets to ensure a balanced representation across all splits.",
  "dataset/redundancy": "The ColoPola dataset was divided into three sets for training, validation, and testing purposes, respectively, in a ratio of approximately 65:15:20. The training set contained 365 slices, with 184 normal slices and 181 cancer slices. The validation set contained 92 slices, with 46 normal and 46 cancerous slices. The testing set contained 115 slices, with 58 normal and 57 cancerous slices. The samples were uniformly distributed between the normal and cancer classes and were randomly assigned to the training, validation, and testing sets.\n\nThe training and test sets are independent. This independence was enforced by randomly assigning the samples to each set, ensuring that there is no overlap between the sets. This random assignment helps to prevent data leakage and ensures that the model's performance on the test set is a true reflection of its generalizability to unseen data.\n\nThe distribution of the ColoPola dataset compares favorably to previously published machine learning datasets in the context of colorectal cancer detection. The dataset includes a balanced number of normal and cancerous samples, which is crucial for training robust and unbiased models. The use of polarimetric imaging provides a unique and rich source of data that can capture subtle differences between normal and cancerous tissues, potentially offering better performance than traditional imaging techniques. The dataset's size and diversity, with samples collected from two different hospitals, also contribute to its robustness and generalizability.",
  "dataset/availability": "The dataset supporting the results of this study is publicly available in the Zenodo repository. This ensures that the data is accessible to the broader research community for further analysis and validation. Additionally, all supporting data are available in the GigaScience repository, GigaDB. This dual availability enhances the reproducibility and transparency of the research.\n\nThe dataset is licensed under the GNU GPL v3.0, which allows for free use, modification, and distribution of the data, provided that the original authors are credited and any modifications are shared under the same license. This licensing approach promotes open science and encourages collaboration and innovation.\n\nTo enforce the proper use of the dataset, the project is hosted on GitHub under the name \"Colorectal Cancer Detection.\" The GitHub repository includes detailed documentation and instructions on how to access and use the dataset. This includes information on the data splits used for training, validation, and testing purposes, ensuring that researchers can replicate the experimental setup accurately.\n\nThe dataset comprises 572 slices of healthy and colorectal cancer tissue, with each slice containing 36 polarimetric images. This results in a total of 10,368 instances of healthy tissue and 10,224 instances of colorectal cancer tissue. The data was collected from two hospitals in Vietnam, with patient consent and following relevant guidelines and regulations. All personal information about the samples was concealed to maintain privacy and ethical standards.\n\nThe dataset is designed to be platform-independent and is provided in a format that can be easily integrated into various machine learning and deep learning workflows. The project homepage on GitHub provides links to additional resources, including the WorkflowHub, which offers further details on the data processing and analysis workflows used in the study. This comprehensive approach ensures that the dataset is not only accessible but also usable by a wide range of researchers.",
  "optimization/algorithm": "The optimization algorithm employed in our study is the AdamW optimizer. This is a well-established algorithm class in the field of machine learning, particularly for training deep neural networks. AdamW is an extension of the Adam optimizer, which includes weight decay regularization to improve generalization performance.\n\nThe AdamW optimizer is not a new algorithm; it has been previously published and is widely used in the machine learning community. The decision to use AdamW was based on its proven effectiveness in various deep learning tasks, including image classification, which is the primary focus of our study.\n\nThe choice of optimizer is crucial for the convergence and performance of deep learning models. AdamW was selected for its ability to handle sparse gradients on noisy problems, making it suitable for the complex task of classifying colorectal cancer tissues. The optimizer's parameters, such as the initial learning rate and weight decay, were carefully tuned to ensure optimal performance across different models.\n\nThe use of a well-known optimizer like AdamW aligns with our goal of leveraging established techniques to achieve robust and reliable results. This approach allows us to focus on the specific challenges of our dataset and the architectural innovations of our models, rather than experimenting with novel optimization algorithms.",
  "optimization/meta": "The models utilized in this study do not function as meta-predictors. Instead, they are individual classifiers that operate independently. Each model—CNN, CNN_2, EfficientFormerV2-S0, DenseNet-121, and EfficientNetV2-M—is trained and evaluated separately on the same dataset of colorectal cancer polarimetric images. The dataset consists of 572 samples, with 457 used for training and validation and 115 reserved for testing.\n\nThe models were implemented using the PyTorch library on a desktop computer equipped with an Intel i7-12700 CPU, 32 GB RAM, and a GeForce RTX 4070 GPU. Different initial learning rates were applied to the models built from scratch versus the pretrained models. Learning rate scheduling and early stopping techniques were employed to optimize training and prevent overfitting.\n\nThe performance of these models was evaluated using metrics such as accuracy, precision, recall, and F1 score. The red channel of the images was chosen as the primary input data to ensure consistent learning performance across healthy and malignant classes. This approach reduces the computational complexity compared to methods that use multiple color channels.\n\nIn summary, the study focuses on the individual performance of five different models without integrating them into a meta-predictor framework. Each model is trained and evaluated independently, with no data from other machine-learning algorithms used as input. The training data is split into training, validation, and testing sets to ensure independence and robustness in the evaluation process.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure optimal performance and efficiency. Initially, images captured by a CCD camera were resized from their original dimensions of 1,280 × 1,024 pixels to 900 × 900 pixels by cropping the center of each image. This reduction in size helped to lower computational costs while retaining sufficient polarimetric information necessary for classification.\n\nEach image in the dataset consisted of three basic color channels: red, green, and blue. However, an analysis of the intensity values revealed that the green channel had negligible information, and the blue channel had limited useful data. In contrast, the red channel exhibited a wide range of intensity values across the images, making it the most informative channel. Therefore, to streamline the analysis and reduce computational demands, the red channel was selected as the primary input data for each image.\n\nThe dataset contained 36 polarimetric images for each sample, and since the red channel was chosen, the input data size was set to 900 × 900 × 36, representing the width, height, and the red channel values of the 36 images, respectively. To align with the standard input size for most deep learning models, these images were further rescaled to 224 × 224 × 36 before processing.\n\nAdditionally, data augmentation techniques such as random rotation, contrast-limited adaptive histogram equalization, and blur were applied to increase the diversity and quantity of the training data. This augmentation helped to improve the robustness of the models by exposing them to a wider variety of image variations during training.\n\nThe dataset was divided into training, validation, and testing sets in a ratio of approximately 65:15:20. Specifically, the training set included 365 slices, the validation set contained 92 slices, and the testing set had 115 slices. The samples were uniformly distributed between normal and cancer classes and were randomly assigned to these sets to ensure a balanced representation in each subset.",
  "optimization/parameters": "In our study, the input parameters for the models were carefully selected to optimize performance. The input images consisted of 36 polarimetric images for each sample, resulting in 36 channels. This is a significant increase from the standard 3 channels (red, green, and blue) typically used in color images.\n\nFor the DenseNet-121 and EfficientNetV2-M models, the number of input channels in the first convolutional layer was increased from 3 to 36. The weights of the first 3 channels were kept unchanged, while the remaining 33 channels were initialized using the He technique. This adjustment was necessary to accommodate the increased dimensionality of the input data.\n\nThe EfficientNetV2 model, in particular, includes several new convolutional blocks, such as Fused-MBConv, which replace the depth-wise 3 × 3 Conv and expansion 1 × 1 Conv found in the original EfficientNet. These modifications were made to optimize training speed and parameter efficiency. Progressive learning with adaptive regularization was also applied to gradually increase image size and regularizations at specific stages, further enhancing the model's performance.\n\nThe hyperparameters used for training the models were largely consistent across all five models. However, different initial learning rates were applied to models built from scratch (CNN, CNN_2, and EfficientFormerV2-S0) and pretrained models (DenseNet-121 and EfficientNetV2-M). This approach allowed for faster model updates in the early epochs for the models built from scratch.\n\nIn summary, the input parameters were selected to handle the increased dimensionality of the polarimetric images, with specific adjustments made to the convolutional layers and learning rates to optimize performance. The use of progressive learning and adaptive regularization further enhanced the efficiency and accuracy of the models.",
  "optimization/features": "In our study, the input features for the deep learning models consist of the red channel values from 36 polarimetric images. This results in an input size of 224 × 224 × 36, where 224 × 224 represents the spatial dimensions of each image, and 36 represents the number of polarimetric images per sample.\n\nFeature selection was performed by choosing the red channel as the primary input data. This decision was based on the observation that the red channel contained the most relevant information for distinguishing between healthy and malignant tissue samples. The green and blue channels were ignored as they did not significantly contribute to the classification task.\n\nThe feature selection process was conducted independently of the training set. The choice of the red channel was made based on the analysis of the intensity values across all color channels, ensuring that the selected feature was consistent and informative for both healthy and malignant classes. This approach helped in reducing the computational cost and improving the efficiency of the models.",
  "optimization/fitting": "In our study, we employed several strategies to address potential overfitting and underfitting issues. The number of parameters in our models was indeed larger than the number of training points, which is a common scenario in deep learning. To mitigate overfitting, we utilized techniques such as early stopping and learning rate scheduling. Early stopping involved monitoring the validation loss and halting the training process when the loss ceased to improve, thereby preventing the model from memorizing the training data. Learning rate scheduling, specifically the ReduceLROnPlateau scheduler, adjusted the learning rate when the metrics plateaued, ensuring that the model continued to learn effectively without overfitting.\n\nAdditionally, we implemented data augmentation techniques, such as random rotation, contrast-limited adaptive histogram equalization, and blur, to artificially increase the diversity of our training dataset. This helped the models generalize better to unseen data.\n\nTo rule out underfitting, we ensured that our models were sufficiently complex and capable of learning the underlying patterns in the data. We used architectures like DenseNet-121 and EfficientNetV2-M, which are known for their efficiency and effectiveness in image classification tasks. Furthermore, we trained the models for a sufficient number of epochs (200) and used an appropriate batch size (16) to ensure thorough learning.\n\nThe performance metrics, including accuracy, precision, recall, and F1 score, were evaluated on both validation and testing sets to ensure that the models generalized well to new data. The consistent performance across these sets indicated that our models were neither overfitting nor underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure robust model performance. One of the key methods used was early stopping. This technique monitors the validation loss during training, and if the loss ceases to improve for a specified number of epochs, the training process is halted. This helps to prevent the model from overfitting to the training data by stopping the training when the model starts to memorize the training set rather than generalizing to new data.\n\nAdditionally, we utilized learning rate scheduling with the ReduceLROnPlateau scheduler. This method adjusts the learning rate during training based on the performance metrics. If the metrics do not improve over successive iterations, the learning rate is reduced, which helps in fine-tuning the model and preventing overfitting.\n\nData augmentation was another crucial technique applied to mitigate overfitting. By augmenting the training data through random rotations, contrast-limited adaptive histogram equalization, and blurring, we increased the diversity of the training samples. This approach helps the model to generalize better by exposing it to a wider variety of input variations.\n\nMoreover, we implemented dropout, a regularization technique that randomly sets a fraction of input units to zero at each update during training time. This prevents units from co-adapting too much, which can lead to overfitting. By applying dropout, we ensured that the model remains robust and does not rely too heavily on any single feature.\n\nThese regularization methods collectively contributed to the development of models that are not only accurate but also generalize well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are thoroughly documented. The specific hyperparameters, such as the initial learning rates, batch sizes, and epochs, are detailed in the publication. For models built from scratch, a higher initial learning rate was applied to accelerate the initial training process, while pretrained models used a lower initial learning rate. Learning rate scheduling with the ReduceLROnPlateau scheduler was employed to adjust the learning rate when metrics ceased to improve. Early stopping was also utilized to prevent overfitting by monitoring the validation loss.\n\nThe models were implemented using the PyTorch library on a desktop computer equipped with an Intel i7-12700 CPU, 32 GB RAM, and a GeForce RTX 4070 GPU. The dataset consisted of 572 samples, with 80% used for training and validation and 20% reserved for testing. Data augmentation techniques, including random rotation, contrast-limited adaptive histogram equalization, and blur, were applied to increase the amount of training data.\n\nThe performance of the models was evaluated using four metrics: accuracy, precision, recall, and F1 score. These metrics were calculated for the training, validation, and testing stages to ensure comprehensive evaluation. The models included CNN, CNN_2, EfficientFormerV2-S0, DenseNet-121, and EfficientNetV2-M. The EfficientNetV2 model achieved the highest F1 score, demonstrating robust performance across both validation and testing sets.\n\nModel files and optimization parameters are available in the study’s GitHub repository. The pretrained models utilized weights trained on the ImageNet-1K dataset, and the repository provides access to these models along with the necessary code for replication. The repository is open-source, allowing researchers to access and utilize the configurations and parameters for their own studies. This transparency ensures that the methods and results can be verified and built upon by the scientific community.",
  "model/interpretability": "The models employed in this study are primarily deep learning architectures, which are often considered black-box models due to their complex, multi-layered structures that make it challenging to interpret how they arrive at specific predictions. The models used include convolutional neural networks (CNN and CNN_2), EfficientFormerV2, DenseNet-121, and EfficientNetV2-M. These models are designed to automatically learn and extract features from input data, but the internal workings and the specific features they focus on are not easily interpretable.\n\nThe CNN and CNN_2 models, for instance, utilize convolutional blocks with layers such as convolutional layers, batch normalization, rectified linear units (ReLU), and dropout. These components work together to process and reduce the dimensionality of the input data, but the exact mechanisms by which they contribute to the final classification are not transparent. Similarly, the pretrained models like DenseNet-121 and EfficientNetV2-M, which were trained on large datasets like ImageNet-1K, leverage pre-existing knowledge to make predictions, but the specific features they use from the training data are not explicitly clear.\n\nWhile these models are powerful in terms of performance, their lack of interpretability means that it is difficult to understand why a particular prediction was made. This can be a limitation in fields where transparency and explainability are crucial, such as medical diagnostics. However, the focus of this study was on the performance metrics such as accuracy, precision, recall, and F1 score, which indicate how well the models can classify normal and cancerous tissue samples rather than on interpretability.\n\nIn summary, the models used in this study are black-box models, and while they demonstrate strong performance in classifying tissue samples, they do not provide clear insights into the decision-making process.",
  "model/output": "The model is a classification model designed to distinguish between normal and cancerous tissue samples. Specifically, it categorizes inputs into two classes: normal tissue and malignant cancer tissue. The performance of the model was evaluated using metrics such as accuracy, precision, recall, and F1 score, which are commonly used in classification tasks. The model's output indicates whether a given tissue sample is likely to be normal or cancerous, based on the features extracted from the input data.\n\nThe models utilized in this study include both custom-built architectures and pretrained models. The custom-built models, such as CNN and CNN_2, were designed with specific convolutional blocks and layers to optimize performance. Pretrained models like DenseNet-121 and EfficientNetV2-M were fine-tuned using weights trained on the ImageNet-1K dataset. These models were implemented using the PyTorch library on a desktop computer equipped with an Intel i7-12700 CPU, 32 GB RAM, and a GeForce RTX 4070 GPU.\n\nDuring the training process, techniques such as learning rate scheduling and early stopping were employed to enhance model performance and prevent overfitting. Data augmentation methods, including random rotation and contrast-limited adaptive histogram equalization, were applied to increase the diversity of the training dataset. The model's performance was assessed on both validation and testing sets, with a focus on metrics that reflect its ability to correctly classify tissue samples.\n\nOverall, the classification model demonstrated varying levels of performance across different architectures. While some models exhibited higher precision and recall, others showed a tendency towards higher false-positive or false-negative rates. Statistical tests, such as McNemar’s and 2-sided binomial tests, were conducted to compare the models' classification abilities, revealing no significant differences in their performance on the testing set. This indicates that all models can be effectively used for detecting colorectal cancer using the ColoPola dataset.",
  "model/duration": "The models were implemented on a desktop computer equipped with an Intel i7-12700 CPU, 32 GB of RAM, and a GeForce RTX 4070 GPU. This hardware setup facilitated efficient training and validation processes. The training involved 572 samples, with 457 used for training and validation (80% of the dataset) and 115 retained for testing (20% of the dataset). Data augmentation techniques, such as random rotation, contrast-limited adaptive histogram equalization, and blur, were applied to increase the amount of training data.\n\nThe models utilized included CNN, CNN_2, EfficientFormerV2-S0, DenseNet-121, and EfficientNetV2-M. Different initial learning rates were applied to the models built from scratch and the pretrained models. For instance, CNN and CNN_2 were trained with a higher learning rate to accelerate the model update in the first few epochs. Learning rate scheduling with the ReduceLROnPlateau scheduler was applied when metrics ceased to improve in successive iterations during the latter stages of training. Additionally, the early stopping technique was used to mitigate overfitting by monitoring the validation loss.\n\nThe performance of the models was evaluated using metrics such as accuracy, precision, recall, and F1 score. The results showed that EfficientFormerV2, DenseNet, and EfficientNetV2 achieved an F1 score of more than 90% on the testing set, while CNN and CNN_2 had lower F1 scores of 87% and 86.2%, respectively. The superior performance of the pretrained models can be attributed to their deeper structures and more sophisticated operations.\n\nThe CNN and CNN_2 models exhibited relatively lower performance, with precision values of 0.862 and 0.847, respectively, for the testing set. The CNN model showed a high false-positive rate (FPR) and false-negative rate (FNR), indicating challenges in accurately classifying malignant and normal samples. The CNN_2 model had similar issues, with slightly poorer classification performance. The EfficientFormerV2 model also had a high FPR, particularly on the testing set.",
  "model/availability": "The source code for the project is publicly available. The project is named \"Colorectal Cancer Detection\" and can be accessed via its homepage on GitHub. The code is written in Python and is licensed under the GNU GPL v3.0. This license allows for the free use, modification, and distribution of the software, provided that the original copyright and license notice are included in all copies or substantial portions of the software.\n\nThe project is platform-independent, meaning it can be run on any operating system that supports Python. This makes it accessible to a wide range of users and researchers. The availability of the source code and the open-source license encourages collaboration and further development of the algorithms for colorectal cancer detection.\n\nThe project also has an identifier in the Research Resource Identifiers (RRID) database, which is SCR_024827, and a biotoolsID, which is colopola_dataset_for_colorectal_cancer_detection. These identifiers help in the unique identification and citation of the software in scientific literature.\n\nAdditionally, the workflow associated with this project is available on WorkflowHub with the DOI https://doi.org/10.48546/WORKFLOWHUB.WORKFLOW.1797.2. This provides a standardized way to access and reproduce the computational workflow used in the study.",
  "evaluation/method": "The evaluation of the models involved several key steps and metrics to ensure robust and reliable performance assessment. Five different deep learning models were evaluated: CNN, CNN_2, EfficientFormerV2-S0, DenseNet-121, and EfficientNetV2-M. These models were trained and validated using 80% of the dataset, consisting of 457 samples, while the remaining 20%, or 115 samples, were reserved for testing.\n\nTo enhance the training data, augmentation techniques such as random rotation, contrast-limited adaptive histogram equalization, and blur were applied. This helped in increasing the diversity of the training samples and improving the models' generalization capabilities.\n\nThe performance of the models was evaluated using four primary metrics: accuracy, precision, recall, and F1 score. These metrics provided a comprehensive view of the models' ability to correctly classify normal and cancerous tissue samples. Accuracy measured the overall correctness of the classifications, precision assessed the correctness of positive predictions, recall evaluated the ability to identify all relevant instances, and the F1 score balanced precision and recall.\n\nLearning rate scheduling and early stopping techniques were employed to optimize the training process. The ReduceLROnPlateau scheduler adjusted the learning rate when the metrics ceased to improve, while early stopping monitored the validation loss to prevent overfitting.\n\nStatistical tests, including McNemar’s and 2-sided binomial tests, were conducted to compare the binary classification performance of the models. These tests helped in determining whether there were significant differences in the classification abilities of the models. The results indicated that there was no significant difference in the performance of the five trained models on the testing set, suggesting that all models could be effectively used with the ColoPola dataset to detect colorectal cancer.\n\nThe models were implemented on a desktop computer equipped with an Intel i7-12700 CPU, 32 GB RAM, and a GeForce RTX 4070 GPU, using the PyTorch library. This setup ensured efficient and reliable model training and evaluation.",
  "evaluation/measure": "In the evaluation of our models, we focused on four key performance metrics to comprehensively assess their effectiveness in detecting colorectal cancer. These metrics are accuracy, precision, recall, and the F1 score. Accuracy is the ratio of correctly predicted observations to the total number of observations, providing an intuitive measure of overall performance. Precision evaluates the proportion of positive class predictions that are truly positive, indicating how often the model is correct when it predicts a sample as malignant. Recall, on the other hand, measures the proportion of actual positives that are correctly identified by the model, highlighting its ability to detect all relevant cases. The F1 score combines precision and recall into a single metric, offering a balanced view of the model's performance, especially useful when dealing with imbalanced datasets.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, particularly in medical imaging and cancer detection. They provide a robust framework for comparing the performance of different models and ensuring that our results are both reliable and representative of the state-of-the-art in the field. By reporting these metrics, we aim to offer a clear and comprehensive understanding of how well our models perform in distinguishing between normal and cancerous tissues.",
  "evaluation/comparison": "In our study, we evaluated the performance of five different deep learning models for colorectal cancer detection using the ColoPola dataset. The models included two convolutional neural networks (CNN and CNN_2) trained from scratch, and three pre-trained models: EfficientFormerV2, DenseNet, and EfficientNetV2. This approach allowed us to compare the effectiveness of models with varying architectures and training strategies.\n\nThe CNN and CNN_2 models were designed to extract 128 output features in their first convolutional layers, balancing information usability and computational complexity. These models were trained with higher initial learning rates to accelerate updates in the early epochs, and learning rate scheduling was applied to optimize performance during training. Early stopping was also used to prevent overfitting.\n\nIn contrast, the pre-trained models—DenseNet, EfficientNetV2, and EfficientFormerV2—were originally designed for color image classification using red, green, and blue channels. For our study, we used the red channel values of the polarimetric images as the primary input data, which reduced computational costs and analysis time. The default number of output features in the first convolutional layers of these models varied: 16 for DenseNet, 24 for EfficientNetV2, and 16 for EfficientFormerV2.\n\nThe performance of these models was evaluated using four key metrics: accuracy, precision, recall, and F1 score. The results showed that EfficientFormerV2, DenseNet, and EfficientNetV2 achieved F1 scores of over 90% on the testing set, outperforming the CNN and CNN_2 models, which had lower F1 scores of 87% and 86.2%, respectively. This superior performance of the pre-trained models can be attributed to their deeper structures and more sophisticated operations.\n\nStatistical tests, including McNemar’s and 2-sided binomial tests, were conducted to compare the binary classification performance of the models. The tests indicated no significant difference in performance among the five models on the testing set, suggesting that all models can be effectively used with the ColoPola dataset for colorectal cancer detection.\n\nIn summary, our evaluation involved a comparison of models with different architectures and training strategies, providing a comprehensive assessment of their performance in detecting colorectal cancer. The use of pre-trained models demonstrated their effectiveness in achieving high accuracy and reliability in this context.",
  "evaluation/confidence": "The evaluation of the models involved several statistical tests to assess the significance of the results. Two primary statistical tests were employed: McNemar’s test and a 2-sided binomial test. These tests were used to compare the binary classification performance of the five trained models on the testing set. The null hypothesis for these tests was that there should be no significant difference in the classification ability between the models. Specifically, the number of samples classified as normal by one model but as cancer by another should be equal to the number of samples classified as cancer by the first model but as normal by the second.\n\nThe significance level (α) was set at 0.05 for both statistical tests. In McNemar’s test, the P value was calculated based on the chi-square (χ2) distribution with continuity correction and 1 degree of freedom. However, for some paired models, such as CNN vs. CNN_2, CNN vs. EfficientFormerV2, and DenseNet vs. EfficientFormerV2, the number of examples (n01 + n10) was very few (≤ 10), necessitating the use of a 2-sided binomial test. The binomial test was calculated with a probability of 0.5, indicating an assumption that there was a 50% chance of the model’s output being true or false.\n\nThe results presented in the table showed no significant difference in the performance of the five trained models on the testing set. This means the null hypothesis could not be rejected, indicating that all five models can be used with the ColoPola dataset to detect colorectal cancer. The significance entries at the foot of the table indicated whether there was a significant difference (Yes) if the P value was less than α or no difference (No) if the P value was greater than α.\n\nOverall, the evaluation confidence is supported by rigorous statistical testing, ensuring that the performance metrics are reliable and that the claims of model superiority are statistically validated.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the dataset supporting the results of this article is available in the Zenodo repository. All additional supporting data are available in the GigaScience repository, GigaDB. The project's source code is available on GitHub under the GNU GPL v3.0 license, which allows for free use, modification, and distribution of the software, provided that the original authors are credited and any modifications are also licensed under the same terms. The project is platform-independent and written in Python. The workflow is also available on WorkflowHub."
}