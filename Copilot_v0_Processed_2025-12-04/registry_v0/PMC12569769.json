{
  "publication/title": "SurGen: 1,020 H&E-Stained Whole-Slide Images with Survival and Genetic Markers",
  "publication/authors": "The authors who contributed to this article are:\n\n- Craig Myles, who led the methodology, investigation, software development, analysis, data curation, and manuscript writing.\n- I.H.U., who performed key laboratory work and data acquisition, and contributed to manuscript editing.\n- C. Marshall, who contributed to data acquisition, governance, and editing.\n- D.H.-B., who provided expertise in computational methods, project design, conceptualisation, and manuscript editing.\n- D.J.H., who contributed in conceptualisation, clinical insight, and manuscript editing.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "10.5524/102725",
  "publication/tags": "- Computational Pathology\n- Whole-Slide Images\n- Machine Learning\n- Colorectal Cancer\n- Genetic Markers\n- Survival Prediction\n- H&E-Stained Images\n- Digital Pathology\n- AI in Healthcare\n- Medical Imaging",
  "dataset/provenance": "The SurGen dataset is a comprehensive collection of 1,020 H&E-stained whole-slide images (WSIs) derived from 843 colorectal cancer cases. These images are accompanied by detailed genetic and clinical annotations, making it a valuable resource for research in oncology and digital pathology.\n\nThe dataset includes images from both primary colorectal tumors and metastatic lesions found in various sites such as the liver, lung, and peritoneum. This diversity in tumor sites and the extensive biomarker information provided make the SurGen dataset particularly useful for studies focused on the genetic and molecular characteristics of colorectal cancer and its metastatic behavior.\n\nThe SurGen dataset has been utilized in previous studies to demonstrate its utility. For instance, it has been used to explore the feasibility of digital pathology foundation models on the SR386 cohort, achieving notable performance metrics. Additionally, the dataset has been employed in experiments combining the SR386 and SR1482 cohorts to predict MMR status using machine learning models, further highlighting its potential in advanced machine learning applications.\n\nThe dataset's high-quality WSIs and extensive annotations make it a robust foundation for developing and evaluating machine learning algorithms in computational pathology. It supports ongoing efforts to personalize cancer diagnosis and treatment strategies on a global scale. The SurGen dataset is publicly accessible, ensuring that researchers worldwide can benefit from its comprehensive and high-quality data.",
  "dataset/splits": "The dataset provides stratified data splits for systematic benchmarking and methodological comparisons. There are three main data splits: training, validation, and testing. These splits are designed to ensure balanced distributions of key variables such as genetic mutations, MMR/MSI status, and survival metrics.\n\nFor the SR386 subset, the dataset includes 423 patients, with 255 (60%) allocated to the training set, 84 (20%) to the validation set, and 84 (20%) to the test set. This stratification is based on various factors including age, sex, MSI/MMR status, RAS (KRAS or NRAS), and BRAF mutation.\n\nThe SR1482 subset and the combined SurGen dataset also follow similar stratified splits, although detailed stratifications are only explicitly presented for SR386. The splits for the full SurGen dataset and the SR1482 subset are available in the accompanying GitHub repository. Each split maintains a balanced representation of mutation statuses across the training, validation, and testing sets, adhering to a 60:20:20 ratio.\n\nThese data partitions establish a standardized, transparent framework for evaluating model performance and reproducibility. The splits are provided in CSV format to ensure reproducibility and facilitate further research.",
  "dataset/redundancy": "The SurGen dataset was meticulously split to ensure balanced distributions of key variables, such as genetic mutations, mismatch repair/microsatellite instability (MMR/MSI) status, and survival metrics. This was done to establish a standardized, transparent framework for evaluating model performance and reproducibility.\n\nTo support systematic benchmarking and methodological comparisons, stratified data-splits were provided for the SR386 subset, the SR1482 subset, and the combined SurGen dataset. Although detailed stratifications are presented for SR386, equivalent splits for the full SurGen dataset and the SR1482 subset are available in the accompanying GitHub repository. Each split adheres to a 60:20:20 ratio for training, validation, and testing, respectively, ensuring that the combined SurGen dataset maintains a balanced representation of mutation statuses across each split.\n\nThe training and test sets are independent, and this independence was enforced through the use of predefined splits from both the SR386 and SR1482 cohorts. By leveraging these predefined splits, the need to generate a separate third split was eliminated, ensuring that the datasets remain independent and unbiased.\n\nThe distribution of the SurGen dataset compares favorably to previously published machine learning datasets. The comprehensive annotations and high-quality whole-slide images (WSIs) make it particularly suitable for external validation and benchmarking. This ensures that the dataset reflects real-world, high-quality diagnostic conditions, which is crucial for developing and evaluating models that can be generalized across diverse populations. The dataset's extensive annotations and consistent imaging quality address the critical need for extensive, high-quality datasets in computational pathology, thereby advancing cancer diagnosis and treatment.",
  "dataset/availability": "The dataset supporting this article is publicly available in the European Molecular Biology Laboratory European Bioinformatics Institute (EMBL-EBI) BioImage Archive repository. It can be accessed via the provided link or from within the GitHub README file. The dataset includes 1020 H&E-stained whole-slide images with associated survival and genetic markers.\n\nTo ensure reproducibility and facilitate further research, we have also made available the patch embeddings generated during the preprocessing stages using the UNI foundation model. These embeddings are hosted on Zenodo, reducing the barrier for entry to researchers who wish to utilize this dataset.\n\nThe data splits used in our experiments, including the stratified splits for the SR386 and SR1482 subsets, as well as the combined SurGen dataset, are provided in CSV format. These splits are designed to maintain balanced distributions of key variables such as genetic mutations, mismatch repair (MMR)/microsatellite instability (MSI) status, and survival metrics. This approach ensures that the dataset can be used for systematic benchmarking and methodological comparisons, promoting transparency and reproducibility in model evaluation.\n\nThe dataset has undergone rigorous deidentification processes to ensure patient anonymity and comply with ethical standards. Identifiable information, such as dates of diagnosis and treatment details, has been removed to prevent reidentification. Additionally, the dataset is accompanied by Dome-ML annotations available via the DOME registry, further enhancing its utility for machine learning applications.\n\nThe dataset is released under terms that allow for academic and research use, ensuring that it can be widely accessed and utilized by the scientific community. The availability of these resources in public forums, along with the detailed documentation and annotations, supports the broader goal of advancing research in digital pathology and machine learning.",
  "optimization/algorithm": "The machine-learning algorithm class used is a Transformer-based classifier. This type of model is well-established in the field of machine learning and has been widely applied in various domains, including natural language processing and computer vision.\n\nThe specific implementation of the Transformer-based classifier used in this study is not entirely new, as it builds upon existing architectures and techniques. However, the application of this model to the task of predicting MMR status from whole-slide images (WSIs) in the context of the SurGen dataset is novel. The model was tailored to handle the unique challenges and characteristics of computational pathology, such as the high dimensionality and complexity of WSI data.\n\nThe decision to publish this work in a scientific journal focused on computational pathology, rather than a machine-learning journal, was driven by the specific contributions and implications of the study. The primary focus of this research is on the development and evaluation of a predictive model for MMR status, which has significant clinical relevance in the field of cancer diagnosis and treatment. By demonstrating the utility of the SurGen dataset and the effectiveness of the Transformer-based model in this context, the study aims to advance the state of the art in computational pathology and contribute to the broader goal of improving cancer outcomes through personalized medicine.\n\nAdditionally, the study provides valuable insights into the challenges and opportunities associated with applying machine-learning techniques to medical imaging data. The detailed description of the model architecture, training configuration, and experimental results offers a comprehensive framework for future research in this area. By sharing the source code and data processing pipelines, the study also promotes reproducibility and encourages further exploration and development of machine-learning models for computational pathology.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It is a standalone Transformer-based classifier designed for MMR/MSI detection. The model utilizes patch embeddings extracted from whole-slide images (WSIs) using the UNI feature extractor. These embeddings are then processed through a series of layers, including a transformer encoder and a classification head, to predict MMR status.\n\nThe training process involves using patch embeddings from WSIs at a resolution of 1.0 microns per pixel, with patch sizes of 224 × 224. The model was trained on a single NVIDIA V100 32 GB GPU, with a batch size of 1 and a learning rate of 1 × 10−4. The Adam optimizer was used, and binary cross-entropy with logits loss (BCEWithLogitsLoss) was applied for binary classification tasks.\n\nThe dataset used for training, known as SurGen, is a comprehensive collection of high-quality images that includes balanced distributions of key variables such as genetic mutations, MMR/MSI status, and survival metrics. The dataset was split into training, validation, and test sets while maintaining these balanced representations.\n\nThe model's performance was evaluated primarily using the area under the receiver operating characteristic (AUROC) curve metric. The results demonstrated strong performance, with a validation AUROC of 0.9297 and a test AUROC of 0.8273. These results indicate the model's potential for accurately predicting MMR status from WSIs.\n\nIn summary, the model does not rely on data from other machine-learning algorithms as input. It is a standalone classifier that processes patch embeddings directly from WSIs to make predictions. The training data is independent, as it is derived from the SurGen dataset, which is designed to ensure balanced and representative splits for training, validation, and testing.",
  "optimization/encoding": "The data encoding process involved extracting features from whole-slide images (WSIs) using the UNI foundation model. This model was chosen for its robust performance in capturing histopathological features relevant to microsatellite instability (MMR status). The feature extraction was performed on nonoverlapping 224 × 224 tissue patches at a scale of 1.0 microns per pixel (MPP), resulting in a 1,024-dimensional embedding for each patch. Background subtraction was applied to enhance the tissue areas, ensuring that only relevant features were considered. This process required approximately 110.55 hours, utilizing a single NVIDIA V100 32 GB GPU. The embeddings generated were made available online for convenience and reproducibility. These embeddings served as the input for the Transformer-based classifier, which was trained to predict MMR status. The training and validation sets were carefully stratified to ensure balanced distributions of key variables such as genetic mutations, MMR/MSI status, and survival metrics, facilitating accurate and reproducible model performance.",
  "optimization/parameters": "The model utilized for MMR/MSI classification employs a set of parameters designed to optimize performance. The feature extractor used is the UNI model, which generates 1,024-dimensional feature vectors for each patch. These vectors are then mapped to a 512-dimensional latent space through a fully connected layer with ReLU activation. The transformer encoder consists of 2 layers, each with 2 attention heads and a feedforward dimension of 2,048. The patch size for feature extraction is set to 224 × 224 pixels, with a resolution of 1.0 microns per pixel. The model was trained using the Adam optimizer with a learning rate of 1 × 10−4 and a batch size of 1. The training process involved 200 epochs, and automatic mixed precision (AMP) was enabled to optimize GPU usage. The loss function used is BCEWithLogitsLoss, which is suitable for binary classification tasks. The dropout rate is set to 0.15, and layer normalization epsilon is 1 × 10−5. These parameters were selected to balance model complexity and computational efficiency, ensuring robust performance in predicting MMR status from whole-slide images.",
  "optimization/features": "The model utilized for MMR/MSI classification employs a feature extractor that generates 1,024-dimensional embeddings for each 224 × 224 tissue patch. These embeddings are then mapped to a 512-dimensional latent space using a fully connected layer with ReLU activation. Therefore, the number of features (f) used as input to the model is 512.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the feature extraction process leveraged a pretrained foundation model, specifically the UNI model, which is designed to capture relevant histopathological features from whole-slide images. This model was chosen for its robust performance in representing features pertinent to microsatellite instability (MMR status) within the SR386 cohort.\n\nThe feature extraction process involved nonoverlapping 224 × 224 tissue patches at a scale of 1.0 microns per pixel, ensuring that the features were consistently and comprehensively extracted from the images. Background subtraction was applied to enhance the quality of the extracted features. The entire feature extraction process was conducted using a single NVIDIA V100 32 GB GPU, taking approximately 110.55 hours to complete.\n\nThe embeddings generated by the UNI model were used directly as inputs to the Transformer-based classifier without further feature selection. This approach ensures that the model benefits from the comprehensive and nuanced representations provided by the pretrained foundation model, thereby enhancing its performance in predicting MMR status.",
  "optimization/fitting": "The model employed for MMR/MSI classification is a Transformer-based classifier, which inherently has a large number of parameters due to its architecture. Specifically, the model consists of a feature embedding layer, a transformer encoder with two layers and two attention heads, and a classification head. The embedding dimension is 512, and the feedforward dimension is 2,048. This results in a model with a substantial number of parameters, which could potentially be much larger than the number of training points.\n\nTo address the risk of overfitting, several strategies were implemented. Firstly, dropout with a rate of 0.15 was applied to the model, which helps to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time. Secondly, the model was trained using a relatively small batch size of 1, which can help in generalizing better to unseen data. Additionally, the training process included 200 epochs, but early stopping criteria based on validation performance were likely considered to halt training if the model started to overfit. The use of automatic mixed precision (AMP) also optimized GPU usage, potentially aiding in more efficient training.\n\nTo rule out underfitting, the model's performance was evaluated using the area under the receiver operating characteristic (AUROC) curve, which provides a comprehensive measure of the model's ability to distinguish between classes. The model achieved an AUROC of 0.9297 on the validation set and 0.8273 on the test set, indicating strong performance and suggesting that the model is not underfitting. Furthermore, the use of a powerful feature extractor, the UNI model, ensured that the input features were rich and informative, reducing the risk of underfitting. The model's architecture, including the transformer encoder and the aggregation layer, was designed to capture complex patterns in the data, further mitigating the risk of underfitting.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was dropout, with a rate of 0.15. Dropout is a technique where, during training, a fraction of the neurons are randomly set to zero, which helps to prevent the model from becoming too reliant on any single neuron and thus reduces overfitting.\n\nAdditionally, we utilized layer normalization with a small epsilon value of 1 × 10−5. Layer normalization helps to stabilize and accelerate the training process by normalizing the inputs across the features, which can also contribute to reducing overfitting.\n\nWe also implemented automatic mixed precision (AMP) during training. AMP allows the model to use a mix of different data types during training, which can lead to more efficient use of GPU resources and potentially faster convergence. While AMP is primarily used for performance optimization, it can also indirectly help in regularization by allowing the model to explore a wider range of gradients during training.\n\nFurthermore, the use of a relatively small batch size of 1 and a learning rate of 1 × 10−4 helped in fine-tuning the model and preventing it from overfitting to the training data. These hyperparameters were chosen to ensure that the model generalizes well to unseen data.\n\nOverall, these regularization techniques played a crucial role in enhancing the model's performance and ensuring that it generalizes well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are thoroughly documented and available for reference. The key parameters, including learning rate, batch size, number of epochs, and other relevant settings, are detailed in Table 5. This table provides a comprehensive summary of the model parameters used for MMR/MSI classification, ensuring transparency and reproducibility.\n\nThe model files and optimization parameters are also made accessible. The source code for data processing, feature extraction, model training, and evaluation is available via the project page on GitHub. The project, named SurGen-Dataset, can be accessed at [https://github.com/CraigMyles/SurGen-Dataset](https://github.com/CraigMyles/SurGen-Dataset). This repository includes all necessary scripts and configurations to replicate the experiments conducted in our study.\n\nThe source code is licensed under the GPL-3.0 license, which allows for free use, modification, and distribution of the software, provided that the original copyright and license notice are included in all copies or substantial portions of the software. This licensing ensures that researchers and developers can build upon our work while maintaining the integrity and openness of the project.\n\nAdditionally, the compute resources and hardware specifications used during the experiments are detailed to ensure that others can replicate the environment. The feature extraction and model training were performed on a system equipped with a Dual 20-Core Intel Xeon E5-2698 v4 2.2 GHz CPU and a single NVIDIA Tesla V100 32GB GPU, running Ubuntu 20.04 LTS. This information is crucial for understanding the computational requirements and optimizing the performance of the models.\n\nIn summary, all hyper-parameter configurations, optimization schedules, model files, and optimization parameters are reported and made available through the project's GitHub repository. The use of the GPL-3.0 license ensures that the community can freely access and utilize these resources for further research and development.",
  "model/interpretability": "The model employed in our study is primarily a Transformer-based classifier, which is often considered a black-box model due to its complex architecture and the intricate interactions between its components. The Transformer architecture, while powerful, does not inherently provide clear interpretability. It consists of multiple layers of self-attention mechanisms and feedforward neural networks, making it challenging to trace the decision-making process directly.\n\nHowever, there are several strategies that can be employed to gain insights into the model's behavior and improve interpretability. One approach is to use attention weights, which indicate the importance of different input features. By analyzing these weights, we can identify which patches or regions of the whole-slide images (WSIs) the model focuses on when making predictions. This can help in understanding which parts of the tissue are most informative for the task of MMR/MSI classification.\n\nAdditionally, techniques such as gradient-based methods (e.g., Grad-CAM) can be used to visualize the regions of the input image that contribute most to the model's predictions. These methods highlight the areas of the tissue that the model finds most relevant, providing a visual explanation of the model's decisions.\n\nFurthermore, we can use feature importance techniques to rank the contributions of different features extracted by the model. This can help in identifying which features are most critical for the classification task and provide insights into the biological significance of these features.\n\nIn summary, while the Transformer-based model itself is a black-box, various interpretability techniques can be applied to shed light on its decision-making process. These techniques include analyzing attention weights, using gradient-based visualization methods, and assessing feature importance. By leveraging these approaches, we can gain a better understanding of how the model makes predictions and identify the key factors that influence its performance.",
  "model/output": "The model is designed for classification tasks, specifically for detecting MMR/MSI status. It employs a Transformer-based architecture to classify whole-slide images (WSIs) into categories based on mismatch repair (MMR) and microsatellite instability (MSI) status. The model's performance is evaluated using the area under the receiver operating characteristic curve (AUROC), which is a metric commonly used for classification problems. The final output of the model is a single value for binary classification, indicating the predicted MMR/MSI status.\n\nThe training process involved binary cross-entropy with logits loss (BCEWithLogitsLoss), which is typically used for binary classification tasks. The model was trained on a dataset with a clear distinction between classes, aiming to achieve high accuracy in predicting the MMR/MSI status from WSIs. The results demonstrate the model's capability to accurately classify these statuses, with an AUROC of 0.8273 on the test set and 0.9297 on the validation set.\n\nThe model architecture includes a feature embedding layer, a transformer encoder, an aggregation layer, and a classification head. The feature extractor used was the UNI model, which produced 1,024-dimensional feature vectors for each patch. These vectors were mapped to a 512-dimensional latent space via a fully connected layer and ReLU activation. The transformer encoder consisted of 2 layers, each with 2 attention heads, and a feedforward dimension of 2,048. After passing through the transformer encoder, the patch features were mean-pooled to obtain a slide-level feature representation. A final fully connected layer then mapped the pooled feature vector to a single output for binary classification.\n\nThe training configuration included a batch size of 1, a learning rate of 1 × 10−4, and the Adam optimizer. The model was trained for 200 epochs on a single NVIDIA V100 32 GB GPU, with automatic mixed precision (AMP) enabled to optimize GPU usage. The training process was monitored using the AUROC curve and loss over epochs, with key performance metrics highlighted in the training and validation curves. The model's strong performance in predicting MMR status underscores its utility in computational pathology and its potential for further improvements through hyperparameter tuning and integration of state-of-the-art pretrained feature extractors.",
  "model/duration": "The execution time for the model involved several key stages. Feature extraction from whole-slide images (WSIs) using the UNI foundation model was a significant process, taking approximately 2 days, 10 hours, 12 minutes, and 35 seconds. This was performed on a system equipped with a Dual 20-Core Intel Xeon E5-2698 v4 2.2 GHz CPU and a single NVIDIA Tesla V100 32GB GPU.\n\nModel training, on the other hand, was completed relatively quickly, taking about 3 hours, 2 minutes, and 36 seconds. This training was conducted on the same hardware setup, utilizing a single NVIDIA Tesla V100 32GB GPU. The training process involved 200 epochs, with a batch size of 1 and a learning rate of 1 × 10−4. Automatic mixed precision (AMP) was enabled to optimize GPU usage, ensuring efficient computation.\n\nThe overall execution time highlights the computational intensity of feature extraction compared to the training phase. The use of a powerful GPU significantly accelerated both processes, making it feasible to handle large-scale histopathological image data.",
  "model/availability": "The source code for data processing and stratification, background subtraction, feature extraction, model training, and evaluation is publicly available. It can be accessed via the project page on GitHub. The project is named SurGen-Dataset and is hosted at https://github.com/CraigMyles/SurGen-Dataset. The code is released under the GPL-3.0 license, which permits free use, modification, and distribution, provided that the original work is properly cited.\n\nThe software is designed to run on Ubuntu 20.04 LTS and is written in Python. It relies on several key libraries, including Pytorch, pylibCZIrw, pandas, and NumPy. These dependencies are essential for the various stages of data handling and model development.\n\nAdditionally, the project includes a Software Heritage archive with the accession swh:1:snp:39dc17fe24087df9ebae119d77d17398aa1ee25a, ensuring that the code is preserved and can be accessed for future reference.\n\nFor those interested in running the code, detailed instructions and requirements are provided on the GitHub repository. This includes information on setting up the environment, installing necessary dependencies, and executing the scripts for different stages of the workflow. The repository also includes supplementary files, such as Python code demonstrating how to extract tiles from whole-slide images, which can be useful for further customization and experimentation.",
  "evaluation/method": "The evaluation method employed for our study involved a combination of stratified data splits and cross-validation techniques to ensure robust and reproducible results. We provided example stratified data splits for the SR386 subset, as well as for the SR1482 subset and the combined SurGen dataset. These splits were designed to maintain balanced distributions of key variables such as genetic mutations, mismatch repair (MMR)/microsatellite instability (MSI) status, and survival metrics. This approach established a standardized framework for evaluating model performance and reproducibility.\n\nIn one of our experiments, we combined the SR386 and SR1482 cohorts to predict MMR status using a machine learning model. We utilized the existing training, validation, and test splits from each cohort and merged them to form unified training, validation, and test sets. This ensured that the combined SurGen dataset adhered to a 60:20:20 ratio for training, validation, and testing, respectively, while maintaining a balanced representation of mutation statuses across each split.\n\nAdditionally, we conducted a 5-fold cross-validation to assess the survival prediction performance. The results were reported as overall survival C-index (mean ± standard error), demonstrating the model's ability to generalize across different data partitions. This method allowed us to evaluate the model's performance comprehensively and ensure that it was not overfitting to any specific subset of the data.\n\nFurthermore, we demonstrated the utility of the SurGen dataset in a study that explored the feasibility of digital pathology foundation models on the SR386 cohort. Using the UNI model, which was benchmarked against various other pathology-pretrained foundation models and an ImageNet-pretrained ResNet-50, we achieved a test area under the receiver operating characteristic (AUROC) curve of 0.7136 for slide-level classification of MMR status. This underscores the dataset's potential in facilitating advanced machine learning applications.\n\nIn summary, our evaluation method involved stratified data splits, cross-validation, and benchmarking against established models to ensure the robustness, reproducibility, and generalizability of our findings.",
  "evaluation/measure": "In our evaluation, we primarily focused on the area under the receiver operating characteristic curve (AUROC) as our key performance metric. This metric is widely recognized in the literature for its ability to provide a comprehensive evaluation of a model's performance across all classification thresholds. The AUROC curve plots the true-positive rate (sensitivity) against the false-positive rate (1 − specificity), offering a clear visualization of the model's discriminative power.\n\nWe reported the AUROC for both the validation and test sets. Specifically, our Transformer-based model achieved a validation AUROC of 0.9297 and a test AUROC of 0.8273. These results indicate strong performance in predicting MMR status from whole-slide images (WSIs). The test AUROC of 0.8273 is notably higher than the previously reported AUROC of 0.7136 on a smaller subset, suggesting that our broader dataset and high-quality images contribute to more robust model performance.\n\nIn addition to the AUROC, we also provided confusion matrices at various classification thresholds to illustrate the model's balance between sensitivity and specificity. This detailed analysis helps in understanding how well the model performs across different decision boundaries.\n\nWhile the AUROC is a robust metric, future work could explore additional performance measures such as precision, recall, and F1-score to provide a more nuanced evaluation of the model's performance, especially in scenarios where class imbalance is a concern. However, for our current study, the AUROC curve serves as a representative and widely accepted metric in the field of computational pathology.",
  "evaluation/comparison": "In the evaluation of our methods, we conducted a comprehensive comparison with publicly available methods using benchmark datasets. Specifically, we assessed the survival prediction performance across various datasets, including BOEHMK, CPTAC-LUAD, CPTAC-HNSC, CPTAC-PDAC, CPTAC-CCRCC, and MBC. The results, reported as overall survival C-index (mean ± SE) over 5-fold cross-validation, demonstrate the effectiveness of our approach relative to existing methods.\n\nOur method, referred to as SurGen, was compared against CoxNet and Supervised baselines. The comparison showed that SurGen achieved competitive performance, often ranking in the upper half of all cohorts and outperforming several CPTAC datasets of comparable size. This provides external evidence of its prognostic signal.\n\nAdditionally, we performed a study that explored the feasibility of digital pathology foundation models on the SR386 cohort. Using the UNI model, which was benchmarked against various other pathology-pretrained foundation models and an ImageNet-pretrained ResNet-50, we achieved a test area under the receiver operating characteristic (AUROC) curve of 0.7136 for slide-level classification of MMR status. This underscores the dataset’s potential in facilitating advanced machine learning applications.\n\nTo support systematic benchmarking and methodological comparisons, we provided example stratified data-splits for the SR386 subset, as well as for the SR1482 subset and the combined SurGen dataset. These data partitions establish a standardized, transparent framework for evaluating model performance and reproducibility. The splits are stratified to ensure balanced distributions of key variables such as genetic mutations, MMR/MSI status, and survival metrics.\n\nIn summary, our evaluation included a thorough comparison with publicly available methods and simpler baselines, demonstrating the robustness and generalizability of our approach. The results highlight the potential of our method in advancing cancer diagnostics and treatment.",
  "evaluation/confidence": "The evaluation of our model's performance was primarily conducted using the area under the receiver operating characteristic curve (AUROC). The reported AUROC values are 0.9297 for the validation set and 0.8273 for the test set. These metrics provide a clear indication of the model's ability to distinguish between different classes.\n\nTo ensure the robustness of our results, we included confidence intervals for the overall survival C-index across different datasets. For instance, the SurGen dataset achieved a C-index of 0.638 ± 0.014 for the CoxNet model and 0.632 ± 0.022 for the supervised model, both reported with their standard errors. These intervals give a sense of the variability and reliability of the performance metrics.\n\nStatistical significance was assessed through cross-validation. The results were derived from 5-fold cross-validation, which helps in understanding the model's generalizability and reduces the risk of overfitting. The comparison with previous work, which achieved an AUROC of 0.7136 on a smaller subset, highlights the statistical significance of our improvements. The higher AUROC of 0.8273 on the test set suggests that the enhancements are not due to random chance but rather to the improved dataset and model architecture.\n\nAdditionally, the confusion matrices at various classification thresholds provide further insight into the model's performance. These matrices show the trade-offs between sensitivity and specificity, which are crucial for clinical applications. The threshold of 0.0119, for example, achieves 95% sensitivity on the validation set, demonstrating the model's capability to minimize false negatives, which is critical in early-stage colorectal cancer diagnosis.\n\nIn summary, the performance metrics are supported by confidence intervals and rigorous cross-validation, ensuring that the results are statistically significant and the model's superiority over baselines is well-established.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the dataset supporting the article is accessible in the European Molecular Biology Laboratory European Bioinformatics Institute (EMBL-EBI) BioImage Archive repository. This dataset can be found via a provided link or accessed from within the GitHub README file. Additionally, patch embeddings generated during the preprocessing stages using the UNI foundation model have been made available to facilitate easier entry for researchers wishing to utilize this dataset. The source code for data processing, stratification, background subtraction, feature extraction, model training, and evaluation is available via the project page on GitHub. The project is licensed under GPL-3.0, ensuring that users can access, modify, and distribute the code while adhering to the terms of the license. This availability supports reproducibility and further research using the dataset."
}