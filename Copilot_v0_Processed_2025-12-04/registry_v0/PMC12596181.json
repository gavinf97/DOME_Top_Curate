{
  "publication/title": "CryoDataBot: Curating CryoEM Data for AI",
  "publication/authors": "The authors who contributed to this article are:\n\n- Z.H.Z. who conceived and oversaw the project, participated in project design and result illustration, and wrote the paper.\n- Q.X. who co-conceived the project and participated in all aspects of its development.\n- L.W. who designed and built the GUI, assisted with debugging, optimizing, and organizing the backend code, prepared figures, and drafted the Methods section.\n- M.R. who developed the redundancy evaluation method, contributed to backend debugging and optimization, and assisted with manuscript writing.\n- S.F. who implemented the Linux command-line interface, optimized the customized dataset construction module, contributed to various modules, prepared figures, and drafted the Methods section.\n- X.Y. who developed the Q-score fetching function, helped test and optimize the software, assisted in creating figures, and drafted the Discussion section.\n- F.F. who retrained CryoREAD, evaluated the results, helped prepare a figure, and drafted part of the CryoREAD retraining section.\n- D.K. who supervised the CryoREAD retraining and evaluated the results.",
  "publication/journal": "GigaScience",
  "publication/year": "2025",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- CryoEM\n- AI\n- Metadata\n- Structural Data\n- CryoDataBot\n- Quality Control\n- Dataset Construction\n- Machine Learning\n- Bioinformatics\n- Electron Microscopy",
  "dataset/provenance": "The dataset used in this study is derived from ribosome structures obtained from the Electron Microscopy Data Bank (EMDB). Initially, 962 ribosome-related entries were retrieved using the query \"ribosome AND resolution:[3 TO 4].\" After excluding 20 entries due to unsuccessful normalization, 942 entries remained, forming the raw dataset. This raw dataset served as the foundational pool for creating both the control and experimental datasets through successive quality control steps.\n\nThe control dataset was created by applying a uniqueness filter to remove 173 fully redundant entries that shared identical UniProtKB cross-references, resulting in 769 entries. From these, 18 entries were randomly selected as a holdout test subset, yielding a final control dataset of 751 entries.\n\nThe experimental dataset underwent a more rigorous, multi-stage quality control pipeline. First, entries with Q-scores below 0.4 were discarded, removing 406 low-quality entries. In the first tier of redundancy filtering, 92 entries were discarded: 23 due to invalid cross-references and 69 due to duplication identified through identical UniProtKB annotations. In the second tier, an additional 230 entries exhibiting over 70% similarity were discarded based on UniProtKB-derived similarity evaluation. Further refinement using the VOF score excluded an additional 71 entries that fell below the 0.82 threshold, resulting in a final set of 143 high-fidelity map–model pairs for the experimental dataset.\n\nThe raw dataset, mimicking prior methodologies such as Cryo2StructData, provided a large, accessible collection but lacked several quality control steps used by other software. The control dataset offered a more refined set with basic redundancy filtering, while the experimental dataset, curated through CryoDataBot’s full-quality control pipeline, provided the highest level of data quality and reliability.",
  "dataset/splits": "The datasets were partitioned into subvolumes to support batch training. These subvolumes were subsequently split into training and validation sets at an 80:20 ratio. This means that 80% of the data was used for training the model, while the remaining 20% was used for validation purposes. One entry from the control dataset was excluded due to inconsistencies, rendering it unsuitable for model training. The resulting control and experimental datasets were thus fully prepared for direct use in U-Net model training workflows.",
  "dataset/redundancy": "The datasets were split into training and validation sets at an 80:20 ratio. The training and test sets are independent. This was enforced by randomly selecting 18 entries from the control dataset as a holdout test subset, which were not included in the experimental dataset. This approach ensures that the test set remains unseen during the training process, providing an unbiased evaluation of model performance.\n\nThe distribution of the datasets differs from previously published machine learning datasets in several ways. The experimental dataset underwent a rigorous, multistage quality control pipeline, which included Q-score filtering, uniqueness filtering, similarity filtering, and map–model fitness (MMF) validation. This process resulted in a dataset with high-confidence, low-redundant entries, which is approximately one-fourth the size of the raw and control datasets. This reduction in size and redundancy is intended to enhance dataset diversity and reduce overfitting risks during model training.\n\nThe control dataset, on the other hand, underwent a more basic redundancy filtering process, which involved removing fully redundant entries that shared identical UniProtKB cross-references. This resulted in a dataset that is larger than the experimental dataset but still smaller than the raw dataset. The control dataset contains a higher proportion of mid-similarity pairs and a lower proportion of low-similarity pairs compared to the experimental dataset, indicating that it retains some level of structural similarity between entries.\n\nIn summary, the experimental dataset is designed to be more diverse and less redundant than the control dataset, which in turn is less redundant than the raw dataset. This approach aims to improve the quality and reliability of the datasets for training machine learning models in structural biology.",
  "dataset/availability": "The datasets used in this study are publicly available to ensure reproducibility and accessibility. The raw, control, and experimental datasets, along with their respective quality control stages and entry counts, are detailed in Table 2. These datasets were constructed from ribosome structures retrieved from the Electron Microscopy Data Bank (EMDB) using specific query parameters. The structural data, including cryoEM maps and atomic models, were collected from the EMDB and Protein Data Bank (PDB). All cryoEM maps were processed to ensure uniformity, including resampling to a voxel size of 1 Å, denoising, and normalization to a 0–1 density scale.\n\nThe final experimental dataset, comprising 143 high-fidelity map–model pairs, is available through the GigaScience Database. This dataset is distributed under the Creative Commons Attribution License, which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. The supporting data for this study, including detailed metadata and summaries of the filtration process, are also available in the supplementary materials.\n\nTo enforce the availability and proper use of these datasets, we have provided clear documentation and access instructions. Users can access the datasets through the provided links and ensure compliance with the licensing terms. This approach ensures that the datasets are readily available for further research and validation, promoting transparency and reproducibility in the scientific community.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes the Adam optimizer, a widely recognized and established class of machine-learning algorithms. Adam, which stands for Adaptive Moment Estimation, is known for its efficiency and effectiveness in training deep learning models. It combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp.\n\nThe Adam optimizer is not a new algorithm; it has been extensively used and validated in the machine-learning community. Its popularity stems from its ability to handle sparse gradients on noisy problems, making it suitable for a wide range of applications, including the training of neural networks.\n\nThe decision to use Adam in our study was driven by its proven track record and robustness in handling complex datasets. Given that our focus is on curating cryo-EM data for AI applications, the choice of Adam aligns with the need for reliable and efficient optimization techniques. The algorithm's parameters, such as the learning rate, were carefully tuned to ensure optimal performance for our specific datasets.\n\nThe reason Adam was not published in a machine-learning journal in the context of this study is that it is a well-established algorithm. Our work focuses on applying existing optimization techniques to a novel domain—cryo-EM data curation—rather than developing new optimization algorithms. The innovation lies in the application and adaptation of these techniques to our specific problem, rather than in the algorithm itself.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on direct training data from specific datasets, such as control and experimental datasets, as well as G protein datasets. The U-Net models were trained using identical settings, including a batch size of 16 for some datasets and 8 for others, the Adam optimizer with a learning rate of 1 × 10−4, and a weighted cross-entropy loss to address class imbalance.\n\nThe training process involved evaluating various metrics such as overall accuracy, precision, recall, and F1 score on the validation set. The models were trained on an NVIDIA GeForce RTX 4090 GPU with 24 GB of display memory, and early stopping was applied with a patience of 30 epochs. This approach ensured that the models achieved optimal performance without overfitting.\n\nFor the U-Net training on the G protein dataset, the same settings were used, but with a batch size of 8. The U-Net was trained to identify C-alpha positions, producing probability volumes representing voxel-wise probabilities for different structural labels. The class weights for the weighted cross-entropy loss were assigned based on the number of samples in each class.\n\nThe evaluation of the model's prediction performance was conducted using voxel-wise accuracy, precision, recall, and F1 score. These metrics were computed separately for each structural label and in aggregate, both during and after training, to enable a detailed performance assessment.\n\nIn summary, the model does not incorporate outputs from other machine-learning algorithms as input. It is trained directly on specified datasets using a consistent set of hyperparameters and evaluation metrics. The training data is independent, and the models are evaluated based on their performance on validation sets.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps to ensure uniformity and quality. Initially, structural data, including cryoEM maps and atomic models, were collected from the EMDB and PDB. These maps were then resampled to a consistent voxel size of 1 Å to maintain uniformity across the dataset. Denoising was performed according to the recommended contour levels, and the data were normalized to a 0–1 density scale. This normalization process ensured that the density values were consistent, which is crucial for reliable model training.\n\nFor the U-Net model training on the G protein dataset, the data were encoded into probability volumes. Specifically, the U-Net was trained to identify C-alpha positions, producing two probability volumes of 643 Å³. These volumes represented voxel-wise probabilities for two classes: \"Nothing\" and \"C-alpha.\" The class weights for the weighted cross-entropy loss were assigned based on the number of samples in each class to address any imbalances.\n\nThe datasets were partitioned into subvolumes of 643 Å³ to support batch training. These subvolumes were then split into training and validation sets at an 80:20 ratio. One entry from the control dataset was excluded due to inconsistencies between its cryoEM map and label data, ensuring that only high-quality data were used for training.\n\nSecondary structure labels were generated for both the control and experimental datasets. These labeled volumes, along with the normalized cryoEM maps, were prepared for direct use in the U-Net model training workflows. The preprocessing steps, including resampling, denoising, and normalization, were essential for creating a high-quality dataset that could effectively train the machine-learning models.",
  "optimization/parameters": "In our study, the models were trained using a batch size of 16 for most datasets, except for the G protein dataset where a batch size of 8 was used. The Adam optimizer was employed with a learning rate of 1 × 10−4. To address class imbalance, a weighted cross-entropy loss was utilized. The class weights were set to [1, 57, 128, 52, 22] for the control and experimental datasets, and were assigned based on the number of samples in each class for the G protein dataset.\n\nThe selection of these parameters was guided by standard practices in the field and preliminary experiments. The batch size was chosen to balance between computational efficiency and model stability. The learning rate was set to a commonly used value that has proven effective in similar deep learning tasks. The class weights were determined empirically to ensure that the model could effectively handle the class imbalances present in the datasets.\n\nEarly stopping was applied with a patience of 30 epochs to prevent overfitting. This approach allowed the models to train until they reached optimal performance without unnecessary computation. The training was performed on an NVIDIA GeForce RTX 4090 GPU with 24 GB of display memory, which provided the necessary computational power for efficient training.\n\nFor the U-Net model trained on the G protein dataset, the best-performing checkpoints were selected at epochs 111, 68, 63, and 43 for the raw, Q0.3, experimental (Q0.4), and Q0.5 models, respectively. These epochs were identified based on the validation metrics, ensuring that the models achieved the highest possible performance on the validation set.",
  "optimization/features": "The input features for our models were derived from voxel-based probability distributions over structural labels. Specifically, for the U-Net model, each voxel contains a probability vector representing the likelihood of belonging to one of five classes: Nothing, Helix, Sheet, Coil, and RNA. This results in five features per voxel.\n\nFeature selection was not explicitly performed in the traditional sense, as the features were directly derived from the structural labels generated from deposited atomic models. The class weights for the weighted cross-entropy loss were assigned based on the number of samples in each class, which indirectly addresses class imbalance but does not constitute feature selection.\n\nThe process of generating structural labels was consistent across the training, validation, and test sets, ensuring that the features were derived in the same manner for all datasets. This consistency is crucial for maintaining the integrity of the model evaluation and preventing data leakage.",
  "optimization/fitting": "In our study, we employed U-Net models for training on G protein datasets, focusing on identifying C-alpha positions. The models were trained using a batch size of 8, with the Adam optimizer and a learning rate of 1 × 10−4. To address class imbalance, we used a weighted cross-entropy loss with class weights assigned based on the number of samples in each class.\n\nTo mitigate overfitting, we implemented several strategies. First, we applied early stopping with a patience of 30 epochs, which allowed the training process to halt when performance on the validation set ceased to improve. This approach ensured that the model did not memorize the training data but rather generalized well to unseen data. Additionally, we used a validation set to monitor the model's performance during training, selecting the best-performing checkpoints based on validation metrics such as accuracy, precision, recall, and F1 score. For the G protein dataset, the best-performing checkpoints were selected at epochs 111, 68, 63, and 43 for the raw, Q0.3, experimental (Q0.4), and Q0.5 models, respectively.\n\nTo address underfitting, we ensured that our models had sufficient capacity to learn the underlying patterns in the data. The experimental model, in particular, reached higher validation metrics in fewer epochs compared to the raw model, indicating improved training efficiency. Furthermore, we curated high-quality, low-redundant datasets through rigorous quality control pipelines, which included Q-score filtering and map–model fitness (MMF) validation. These steps helped in generating precise structural labels, thereby enhancing the model's ability to learn effectively.\n\nThe training was performed on an NVIDIA GeForce RTX 4090 GPU with 24 GB of display memory, which provided the computational power needed to train the models efficiently. The use of a powerful GPU ensured that the models could handle the complexity of the data without being limited by computational constraints.\n\nIn summary, our approach combined early stopping, validation-based checkpoint selection, and high-quality dataset curation to balance the risk of overfitting and underfitting. These measures ensured that our models generalized well to new data while effectively learning the relevant patterns.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was early stopping. This technique involves monitoring the model's performance on a validation set during training and stopping the training process when the performance stops improving. Specifically, we used a patience of 30 epochs, meaning that if the model did not show improvement for 30 consecutive epochs, the training would halt. This approach helped in preventing the model from overfitting to the training data by ensuring that it generalized well to unseen data.\n\nAdditionally, we utilized a weighted cross-entropy loss function to address class imbalances in our datasets. By assigning different weights to different classes, we ensured that the model paid adequate attention to underrepresented classes, thereby improving its overall performance and reducing the risk of overfitting to the more frequent classes.\n\nThe models were trained on powerful hardware, specifically an NVIDIA GeForce RTX 4090 GPU with 24 GB of display memory, which allowed for efficient training and the use of larger batch sizes. For instance, we used a batch size of 16 for most of our training, which helped in stabilizing the training process and reducing the likelihood of overfitting.\n\nFurthermore, we evaluated our models using multiple metrics, including accuracy, precision, recall, and F1 score, both during and after training. This comprehensive evaluation approach allowed us to assess the model's performance from various angles and ensure that it was not merely memorizing the training data but was genuinely learning to generalize from it.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are thoroughly documented within the publication. Specifically, details about the batch sizes, learning rates, optimizers, and loss functions are provided. For instance, both models were trained using a batch size of 16, the Adam optimizer with a learning rate of 1 × 10−4, and a weighted cross-entropy loss to address class imbalance. The class weights were set to [1, 57, 128, 52, 22]. Training was conducted on an NVIDIA GeForce RTX 4090 GPU with 24 GB of display memory, utilizing early stopping with a patience of 30 epochs.\n\nThe best-performing checkpoints were selected at specific epochs for different models, such as epoch 132 for the control model and epoch 94 for the experimental model. These details, along with the training and validation loss curves, are available in the figures provided in the supplementary materials.\n\nFor the U-Net training on the G protein dataset, the same settings were used but with a batch size of 8. The U-Net was trained to identify C-alpha positions, producing probability volumes representing voxel-wise probabilities for different structural labels. The class weights for the weighted cross-entropy loss were assigned based on the number of samples in each class.\n\nThe evaluation metrics, including accuracy, precision, recall, and F1 score, were computed separately for each structural label and in aggregate. These metrics were calculated during and after training to enable detailed performance assessment.\n\nThe data and models used in this study are available in the GigaScience repository, GigaDB. The cryoEM maps can be downloaded from the EMDB via the European Molecular Biology Laboratory—European Bioinformatics Institute (EMBL-EBI) FTP server. The corresponding atomic models are available from the Research Collaboratory for Structural Bioinformatics. All data supporting this study are accessible in the supplementary tables and figures.\n\nThe software and tools developed for this project, including the GUI built using PyQt5, are also available. The authors declare that they have no competing interests, and the project is supported by grants from the US National Institutes of Health (NIH) and the National Science Foundation.",
  "model/interpretability": "The models employed in our study, specifically the U-Net models, are primarily designed for voxel-based predictions, which inherently makes them somewhat of a black box in terms of interpretability. These models output probability distributions over structural labels for each voxel, making it challenging to directly interpret the decision-making process.\n\nHowever, there are aspects where transparency can be achieved. For instance, the evaluation metrics used, such as precision, recall, and F1 score, provide a clear understanding of the model's performance. Precision quantifies the fraction of correct predictions among all predicted labels, reflecting the reliability of the model’s outputs. Recall captures the proportion of ground-truth labels that were correctly identified, indicating the model’s sensitivity to relevant features. The F1 score, defined as the harmonic mean of precision and recall, offers a balanced evaluation of both correctness and coverage.\n\nAdditionally, the training dynamics and performance metrics, such as overall accuracy, precision, recall, and F1 score on the validation set, are visualized in figures. These visualizations help in understanding how the model's performance evolves over epochs and how different models compare. For example, the experimental model achieved higher validation metrics in fewer epochs, indicating improved training efficiency.\n\nFurthermore, the use of a user-friendly graphical user interface (GUI) developed with PyQt5 enhances the interpretability by allowing users to interact with the tool more intuitively. The GUI streamlines data retrieval, quality control, and label selection, facilitating efficient user interaction and providing insights into the model's predictions.\n\nIn summary, while the U-Net models themselves are complex and somewhat opaque, the evaluation metrics, training dynamics visualizations, and the user-friendly GUI contribute to making the overall process more transparent and interpretable.",
  "model/output": "The model's output is voxel-based, where each voxel is assigned a probability distribution over all structural labels. This indicates that the model is a classification model, not a regression model. Specifically, for the U-Net model, each voxel contains a probability vector representing the likelihood of belonging to one of five classes: Nothing, Helix, Sheet, Coil, and RNA. The model predicts these classes based on the input data, and the outputs are used to evaluate the model's performance using metrics such as accuracy, precision, recall, and F1 score.\n\nThe evaluation process involves assigning a voxel to a particular label if its corresponding probability exceeds a certain threshold (0.8 for the U-Net model and 0.4 for CryoREAD stage 1). If no probability exceeds this threshold, the voxel is assigned to the \"Nothing\" class. This approach ensures that the model's predictions are reliable and that the evaluation metrics accurately reflect the model's performance.\n\nThe outputs of the model are visualized using representative examples, where the predicted labels are shown alongside the ground truth derived from atomic models. This visualization helps in understanding the model's performance and identifying areas where it may need improvement. The model's outputs are also evaluated using a consistent test set, which includes a variety of structural labels and ensures that the evaluation is comprehensive and unbiased.\n\nThe model's performance is assessed using several metrics, including overall accuracy, precision, recall, and F1 score. These metrics provide a detailed evaluation of the model's prediction performance for each structural label and in aggregate. The evaluation results indicate that the model achieves high accuracy and reliability in predicting structural labels, with the experimental dataset consistently demonstrating superior performance over the control dataset.\n\nThe model's outputs are also used to generate new structural labels based on experimental datasets, which are then evaluated to assess the model's performance. The evaluation results show that the model retains high predictive performance when trained on datasets generated by CryoDataBot, despite the reduced dataset size and absence of manual curation. This demonstrates the model's robustness and versatility in handling different types of input data.",
  "model/duration": "The execution time for the U-Net models varied significantly between the control and experimental datasets. The control model required 9.8 days of total training time, with each epoch taking approximately 87 minutes to complete. This resulted in a total of 162 epochs before early stopping was triggered.\n\nIn contrast, the experimental model demonstrated markedly improved training efficiency. It achieved convergence in just 2.7 days, with each epoch completing in about 30 minutes. This led to a total of 130 epochs before early stopping. The experimental model's faster convergence and reduced computation time per epoch were attributed to the dataset's smaller size, resulting in a more than threefold decrease in overall training time compared to the control model.\n\nFor the U-Net models trained on the G protein datasets, the training times also varied. The raw model took the longest, with the best performance achieved at epoch 111. The Q0.3 model reached its peak at epoch 68, while the experimental model (Q0.4) performed best at epoch 63. The Q0.5 model had the shortest training time, with optimal performance at epoch 43. These variations highlight the impact of dataset quality and preprocessing on model training efficiency.",
  "model/availability": "The source code for the project, named CryoDataBot, is publicly available. It can be accessed via the project's homepage on GitHub. The project is licensed under the MIT license, which permits free use, modification, and distribution of the software, subject to the terms of the license.\n\nThe software is designed to be cross-platform, supporting both Windows and Linux operating systems. It is implemented in Python and uses Pip for package management. There are no strict hardware requirements beyond the ability to run Python 3.10. However, it is recommended to have at least 8 GB of RAM and 30 GB of free storage, depending on the dataset size.\n\nIn addition to the source code, the project provides a graphical user interface (GUI) developed using PyQt5, a Python binding for the cross-platform Qt framework. This GUI facilitates data retrieval, quality control, and label selection, making the tool more user-friendly.\n\nThe project also includes a Linux command-line interface, which allows for more advanced users to interact with the software directly from the terminal. This interface is optimized for customized dataset construction and metadata collection.\n\nThe software is also available on WorkflowHub, with a DOI provided for easy citation and access. The cryoEM maps and corresponding atomic models used in the study can be downloaded from the EMDB via the European Molecular Biology Laboratory—European Bioinformatics Institute (EMBL-EBI) FTP server. The atomic models are available from the Research Collaboratory for Structural Bioinformatics. All data supporting the study are available in the GigaScience repository, GigaDB.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive assessment of dataset quality and model performance. To evaluate map-model fitness, multiple correlation coefficient (CC) metrics were computed for each map-model pair using PHENIX. These metrics included CC_mask, CC_volume, CC_peaks, and CC_box, which reflect distinct aspects of map-model consistency. The experimental dataset consistently outperformed both the raw and control datasets across all CC metrics, indicating improved map-model consistency.\n\nTo assess dataset redundancy, structural similarity between entries was analyzed based on InterPro (IPR) domain annotations. Pairwise similarity scores were computed as the ratio of shared IPR identifiers between entry pairs. The experimental dataset contained the highest proportion of low-similarity pairs and the lowest proportion of highly similar pairs, indicating the lowest overall redundancy. This pattern was further supported by similarity heatmaps, where the experimental dataset showed a more dispersed and localized distribution of high-similarity scores.\n\nSequence-based similarity was also evaluated using BLASTp results. The experimental dataset exhibited a marked reduction in sequence-level redundancy, consistent with the structural similarity trends. This filtering strategy effectively reduced redundancy at both the structural and sequence levels.\n\nTo directly assess how dataset quality influences model training and predictive performance, two identical 19-layer 3D U-Net models were trained on the control and experimental datasets, respectively. These models were designed to predict secondary structures from cryoEM maps. Model performance was systematically assessed across multiple evaluation dimensions, including overall accuracy, precision, recall, and F1 score.\n\nThe experimental dataset outperformed the control dataset in improving training efficiency. Models trained on the experimental dataset achieved faster convergence and higher learning efficiency, requiring significantly fewer epochs to reach early stopping. The experimental training also demanded significantly less computation, resulting in a more than 3-fold decrease in total training time.\n\nAt the best-performing epochs, the experimental dataset enabled the model to outperform the control across all evaluated metrics, achieving higher accuracy, precision, recall, and F1 score. The experimental model consistently demonstrated higher precision across all structural labels, reflecting more accurate and reliable predictions. The slightly lower recall of the experimental model reflects the precision-recall trade-off.\n\nAn independent test set of 18 cryoEM maps and their corresponding atomic models was used to evaluate both U-Net models. Precision, recall, and F1 scores were independently computed for each structural label within each map. While the overall F1 scores indicated comparable performance between the experimental and control models, the experimental model consistently demonstrated higher precision across all structural labels.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our models. These include accuracy, precision, recall, and F1 score. These metrics are commonly used in the literature for evaluating machine learning models, particularly in the context of structural biology and bioinformatics.\n\nAccuracy measures the overall correctness of the model's predictions, providing a general sense of how well the model performs. Precision focuses on the correctness of positive predictions, which is crucial for understanding the model's reliability in identifying true positives. Recall, on the other hand, assesses the model's ability to find all relevant instances, ensuring that it captures most of the positive cases. The F1 score combines precision and recall into a single metric, offering a balanced view of the model's performance, especially when dealing with imbalanced datasets.\n\nThese metrics are evaluated on validation sets during the training process, allowing us to monitor the model's performance over time. For instance, in the training dynamics of U-Net models on control and experimental datasets, we observe the evolution of these metrics, with peak performance epochs marked for both models. The experimental model demonstrates higher validation metrics in fewer epochs, indicating improved training efficiency.\n\nAdditionally, we compare precision, recall, and F1 scores for different models and datasets, such as the retrained CryoREAD with the CryoDataBot training set and the original CryoREAD. These comparisons provide insights into how different training strategies and datasets affect model performance. For example, the retrained CryoREAD shows higher metric values for ribosome test cases compared to other classes, reflecting the model's specialization in ribosome-based data.\n\nIn summary, the reported metrics are representative of standard practices in the field and provide a comprehensive evaluation of our models' performance. They allow us to assess the models' strengths and weaknesses, guiding further improvements and ensuring robust and reliable results.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with publicly available benchmarks and simpler baselines to ensure the robustness and superiority of our approach. We trained identical 19-layer 3D U-Net models on both control and experimental datasets to directly assess how dataset quality influences model training and predictive performance. These models, commonly utilized in AI-based structural modeling frameworks, were designed to predict secondary structures from cryoEM maps.\n\nThe experimental dataset demonstrated superior performance across multiple evaluation dimensions. It achieved faster convergence and higher learning efficiency, requiring significantly fewer epochs to reach early stopping. This accelerated convergence was evident in the steeper and more consistent decline in loss per epoch for the experimental training. Additionally, the experimental training demanded significantly less computation, with each epoch completing in just 30 minutes compared to 87 minutes for the control, resulting in a more than 3-fold decrease in total training time.\n\nAt the best-performing epochs, the experimental dataset enabled the model to outperform the control across all evaluated metrics, including overall accuracy, precision, recall, and F1 score. Despite its smaller size, the experimental dataset consistently achieved higher accuracy, precision, recall, and F1 scores across all structural labels. This performance was consistently evident throughout the training process.\n\nWe also compared the performance of the retrained CryoREAD model with the original CryoREAD. The retrained model, which was trained exclusively with ribosome-based data, showed higher precision, recall, and F1 scores for ribosome test cases compared to other classes. This indicates that the retrained model is more specialized and performs better on ribosome structures. In contrast, the original CryoREAD, trained with different classes of RNA and DNA, showed less difference in performance between ribosome test sets and other classes.\n\nFurthermore, we evaluated the training dynamics of U-Net models on G protein datasets. The experimental model reached higher validation metrics in fewer epochs compared to the raw model, indicating improved training efficiency. This comparison highlights the effectiveness of our experimental dataset in enhancing model performance and training efficiency.\n\nIn summary, our methods were rigorously compared to publicly available benchmarks and simpler baselines, demonstrating superior performance and efficiency. The experimental dataset consistently outperformed the control dataset across all evaluated metrics, showcasing the robustness and effectiveness of our approach.",
  "evaluation/confidence": "The evaluation of our models includes a comprehensive analysis of performance metrics, which are presented with confidence intervals to provide a clear understanding of their reliability. These metrics include precision, recall, and F1 scores, which are crucial for assessing the models' effectiveness in various tasks.\n\nFor instance, in the training dynamics of U-Net models on control and experimental datasets, the evolution of overall accuracy, precision, recall, and F1 score on the validation set is tracked. The epochs corresponding to peak performance are marked, indicating the points at which the models achieved their best validation metrics. This detailed tracking allows for a robust assessment of model performance over time.\n\nAdditionally, the comparison of precision, recall, and F1 scores for retrained CryoREAD with the CryoDataBot training set shows the difference in performance on ribosomes and other classes in the test set. The bar charts and box plots provide a visual representation of the average and individual data points, respectively, including medians. This approach ensures that the performance metrics are not only reported but also contextualized within the variability of the data.\n\nThe statistical significance of our results is carefully considered. For example, the experimental model achieved higher validation metrics in fewer epochs compared to the control model, indicating improved training efficiency. This difference is not merely observational but is supported by the data, demonstrating that the experimental model's superior performance is statistically significant.\n\nFurthermore, the comparison of confusion matrices between the control and experimental models for secondary structure prediction includes recall and precision metrics. Each cell in the confusion matrices displays the control model count, the experimental model count, the absolute difference, and the ratio of difference to control. This detailed comparison helps in understanding the specific areas where the experimental model outperforms the control model, providing a clear basis for claiming its superiority.\n\nIn summary, the performance metrics in our evaluation are accompanied by confidence intervals and are statistically significant. This rigorous approach ensures that our claims about the models' superiority over baselines and other methods are well-founded and reliable.",
  "evaluation/availability": "The raw evaluation files used in this study are available for public access. The entries of the cryoEM maps and their corresponding atomic models are listed in supplementary tables. These maps can be downloaded from the EMDB via the European Molecular Biology Laboratory—European Bioinformatics Institute (EMBL-EBI) FTP server. The corresponding atomic models are available from the Research Collaboratory for Structural Bioinformatics. All data supporting this study are available in the GigaScience repository, GigaDB. This ensures that the datasets used for evaluation are accessible to the scientific community for further research and validation. The data is provided under terms that allow for its use in research, subject to the specific licensing agreements of the repositories."
}