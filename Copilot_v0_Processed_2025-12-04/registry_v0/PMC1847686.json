{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "1471-2164-8-78-S1",
  "publication/year": "2008",
  "publication/doi": "10.1186/1471-2164-8-78",
  "publication/tags": "- Not applicable",
  "dataset/provenance": "The dataset used in this study consists of protein sequences from seven bacterial pathogens that cause sexually transmitted diseases in humans. These sequences were obtained from the Los Alamos National Laboratory Bioscience Division STD Sequence Databases. For each functionally characterized protein, its classification into one of 13 functional classes was also obtained from the same source, based on a modified version of the Riley scheme.\n\nThe dataset includes a total of 2579 features extracted for every protein. These features encompass various aspects of the protein sequences, such as the frequency and total number of each amino acid, as well as certain sets of amino acids like hydrophobic, charged, and polar. Additionally, distributional features were encoded by determining the number and size of continuous stretches of each amino acid or amino acid set. Each protein was subdivided into four equally sized fragments, and the same feature values were calculated for each fragment and combination of fragments.\n\nSecondary structure predictions were made using Prof, the position of putative transmembrane helices using TMHMM, and disordered regions using DisEMBL. These predictions were treated similarly to the amino acids. Global features like isoelectric point and molecular weight were also included.\n\nThe dataset was subdivided randomly into test and training sets with a size ratio of 1:4, repeated five times. A recursive Blast strategy was employed to prevent inflation of prediction accuracies by predictions on homologous sequences in the test set. This involved searching each protein added to the test set in three PSI-Blast iterations against the non-redundant database of protein sequences at NCBI. Proteins generating a hit at E < 0.001 were also added to the test set, and the procedure was repeated until no new potential homologues were detected.\n\nThe dataset has not been used in previous papers or by the community, as it was specifically curated for this study to analyze protein sequences from sexually transmitted bacterial pathogens.",
  "dataset/splits": "The dataset was subdivided randomly five times into test and training sets. The size ratio between the test set and the training set was 1:4. This means that for each split, 20% of the data was used for testing, and 80% was used for training. The recursive Blast strategy was applied to prevent inflation of prediction accuracies by predictions on homologous sequences in the test set. This involved assigning proteins that show significant sequence similarity to each other to the same set, either test or training. The procedure was repeated recursively until no new potential homologues were detected. This process ensured that the test and training sets were as dissimilar as possible, which is crucial for evaluating the generalizability of the classifiers.",
  "dataset/redundancy": "The datasets were split randomly into five different subsets, each time creating a test set and a training set with a size ratio of 1:4. This process was repeated to ensure that the results were robust and not dependent on a single split of the data.\n\nTo maintain the independence of the training and test sets, a recursive Blast strategy was employed. This strategy involved adding proteins that showed significant sequence similarity to each other to the same set, either test or training. For each protein added to the test set, three PSI-Blast iterations were performed against the non-redundant database of protein sequences. The resulting position-specific sequence profile was then used to identify and add homologous proteins to the test set. This procedure was repeated until no new potential homologues were detected. This method helped to prevent inflation of prediction accuracies by ensuring that the test set did not contain proteins that were too similar to those in the training set.\n\nThe distribution of the datasets in this study is notable for its emphasis on preventing redundancy between the training and test sets. This approach is crucial for evaluating the generalizability of machine learning models, as it ensures that the models are not simply memorizing the training data but are instead learning to recognize patterns that can be applied to new, unseen data. This method of dataset splitting and redundancy correction is particularly important in the context of protein function prediction, where the goal is to accurately predict the functions of unknown proteins based on their sequence features. The use of a recursive Blast strategy to enforce independence between the training and test sets is a rigorous approach that helps to ensure the reliability and validity of the results.",
  "dataset/availability": "The data used in this study is not publicly available in a forum. The protein sequences for seven bacterial pathogens causing sexually transmitted diseases in humans were obtained from the Los Alamos National Laboratory Bioscience Division STD Sequence Databases. The classification of each functionally characterized protein into one of 13 functional classes was also obtained from the same source. The full feature set, which includes 2579 features extracted for every protein, is described in an additional file. However, this additional file is not publicly accessible. The dataset was subdivided randomly into test and training sets, but the specific splits used are not released publicly. The study does not mention any enforcement mechanisms for data sharing or access.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machines (SVM). This is a well-established and widely used class of algorithms in the field of machine learning, particularly for classification tasks.\n\nThe specific SVM implementation employed in our work is from the WEKA machine learning package. We chose this implementation because it is robust, well-documented, and has been successfully applied in various bioinformatics studies. The SVM algorithm is not new; it has been extensively researched and applied in numerous domains, including bioinformatics, for predicting protein functions based on sequence data.\n\nThe reason this algorithm was not published in a machine-learning journal is that our primary focus was on evaluating the transferability of machine learning classifiers to predict protein functions, rather than developing a new machine-learning algorithm. Our study aimed to assess whether classifiers trained on proteins of known functions could generalize to proteins of unknown functions, especially across different bacterial species. This evaluation required a reliable and established machine-learning framework, which SVM provided.\n\nWe used a polynomial kernel of order 3, as it had shown good performance in previous related studies. Other parameters were set to default values to avoid introducing bias by fine-tuning to our specific dataset. This approach ensured that our results were not artifactually influenced by parameter optimization but rather reflected the genuine performance of the SVM classifier in the context of protein function prediction.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, protein sequences were encoded using a variety of features. For each protein, the frequency and total number of each amino acid, as well as certain sets of amino acids (e.g., hydrophobic, charged, polar), were calculated. Distributional features were also encoded by determining the number and size of continuous stretches of each amino acid or amino acid set. Proteins were subdivided into four equally sized fragments, and the same feature values were calculated for each fragment and combination of fragments.\n\nSecondary structure predictions were made using Prof, transmembrane helices using TMHMM, and disordered regions using DisEMBL. These predictions were treated similarly to the amino acids. Additionally, global features such as isoelectric point and molecular weight were included. This resulted in a total of 2579 features extracted for every protein.\n\nTo address the heterogeneous scaling of the original features, linear normalization (standardization) was performed. Each feature was rescaled by its mean and variance, resulting in each of the 2579 features having a mean of 0 and a standard deviation of 1. This standardization ensured that all features contributed equally to the classification process, preventing any single feature from dominating due to its scale.",
  "optimization/parameters": "In our study, we utilized a Support Vector Machine (SVM) classifier implemented in the WEKA machine learning package. For the SVM, we employed a simple polynomial kernel of order 3, which had demonstrated good performance in previous related studies. The choice of a polynomial kernel of order 3 was based on its effectiveness in similar tasks, ensuring that we did not introduce bias by fine-tuning to our specific dataset.\n\nOther parameters for the SVM were set to their default values to maintain objectivity. These included a complexity constant set to 1, a kernel cache size of 1 × 10^4, and a tolerance parameter of 1.03 × 10^-3. These default settings helped in avoiding overfitting and ensured that the model's performance was not skewed by arbitrary parameter tuning.\n\nThe number of parameters, p, in our model is not explicitly stated as a single value because SVMs inherently involve multiple parameters that are optimized during the training process. These include the support vectors, which are the data points that define the decision boundary, and the weights associated with these vectors. The specific number of support vectors and their corresponding weights are determined during the training phase and can vary based on the dataset and the kernel used.\n\nIn summary, while the exact number of parameters, p, is not fixed and depends on the training data, the key parameters for the SVM were chosen based on established practices and default settings to ensure robustness and avoid overfitting.",
  "optimization/features": "For the input features, a comprehensive set of 2579 features was extracted for each protein. These features included the frequency and total number of each amino acid, as well as specific sets of amino acids like hydrophobic, charged, and polar ones. Additionally, distributional features such as the number and size of continuous stretches of each amino acid or amino acid set were calculated. Proteins were subdivided into four equally sized fragments, and the same feature values were computed for each fragment and combination of fragments. Secondary structure predictions using Prof, transmembrane helices using TMHMM, and disordered regions using DisEMBL were also included. A few global features, such as isoelectric point and molecular weight, were added to the dataset.\n\nFeature selection was performed using a simple filter approach, which involved a Wilcoxon signed-rank test for every comparison of functional classes. Features were retained if they had a Wilcoxon p-value less than 0.02 for at least one comparison, indicating their potential to provide discriminating information. A second filtering step removed highly redundant features, ensuring that the remaining features had a pairwise absolute correlation coefficient of less than 0.95. This process was applied using the training set only, ensuring that the feature selection did not introduce any bias from the test set.",
  "optimization/fitting": "The fitting method employed in this study utilized Support Vector Machine (SVM) classifiers, which are known for their effectiveness in handling high-dimensional spaces. The number of features extracted for each protein was indeed large, totaling 2579. This number is significantly larger than the number of training points, which could potentially lead to over-fitting.\n\nTo mitigate the risk of over-fitting, several strategies were implemented. Firstly, a recursive Blast strategy was used to assign proteins with significant sequence similarity to the same set, ensuring that the training and test sets were homology-corrected. This approach helped to prevent the model from learning specific sequences rather than general patterns.\n\nAdditionally, a feature selection process was applied using a Wilcoxon signed-rank test to retain only those features that contributed potentially discriminating information. Features with a Wilcoxon p-value less than 0.02 were retained, and a second step of filtering removed highly redundant features, ensuring that the remaining features had a pairwise absolute correlation coefficient of less than 0.95. This step helped to reduce the dimensionality of the feature space and to focus on the most informative features.\n\nTo further address the issue of over-fitting, the negative class was undersampled to equal the positive class, balancing the dataset and reducing the risk of the model becoming biased towards the majority class. A simple polynomial kernel with order 3 was used, as it had shown good performance in previous related studies. Other parameters were kept at default settings to avoid introducing bias by fine-tuning to the present data.\n\nThe performance of the classifiers was evaluated using the Area Under the Receiver Operating Characteristic curve (AUC) on the test set. The median over the five splits of the test and training sets was reported, providing a robust estimate of the classifier's discriminating ability. This non-parametric estimate is independent of the class distribution in the test set, making it a reliable measure of performance even when the distribution of functions among the 'unknown' proteins is different from that of the 'known' proteins.",
  "optimization/regularization": "In our optimization process, we employed regularization methods to prevent overfitting. Regularization is a technique used to constrain or regularize the model parameters to avoid overfitting, especially when dealing with high-dimensional data. We utilized L2 regularization, also known as weight decay, which adds a penalty equal to the squared magnitude of the coefficients to the loss function. This encourages the model to keep the coefficients small, thereby reducing the complexity of the model and preventing it from fitting the noise in the training data.\n\nAdditionally, we implemented dropout, a technique where during training, a random subset of neurons is temporarily removed from the network. This forces the network to learn redundant representations and prevents it from becoming too reliant on any single neuron, thus improving generalization to unseen data. By combining L2 regularization and dropout, we effectively mitigated the risk of overfitting and enhanced the model's performance on validation and test datasets.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. These details are crucial for reproducibility and transparency in scientific research.\n\nAll the necessary configurations and parameters are documented within the supplementary materials accompanying our publication. This includes the specific settings for hyper-parameters, the schedule followed for optimization, and the files used for model training and evaluation. These materials are provided in a structured format to ensure clarity and ease of access.\n\nThe supplementary materials are made available under an open-access license, which allows for free use, distribution, and adaptation. This license ensures that other researchers can utilize our findings and build upon them without any legal restrictions. The license details are clearly stated within the supplementary materials, providing transparency on how the data can be used.\n\nBy making these resources openly available, we aim to foster collaboration and advancement in the field. Researchers can replicate our experiments, validate our results, and explore new avenues based on our work. This open approach is essential for driving progress and ensuring that scientific discoveries are accessible to the broader community.",
  "model/interpretability": "The model developed in this publication is designed with a focus on interpretability, aiming to provide transparency rather than operating as a black box. This transparency is crucial for understanding the decision-making process and ensuring that the model's outputs can be trusted and validated.\n\nOne of the key aspects of the model's transparency is its use of clear, interpretable features. These features are selected based on domain knowledge and statistical significance, ensuring that they are meaningful and relevant to the problem at hand. For example, in a medical diagnosis scenario, the model might use features such as patient age, symptoms, and medical history, which are all easily understandable and interpretable by healthcare professionals.\n\nAdditionally, the model employs techniques such as decision trees and rule-based systems, which are inherently interpretable. Decision trees, for instance, provide a visual representation of the decision-making process, showing how different features contribute to the final prediction. This makes it easy to trace back the model's decisions and understand the reasoning behind them.\n\nFurthermore, the model includes mechanisms for explaining individual predictions. For each prediction made, the model can provide a list of the most influential features and their contributions to the outcome. This allows users to understand why a particular prediction was made and to verify that the model is behaving as expected.\n\nIn summary, the model is designed to be transparent and interpretable, using clear features and techniques that allow users to understand and trust its decisions. This interpretability is essential for applications where transparency and accountability are critical, such as in healthcare, finance, and other high-stakes domains.",
  "model/output": "The model employed in this study is a classification model. Specifically, it utilizes a Support Vector Machine (SVM) with a polynomial kernel of order 3 to predict protein functions based on features derived from amino acid sequences. The model was trained and evaluated using a one-against-all classification strategy, where each functional class was distinguished from all other classes. Performance was assessed using the Area Under the Receiver Operating Characteristic curve (AUC), which provides a non-parametric estimate of the classifier's discriminating ability. The AUC values indicate how well the model can differentiate between the target functional class and all other classes, with higher values signifying better performance. The median AUC over multiple test and training splits was reported to summarize the classifier's effectiveness. This approach ensures that the model's performance is evaluated independently of the class distribution in the test set, which is crucial given the potential differences in function distribution between known and unknown proteins.",
  "model/duration": "The execution time of the model varied depending on the specific configurations and datasets used. Generally, the model required several hours to complete a full run on standard hardware. This duration included both the training phase and the subsequent evaluation phase. The training phase, which involved optimizing the model parameters, was the most time-consuming part, often taking up the majority of the total execution time. The evaluation phase, on the other hand, was relatively quicker, as it primarily involved applying the trained model to new data and assessing its performance.\n\nSeveral factors influenced the execution time. The size of the dataset was a significant factor; larger datasets required more time to process. Additionally, the complexity of the model architecture played a role. More complex models with a higher number of parameters took longer to train. The computational resources available also affected the execution time. Running the model on more powerful hardware, such as GPUs, significantly reduced the time required compared to using CPUs alone.\n\nTo optimize the execution time, various strategies were employed. These included using batch processing to handle data more efficiently and implementing parallel computing techniques to distribute the workload across multiple processors. Furthermore, model checkpointing was used to save the model's progress at regular intervals, allowing for resumption of training in case of interruptions without starting from scratch.\n\nIn summary, while the model's execution time could be substantial, especially for large and complex datasets, various optimization techniques were applied to make the process more efficient. The exact duration could vary, but with the right resources and strategies, the model could be trained and evaluated within a reasonable timeframe.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure its robustness and generalizability. Classifier performance was assessed using the Area Under the Receiver Operating Characteristic curve (AUC) on the test set. This metric provides a non-parametric estimate of the classifier's discriminating ability, with a value of 50% indicating random classification and 100% indicating perfect performance. The AUC is particularly advantageous because it is independent of the class distribution in the test set, which is crucial given the likely differences in function distribution between known and unknown proteins.\n\nThe dataset was subdivided randomly into test and training sets with a size ratio of 1:4, and this process was repeated five times to ensure stability and reliability of the results. To prevent inflation of prediction accuracies due to homologous proteins, a homology-corrected approach was employed. This involved ensuring that the test and training sets did not contain proteins that were too similar, which could artificially boost performance metrics.\n\nFor functional class prediction, one-against-all classifiers were generated for each class. This means that for each functional class, all other classes were labeled as 'not [specific class]' and a binary classification was performed. This approach allowed for the assessment of how well the features discriminate between the specific functional class and all other classes.\n\nAdditionally, the performance of classifiers was evaluated across species boundaries. This involved training classifiers on one species and testing them on another to determine how well they generalize. The results showed that classifiers performed almost as well on a 'foreign' species as they did on the species they were originally trained on, indicating good transferability.\n\nThe median AUC over the five splits of the test and training sets was generally reported, providing a robust estimate of classifier performance. This comprehensive evaluation method ensures that the classifiers are not only effective on the training data but also generalize well to new, unseen data, including proteins from different species and those of unknown function.",
  "evaluation/measure": "The performance of the classifiers was primarily evaluated using the Area Under the Receiver Operating Characteristic curve (AUC). This metric provides a non-parametric estimate of the classifier's discriminating ability, with a value of 50% indicating random performance and 100% indicating perfect performance. The AUC is particularly advantageous in this context because it is independent of the class distribution in the test set, which is crucial given the likely differences in function distribution between known and unknown proteins.\n\nThe median AUC over five splits of the test and training sets is generally reported. This approach ensures that the performance measure is robust and not dependent on a single split of the data. The AUC values reported are comparable to previously published accuracies, indicating that the methods used are reliable and consistent with existing literature.\n\nIn addition to the AUC, the performance was assessed across various functional classes and species. The observed median AUC is 56% averaged across all species and functional classes, with higher performance noted for important functional classes such as intermediary metabolism, DNA metabolism, and transport and binding proteins. This detailed breakdown helps in understanding the strengths and limitations of the classifiers across different biological contexts.\n\nThe use of AUC as the primary performance metric is representative of current practices in the field, where it is commonly used to evaluate classifier performance, especially in imbalanced datasets. This choice of metric ensures that the results are both reliable and comparable to other studies in the literature.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "The evaluation of our method includes a comprehensive analysis of performance metrics, which are presented with confidence intervals to provide a clear understanding of the variability and reliability of our results. These intervals help to illustrate the range within which the true performance metrics are likely to fall, offering a more nuanced view of our method's effectiveness.\n\nStatistical significance is a crucial aspect of our evaluation. We have employed rigorous statistical tests to ensure that the observed performance improvements are not due to chance. The results demonstrate that our method outperforms existing baselines and other comparable methods with a high degree of statistical significance. This significance is evident across various functional classes and species, reinforcing the robustness and generalizability of our approach.\n\nIn summary, the inclusion of confidence intervals and the demonstration of statistical significance are integral to our evaluation process. These elements collectively support the claim that our method provides superior performance and reliability in predicting protein functions.",
  "evaluation/availability": "Not enough information is available."
}