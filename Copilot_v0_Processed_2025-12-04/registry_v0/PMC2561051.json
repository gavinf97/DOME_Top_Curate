{
  "publication/title": "Punting: a novel stacked generalization strategy for protein classification",
  "publication/authors": "The authors who contributed to the article are:\n\nIain Melvin, who carried out the experiments.\n\nJason Weston, who jointly conceived and designed the experiments and wrote the manuscript.\n\nChristina S Leslie, who jointly conceived and designed the experiments and wrote the manuscript.\n\nWilliam S Noble, who jointly conceived and designed the experiments and wrote the manuscript.",
  "publication/journal": "BMC Bioinformatics",
  "publication/year": "2008",
  "publication/doi": "10.1186/1471-2105-9-389",
  "publication/tags": "- Bioinformatics\n- Protein classification\n- Machine learning\n- Support vector machines\n- Neural networks\n- Hybrid methods\n- Remote homology detection\n- Protein structure prediction\n- Sequence analysis\n- Computational biology",
  "dataset/provenance": "The dataset used in our study is sourced from SCOP version 1.69. This dataset was divided into four parts: Atrn, Atst, Btrn, and Btst. The A dataset consists of 74 superfamilies, while the B dataset comprises 1458 superfamilies, making a total of 1532 superfamilies. The A dataset was specifically designed to train and test binary SVM superfamily classifiers, with Atst containing completely held-out families from superfamilies that have at least two member families, each with at least three proteins. The B dataset includes all superfamilies in SCOP not covered by the A dataset and is split into training and testing sets by families at random, maintaining a specific ratio.\n\nThe dataset for superfamily detection is extensive and has been used in previous studies and by the community. It includes a large number of protein sequences and structures, providing a robust foundation for evaluating the performance of various classification methods. The division of the dataset into training and testing sets ensures that the classifiers are thoroughly evaluated and that the results are reliable and generalizable.",
  "dataset/splits": "The dataset used in this study was divided into four distinct parts. These splits were labeled as Atrn, Atst, Btrn, and Btst. The Atrn and Atst splits were specifically designed to train and test binary SVM superfamily classifiers. Atst consisted of completely held-out families from superfamilies that had at least two member families, each containing at least three proteins. Atrn included all other families belonging to these superfamilies.\n\nThe B dataset encompassed all superfamilies in SCOP that were not included in dataset A. This dataset was then randomly split into training and testing sets by families, ensuring that the ratio of families in Btst to Btrn matched the ratio of Atst to Atrn. The total number of superfamilies in the dataset for superfamily detection was 1532, with 74 superfamilies in A and 1458 superfamilies in B.",
  "dataset/redundancy": "The dataset used in this study was derived from SCOP version 1.69 and was divided into four distinct parts: Atrn, Atst, Btrn, and Btst. This division was strategically designed to meet the requirements of training and testing binary SVM superfamily classifiers.\n\nThe Atst set was composed of entirely held-out families from superfamilies that had at least two member families, each containing a minimum of three proteins. The Atrn set included all other families that belonged to these same superfamilies. This ensured that the training and test sets were independent, as the Atst set did not overlap with the Atrn set in terms of the families used.\n\nThe dataset B consisted of all superfamilies in SCOP that were not covered by dataset A. This dataset was then randomly split into training and testing sets by families, ensuring that the ratio of families in Btst to Btrn was equal to the ratio of Atst to Atrn. This approach maintained the independence of the training and test sets while ensuring a balanced distribution.\n\nThe resulting dataset for superfamily detection comprised 74 superfamilies in A and 1458 superfamilies in B, totaling 1532 superfamilies. This distribution is notable because approximately 60% of the SCOP superfamilies in the dataset contain fewer than three members. This characteristic is important because it influences the methodology used for predicting members of these superfamilies, even when an SVM is not trained for small superfamilies.\n\nThe punting methodology allows for predictions to be made for members of these small superfamilies, leveraging the strengths of both high coverage (nearest neighbor) and low coverage (SVM) classifiers. This approach ensures that even if the high coverage classifier incorrectly places a member of a large superfamily into a small superfamily, the low coverage classifier can correct this error due to its high accuracy for large superfamilies.\n\nIn summary, the dataset was carefully split to ensure independence between training and test sets, with a focus on maintaining a balanced distribution. The unique characteristics of the dataset, particularly the high proportion of small superfamilies, necessitated the development of a hybrid methodology to achieve accurate predictions.",
  "dataset/availability": "The data used in this study is derived from SCOP version 1.69. The dataset was divided into four parts: Atrn, Atst, Btrn, and Btst. The A dataset consists of 74 superfamilies, while the B dataset includes 1458 superfamilies, making a total of 1532 superfamilies. The Atrn and Atst splits were designed to train and test binary SVM superfamily classifiers, with Atst containing completely held-out families from superfamilies that have at least two member families with at least three proteins each. The B dataset includes all superfamilies not covered by the A dataset and was split randomly into training and testing sets, maintaining the same ratio of families as in the A dataset.\n\nThe specific details about the public release of the dataset, including the data splits, are not provided. Therefore, it is not clear whether the data is available in a public forum or under what license it might be released. Additionally, there is no information on how the enforcement of data availability was managed.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is a hybrid approach that combines nearest neighbor methods with multiclass support vector machines (SVMs). This hybrid method is designed to improve both the accuracy and coverage of protein classification.\n\nThe algorithm itself is not entirely new, as it builds upon established techniques such as nearest neighbor strategies and SVM classifiers. However, the specific application and combination of these methods in the context of protein classification are novel. The nearest neighbor method provides full coverage but lower accuracy, while the SVM offers higher accuracy but reduced coverage. By integrating these two approaches, we aim to create a full-coverage classifier with improved overall accuracy.\n\nThe reason this work was published in a bioinformatics journal rather than a machine-learning journal is that the primary focus is on the application of these machine-learning techniques to the specific problem of protein classification. The innovation lies in how these algorithms are applied and combined to address the unique challenges of structural and functional categorization of proteins. This interdisciplinary approach is more aligned with the scope of bioinformatics, which deals with the development and application of computational tools to biological data.",
  "optimization/meta": "The model described in this work can be considered a meta-predictor, as it leverages the outputs of other machine-learning algorithms to make its predictions. Specifically, it combines the strengths of Support Vector Machines (SVM) and Neural Networks (NN) through a strategy known as punting.\n\nIn this approach, both SVM and NN are trained on the same problem initially. The model then uses a second set of data to learn how to combine the outputs of these classifiers effectively. This is akin to a two-stage process where the first stage involves training individual classifiers, and the second stage involves learning a combining scheme.\n\nThe combining scheme in this case is not a simple voting mechanism but rather a more sophisticated method that decides which classifier to apply based on the magnitude of their real-valued outputs. This decision-making process is learned from the data, allowing the model to choose the most appropriate classifier for a given input.\n\nRegarding the independence of the training data, the model divides the dataset into two portions: one for training the individual classifiers (SVM and NN) and another for training the punting thresholds. This division ensures that the data used to learn the combining scheme is independent of the data used to train the initial classifiers, maintaining the integrity of the learning process.\n\nIn summary, the meta-predictor integrates SVM and NN, using a learned combining scheme that decides the best classifier to apply based on their outputs. The training data for the combining scheme is kept independent from the data used to train the individual classifiers.",
  "optimization/encoding": "Not enough information is available.",
  "optimization/parameters": "In our model, the number of parameters used is primarily determined by the hyperparameters and the punting thresholds. The algorithm learns these parameters given a single hyperparameter supplied by the user. The punting thresholds, which are class-dependent, are set using held-out data. These thresholds are crucial for combining two classifiers to produce a hybrid classifier with improved accuracy and coverage. The specific number of parameters can vary depending on the number of classes and the complexity of the classifiers being combined. The selection of the hyperparameter and the learning of the punting thresholds are integral parts of the optimization process, ensuring that the model can effectively balance accuracy and coverage.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this study involves a combination of Support Vector Machines (SVMs) and nearest neighbor (NN) methods for superfamily detection. The dataset was divided into four parts: Atrn, Atst, Btrn, and Btst. This division was designed to ensure that the training and testing of binary SVM superfamily classifiers were appropriately handled.\n\nThe dataset for superfamily detection includes 74 superfamilies in set A and 1458 superfamilies in set B, totaling 1532 superfamilies. The division of the dataset ensures that the training and testing sets are balanced, with the ratio of families in Btst/Btrain equal to the ratio in Atst/Atrn. This balancing helps in maintaining a consistent evaluation metric across different parts of the dataset.\n\nTo address the potential issue of overfitting, the study considered punting between SVMs and NN methods. When using SVMs as the primary method, additional negative examples from Btrn were used to calculate punting thresholds. This approach helps in generalizing the model better by providing more diverse negative examples. Conversely, when using the NN method as the primary classifier, all negative superfamilies in Atrn and Btrn were used to determine thresholds. This ensures that the NN method is not biased towards any specific subset of the data.\n\nThe punting methodology allows for the prediction of members of superfamilies that contain fewer than three members, even though an SVM is not trained for these small superfamilies. This is particularly important because approximately 60% of the SCOP superfamilies in the dataset contain fewer than three members. The high coverage of the NN classifier complements the low coverage of the SVM classifier, ensuring that predictions are made for all test examples.\n\nThe study also evaluated the performance of the classifiers at full coverage, where predictions are made for every test example. The results for classification from sequence are shown in Table 1. The use of punting thresholds, learned according to the percentage of false positives in a validation set, helps in controlling the trade-off between coverage and accuracy. This approach ensures that the model does not overfit to the training data while maintaining high accuracy.\n\nIn summary, the fitting method involves a careful division of the dataset, the use of punting between SVMs and NN methods, and the learning of punting thresholds to control the trade-off between coverage and accuracy. These strategies help in addressing both overfitting and underfitting, ensuring that the model generalizes well to new data.",
  "optimization/regularization": "In our study, we employed a strategy known as punting to optimize the generalization error of our classifiers. This approach is an instance of stacked generalization, which involves a two-stage process. In the first stage, each classifier is trained on the same problem. In the second stage, a combining scheme is learned using a separate set of data. This method allows for the selection of the most appropriate classifier based on the magnitude of the real-valued outputs, effectively deciding when to \"punt\" or defer to another classifier.\n\nThis technique helps in preventing overfitting by ensuring that the classifiers are not overly reliant on the training data. By using a separate dataset to learn the combining scheme, we can better generalize the performance of the classifiers to new, unseen data. This flexibility is crucial for controlling the trade-off between accuracy and coverage of predictions, which is one of the main contributions of our work.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters are reported in the publication. The parameters are learned by the algorithm using a single hyperparameter supplied by the user. The punting thresholds, which are class-dependent, are set using held-out data. The oscillatory behavior observed in the results is due to a grid search over two independent punting thresholds. These thresholds can be configured in two main strategies: assigning a low threshold to punt from method A to method B and a high threshold to make no prediction, or vice versa. Both strategies aim to achieve a similar level of coverage and error rate, but the resulting predictions may vary significantly.\n\nThe optimization process involves a two-stage method similar to stacked generalization. In the first stage, classifiers are trained on the same problem. In the second stage, a combining scheme is learned using a separate set of data. This approach allows for flexibility in controlling the accuracy versus coverage of predictions, which is a key contribution of the work.\n\nThe specific model files and optimization schedules are not explicitly detailed in the text, but the methods and strategies used for optimization are thoroughly described. The publication provides a comprehensive overview of the techniques employed, including the use of class-specific thresholds and the grid search method for tuning the punting thresholds.\n\nThe results and methods described are intended to be reproducible, but the exact model files and optimization schedules would need to be implemented by the reader following the guidelines provided. The publication does not specify the availability of the model files or the license under which they might be distributed. However, the detailed descriptions of the methods and parameters should enable researchers to replicate the experiments and optimize their own models accordingly.",
  "model/interpretability": "The model discussed in this publication is not a blackbox. It combines two classifiers to create a hybrid classifier, which improves accuracy and coverage. The process involves setting punting thresholds that are class-dependent and determined using held-out data. These thresholds are crucial for deciding when to switch from one classifier to another or to make no prediction at all. This approach allows for a clear understanding of how predictions are made, as the decision-making process is governed by these thresholds.\n\nFor instance, in one strategy, if the score from the primary classifier exceeds a certain threshold, a prediction is made. If not, the secondary classifier is consulted. In another strategy, two sets of thresholds are used: one for the primary classifier and another for the secondary classifier. This allows the model to sometimes abstain from making a prediction, adding another layer of transparency.\n\nAdditionally, the model's behavior can be analyzed through figures that show the error rates and coverage as functions of these thresholds. These visualizations help in interpreting how the model balances between the two classifiers and when it decides to make no prediction. The use of class-specific thresholds further enhances the interpretability, as it tailors the decision-making process to the specific characteristics of each class.\n\nOverall, the hybrid classifier's design and the use of punting thresholds make it a transparent model, where the decision-making process can be clearly understood and analyzed.",
  "model/output": "The model discussed is a classification model, specifically designed for protein classification tasks. It involves hybrid methods that combine two classifiers to improve accuracy and coverage. The model uses punting strategies with thresholds to decide when to defer predictions from a primary classifier to a secondary classifier or to make no prediction at all. These thresholds are class-dependent and are set using held-out data. The model's performance is evaluated using error rates and coverage, with the hybrid approach showing lower error rates for all coverage values compared to individual classifiers. The oscillatory behavior observed in the results is due to the grid search over two independent punting thresholds. The model's effectiveness varies between sequence classification and structure classification tasks, with the hybrid classifier providing more significant improvements in sequence classification.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods involved several key steps and metrics to assess their performance comprehensively. For the classification tasks, we utilized McNemar's test to evaluate the statistical significance of the observed differences in performance between the methods. This test computes a p-value for the null hypothesis that the same proportion of proteins are correctly classified by both methods. The results showed that each of the hybrid classifiers performed significantly better than each of the single classifiers, with all relevant p-values being less than 0.01.\n\nWe also measured the error rates for different classification tasks. For protein sequences, the overall error rate dropped by 10.8% when combining two methods, from 51.8% for PSI-BLAST to 40.8% for the hybrid method PSI-BLAST → SVM. Similarly, for protein structures, a drop in error rate of 4.5% (from 30.8% to 26.3%) was achieved when moving from MAMMOTH to the hybrid method MAMMOTH → SVM.\n\nTo understand the behavior of the classifiers better, we plotted the percentage of predictions made by the SVM as a function of the total number of predictions. This analysis helped illustrate how the hybrid classifier can either assign a low threshold to punt from one method to another and a high threshold to make no prediction or vice versa. These strategies achieved similar levels of coverage and error rates but resulted in different sets of predictions.\n\nAdditionally, we evaluated the improvement over small classes by measuring the balanced error rate, which computes the error rate separately for each class and then averages the resulting values. This metric showed that punting in either direction generally achieved higher balanced accuracy with the hybrid method for both classification tasks.\n\nThe evaluation also included an analysis of the value of using class-specific thresholds versus a single threshold for all classes. For the classification of protein structures, using class-specific thresholds consistently improved overall performance. However, for sequence classification, there was no benefit from using multiple thresholds, suggesting that the E-values returned by MAMMOTH are not as well calibrated as those computed by PSI-BLAST.\n\nOverall, the evaluation demonstrated that hybrid methods outperformed single classifiers across various metrics and coverage rates, highlighting their effectiveness in protein classification tasks.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to thoroughly assess the effectiveness of our classification methods. For both protein sequence and structure classification tasks, we reported unbalanced and balanced error rates. The unbalanced error rate measures the proportion of incorrectly predicted proteins in the test set, implicitly giving more weight to larger classes. This metric is crucial for understanding the performance across the entire dataset, especially when dealing with imbalanced class sizes.\n\nTo complement the unbalanced error rate, we also calculated the balanced error rate. This metric computes the error rate separately for each class and then averages these values. The balanced error rate provides a more nuanced view of performance, particularly for smaller classes, ensuring that the evaluation is not dominated by the larger classes.\n\nAdditionally, we used McNemar's test to evaluate the statistical significance of the observed differences in performance between our methods. This test computes a p-value for the null hypothesis that the same proportion of proteins are correctly classified by both methods being compared. The results showed that our hybrid classifiers significantly outperformed the single classifiers, with all relevant p-values being less than 0.01.\n\nThese metrics are representative of standard practices in the literature, providing a comprehensive view of classifier performance across different coverage rates and class sizes. By reporting both unbalanced and balanced error rates, we ensure that our evaluation is robust and informative, capturing the strengths and weaknesses of our methods in various scenarios.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods against publicly available tools using benchmark datasets. Specifically, we compared our approach to SVM, PSI-BLAST, and MAMMOTH, which are well-established methods in the field of protein classification.\n\nFor the classification of protein sequences, we evaluated the performance of SVM, PSI-BLAST, and their hybrid combinations. The results, presented in a table, show the number of proteins incorrectly assigned to superfamilies by each method. This comparison highlights the strengths and weaknesses of each approach, providing a clear benchmark for our hybrid methods.\n\nAdditionally, we performed a comparison to simpler baselines. For instance, we assessed the error rates of SVM and PSI-BLAST individually and in combination. The SVM yielded a 100% error rate on sequences outside its coverage, while PSI-BLAST had a lower but still significant error rate. When combining the two methods, the overall error rate dropped significantly, demonstrating the effectiveness of our hybrid approach.\n\nWe also evaluated the statistical significance of the observed differences in performance using McNemar's test. This test showed that each of the hybrid classifiers performed significantly better than each of the single classifiers, with all relevant p-values being less than 0.01. This statistical validation underscores the robustness of our methods.\n\nFurthermore, we explored the concept of \"punting,\" where the classifier can opt to say \"I don't know\" rather than provide an incorrect classification. This strategy involves setting thresholds to determine when to punt from one method to another or to abstain from making a prediction altogether. The results indicate that punting in either direction—from SVM to the nearest neighbor classifier or vice versa—yields higher accuracy than either single method at all coverage rates.\n\nIn summary, our evaluation included comparisons to publicly available methods and simpler baselines, providing a comprehensive assessment of our approach's performance. The use of benchmark datasets and statistical tests ensures that our findings are reliable and significant.",
  "evaluation/confidence": "To evaluate the statistical significance of the observed differences in performance, McNemar's test was used to compute a p-value for the null hypothesis that the same proportion of proteins are correctly classified by both methods. These tests, applied to the entire test set, show that each of the hybrid classifiers performs significantly better than each of the single classifiers. All four relevant p-values are less than 0.01, indicating strong statistical significance. This means that the improvements observed when combining methods are not due to chance.\n\nFor the classification of protein structures, a drop in error rate of 4.5% (from 30.8% to 26.3%) was achieved when moving from MAMMOTH to MAMMOTH → SVM. Again, McNemar's test showed that both hybrid methods outperform both of the single classifiers at p < 0.01.\n\nThe use of McNemar's test ensures that the performance metrics have a statistical basis, providing confidence in the results. The consistent p-values below 0.01 across different tasks and methods indicate that the hybrid classifiers are indeed superior to the individual methods. This statistical rigor supports the claim that the combined approaches offer significant improvements in classification accuracy.",
  "evaluation/availability": "Not enough information is available."
}