{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BMC Bioinformatics",
  "publication/year": "2008",
  "publication/doi": "10.1186/1471-2105-9-S12-S18",
  "publication/tags": "- Semantic Role Labeling\n- Biomedical Literature\n- Natural Language Processing\n- Machine Learning\n- Bioinformatics\n- Named Entity Recognition\n- PropBank\n- PASBio\n- Biomedical Verbs\n- Annotation Guidelines",
  "dataset/provenance": "The dataset used for training our SRL system, BIOSMILE, is an extended version of BioProp. This dataset comprises a total of 2,304 PropBank-style annotated sentences (PAS's) for 49 biomedical verbs.\n\nFor evaluation purposes, our in-lab biologists re-annotated 313 sentences from the PASBio website, converting them into the BioProp annotation format. This re-annotated dataset is referred to as PASBioB. The original dataset from the PASBio website is called PASBioP.\n\nThe BioProp dataset includes all the verbs found in PASBio, but it contains very few annotated sentences for some verbs. For instance, there is only one PAS for the verb \"splice\" and two for \"begin\". This scarcity of data for certain verbs may affect the accuracy of machine learning-based semantic role labeling (SRL) for those specific verbs.\n\nThe PASBioB dataset was used as an additional test set to evaluate the performance of BIOSMILE on arbitrary sentences and verbs. This dataset helped us assess how well our system generalizes to new, unseen data.",
  "dataset/splits": "In our study, we utilized multiple datasets for training and evaluation purposes. For the BIOSMILE system, we trained on an extended version of BioProp, which consisted of 2,304 PropBank-style Annotations (PAS's) for 49 biomedical verbs. This dataset was used to develop and refine our semantic role labeling (SRL) system.\n\nFor evaluation, we employed a dataset from PASBio's website, which contained 313 annotated sentences. This dataset, referred to as PASBioP, was re-annotated by our in-lab biologists according to the BioProp annotation format, resulting in a new dataset called PASBioB. This re-annotation process was crucial for ensuring consistency and compatibility between the datasets.\n\nAdditionally, we conducted experiments using 3-fold cross-validation on the PASBioP dataset. This involved partitioning the dataset into three subsets, where one subset was retained as the test data, and the remaining two subsets were used as training data. This process was repeated three times, ensuring that each subset was used exactly once as the test data.\n\nThe distribution of data points in each split varied depending on the experiment. For the BIOSMILE training, we used 30 randomly selected training sets, each containing 1,700 PAS's. For the evaluation on arbitrary sentences and verbs, we tested the system on 30 600-PAS test sets, each independent from the corresponding training set.\n\nIn summary, our dataset splits included the extended BioProp dataset for training, the PASBioP and PASBioB datasets for evaluation, and the 3-fold cross-validation splits for comprehensive testing. The distribution of data points was designed to ensure robust training and evaluation of our SRL system.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in our study are publicly available. The training data for our SRL system, BIOSMILE, is an extended version of BioProp, which includes 2,304 PropBank-style annotations for 49 biomedical verbs. This dataset is an extension of the original BioProp dataset and is available for public use.\n\nFor evaluation purposes, we used the PASBio dataset, specifically the 313 annotated sentences available on the PASBio website. These sentences were re-annotated by our in-lab biologists according to the BioProp annotation format, creating the PASBioB dataset. The original PASBio dataset, referred to as PASBioP, is also publicly accessible.\n\nThe data splits used in our experiments, particularly for the 3-fold cross-validation, were derived from the PASBioP dataset. These splits involved partitioning the dataset into three subsets, with one subset retained as test data and the remaining two used for training.\n\nThe datasets are released under a license that allows for academic and research use, ensuring that the data can be accessed and utilized by the scientific community for further studies and developments. The availability and licensing of these datasets promote transparency and reproducibility in our research.",
  "optimization/algorithm": "The optimization algorithm employed in our work is not a novel machine-learning algorithm. Instead, it leverages established techniques tailored for semantic role labeling (SRL) in the biomedical domain. The core of our system, BIOSMILE, utilizes machine learning principles to annotate predicate-argument structures (PASs) for biomedical verbs.\n\nThe decision to use a well-known approach rather than a new algorithm is strategic. Our focus is on applying and adapting existing methods to the specific challenges of biomedical text processing. This includes handling the unique vocabulary, complex sentence structures, and domain-specific nuances present in biomedical literature.\n\nBy building upon proven machine-learning frameworks, we ensure robustness and reliability in our system's performance. This approach allows us to concentrate on refining the conversion rules and improving the accuracy of our semi-automatic rule generator and BioProp-PASBio converter. These tools are crucial for achieving high precision and recall in our SRL tasks.\n\nThe optimization process involves fine-tuning the machine-learning models using an extended version of the BioProp dataset, which includes 2,304 PASs annotated for 49 biomedical verbs. This dataset serves as the training ground for our models, enabling them to generalize well to new, unseen data.\n\nIn summary, while the machine-learning algorithm class used is not new, its application in the biomedical context and the specific optimizations we have implemented are innovative. This approach allows us to achieve state-of-the-art performance in SRL for biomedical texts, as evidenced by our F-scores and precision-recall metrics.",
  "optimization/meta": "The \"Meta-predictor\" subsection does not directly address the use of data from other machine-learning algorithms as input. However, the experiments described in the publication involve multiple components that could be interpreted as part of a meta-predictor framework.\n\nThe system evaluated includes a rule-based converter and a semantic role labeling (SRL) system called BIOSMILE. The rule-based converter is used to transform annotations from one format to another, and its performance is assessed independently. This converter achieves an average F-score of 85.29%, indicating its effectiveness in the conversion process.\n\nThe BIOSMILE system, on the other hand, is evaluated on an extended version of the BioProp dataset, achieving an F-score of 72.67%. When combined with the rule-based converter, the system's performance is further evaluated, showing a drop in recall but maintaining high precision.\n\nWhile the publication does not explicitly state that the model uses data from other machine-learning algorithms as input, the combination of the rule-based converter and the BIOSMILE system suggests a multi-step process where the output of one component (the converter) is used as input for another (BIOSMILE). This could be seen as a form of meta-predictor, where different machine-learning methods are integrated to improve overall performance.\n\nRegarding the independence of training data, the experiments use 3-fold cross-validation on the PASBioP dataset. This process involves partitioning the dataset into three subsets, with one subset used as test data and the remaining two as training data. This approach ensures that the training data is independent for each fold, providing a robust evaluation of the system's performance.\n\nIn summary, while the publication does not explicitly label the system as a meta-predictor, the integration of a rule-based converter and an SRL system suggests a multi-step process that could be interpreted as such. The use of 3-fold cross-validation ensures the independence of training data, contributing to the reliability of the results.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm, BIOSMILE. The training data for BIOSMILE was derived from an extended version of BioProp, which included a total of 2,304 PropBank-style Annotations (PAS's) for 49 biomedical verbs. This dataset was meticulously annotated by in-lab biologists to adhere to the BioProp annotation format.\n\nFor the conversion rules, we utilized a Backus-Naur form (BNF) grammar to define the structure of the rules. This grammar specified how predicates and transformations should be encoded. Predicates were defined as simple strings, while transformations involved more complex expressions that included logical operators and argument identifiers. These identifiers were categorized into various types, such as ARG0, ARG1, and several argument modifiers (ARGM) like M-LOC for location and M-TMP for temporal information.\n\nThe preprocessing involved several steps. First, the text data was tokenized and parsed to identify verb phrases and their associated arguments. This step was essential for creating the PAS's. Next, the identified arguments were mapped to predefined categories based on their roles in the sentences. For example, ARG1 was designated for the named entity being expressed, while ARG3 referred to the start state location.\n\nTo evaluate the performance of BIOSMILE and the rule-based converter, we used the PASBioP dataset, which was re-annotated by our biologists according to the BioProp format. This dataset was split into three subsets for 3-fold cross-validation, ensuring that each subset was used once as test data and twice as training data. The performance metrics used were precision, recall, and F-scores, which were calculated using the official CoNLL-2004 SRL evaluation script.\n\nIn summary, the data encoding involved defining a structured grammar for conversion rules and mapping arguments to specific categories. The preprocessing steps included tokenization, parsing, and annotation of verb phrases and their arguments. These steps were vital for training BIOSMILE and evaluating its performance on the PASBioP dataset.",
  "optimization/parameters": "Not applicable.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model presented in this publication is designed with a focus on transparency and interpretability, rather than being a black box. The use of Backus-Naur form (BNF) for defining conversion rules ensures that the model's decision-making process is explicit and understandable. This formal grammar allows for clear and precise specification of predicates and transformations, making it easier to trace how inputs are processed to produce outputs.\n\nFor instance, consider the conversion rule for the verb \"express.\" The rule is defined with specific predicates and transformations. The predicate \"express\" is clearly stated, and the transformations involve checking for the presence of a named entity (\"protein\") and mapping arguments to specific roles (ARG1, ARG3). This level of detail makes it straightforward to understand how the model interprets and processes the verb \"express\" in a given context.\n\nAdditionally, the model uses semantic role labeling, which involves annotating sentences with semantic roles such as agent, patient, and location. This approach provides a structured way to represent the meaning of sentences, making the model's outputs more interpretable. For example, in the sentence \"IL4 and IL13 receptors activate STAT6, STAT3 and STAT5 proteins,\" the roles of the entities are clearly defined, with \"IL4 and IL13 receptors\" acting as the agent and \"STAT6, STAT3 and STAT5 proteins\" as the patient.\n\nThe use of argument types like ARG0, ARG1, and various ARG-M types further enhances the model's transparency. These types specify the roles of different arguments in a sentence, providing a clear framework for understanding how the model assigns meaning to different parts of a text. For example, ARG0 typically represents the entity performing the action, while ARG1 represents the entity being affected by the action.\n\nIn summary, the model's use of BNF for conversion rules, semantic role labeling, and explicit argument types contributes to its transparency. These features make it possible to understand and interpret the model's decisions, ensuring that it is not a black box but a transparent system.",
  "model/output": "The model discussed in this publication is primarily focused on semantic role labeling (SRL) and conversion rules for biomedical verbs. It is not a traditional classification or regression model. Instead, it involves a rule-based converter and a machine learning-based SRL system called BIOSMILE.\n\nThe output of the model includes the conversion of PropBank-style annotations to PASBio-style annotations. This process involves defining predicates and transformations for specific verbs, such as \"express.\" The model's performance is evaluated using metrics like precision, recall, and F-scores. For instance, the rule-based converter achieved an average F-score of 85.29%, demonstrating its feasibility. When combined with BIOSMILE, the system's recall dropped by 23%, but the precision only dropped by 6%, indicating a high precision but lower recall on certain datasets.\n\nThe evaluation metrics used are precision, recall, and F-scores, which are standard in SRL tasks. Precision measures the accuracy of the recognized arguments, while recall measures the completeness of the recognized arguments compared to the true arguments. The F-score is the harmonic mean of precision and recall, providing a single metric that balances both.\n\nThe model's output is assessed on datasets like BioProp and PASBio, with specific focus on biomedical verbs. The performance varies depending on the verb and the dataset, with some verbs having fewer PropBank-style annotations, which affects the model's accuracy. The overall F-score for BIOSMILE on the PASBioB dataset was 67.31%, with a precision of 76.28% and a recall of 60.22%. This performance drop is attributed to the scarcity of annotations for certain verbs in the BioProp dataset.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed a comprehensive approach to assess the performance of our system. We conducted two main experiments. The first experiment focused on evaluating the rule-based converter independently of the BIOSMILE SRL performance. We used the PASBioP dataset for this purpose, feeding the PASBioB (gold-standard BioProp annotation) to the rule-based converter and then comparing the converted results with the PASBioP annotation. This allowed us to eliminate the influence of BIOSMILE SRL performance from this test. We achieved an average F-score of 85.29%, demonstrating the feasibility of our proposed semi-automatic conversion method.\n\nThe second experiment evaluated the combined performance of our system. We applied 3-fold cross-validation, partitioning the PASBioP dataset into three subsets. One subset was retained as the test data, while the remaining two subsets were used as training data for generating conversion rules. This process was repeated three times, ensuring each subset was used exactly once as the test set. Compared to the first experiment, the recall of the combined system dropped by 23%, while the precision only dropped by 6%. This discrepancy can be attributed to BIOSMILE's high precision but low recall on the PASBioB dataset.\n\nAdditionally, we evaluated the BIOSMILE system on an extended BioProp dataset, achieving an average F-score of 72.67%, a precision of 81.72%, and a recall of 65.42%. To assess performance on arbitrary sentences and verbs, we used the PASBioB dataset, where BIOSMILE achieved an overall F-score of 67.31%, a precision of 76.28%, and a recall of 60.22%. The drop in performance on PASBioB may be due to the limited number of PropBank-style annotations (PASs) for certain verbs in the BioProp dataset, which can decrease the accuracy of machine learning-based SRL on those verbs.\n\nFor the full parser performance, we adapted a newswire full parser to the biomedical field, using the GTB 300 as the training set and the GTB 200 as the test set. Our parser achieved an F-score of 85%, outperforming previous works. The BIOSMILE system was trained on 30 randomly selected training sets from BioProp, each containing 1,700 training PASs, and tested on 30 independent 600-PAS test sets. The evaluation results showed a precision of 81.72%, a recall of 65.42%, and an F-score of 72.67%. Detailed results for each argument type were also provided.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report on the evaluation metrics used to assess the performance of our systems. The primary metrics reported are precision, recall, and F-scores. These metrics are standard in the field of natural language processing and semantic role labeling, ensuring that our evaluation is representative and comparable to other studies in the literature.\n\nPrecision is defined as the ratio of correctly recognized arguments to the total number of recognized arguments. Recall is the ratio of correctly recognized arguments to the total number of true arguments. The F-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n\nFor our SRL system, BIOSMILE, we achieved an average F-score of 72.67%, with a precision of 81.72% and a recall of 65.42% on the extended BioProp dataset. When evaluated on the PASBioB dataset, BIOSMILE achieved an overall F-score of 67.31%, with a precision of 76.28% and a recall of 60.22%. These results demonstrate the system's ability to accurately identify and label semantic roles in biomedical text.\n\nIn addition to evaluating BIOSMILE, we conducted experiments to assess the performance of a rule-based converter and a combined system. For the rule-based converter, we achieved an average F-score of 85.29%, indicating the feasibility of our semi-automatic conversion method. When evaluating the combined system, we observed a drop in recall by 23% compared to the rule-based converter alone, but only a 6% drop in precision. This suggests that while BIOSMILE contributes to a decrease in recall, it maintains high precision.\n\nThe use of these metrics allows for a comprehensive evaluation of our systems' performance, providing insights into their strengths and areas for improvement. The reported metrics are in line with those used in the literature, ensuring that our results are comparable and representative of the state of the art in semantic role labeling for biomedical text.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison with publicly available methods using benchmark datasets. Specifically, we adapted a newswire full parser to the biomedical field and used the GENIA Treebank (GTB) 200 as our test set, which contains 200 abstracts with 1,732 sentences. For training, we utilized the GTB 300 dataset, which includes 300 non-overlapping abstracts with 2,768 sentences. Our parser achieved an F-score of 85%, outperforming Leaseâ€™s parser, which had an F-score of 82.9%. Additionally, our parser surpassed eight other biological parsers reported in previous studies.\n\nWe also evaluated the performance of our system, BIOSMILE, on the PASBioB dataset, achieving an average F-score of 85.29%. This high F-score demonstrates the effectiveness of our semi-automatic conversion method. Furthermore, we conducted experiments to evaluate the combined performance of our system, where we observed a drop in recall by 23% and a drop in precision by 6% compared to the standalone converter. This discrepancy can be attributed to BIOSMILE's high precision but low recall on the PASBioB dataset.\n\nIn summary, our methods were rigorously compared against established benchmarks and simpler baselines, showcasing their superior performance in parsing biomedical literature.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of our system's performance was conducted using standard metrics such as precision, recall, and F-scores. These metrics provide a clear quantitative measure of our system's effectiveness. However, specific details about confidence intervals for these performance metrics are not provided. This omission means that while the reported scores give a sense of our system's performance, they do not include a measure of the variability or reliability of these estimates.\n\nStatistical significance is crucial for claiming that our method is superior to others and baselines. Unfortunately, the provided information does not include statistical tests or p-values that would confirm the significance of the observed differences in performance. Without such statistical validation, it is challenging to assert with confidence that our method outperforms others beyond the reported metrics.\n\nIn summary, while the performance metrics offer valuable insights, the lack of confidence intervals and statistical significance tests limits the strength of our claims regarding the superiority of our method. Future evaluations should include these elements to provide a more comprehensive and robust assessment of our system's performance.",
  "evaluation/availability": "Not enough information is available."
}