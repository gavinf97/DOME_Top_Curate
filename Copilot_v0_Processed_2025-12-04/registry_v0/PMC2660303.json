{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BMC Bioinformatics",
  "publication/year": "2021",
  "publication/doi": "10.1186/1471-2105-10-25-S2",
  "publication/tags": "- DNA repair\n- Protein classification\n- Sequence similarity\n- Machine learning\n- Bioinformatics\n- Kernel methods\n- Gene Ontology\n- Protein prediction\n- DNA synthesis\n- Classification experiments\n- Single Strand Break repair\n- Double Strand Break repair\n- Error-prone DNA repair\n- SVM algorithms\n- Genome scanning",
  "dataset/provenance": "For our identification experiments, we utilized two primary data sources: the Protein Data Bank (PDB) and UniProt. The PDB was chosen for its reliability, as it contains proteins that have been experimentally observed and confirmed, often with X-ray crystallography data. This ensures a high level of confidence in the structural information provided. The PDB dataset included approximately 49,000 protein structures as of February 2008, from which 245 matched the \"DNA repair\" Gene Ontology (GO) label, resulting in 557 sequences after extracting multiple chains. Additionally, 447 protein structures with the \"nucleus\" GO label were extracted, resulting in 1443 sequences after removing those that also matched the repair GO label.\n\nUniProt, on the other hand, was selected for its vast sequence repository, offering a significantly larger number of sequences for analysis. From the UniProt Knowledge Base (KB) database, 17,828 DNA repair and 19,348 nuclear non-repair GO-based sequences were retrieved. The PDB sequences are generally shorter in length because they are divided into separate sequences for each chain, and some PDB structures contain multiple chains.\n\nFor our classification experiment, we focused on the UniProt database due to its comprehensive coverage of DNA repair pathways. We used the 20 DNA repair pathway categories listed in Gene Ontology and extracted proteins from UniProt with the matching ontology IDs. The PDB database was not used for this experiment because it currently contains data for only four major repair pathways: base-excision repair, mismatch repair, single strand break repair, and double strand break repair. For classification experiments, a minimum of 25 proteins in the pathway dataset was required, and some pathways were not used due to insufficient data.\n\nThe datasets used in this work are available as an additional file, providing transparency and allowing for reproducibility of our results. The reliability of our datasets was ensured by requiring that proteins be catalogued using the Gene Ontology annotation system and by removing proteins with uncertain descriptions, such as those containing keywords like \"putative,\" \"similar,\" \"possible,\" \"probable,\" \"theoretical,\" and \"hypothetical.\"",
  "dataset/splits": "In our experiments, we employed a cross-validation technique known as one-versus-rest (1vR) cross-validation. This method is particularly useful in data-based inference experiments to measure performance. The data division scheme assumes an f-fold division, where f represents the number of splits. However, the specific number of folds (f) used in our experiments is not explicitly stated.\n\nFor each protein in the dataset, we utilized SSPro to determine the secondary structure role of each amino acid, classifying them into alpha helix, beta sheet, or other structural functions. This process was applied to every protein in each dataset.\n\nThe distribution of data points across the splits is not detailed, but it is implied that the data is divided in such a way that each fold contains a representative sample of the overall dataset. This ensures that the performance metrics, such as true positive rate and false positive rate, are reliably estimated.\n\nIn summary, while the exact number of data splits and the distribution of data points in each split are not specified, the use of 1vR cross-validation indicates a systematic approach to evaluating the performance of our classification methods.",
  "dataset/redundancy": "The datasets used in our study were filtered at three different sequence similarity levels: 50%, 90%, and unfiltered (0%). This filtering process was applied to both the GO-PDB and GO-UniProt datasets, which were used for identification and classification experiments, respectively.\n\nTo ensure the independence of training and test sets, we employed a 5-fold cross-validation strategy. This method involves splitting the dataset into five equally sized folds. The model is then trained on four of these folds and tested on the remaining fold. This process is repeated five times, with each fold serving as the test set once. This approach helps to ensure that the training and test sets are independent and that the model's performance is evaluated on different subsets of the data.\n\nThe distribution of our datasets is comparable to previously published machine learning datasets in the field of bioinformatics. The datasets were carefully selected to include a diverse range of DNA repair proteins, ensuring that the models trained on these datasets would be robust and generalizable. The filtering process at different sequence similarity levels allowed us to assess the performance of our methods under varying conditions of data redundancy.\n\nIn summary, the datasets were split using a 5-fold cross-validation strategy to ensure the independence of training and test sets. The distribution of the datasets is comparable to previously published machine learning datasets in the field, with a focus on including a diverse range of DNA repair proteins.",
  "dataset/availability": "The datasets used in our experiments are publicly available to ensure transparency and reproducibility. The identification and classification datasets, including protein identifiers from PDB and UniProt, are provided in Additional File 7. This file contains the datasets for each sequence similarity used in the report.\n\nFor the classification experiments, we required a minimum of 25 proteins in the pathway dataset. Due to insufficient data, some pathways were not used, and these are detailed in Additional File 2, which includes results for all GO-based repair pathway classification experiments, even those not shown in the main results section.\n\nThe data division technique employed was cross-validation, specifically one-versus-rest (1vR) cross-validation. This method ensures that the performance metrics are robust and not dependent on a particular data split.\n\nAdditionally, we provide detailed results for identification experiments at various dataset filters (50%, 90%, and unfiltered) in Additional File 1, which includes ROC curves similar to those presented in the main figures.\n\nAll supplementary files are available through the Biomed Central website, ensuring that the data is accessible to the scientific community. The files are provided under the terms that allow for their use in research and educational purposes, adhering to standard academic sharing practices.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes Support Vector Machines (SVMs) for processing protein data. SVMs are a well-established class of machine-learning algorithms known for their effectiveness in decision and regression problems. They work by deriving decision functions through solving quadratic programming problems, which allows them to classify new data points accurately.\n\nThe specific SVM implementation used in our research is SVMlight. This software is chosen for its robustness and efficiency in handling large datasets. One key parameter that we manually set in SVMlight is the gamma value of the radial basis function (RBF) kernel. This parameter is crucial as it influences the SVM's ability to fit the model to the data.\n\nThe use of SVMs in our work is not novel in the context of bioinformatics and protein analysis. However, our approach involves a unique methodology called one-versus-one-versus-rest (11R) cross-validation. This method is designed to combine homology and sequence data in an unbiased manner, providing a realistic estimate of method performance. The 11R technique differs from traditional one-versus-rest (1vR) methods by using a single portion of data for training and the remaining portions as a reference homology database. This approach helps in reducing the amount of training data and enhances the integration of homology information.\n\nThe reason for developing and using this specific cross-validation technique is rooted in the need to address the challenges posed by homology in protein data. By incorporating homology data, we aim to improve the accuracy and reliability of our protein identification and classification experiments.\n\nThe decision to publish this work in a bioinformatics journal rather than a machine-learning journal is driven by the specific application of the SVM and cross-validation techniques to biological data. Our focus is on the biological significance and the practical applications of these methods in the field of bioinformatics, particularly in the identification and classification of DNA repair proteins. This aligns with the scope and readership of bioinformatics journals, which are more likely to appreciate the biological implications of our findings.",
  "optimization/meta": "The optimization process for our model incorporates a meta-predictor approach, which leverages data from multiple machine-learning algorithms to enhance prediction accuracy. Specifically, the meta-predictor combines outputs from various methods, including Support Vector Machines (SVMs) and homology-based techniques.\n\nSeveral machine-learning methods constitute the whole meta-predictor framework. These include primary sequence data (Method P), secondary structure information (Method PS), amino acid frequency prior knowledge (Method PF), and homology information (Method PH). Additionally, a combination of primary structure, secondary structure, and homology information (Method PSH) is used. The SVM implementation employed is SVM light, with the gamma value of the radial basis function (RBF) similarity manually set.\n\nThe training data for the meta-predictor is designed to be independent through the use of one-versus-one-versus-rest (11R) cross-validation. This methodology ensures that for f-fold validation, one portion of the data is set aside for evaluation, while only a single portion is used for training. The remaining f - 2 portions serve as a reference homology database for querying both training and test data. This approach aims to combine homology and sequence data in an unbiased manner, providing a realistic estimate of method performance. The reduction in the amount of training data by 60% does not significantly impact the performance, as evidenced by the high true positive rates achieved.",
  "optimization/encoding": "In our study, we employed several data transformation techniques to encode and pre-process the data for input into Support Vector Machines (SVMs). These techniques incorporated combinations of primary sequence, predicted secondary structure, and homology search information to create feature vectors.\n\nWe utilized five main data transformation methods. These methods included the use of spectrum kernels, secondary structure spectrum kernels, and homology information. By applying these transformations, we aimed to maximize the amount of data (features) available to the SVM, which is particularly useful in situations with reduced datasets.\n\nThe transformation-based SVM experimental results were compared with independent BLAST trials. For the BLAST trials, we used only DNA repair protein training data as a reference database, excluding non-repair training data. This approach allowed us to assess the similarity of query proteins to known DNA repair proteins using e-values.\n\nAdditionally, we proposed a one-versus-one-versus-rest (11R) cross-validation technique. This method involves setting aside one portion of the data for performance evaluation (test data) while using a single portion for training and the remaining portions as a reference homology database. The goal of this technique is to combine homology and sequence data in an unbiased way, providing a realistic estimate of method performance.\n\nThe SVM implementation used was SVM light, with the gamma value of the radial basis function (RBF) kernel being the only manually set parameter. This setup allowed us to derive decision functions by solving a quadratic programming problem, enabling us to classify new data effectively.",
  "optimization/parameters": "In our model, we primarily focus on optimizing a single key parameter, which is the gamma value (Î³) of the radial basis function (RBF) kernel used in our Support Vector Machine (SVM) implementation. This parameter is crucial as it controls the learning balance between over-fitting and over-generalizing.\n\nThe selection of the gamma value was not arbitrary but rather a result of careful consideration and experimentation. We manually set this parameter to find an optimal balance that would yield the best prediction performance. The effect of this parameter on prediction performance was thoroughly analyzed and is discussed in detail in the Results and Discussion sections of our publication. This approach ensures that our model is finely tuned to handle the complexity of the data while maintaining robustness and generalizability.",
  "optimization/features": "In our study, we employed several methods to create feature vectors from protein sequences. The first and simplest method involved counting the occurrences of each amino acid in a protein and normalizing these counts by the protein's length. This approach yields a feature vector with a length equal to the number of different amino acids, which is 20.\n\nFor more complex feature extraction, we utilized various kernels such as the amino acid spectrum kernel, the secondary structure spectrum kernel, and incorporated homology information. These transformations allowed us to maximize the amount of data available to the Support Vector Machines (SVMs), which is particularly useful in scenarios with reduced datasets.\n\nFeature selection was not explicitly performed as a separate step. Instead, the feature vectors were directly fed into the SVMs for decision-making. The SVMs themselves inherently perform a form of feature selection by identifying the most relevant features during the training process. This is achieved through the optimization of the decision function, which involves solving a quadratic programming problem.\n\nThe parameter settings, such as the gamma value of the radial basis function (RBF) kernel, were manually adjusted to balance between over-fitting and over-generalizing. This tuning was done to optimize the prediction performance of the SVMs, as elaborated in the results and discussion sections.\n\nIn summary, the number of features used as input varies depending on the method of feature vector creation. The simplest method results in 20 features, while more complex methods involve additional transformations that increase the dimensionality of the feature vectors. Feature selection was not performed as a standalone process but was integrated into the SVM training procedure.",
  "optimization/fitting": "In our study, we employed Support Vector Machines (SVMs) for processing protein data, utilizing the SVM light implementation. One critical aspect of using SVMs is managing the balance between over-fitting and under-fitting, which is influenced by the gamma parameter of the Radial Basis Function (RBF) kernel.\n\nThe RBF kernel measures the similarity between two feature vectors, and the gamma parameter controls this learning balance. A low gamma value can lead to over-fitting, where the model becomes too complex and captures noise in the training data. Conversely, a high gamma value can result in under-fitting, where the model is too simplistic and fails to capture the underlying patterns in the data.\n\nTo address over-fitting, we carefully selected the gamma value and evaluated its impact on prediction performance. The effect of this parameter was thoroughly examined in the Results section and elaborated upon in the Discussion section. By tuning the gamma parameter, we ensured that the model generalized well to unseen data, thus mitigating the risk of over-fitting.\n\nAdditionally, we implemented a one-versus-one-versus-rest (11R) cross-validation technique. This method sets aside a portion of the data for performance evaluation while using a single portion for training and the remaining portions as a reference homology database. This approach helps in combining homology and sequence data in an unbiased manner, providing a realistic estimate of the method's performance and further reducing the likelihood of over-fitting.\n\nTo rule out under-fitting, we ensured that the feature vectors were appropriately transformed and that the SVM was fed with meaningful data. The first method for creating feature vectors involved counting the occurrences of each amino acid in a protein sequence and normalizing these counts by the protein's length. This simple yet effective approach provided a robust foundation for the SVM to make accurate decisions.\n\nIn summary, by carefully tuning the gamma parameter, employing 11R cross-validation, and using well-constructed feature vectors, we effectively managed the balance between over-fitting and under-fitting, ensuring that our model performed reliably and generalized well to new data.",
  "optimization/regularization": "In our study, we employed a regularization technique to prevent over-fitting. Specifically, we used the radial basis function (RBF) kernel in our Support Vector Machine (SVM) implementation, which includes a parameter that controls the learning balance. This parameter helps to manage the trade-off between over-fitting (low value) and over-generalizing (high value). By carefully tuning this parameter, we aimed to achieve a realistic estimate of method performance, ensuring that our model generalizes well to unseen data. The effect of this parameter on prediction performance is discussed in detail in the Results and Discussion sections.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our experiments are reported in the supplementary materials. Specifically, the detailed results of our classification experiments, including the hyper-parameters used for each method, are available in Additional File 2. This file contains results for all GO-based repair pathway classification experiments, providing a comprehensive overview of the configurations employed.\n\nThe optimization schedule and model files are not explicitly detailed in the main text but can be inferred from the methods and results presented. For instance, the ROC curves and additional classification experiments in Additional File 1 and Additional File 2 provide insights into the performance of different models under various configurations.\n\nRegarding the availability and licensing of these materials, the supplementary files are accessible through the provided links. These files are hosted on a reputable platform, ensuring that they are freely available to the scientific community. The licensing terms typically allow for academic use and sharing, but specific details should be verified through the platform's terms of service.\n\nFor those interested in replicating our experiments or using our models, the supplementary files serve as a valuable resource. They include the necessary data and results to understand the configurations and optimization processes used in our study.",
  "model/interpretability": "The models employed in our study, particularly Methods P, PH, and BLAST, offer a degree of interpretability that allows users to understand the underlying mechanisms of protein identification. These methods provide clear and actionable insights into the classification process, making them less of a black box compared to some other machine learning approaches.\n\nMethod P, for instance, utilizes a spectrum kernel that focuses on the frequency of k-mers in protein sequences. This approach is transparent in the sense that the features used for classification are directly derived from the sequence data, making it easier to trace back the decision-making process. Users can see which specific k-mers are contributing to the classification of a protein as DNA repair-related.\n\nMethod PH extends this by incorporating additional biological information, such as physicochemical properties of amino acids. This method not only considers the sequence but also the functional aspects of the proteins, providing a more comprehensive and interpretable model. The inclusion of secondary structure information in Methods PS and PSH further enhances the interpretability by adding another layer of biological relevance to the classification process.\n\nBLAST, a well-established tool in bioinformatics, is also highly interpretable. It compares query sequences against a database of known sequences, providing alignment scores and e-values that indicate the significance of the matches. This makes it straightforward for users to understand why a particular protein is classified as DNA repair-related based on its similarity to known repair proteins.\n\nIn summary, the models used in our study are designed to be transparent and interpretable, allowing users to gain insights into the classification process. The use of sequence-based features, physicochemical properties, and secondary structure information ensures that the models are not black boxes but rather provide clear and understandable results.",
  "model/output": "The model employed in this study is designed for classification tasks. Specifically, it focuses on classifying proteins based on their involvement in various DNA repair processes. The experiments detailed in the publication involve different types of DNA repair mechanisms, such as Base Excision Repair, DNA Dealkylation, Single Strand Break Repair, and others. For each type of repair, the model aims to identify proteins related to that specific repair process.\n\nThe classification performance is evaluated using several metrics, including the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, True Positive Rates (TPR) at different false positive rates (0%, 1%, and 5%), and the percentage of positive cases. These metrics provide a comprehensive view of the model's ability to correctly identify proteins involved in the specified DNA repair processes.\n\nThe model utilizes different methods for classification, including P, PS, PF, BLAST, PH, and PSH. Each method is evaluated under varying sequence similarity thresholds (50%, 90%, and 0%), and the results are presented in tables for each type of DNA repair. The top-performing method for each metric is highlighted, indicating the method's effectiveness in classifying the proteins accurately.\n\nIn summary, the model is a classification model tailored to identify proteins associated with specific DNA repair mechanisms. The performance is assessed through multiple metrics and methods, providing a robust evaluation of the model's classification capabilities.",
  "model/duration": "The execution time for the model varied depending on the specific implementation and dataset used. Notably, significant improvements were achieved by utilizing an alternative data structure, which resulted in a 60% reduction in spectrum kernel computation time. This optimization was also integrated into the publicly available INTREPED web server, enhancing its efficiency. The detailed runtimes and performance metrics for different methods and datasets can be found in the supplementary files, which provide comprehensive insights into the model's execution time across various scenarios.",
  "model/availability": "The INTREPED web server is publicly accessible and can be found on the \"Prediction Servers\" page. Users can input a series of FASTA-format sequences for analysis. There are two processing options available: immediate results in the web browser using Methods P, PH, and BLAST, or submission to a queue with an email address for results that include secondary structure analysis using Methods PS and PSH.\n\nThe web server utilizes a rapid spectrum kernel implementation to ensure quick prediction responses and support for large queries. This implementation is designed to be efficient and is publicly available, allowing users to access and utilize the tool for their research needs.\n\nThe web server is free and publicly available, providing a user-friendly interface for inputting sequences and obtaining results. The use of a rapid spectrum kernel implementation ensures that the server can handle large queries efficiently, making it a valuable resource for researchers in the field.",
  "evaluation/method": "The evaluation of the proposed methods involved a rigorous cross-validation approach. Specifically, a one-versus-one-versus-rest (11R) cross-validation technique was employed. This method differs from the traditional one-versus-rest (1vR) technique by using only a single portion of the data for training, while the remaining portions are used as a reference homology database. This approach aims to combine homology and sequence data in an unbiased manner, providing a realistic estimate of method performance.\n\nThe evaluation process involved setting aside a portion of the data for performance evaluation (test data) while using the remaining portions for training. This process was repeated multiple times with different test and training datasets to ensure robustness. The amount of training data was reduced, and the end goal was to obtain a realistic estimate of method performance by combining homology and sequence data.\n\nStatistical analysis was conducted using the Kruskal-Wallis test on various datasets at different sequence similarity levels (50%, 90%, and 0%). The test was used to determine the statistical likelihood that all methods produced the same average performance. For each dataset and sequence similarity pair, the Area Under the Curve (AUC) and true positive rates (TPR) were used as performance metrics. When the AUC-based probability was less than 0.05, the null hypothesis that the methods were equivalent was rejected, indicating a statistically significant difference.\n\nAdditionally, pairwise comparisons of classifiers were performed using both the parametric t-test and the non-parametric Wilcoxon signed-rank test. These comparisons further validated the statistical significance of the differences observed among the methods. The results showed that certain methods, such as P and PS, demonstrated statistically significant improvements when analyzing GO-based DNA repair proteins in the unfiltered GO-PDB database.",
  "evaluation/measure": "In the evaluation of our classifiers, we focus on several key performance metrics to ensure a comprehensive assessment of their effectiveness. The primary metric reported is the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. This metric provides a single scalar value that summarizes the performance of the classifier across all classification thresholds, offering a robust measure of its ability to distinguish between positive and negative classes.\n\nIn addition to AUC, we also report the True Positive Rate (TPR) at different false positive rate thresholds. Specifically, we evaluate TPR at 0%, 1%, and 5% false positive rates. These metrics are crucial for understanding the classifier's performance in practical scenarios where the cost of false positives is significant. For instance, TPR-0% indicates the proportion of true positives identified without allowing any false positives, which is particularly important in applications where false positives are highly undesirable.\n\nThe choice of these metrics is representative of common practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By reporting AUC and TPR at various false positive rates, we provide a detailed view of our classifiers' performance, highlighting their strengths and areas for potential improvement. This approach allows for a nuanced understanding of how well our methods perform under different conditions and constraints, making our results relevant and applicable to a wide range of real-world scenarios.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of our proposed methods against both publicly available methods and simpler baselines. We performed comparisons on benchmark datasets, specifically the GO-PDB and GO-UniProt datasets, at different sequence similarity thresholds (50%, 90%, and 0%).\n\nFor the GO-PDB dataset, we used the Kruskal-Wallis test to assess the statistical significance of the differences in performance among the methods. The results indicated that there were statistically significant differences in the average performance of the methods, particularly for the unfiltered dataset. We further performed pairwise comparisons using both the parametric t-test and the non-parametric Wilcoxon signed-rank test. These comparisons revealed that Methods P and PS showed statistically significant improvements over other methods when analyzing GO-based DNA repair proteins.\n\nAdditionally, we compared our methods to simpler baselines, including BLAST, which is a widely used tool for sequence alignment. The results showed that our methods generally outperformed BLAST in terms of AUC and true positive rates, especially at higher sequence similarity thresholds.\n\nFor the GO-UniProt dataset, the statistical results suggested that while there were some variations in prediction performance among the classifiers, the large size of the UniProt datasets provided enough training examples to ensure that the various classification models produced similar results. No classifier produced average results identical to another classifier with a probability of less than 0.05, using either statistical test.\n\nOverall, our evaluation demonstrated that the proposed methods offer significant advantages over publicly available methods and simpler baselines, particularly in the context of identifying DNA repair proteins.",
  "evaluation/confidence": "The evaluation of our methods involved rigorous statistical analysis to ensure the reliability and significance of our results. We employed the Kruskal-Wallis test to assess whether there were statistically significant differences among the various methods. This non-parametric test was chosen because we could not assume that the AUC scores or other metrics were normally distributed. The Kruskal-Wallis test was applied to datasets filtered at different sequence similarities (50%, 90%, and unfiltered) using 5-fold cross-validation. For the unfiltered GO-PDB dataset, the test yielded a significant p-value of 0.0191, indicating that it is statistically very unlikely that all classifiers are equivalent on average.\n\nTo further validate our findings, we conducted pairwise comparisons using both the parametric t-test and the non-parametric Wilcoxon signed-rank test. These tests were applied to the GO-PDB and GO-UniProt datasets at various sequence similarities. The results from these tests were consistent, showing that Methods P and PS demonstrated statistically significant improvements when analyzing GO-based DNA repair proteins in the unfiltered GO-PDB database. The pairwise comparisons helped to identify specific methods that outperformed others, providing a more granular understanding of their relative performance.\n\nAdditionally, we presented the true positive rates (TPR) at different false positive rates (0%, 1%, and 5%) alongside the AUC values. These metrics were crucial in evaluating the performance of our classifiers across different scenarios. The statistical significance of our results was highlighted by bold values in the tables, indicating where the p-values were less than 0.05, thereby rejecting the null hypothesis that the methods were equivalent.\n\nIn summary, our evaluation process included comprehensive statistical testing to ensure the confidence and significance of our performance metrics. The use of non-parametric tests, along with pairwise comparisons, provided robust evidence supporting the superiority of certain methods over others and baselines.",
  "evaluation/availability": "The raw evaluation files for the experiments conducted in this study are available as supplementary materials. These files include detailed results and datasets used for the identification and classification of DNA repair proteins. Specifically, Additional File 1 provides ROC curves for identification experiments performed at different sequence similarities. Additional File 2 contains results for all GO-based repair pathway classification experiments, including those not shown in the main results section. Additional File 3 offers statistical analysis of multiple classifier performances on identification and classification datasets, along with pairwise comparisons of classifiers on identification data. Additional File 7 includes the PDB and UniProt identification datasets, as well as the UniProt classification datasets, for each sequence similarity used in the report.\n\nThese supplementary files are publicly accessible through the provided links, ensuring transparency and reproducibility of the research findings. The datasets and results can be downloaded and used by other researchers for further analysis or validation of the methods presented in the paper. The availability of these files supports the rigor and reliability of the study, allowing for independent verification and potential extension of the research."
}