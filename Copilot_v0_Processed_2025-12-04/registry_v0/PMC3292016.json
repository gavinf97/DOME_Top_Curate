{
  "publication/title": "Using Support Vector Machine and Evolutionary Profiles to Predict Antifreeze Protein Sequences",
  "publication/authors": "The authors who contributed to this article are:\n\n- Kandaswamy, K.K.\n- Chou, K.C.\n- Martinetz, T.\n- Möller, S.\n- Suganthan, P.N.\n- Sridharan, S.\n- Ganesan, P.\n\nTheir contributions to the paper include the development of a predictor for antifreeze proteins using a random forest approach based on sequence-derived properties. They also contributed to the creation of the AFP_PSSM predictor, which utilizes PSSM profiles and a PSSM-400 vector for protein sequence encoding. Additionally, they conducted experiments to evaluate the performance of their models using various encoding schemes and compared their predictor with previous work.",
  "publication/journal": "International Journal of Molecular Sciences",
  "publication/year": "2012",
  "publication/doi": "10.3390/ijms13022196",
  "publication/tags": "- Antifreeze proteins\n- Protein prediction\n- Machine learning\n- Support vector machines\n- Bioinformatics\n- Protein encoding\n- Evolutionary information\n- Pseudo amino acid composition\n- Sequence analysis\n- Computational biology",
  "dataset/provenance": "The dataset used in this study was originally compiled by Kandaswamy et al. It comprises 481 antifreeze proteins and 9493 non-antifreeze proteins. To mitigate redundancy and homology bias, sequences with 40% or more similarity were removed using the CD-HIT program. The final dataset was divided into training and testing sets. The training set includes 300 antifreeze proteins and 300 non-antifreeze proteins, both randomly selected from the original pools. The testing set consists of the remaining 181 antifreeze proteins and 9193 non-antifreeze proteins. This dataset is freely accessible for download.",
  "dataset/splits": "There are two main data splits: the training dataset and the test dataset.\n\nThe training dataset contains 300 antifreeze proteins and 300 non-antifreeze proteins. These were randomly selected from a larger pool of 481 antifreeze proteins and 9493 non-antifreeze proteins, respectively.\n\nThe test dataset includes the remaining 181 antifreeze proteins and 9193 non-antifreeze proteins. This split was designed to evaluate the model's performance on data it has not seen during training, ensuring a robust assessment of its predictive power.",
  "dataset/redundancy": "The datasets used in this study were retrieved from a previously published work, consisting of 481 antifreeze proteins and 9493 non-antifreeze proteins. To mitigate redundancy and homology bias, sequences with 40% or more similarity were removed using the CD-HIT program. This process ensures that the datasets are independent and reduces the risk of overfitting.\n\nThe dataset was then split into training and testing sets. The training dataset contains 300 antifreeze proteins, randomly selected from the 481 available, and 300 non-antifreeze proteins, randomly selected from the 9493 available. The test dataset includes the remaining 181 antifreeze proteins and 9193 non-antifreeze proteins. This split ensures that the training and test sets are independent, with no overlap in sequences.\n\nThe distribution of the datasets is notably imbalanced, with a much higher number of non-antifreeze proteins compared to antifreeze proteins. This imbalance is intentional and reflects real-world scenarios, where the number of non-antifreeze proteins is significantly higher. This approach is different from many previously published machine learning datasets, which often aim for balanced distributions. The imbalanced distribution challenges the model to perform well even when the classes are not equally represented, making the results more applicable to real-world applications.",
  "dataset/availability": "The datasets utilized in this study are publicly available for download. The datasets consist of 481 antifreeze proteins and 9493 non-antifreeze proteins, with redundancy and homology bias removed using the CD-HIT program. The training dataset includes 300 antifreeze proteins and 300 non-antifreeze proteins, while the test dataset contains the remaining 181 antifreeze proteins and 9193 non-antifreeze proteins.\n\nThese datasets can be freely accessed from a specified online repository. The availability of these datasets ensures that other researchers can replicate and validate the findings presented in this study. The datasets are provided under terms that allow for their use in research and development, promoting transparency and reproducibility in scientific research.",
  "optimization/algorithm": "The machine-learning algorithm class used is support vector machines (SVM). This is a well-established family of margin-based classifiers known for their effectiveness in prediction, classification, and regression problems. The specific implementation used is the LIBSVM package, which is widely recognized and utilized in the field.\n\nThe SVM algorithm employed is not new; it has been extensively studied and applied in various domains. The choice to use SVM in this context is likely due to its proven capability to handle complex classification tasks, particularly when dealing with high-dimensional data, as is common in bioinformatics.\n\nThe decision to use SVM in a molecular science context, rather than a machine-learning journal, is driven by the specific application and the nature of the data. The focus of the study is on predicting properties of proteins, which is a specialized area within molecular science. The use of SVM is justified by its suitability for the type of data and the problem at hand, rather than the novelty of the algorithm itself. The optimization of parameters such as the regularization parameter C and the kernel width parameter γ is performed using a grid search strategy with 5-fold cross-validation, ensuring robust performance for the specific task of protein sequence encoding and classification.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to transform protein sequences into a format suitable for machine learning algorithms, particularly Support Vector Machines (SVM). The primary goal was to convert variable-length protein sequences into fixed-length feature vectors, which is essential for SVM training.\n\nFirst, we utilized Position Specific Iterated BLAST (PSI-BLAST) to generate evolutionary information for each protein sequence. This information was represented as a Position-Specific Scoring Matrix (PSSM), which captures the evolutionary conservation of amino acids at each position in the sequence. The PSSM is a two-dimensional matrix where rows correspond to protein residues and columns represent the 20 native amino acids. Each element in the matrix indicates the probability of an amino acid being substituted by another during evolution.\n\nTo convert the PSSM into a fixed-length vector, we created a PSSM-400 vector. This vector encapsulates the composition of occurrences of each type of amino acid corresponding to each type of amino acids in the protein. The PSSM-400 vector was chosen because it performed better than other encoding schemes, such as amino acid composition, dipeptide composition, and Chou’s Pseudo Amino Acid Composition (PseAAC).\n\nAdditionally, we considered amino acid composition and dipeptide composition as alternative encoding schemes. Amino acid composition transforms each protein sequence into a 20-dimensional vector, reflecting the fraction of each amino acid in the sequence. Dipeptide composition extends this by considering pairs of amino acids, resulting in a 400-dimensional vector that captures both the fraction information of amino acids and the local order information of the protein sequence.\n\nThe choice of PSSM-400 as the final encoding scheme was validated through extensive testing. Four SVM models were constructed using different encoding schemes, and their performances were evaluated using accuracy and the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curves. The PSSM-400 encoding scheme demonstrated the highest accuracy and AUC, making it the preferred method for representing antifreeze protein sequences in our study.",
  "optimization/parameters": "In our model, three primary parameters are used to measure performance: sensitivity, specificity, and accuracy. These parameters are defined using formulas that involve true positives, true negatives, false positives, and false negatives.\n\nThe selection of these parameters was guided by their widespread use and effectiveness in evaluating the performance of predictive models in bioinformatics. Sensitivity and specificity provide insights into the model's ability to correctly identify positive and negative instances, respectively, while accuracy offers an overall measure of the model's correctness.\n\nAdditionally, two other parameters, the regularization parameter C and the kernel width parameter γ, are optimized using a grid search strategy based on 5-fold cross-validation. These parameters are crucial for the support vector machine (SVM) model, which is employed in our study. The optimization process ensures that the model achieves the best possible performance by balancing the trade-off between the margin and the training error.",
  "optimization/features": "In the optimization process, the input features used are derived from the PSSM-400 encoding scheme. This scheme generates a fixed-length feature vector of 400 dimensions for each protein sequence. These 400 features are used as input for the support vector machine (SVM) model.\n\nFeature selection was not explicitly performed in this study. The PSSM-400 vector was chosen as the final encoding scheme based on its superior performance compared to other encoding methods, such as amino acid composition, dipeptides composition, and Chou’s Pseudo Amino Acid Composition (PseAAC). The selection of PSSM-400 was determined through a comparative analysis of different SVM models, where PSSM-400 demonstrated the highest accuracy and area under the curve (AUC) in ten-fold cross-validation tests.\n\nThe PSSM-400 vector is derived from the Position Specific Scoring Matrix (PSSM) profiles generated by the PSI-BLAST program. These profiles capture evolutionary information, which is crucial for reflecting the intrinsic correlation of protein sequences with the attribute to be predicted. The PSSM-400 vector is composed of the occurrences of each type of amino acid corresponding to each type of amino acids in the protein, providing a comprehensive representation of the protein sequence.",
  "optimization/fitting": "The fitting method employed in this study utilizes a Support Vector Machine (SVM) with a radial basis kernel function. The SVM is a powerful tool for classification problems, and it is particularly effective in handling high-dimensional spaces and cases where the number of parameters (features) is much larger than the number of training points.\n\nTo address the potential issue of over-fitting, a regularization parameter C and a kernel width parameter γ were optimized using a grid search strategy with 5-fold cross-validation. This approach helps to find the best combination of parameters that generalize well to unseen data, thereby mitigating over-fitting. Additionally, the use of a radial basis kernel function allows the SVM to model complex relationships in the data without over-fitting to the training set.\n\nUnder-fitting was addressed by ensuring that the model had sufficient complexity to capture the underlying patterns in the data. The use of PSSM-400, which provides a rich representation of protein sequences, helped in capturing the necessary features. The model's performance was further validated using ten-fold cross-validation, which ensures that the model generalizes well across different subsets of the data. The final model achieved an accuracy of 82.67% and an AUC of 0.926, indicating that it effectively captures the relevant patterns without under-fitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictor. One of the key methods used was the support vector machine (SVM) with a radial basis function kernel. The SVM inherently helps in preventing overfitting by finding the optimal hyperplane that maximizes the margin between classes, which reduces the complexity of the model.\n\nAdditionally, we utilized a grid search strategy combined with 5-fold cross-validation to optimize the parameters of the SVM. This process involved systematically working through multiple combinations of the regularization parameter C and the kernel width parameter γ to find the best-performing model. By doing so, we ensured that the model generalized well to unseen data rather than just fitting the training data.\n\nFurthermore, we performed ten-fold cross-validation to evaluate the model's performance. This involved dividing the dataset into ten equal parts, training the model on nine parts, and testing it on the remaining part. This procedure was repeated ten times, with each part serving as the test set once. The final prediction result was the average accuracy of these ten testing sets, providing a more reliable estimate of the model's performance.\n\nTo further reduce computational time and ensure the model's generalizability, we also adopted independent testing dataset cross-validation. This approach involved using a separate, independent dataset for testing, which helped in assessing the model's performance on data it had not seen during training.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the publication. Specifically, we utilized the LIBSVM package with a radial basis kernel function. The two key parameters optimized were the regularization parameter C and the kernel width parameter γ. These parameters were tuned using a grid search strategy based on 5-fold cross-validation.\n\nThe details of the optimization process, including the specific values of C and γ, are provided within the text. However, the exact model files and the optimization schedule are not explicitly detailed in the publication. The datasets used for training and testing are available for download, which includes the training dataset consisting of 300 antifreeze proteins and 300 non-antifreeze proteins, and the testing dataset with the remaining sequences.\n\nFor those interested in replicating or building upon our work, the datasets can be accessed freely. The methods and encoding schemes, such as the PSSM-400 vector, are described in sufficient detail to allow for reproduction. The LIBSVM package, which is open-source and widely used, can be obtained under its respective license.",
  "model/interpretability": "The model presented in our study is primarily based on Support Vector Machines (SVM), which are known for their robustness and effectiveness in handling high-dimensional spaces. However, SVMs are generally considered black-box models, meaning that the internal workings and decision-making processes are not easily interpretable. The model uses various encoding schemes, such as amino acid composition, dipeptides composition, Chou’s Pseudo Amino Acid Composition (PseAAC), and PSSM-400, to represent protein sequences. Among these, the PSSM-400 encoding scheme was found to perform the best, achieving an accuracy of 82.67% and an AUC of 0.926.\n\nThe use of PSSM-400 involves transforming the Position Specific Scoring Matrix (PSSM) profiles generated by PSI-BLAST into a fixed-length vector. This vector captures the evolutionary information of the protein sequences, which is crucial for predicting antifreeze proteins. While this approach enhances the model's predictive power, it does not provide clear, human-interpretable insights into why a particular sequence is classified as an antifreeze protein.\n\nThe model's transparency is further limited by the use of the radial basis kernel function in the SVM, which adds another layer of complexity. The parameters of the SVM, such as the regularization parameter C and the kernel width parameter γ, are optimized using a grid search strategy based on cross-validation. This optimization process is essential for achieving high accuracy but does not contribute to the interpretability of the model.\n\nIn summary, while the model is highly effective in predicting antifreeze proteins, it lacks transparency. The use of complex encoding schemes and the inherent nature of SVMs as black-box models make it challenging to interpret the decision-making process. Future work could focus on developing more interpretable models or techniques to explain the predictions made by the current model.",
  "model/output": "The model discussed in this publication is a classification model. Specifically, it is a support vector machine (SVM) model used for predicting antifreeze proteins. The SVM is a type of margin-based classifier that is particularly effective for classification problems. It works by finding an optimal hyperplane that maximizes the distance between the hyperplane and the nearest samples from each of the two classes. In this context, the model is used to classify protein sequences into antifreeze proteins and non-antifreeze proteins.\n\nThe performance of the model is evaluated using metrics such as sensitivity, specificity, and accuracy. These metrics are calculated based on the true positive, true negative, false positive, and false negative rates obtained from the model's predictions. Additionally, receiver operating characteristic (ROC) curves are used to visualize the performance of the model under different encoding schemes.\n\nThe model building process involves several steps, including generating sequential evolution information in the form of PSSM profiles for the input sequence using PSI-BLAST, transforming the PSSM into a PSSM-400 vector, and then applying the predictor to output the test results. The final encoding scheme used is the PSSM-400, which demonstrated the highest accuracy and area under the curve (AUC) in the evaluation.\n\nThe model's performance is further validated through ten-fold cross-validation and independent testing dataset cross-validation. The results show that the model achieves an accuracy of 82.67% with an AUC of 0.926, indicating its effectiveness in predicting antifreeze proteins.",
  "model/duration": "The execution time of the AFP_PSSM predictor is relatively efficient. For a protein sequence consisting of 300 amino acids, it takes approximately 15 seconds to obtain the predicted result. This duration includes the time required for the model to process the input sequence and generate the output. The predictor is designed to handle sequences in FASTA format, ensuring compatibility with standard bioinformatics tools and practices. The web server implementation of AFP_PSSM is optimized to provide results promptly, making it practical for experimental scientists who need quick predictions for their research.",
  "model/availability": "To facilitate the use of our predictor by experimental scientists, a user-friendly web server has been established. This web server, named AFP_PSSM, is accessible to the public and provides a straightforward interface for users to input their protein sequences and obtain predictions.\n\nThe web server requires users to input their email address, as the prediction process may take some time. Users can input their query protein sequence in FASTA format, which consists of a single initial line beginning with a greater-than symbol (“>”), followed by lines of amino acid sequence. An example protein sequence is provided to guide users.\n\nAdditionally, users can choose a threshold value from a drop-down list to adjust the confidence level of the predictions. After submitting the query, the server processes the input and returns the predicted result, indicating whether the protein is an antifreeze protein and the associated probability.\n\nThe web server is designed to handle protein sequences efficiently, with a typical prediction for a sequence of 300 amino acids taking approximately 15 seconds.\n\nThe source code and detailed implementation specifics are not publicly released. However, the web server itself serves as the primary method for users to run the algorithm and obtain predictions. The web server is distributed under the terms and conditions of the Creative Commons Attribution license, allowing for its use and sharing with proper attribution.",
  "evaluation/method": "The evaluation of our method involved a comprehensive approach to ensure the robustness and accuracy of our predictor. We employed ten-fold cross-validation, where the dataset was randomly divided into ten equal sets. Nine of these sets were used for training, while the remaining set was used for testing. This procedure was repeated ten times, with each set serving as the test set once. The final prediction result was the average accuracy of these ten testing sets. This method helps to ensure that the model's performance is consistent and not dependent on a particular subset of the data.\n\nAdditionally, to further validate our model and reduce computational time, we adopted an independent testing dataset cross-validation. This approach involved using a separate, independent dataset to evaluate the model's performance, providing an unbiased assessment of its predictive power.\n\nTo measure the performance of our model, we used three key parameters: sensitivity, specificity, and accuracy. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. Accuracy is the overall proportion of true results (both true positives and true negatives) among the total number of cases examined.\n\nWe also created receiver operating characteristic (ROC) curves for all models to evaluate their performance using different encoding schemes. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the model's diagnostic ability.\n\nIn summary, our evaluation method combined cross-validation techniques with independent testing and detailed performance metrics to thoroughly assess the accuracy and reliability of our predictor.",
  "evaluation/measure": "In our study, we employed three primary performance metrics to evaluate the effectiveness of our model: sensitivity, specificity, and accuracy. These metrics are defined using the standard terms true positive (TP), true negative (TN), false positive (FP), and false negative (FN).\n\nSensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. It is calculated as 100 times the ratio of true positives to the sum of true positives and false negatives.\n\nSpecificity, or the true negative rate, assesses the proportion of actual negatives that are correctly identified. It is computed as 100 times the ratio of true negatives to the sum of true negatives and false positives.\n\nAccuracy provides an overall measure of the model's performance by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. It is determined as 100 times the ratio of the sum of true positives and true negatives to the total number of cases.\n\nAdditionally, we utilized the receiver operating characteristic (ROC) curve to evaluate the performance of our models using different encoding schemes. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, providing a visual representation of the model's diagnostic ability.\n\nThese metrics are widely used in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies in the field.",
  "evaluation/comparison": "In the evaluation of our predictor, a comparison was conducted with a recently published method by Kandaswamy et al. This comparison was performed on a testing dataset that included 181 antifreeze proteins and 9193 non-antifreeze proteins. The results demonstrated that our predictor, AFP_PSSM, achieved an accuracy of 90.17%, which is higher than the 83.38% accuracy reported by Kandaswamy et al. This improvement in performance can be attributed to the effective protein sequence encoding scheme used in our prediction model.\n\nRegarding simpler baselines, our study explored various encoding schemes, including amino acid composition, dipeptides composition, and Chou’s Pseudo Amino Acid Composition (PseAAC). These methods were evaluated using Support Vector Machines (SVM) to assess their predictive power. The PSSM-400 encoding scheme, which utilizes evolutionary information, outperformed the other methods with an accuracy of 82.67% and an Area Under the Curve (AUC) of 0.926. This indicates that the PSSM-400 scheme provides a more robust representation of protein sequences for prediction purposes.\n\nThe comparison with publicly available methods and simpler baselines underscores the effectiveness of our approach in accurately predicting antifreeze proteins. The use of the PSSM-400 encoding scheme, combined with SVM, has shown superior performance in handling the complexity and variability of protein sequences.",
  "evaluation/confidence": "Evaluation Confidence\n\nTo assess the confidence in our evaluation, we employed ten-fold cross-validation, a robust statistical method that helps to ensure the reliability and generalizability of our results. This process involves randomly dividing the dataset into ten equal parts, using nine parts for training and the remaining one for testing, and repeating this procedure ten times. The final prediction result is the average accuracy of these ten testing sets, providing a comprehensive measure of our model's performance.\n\nIn addition to cross-validation, we also conducted independent testing dataset cross-validation. This approach further validates our model by testing it on a separate dataset that was not used during the training phase. This method helps to confirm that our model's performance is not due to overfitting and that it can generalize well to new, unseen data.\n\nRegarding the performance metrics, we calculated sensitivity, specificity, and accuracy, which are standard measures in the field. However, confidence intervals for these metrics were not explicitly provided in our study. To determine statistical significance, we compared our predictor, AFP_PSSM, with a recent work by Kandaswamy et al. The results showed that our predictor achieved a higher accuracy of 90.17%, compared to 83.38% reported by Kandaswamy et al. This difference suggests that our method is superior, although formal statistical tests (such as McNemar's test) were not mentioned to confirm the significance of this improvement.\n\nOverall, while our evaluation methods are rigorous and provide strong evidence of our model's performance, the lack of confidence intervals and explicit statistical significance tests means that some caution is warranted in interpreting the results as definitively superior to other methods. Future work could include these additional statistical analyses to further bolster confidence in our findings.",
  "evaluation/availability": "The evaluation process for our predictor involved ten-fold cross-validation, where the dataset was randomly divided into ten equal sets. Nine of these sets were used for training, and the remaining one for testing. This procedure was repeated ten times, and the final prediction result was the average accuracy of the ten testing sets. To further validate our model, we also employed independent testing dataset cross-validation.\n\nThe raw evaluation files, including the datasets used for training and testing, are freely available for download. This allows other researchers to replicate our results and further build upon our work. The datasets consist of antifreeze proteins and non-antifreeze proteins, with redundancy and homology bias removed using the CD-HIT program. The training dataset contains 300 antifreeze proteins and 300 non-antifreeze proteins, while the test dataset includes the remaining 181 antifreeze proteins and 9193 non-antifreeze proteins.\n\nThe datasets can be accessed publicly, ensuring transparency and reproducibility in our research. The specific details and access information for downloading these datasets are provided in the publication, enabling other scientists to utilize them for their own studies. The availability of these datasets supports the open science initiative, fostering collaboration and advancement in the field."
}