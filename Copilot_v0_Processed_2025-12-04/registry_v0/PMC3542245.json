{
  "publication/title": "NetiNeti: discovery of scientific names from text using machine learning methods",
  "publication/authors": "The authors who contributed to the article are:\n\n- Lakshmi Manohar Akella\n  - Corresponding author\n  - Developed the machine learning approach for scientific name discovery\n  - Evaluated the system on various datasets\n  - Contributed to the writing and editing of the manuscript\n\n- Catherine N Norton\n  - Provided expertise in biodiversity and biological data\n  - Assisted in the evaluation of the system on biodiversity texts\n  - Contributed to the writing and editing of the manuscript\n\n- Holly Miller\n  - Assisted in the development and evaluation of the system\n  - Contributed to the writing and editing of the manuscript",
  "publication/journal": "BMC Bioinformatics",
  "publication/year": "2012",
  "publication/doi": "10.1186/1471-2105-13-211",
  "publication/tags": "- Scientific name recognition\n- Machine learning\n- Text mining\n- Biodiversity\n- Biomedical literature\n- Name extraction\n- OCR errors\n- Probabilistic methods\n- Contextual features\n- Taxonomic indexing",
  "dataset/provenance": "The dataset used in our study was sourced from a variety of text corpora, including the Biodiversity Heritage Library (BHL), MEDLINE, and PubMed Central (PMC). The BHL provided a rich source of biodiversity data, with over 80,000 volumes corresponding to over 30 million scanned pages converted to text. For our evaluation, we manually annotated a 600-page book from BHL titled \"American Seashells\" to create a gold-standard biodiversity corpus marked with scientific names. This was necessary because there were no previously reported annotated corpora specifically for biodiversity information that included scientific names with errors and variations.\n\nIn addition to BHL, we also utilized MEDLINE, which contains over 18 million bibliographic records from journal articles in life sciences, with a focus on biomedicine. We ran our system on the full MEDLINE database and found over 190,000 unique binomial and trinomial names in more than 1,880,000 PubMed records. Furthermore, we evaluated our system on a small subset of 136 tagged PMC open-access full-text articles, which were selected from the evaluation set used by the Linnaeus species identification system.\n\nThe training set for our machine learning classifiers consisted of approximately 83,000 examples, with about 40,000 positive examples and 43,000 negative examples. The positive examples included scientific names with contextual information, scientific names without contextual information, and abbreviated names. The negative examples were derived from a tokenized geography book from the Internet Archive, using unigrams, bigrams, and trigrams from the text.\n\nPrevious work in the community has relied on dictionary-based approaches and tools like TaxonFinder and the FAT tool integrated into the GoldenGATE editor. However, these methods often struggle with handling OCR errors, spelling variations, and new species names. Our approach, NetiNeti, addresses these challenges by using machine learning methods to recognize scientific names, including those with errors and variations, from text sources across domains like biodiversity and biomedicine.",
  "dataset/splits": "The dataset used for training and evaluation consists of multiple splits. Initially, an initial set of about 5,000 names was used as a positive example set. Candidate strings from unigram, bigram, and trigrams of a tokenized book, which does not contain any scientific names, were used as an initial negative example set.\n\nFor the final training set, about 10,000 positive examples with contextual information and another 10,000 examples from scientific names without contextual information were used. Additionally, abbreviated names from these examples were included in the positive example set. This resulted in a total of about 40,000 positive examples.\n\nThe negative example set was derived from a geography book, with strings from word unigrams, bigrams, and trigrams forming the negative examples. This resulted in about 43,000 negative examples.\n\nTogether, these positive and negative examples formed a training set of 83,000 examples for the two class labels. The evaluation sets included a 600-page biodiversity book, MEDLINE, and a subset of 136 tagged PubMed Central’s open access full-text articles. The evaluation on the biodiversity book involved comparing the results with manually extracted names and other tools like TaxonFinder and the FAT tool. The evaluation on MEDLINE and PubMed Central articles focused on precision and recall measures.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in our work are probabilistic classifiers, specifically Naïve Bayes and Maximum Entropy. These are well-established algorithms in the field of machine learning and natural language processing.\n\nThe Naïve Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features. Despite its simplicity, it often performs surprisingly well in various document classification tasks.\n\nThe Maximum Entropy algorithm, on the other hand, is a more complex probabilistic classifier that makes no independence assumptions. It estimates the probability distribution that maximizes entropy, subject to constraints derived from the training data. This approach allows it to capture more complex relationships between features.\n\nBoth algorithms are not new; they have been extensively studied and used in various applications. The choice to use these algorithms in our work was driven by their effectiveness in handling text classification tasks, particularly in the context of identifying scientific names from text.\n\nThe reason these algorithms were not published in a machine-learning journal is that our focus was on applying these established methods to a specific problem in bioinformatics—namely, the discovery of scientific names from text. The novelty of our work lies in the application of these algorithms to this particular domain, rather than in the development of new machine-learning algorithms. Therefore, publishing in a bioinformatics journal was more appropriate for our target audience and the specific contributions of our research.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it employs probabilistic machine learning algorithms, specifically Naïve Bayes and Maximum Entropy, to classify candidate names as scientific or not. These classifiers are trained independently on a dataset that includes both positive and negative examples of scientific names, along with their contextual information.\n\nThe Naïve Bayes classifier models the joint probability of a class and a string, assuming that the features of the string and its contexts are independent given the class label. This independence assumption simplifies the estimation of probabilities from a training set of labeled examples.\n\nThe Maximum Entropy classifier, also known as a logistic regression classifier, models the conditional probability of a class label given an input string and its contexts. It aims to create a model that is consistent with the training data while being as uniform as possible, adhering to the principle of modeling only what is known and making no assumptions about what is unknown.\n\nBoth classifiers were implemented using Python and the Natural Language Toolkit (NLTK) package. For the Maximum Entropy classifier, different parameter estimation methods were compared, including Improved Iterative Scaling (IIS), Generalized Iterative Scaling (GIS), and Limited-Memory Variable Metric optimization methods like L-BFGS. The training data for these classifiers included a variety of sources, such as BHL books, MEDLINE abstracts, and EOL content, which were manually annotated to ensure the accuracy of the training examples.",
  "optimization/encoding": "The data encoding process involved creating a training set with both positive and negative examples. Positive examples were derived from scientific names, including those with spelling and OCR errors, and their surrounding contextual information from sentences. These examples were augmented with abbreviated names and additional instances without contextual information. Negative examples were generated from a geography book, using unigrams, bigrams, and trigrams from the tokenized text.\n\nFeatures were extracted from both the scientific names and their contextual neighborhoods. Structural features included the last three, last two, and last characters, as well as the first and second characters of unigram, bigram, and trigram candidates. Binary features indicated the presence of specific characters in different partitions of the set. Additionally, the presence or absence of words in a dictionary of genus and species combinations was used as a binary feature. Numerical features, such as the count of vowels in various parts of the candidate names, were also included. Contextual features comprised words appearing near candidate names and their parts-of-speech tags.\n\nThe training set consisted of approximately 83,000 examples, with about 40,000 positive examples and 43,000 negative examples. This encoded data was used to train probabilistic machine learning algorithms, specifically Naïve Bayes and Maximum Entropy classifiers, to estimate the probability of a label (whether a name is scientific or not) given a candidate string and its contextual information.",
  "optimization/parameters": "In our study, the number of parameters (p) in the model varied depending on the specific algorithm and the context span used. For the Maximum Entropy classifier, the parameters λm are associated with the feature functions gm(ci; sj). The exact number of these parameters depends on the features extracted from the training data. We used a context span of 1, which means features were derived from a word on either side of the candidate name. This configuration was chosen because it provided a good balance between recall and precision.\n\nThe selection of the context span was based on experimental results. We compared different context spans and found that using a context span of 1 yielded higher recall with good precision (>0.8) compared to other configurations. This approach allowed us to effectively capture relevant contextual information without overcomplicating the model.\n\nFor the Naïve Bayes classifier, the parameters are the probabilities of the features given the class labels. The number of parameters in this case depends on the number of features and the number of classes. We used the same context span of 1 for consistency in our comparisons.\n\nIn summary, the number of parameters in our models was determined by the features extracted from the training data and the context span used. The context span of 1 was selected based on its performance in terms of recall and precision.",
  "optimization/features": "The input features used in our study are quite extensive and varied. We utilized both structural and contextual features derived from scientific names and their surrounding text. Structural features include specific characters from unigrams, bigrams, and trigrams, such as the last three, last two, and last characters, as well as the first and second characters. Binary features were also employed, such as the presence of certain characters in specific partitions of the set, and whether unigrams, bigrams, and trigrams are part of a dictionary of genus and species combinations. Additionally, numerical features like the number of vowels in various parts of the candidate names were considered. Contextual features included words appearing in the neighborhood of candidate names and their parts-of-speech tags.\n\nFeature selection was not explicitly mentioned as a separate process. However, the features used were carefully chosen based on their relevance to the task of identifying scientific names. The training set was used to generate these features, ensuring that the model was trained on a comprehensive and relevant set of inputs. The features were derived from both positive and negative example sets, which included scientific names with and without contextual information, as well as abbreviated names and strings from tokenized texts that do not contain scientific names. This approach ensured that the model was robust and could handle a variety of input scenarios.",
  "optimization/fitting": "In the fitting method used for our scientific name recognition task, we employed both Naïve Bayes and Maximum Entropy classifiers. The Maximum Entropy approach, in particular, involves a large number of parameters due to the feature functions defined on the class label and the string context. To address potential overfitting, we utilized regularization techniques and compared different parameter estimation methods, including Improved Iterative Scaling (IIS), Generalized Iterative Scaling (GIS), and Limited-Memory Variable Metric optimization methods like L-BFGS. These methods help in finding a balance between fitting the training data well and generalizing to unseen data.\n\nTo rule out overfitting, we performed cross-validation and evaluated the models on a separate test set that was manually annotated with scientific names. This ensured that the models were not merely memorizing the training data but were capable of generalizing to new, unseen examples. Additionally, we used a stop-list of English words during the pre-filtering process, which helped in generating a cleaner set of candidate names and reducing noise in the training data.\n\nUnderfitting was addressed by carefully selecting and engineering features that captured the structural and contextual information of scientific names. We experimented with different neighborhood sizes for contextual features and found that using features from a word on either side of the candidate name (context span of 1) provided a good balance between precision and recall. Furthermore, we increased the size of the positive example set during training, which contributed to better precision of the classifiers.\n\nIn summary, by employing regularization, cross-validation, and careful feature engineering, we ensured that our models neither overfit nor underfit the data, leading to robust performance in identifying scientific names from text.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "In our study, we explored various hyper-parameter configurations and optimization schedules to enhance the performance of our machine learning classifiers for scientific name recognition. Specifically, we compared different parameter estimation methods for the Maximum Entropy classifier, including Improved Iterative Scaling (IIS), Generalized Iterative Scaling (GIS), and Limited-Memory Variable Metric optimization methods like L-BFGS. These methods were implemented using the Python programming language and the Natural Language Toolkit (NLTK) package. Additionally, the MEGAM optimization package was utilized for L-BFGS optimization.\n\nThe configurations and optimization parameters used in our experiments are detailed within the publication. However, the specific model files and exact hyper-parameter settings are not explicitly provided in a downloadable format. The methods and approaches described can be replicated using the tools and libraries mentioned, such as Python, NLTK, and MEGAM. These tools are freely available and can be accessed through their respective websites. The Python programming language and the NLTK package are open-source and can be obtained from their official repositories. Similarly, the MEGAM optimization package is available for download and use.\n\nFor those interested in replicating our work, the detailed descriptions of the methods and the steps involved in our experiments are provided in the publication. This includes the training set generation, feature extraction, and the evaluation metrics used. While the exact model files are not available for download, the methodologies and tools used are well-documented and can be implemented by following the guidelines provided in the study.",
  "model/interpretability": "The model employed in our work is not a black box but rather a transparent approach that leverages probabilistic machine learning algorithms, specifically Naïve Bayes and Maximum Entropy. These algorithms are well-known for their interpretability, as they provide clear insights into how decisions are made.\n\nNaïve Bayes, for instance, operates by estimating the probability of a label (whether a name is scientific or not) given a candidate string and its contextual information. It assumes that the features of the string are independent, which simplifies the probability calculations and makes the model easier to understand. The model's decisions can be traced back to the individual features and their contributions to the final probability.\n\nMaximum Entropy, on the other hand, also provides a transparent framework. It estimates the probability distribution that maximizes entropy, subject to constraints derived from the training data. This method ensures that the model does not make strong assumptions about the data and can be interpreted in terms of the features and their weights.\n\nBoth algorithms allow for the examination of the features that contribute most to the classification of a name as scientific or not. For example, in the case of Naïve Bayes, one can look at the conditional probabilities of the features given the class label to understand which features are most indicative of a scientific name. Similarly, in Maximum Entropy, the weights assigned to the features can be inspected to see which features have the most significant impact on the model's predictions.\n\nIn summary, the use of Naïve Bayes and Maximum Entropy in our model ensures that it is transparent and interpretable. The decisions made by the model can be traced back to the individual features and their contributions, providing clear examples of how the model arrives at its classifications.",
  "model/output": "The model is classification. It is designed to determine whether a given string is a scientific name or not. This is achieved by estimating the probability of a label (scientific or not) given a candidate string and its contextual information. The model uses probabilistic machine learning algorithms, specifically Naïve Bayes and Maximum Entropy, to classify candidate names. These algorithms learn or estimate probabilities from a training set to make predictions about new, unseen data. The output of the model is a classification label indicating whether a string is a scientific name or not, based on the highest probability calculated from the contextual features.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps and datasets to ensure its robustness and accuracy. Initially, the method was tested on a 600-page book titled \"American Seashells,\" where all scientific names, including those with OCR errors, were manually extracted. This manual extraction served as a gold standard for comparison. The results obtained from the method were then compared with those from other tools like TaxonFinder and the FAT tool integrated into the GoldenGATE editor. This comparison was summarized in a table to highlight the precision, recall, and F-score of each approach.\n\nAdditionally, the method was evaluated on the MEDLINE database, which contains over 18 million bibliographic records from journal articles in life sciences, with a focus on biomedicine. The performance of two of the best-performing algorithms was summarized in another table, showing the number of unique binomial and trinomial names identified and the number of PMIDs covered.\n\nFurthermore, the method was tested on a small subset of 136 tagged PubMed Central’s open-access full-text articles. These articles were selected from the evaluation set used by the Linnaeus species identification system, ensuring that only scientific name tags were considered, as the full evaluation set also included articles tagged with common names.\n\nThe evaluation also involved a series of training experiments with the Naïve Bayes classifier using different neighborhoods for contextual features and varying sizes of positive and negative training examples. The precision and recall measures were used to evaluate the resulting classifiers on the \"American Seashells\" book. The impact of using contextual features and the size of the training set on the performance of the classifiers was analyzed and visualized in precision-recall plots.\n\nIn subsequent experiments, the precision and recall values of Naïve Bayes and Maximum Entropy classification algorithms were compared using various parameter estimation methods. The Decision Tree Learning algorithm implemented in the NLTK toolkit was also included in this comparison. The evaluation was conducted both with and without the use of a stop-list of English words as part of the pre-filtering process. The results were summarized in a table, showing the performance of each algorithm under different conditions.",
  "evaluation/measure": "In our evaluation, we primarily focused on precision and recall as our key performance metrics. Precision measures the fraction of retrieved names that are relevant scientific names, while recall indicates the fraction of scientific names retrieved from all the scientific names in a document. These metrics are widely used in the field of information retrieval and text mining, making our evaluation representative of standard practices in the literature.\n\nAdditionally, we reported the F-score, which is the harmonic mean of precision and recall. The F-score provides a single metric that balances both precision and recall, giving a comprehensive view of the system's performance. This is particularly useful when comparing different algorithms or configurations, as it allows for a straightforward comparison.\n\nWe also compared our results with other tools and approaches, such as TaxonFinder and FAT, using the same metrics. This comparison helps to contextualize our results and demonstrate the relative performance of our system. Furthermore, we evaluated the impact of using a stop-list during the pre-filtering process, which can affect both precision and recall.\n\nIn summary, our use of precision, recall, and F-score as performance metrics aligns with established practices in the field. These metrics provide a clear and comprehensive evaluation of our system's ability to accurately identify scientific names in text.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of various machine learning algorithms for scientific name recognition. We compared the precision and recall values of Naïve Bayes and Maximum Entropy classification algorithms with different parameter estimation methods, such as GIS, IIS, and L-BFGS. These comparisons were conducted on the manually annotated \"American Seashells\" book, which served as our benchmark dataset.\n\nAdditionally, we included a comparison with the Decision Tree Learning algorithm implemented in the NLTK toolkit. For this evaluation, we used a context span of 1, which corresponds to features derived from a word on either side of the candidate name. This configuration yielded higher recall with good precision (greater than 0.8).\n\nWe also performed comparisons both with and without the use of a stop-list of English words as part of the pre-filtering process. The results indicated that all algorithms, except for the Decision Tree learning algorithm, performed better with a stop-list, achieving higher precision without significantly impacting recall. The stop-list helps eliminate common English words, generating a cleaner set of candidate names.\n\nFurthermore, we compared our results with those of publicly available methods, including the dictionary-based TaxonFinder and the FAT tool integrated into the GoldenGATE editor. These comparisons were summarized in Table 1, where NetiNeti demonstrated superior performance in terms of both precision and recall.\n\nIn summary, our evaluation included comparisons with both publicly available methods and simpler baselines, providing a comprehensive assessment of the different algorithms' effectiveness in scientific name recognition.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}