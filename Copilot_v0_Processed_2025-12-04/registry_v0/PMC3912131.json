{
  "publication/title": "Classifying Microbial Communities by Bacterial Vaginosis",
  "publication/authors": "The authors who contributed to the article are David B. and Jacob A. F. Both authors were involved in conceiving and designing the experiments, analyzing the data, contributing reagents/materials/analysis tools, and writing the paper.",
  "publication/journal": "PLOS ONE",
  "publication/year": "2014",
  "publication/doi": "10.1371/journal.pone.0087830",
  "publication/tags": "- Machine Learning\n- Microbial Communities\n- Bacterial Vaginosis\n- Classification Models\n- Genetic Programming\n- Random Forests\n- Logistic Regression\n- Microbial Taxa\n- Vaginal Microbiome\n- Cross-Validation",
  "dataset/provenance": "The datasets used in this study were drawn from two previously published studies. The first dataset is from a study by Ravel et al. published in 2011, which sampled the microbiome of 396 asymptomatic women. The second dataset is from a study by Srinivasan et al. published in 2012, which sampled the microbiome of 220 women. Both studies used Roche’s 454 FLX sequencer to amplify and sequence different variable regions of the 16S rRNA gene. The Ravel et al. study sequenced the V1–V2 regions, while the Srinivasan et al. study sequenced the V3–V4 regions. The Ravel et al. study classified reads at the genus level using the RDP classifier, while the Srinivasan et al. study classified reads at the species or genus level using pplacer. The Ravel et al. study identified 282 microbial taxa, with 97 samples classified as BV+ using a Nugent score definition. The Srinivasan et al. study identified 155 unique microbial taxa, with 97 samples classified as BV+ using Amsel criteria and 117 samples classified as BV+ using Nugent score. These datasets have been used in the original studies and are now being reanalyzed using different machine-learning techniques to classify microbial communities by bacterial vaginosis characteristics.",
  "dataset/splits": "Two different datasets were used in this study. The first dataset, from a study by Ravel et al., included samples from 396 asymptomatic women. Out of these, 97 samples were identified as BV+ using the Nugent score definition. The second dataset, from a study by Srinivasan et al., included samples from 220 women. In this dataset, 97 women were BV+ using the Amsel criteria, and 117 women were BV+ using the Nugent score definition. The datasets were used to train and evaluate classification models, with the specific splits and distributions of data points as described.",
  "dataset/redundancy": "To avoid model overfitting, we employed a ten-fold cross-validation approach. This method involves randomly dividing the data into ten distinct parts. For each iteration of the cross-validation process, nine of these parts are used to train the model, while the remaining part is reserved for testing the model's performance. This procedure is repeated ten times, with each of the ten parts serving as the testing set exactly once. The accuracy of the model in classifying the testing samples is then averaged across all ten iterations to obtain a measure of the model's overall accuracy.\n\nThis approach ensures that the training and test sets are independent in each iteration, as no sample from the test set is used in the training process. The distribution of the data in each fold is designed to be representative of the entire dataset, maintaining the same proportions of diseased and non-diseased samples. This method is widely used in machine learning to provide a robust estimate of a model's performance on unseen data.\n\nThe datasets used in this study were drawn from two previously published studies. The first dataset, from Ravel et al., included samples from 396 asymptomatic women, with 97 classified as BV+ using the Nugent score. The second dataset, from Srinivasan et al., included samples from 220 women, with 97 classified as BV+ using Amsel criteria and 117 using the Nugent score. Both studies used Roche’s 454 FLX sequencer for sequencing, but they amplified different regions of the 16S rRNA gene and used different methods for classifying reads into taxonomic groups. This diversity in the datasets helps to ensure that the models are generalizable and not overly dependent on the specifics of any single dataset.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in this study falls under the category of machine learning algorithms known as ensemble methods and evolutionary algorithms. Specifically, three different machine learning algorithms were employed: genetic programming (GP), random forests (RF), and logistic regression (LR).\n\nGenetic programming is an evolutionary algorithm that uses computational analogs of evolutionary processes to search for highly fit models. It is not a new algorithm, but rather a well-established method in the field of machine learning and evolutionary computation. The GP algorithm used in this study is implemented in C++ and involves processes such as tournament selection, mutation, and crossover to evolve a population of decision tree classifiers. The fitness of each model is evaluated based on its classification accuracy and model complexity, with the goal of minimizing the number of incorrectly classified samples and penalizing larger models.\n\nRandom forests is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes output by individual trees. It is also a well-established algorithm in the machine learning community. The random forest classifiers in this study were implemented using the R package randomForest with default parameters.\n\nLogistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible classes). It is a widely used and well-understood algorithm in both statistics and machine learning. In this study, logistic regression was implemented using the R package glmnet, which employs a maximum likelihood method and lasso for feature selection.\n\nThe choice of these algorithms was driven by their proven effectiveness in discovering complex interactions in biological data, rather than the development of a new algorithm. The focus of this study is on applying these established methods to the specific problem of classifying microbial communities associated with bacterial vaginosis (BV), rather than on the development of new machine-learning techniques. Therefore, the algorithms were not published in a machine-learning journal but rather in a journal focused on the biological application of these methods.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, we began by collapsing many of the microbes into groups based on their correlations. This step was crucial for reducing the number of factors and increasing the interpretability of our classification models. We used the sparCC method to calculate pairwise correlations on microbial relative abundances, converting these correlations into dissimilarities by subtracting the magnitude of the correlation from one. We then employed average hierarchical clustering and a dynamic tree-cutting algorithm to break the microbes into correlation groups. Specifically, we used the function cutreeDynamic from the R package dynamicTreeCut with a 0.9 cut height and a three taxa minimum group size. This cutoff was chosen to account for nearly all of the correlation present between microbes. Uncorrelated microbes were left as individuals, and a single feature in the dataset represented each correlated microbe group.\n\nBefore generating classification models, we used two different datasets to train and evaluate the models: one from Srinivasan et al. and one from Ravel et al. These datasets produced different correlated microbe groups, although there was some similarity between them. For instance, CG1 in the Srinivasan et al. dataset shared many microbes with CG4 in the Ravel et al. dataset.\n\nWe implemented three machine learning algorithms: genetic programming (GP), random forests (RF), and logistic regression (LR). Each algorithm had its own method for selecting and weighting features, so our analysis of feature usage was algorithm-specific. For GP, we used a fitness function that minimized the number of incorrectly classified samples and penalized larger models. For RF, we used the randomForest function in R with default parameters. For LR, we employed a maximum likelihood method implemented in the R package glmnet, using the lasso for feature selection.\n\nTo ensure the robustness of our models, we used ten-fold cross-validation. This involved randomly breaking the data into ten parts, using nine parts to train the model and the remaining part to test its performance. This process was repeated ten times, with each part used as the testing data once. The accuracy of each model was then averaged over the ten datasets to obtain a measure of its performance. This approach helped us avoid overfitting and provided a reliable estimate of how well our models would perform with new data.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the machine learning algorithm employed. For the logistic regression model, the number of parameters was determined by the features selected through the lasso method implemented in the glmnet package. This method automatically selects a subset of features, effectively reducing the number of parameters in the model.\n\nFor the random forest classifier, the number of parameters was influenced by the features that contributed to the increase in node purity across all trees in the forest. The importance of each feature was averaged over all trees to determine its total contribution to the classification model.\n\nIn the case of genetic programming, the number of parameters was more flexible and could vary significantly. The genetic programming algorithm searches for highly fit models by combining substructures from multiple parent models and modifying individual models randomly. The best model in the final population tended to be a very good predictor of BV, but the number of parameters could be large and varied.\n\nTo reduce the number of parameters and increase the interpretability of our results, we collapsed highly correlated microbes into groups. This was done by calculating pairwise correlations on microbial relative abundances using sparCC and then converting these correlations into dissimilarities. We used average hierarchical clustering and a dynamic tree-cutting algorithm to break the microbes into correlation groups, with a cut height of 0.9 and a minimum group size of three taxa. This process ensured that nearly all the correlation present between microbes was accounted for, thereby reducing the number of parameters in our models.",
  "optimization/features": "In our study, we utilized a substantial number of features as inputs for our classification models. Initially, we had a large set of microbial taxa, but to enhance interpretability and reduce the number of parameters, we collapsed highly correlated microbes into groups. This process resulted in a more manageable set of features for our models.\n\nFeature selection was indeed performed, but it was integrated into the model-building process rather than as a separate step. For the genetic programming (GP) classifier, feature importance was determined by varying the values of each feature individually and observing the impact on sample classification. This method allowed us to identify which features were crucial for the model's decisions.\n\nFor the random forest (RF) classifier, feature importance was assessed by measuring the increase in node purity, which indicates how much each feature contributes to separating the samples into diseased and non-diseased categories. This was averaged over all trees in the forest to determine the overall importance of each feature.\n\nIn the case of logistic regression (LR), feature importance was ranked by the magnitude of the mean coefficient across cross-validation replicates, divided by the standard deviation. This approach helped us understand which features were most influential in the logistic model.\n\nIt is important to note that the feature selection process was conducted using the training set only, ensuring that the models were not biased by the testing data. This approach helped us maintain the integrity of our cross-validation process and provided a more reliable measure of model performance.",
  "optimization/fitting": "In our study, we employed three machine learning algorithms—genetic programming (GP), random forests (RF), and logistic regression (LR)—to classify microbial communities into BV+ and BV2 categories. Each method has its own approach to handling the complexity of the data and the potential for overfitting or underfitting.\n\nFor GP, the model complexity can be very high due to its flexibility in creating decision trees. To mitigate overfitting, we used a fitness function that penalized larger models, encouraging simpler and more generalizable solutions. Additionally, we repeated the analysis ten times and selected the model with the highest training fitness for evaluation with the testing dataset. This approach helped to ensure that the model was not overly complex and could generalize well to new data.\n\nRandom forests, being an ensemble method, inherently reduces the risk of overfitting by averaging the results of multiple decision trees. Each tree is built using a random subset of features and samples, which helps to capture a broader range of patterns in the data. To further ensure robustness, we used ten-fold cross-validation. This technique involves splitting the data into ten parts, training the model on nine parts, and testing it on the remaining part. This process is repeated ten times, with each part serving as the test set once. The average accuracy across all folds provides a reliable estimate of the model's performance on new data.\n\nLogistic regression, particularly when implemented with the lasso (least absolute shrinkage and selection operator), is designed to handle high-dimensional data by performing feature selection. The lasso adds a penalty term to the regression equation, which shrinks some coefficients to zero, effectively selecting a subset of the most important features. This not only helps in reducing overfitting but also makes the model more interpretable. We used the glmnet package in R, which implements the lasso and other regularization paths, to build our logistic regression models. The maximum likelihood method ensures that the model parameters are estimated in a way that maximizes the probability of the observed data, further enhancing the model's reliability.\n\nIn summary, we addressed the potential for overfitting through model regularization, ensemble methods, and cross-validation. These techniques ensured that our models were not only accurate but also generalizable to new datasets. Underfitting was mitigated by the flexibility of GP and RF, which can capture complex interactions in the data, and by the feature selection capabilities of logistic regression with the lasso.",
  "optimization/regularization": "In our study, we employed several techniques to prevent over-fitting and ensure the robustness of our models. One of the key methods used was ten-fold cross-validation. This technique involves randomly dividing the data into ten parts, using nine parts to train the model and the remaining part to test its performance. This process is repeated ten times, with each part serving as the testing data once. The accuracy of the model is then averaged over these ten datasets, providing a reliable measure of the model's performance with new data.\n\nAdditionally, we used regularization methods in our logistic regression model. Specifically, we implemented the lasso (least absolute shrinkage and selection operator) technique, which adds a penalty equal to the absolute value of the magnitude of coefficients. This method helps to shrink some coefficients to zero, effectively performing feature selection and reducing the complexity of the model. By doing so, it helps to prevent over-fitting by simplifying the model and focusing on the most relevant features.\n\nFurthermore, we reduced the number of parameters and increased the interpretability of our results by collapsing highly correlated microbes into groups. We calculated pairwise correlations on microbial relative abundances using a method called sparCC. These correlations were then converted into dissimilarities and clustered using average hierarchical clustering and a dynamic tree-cutting algorithm. This approach ensured that our models were not overly complex and were more interpretable, further aiding in the prevention of over-fitting.",
  "optimization/config": "In our study, we have provided detailed information about the configuration and parameters used for our classifiers. For the genetic programming (GP) classifier, we implemented it in C++ and reported many of the parameters used in Table 2 of our publication. These parameters include details about the tournament selection process, mutation, crossover, and the fitness calculation method. We also mentioned that due to the high variability in GP results, we repeated the analysis ten times and selected the model with the highest training fitness for evaluation.\n\nFor the random forest (RF) classifiers, we used the R package `randomForest` with default parameters. This ensures reproducibility, as anyone can use the same package and default settings to replicate our results.\n\nRegarding the logistic regression (LR) models, we used the R package `glmnet` with a maximum likelihood method. The final model was parameterized to maximize the probability that the set of features was associated with BV. The features were selected using the lasso method implemented in `glmnet`.\n\nThe datasets used in our study are from published works by Ravel et al. and Srinivasan et al. The details of these datasets, including the sequencing methods and classification techniques, are thoroughly described in the \"Materials and Methods\" section. We also discussed the preprocessing steps, such as collapsing microbes into groups based on correlations, which were done to reduce the number of factors and increase interpretability.\n\nAll the code and datasets used in this study are available upon request. However, specific model files and optimization parameters are not explicitly provided in the publication. Interested researchers can contact the authors for access to the detailed configurations and any additional information needed to replicate the study. The datasets themselves are drawn from publicly available studies, and the methods used are standard and well-documented in the field.",
  "model/interpretability": "The models employed in this study vary in their interpretability, ranging from highly transparent to more opaque.\n\nLogistic regression (LR) is the most interpretable model among the three used. It fits a linear model to the data, producing a straightforward combination of features and regression coefficients. The magnitude of these coefficients indicates the importance of each feature in diagnosing BV. This makes it easy to understand which microbial populations or patient behaviors are most closely associated with the condition. The LR model's transparency allows for clear insights into feature importance and the relationships between variables.\n\nRandom forests (RF), while not as transparent as LR, still offer a good level of interpretability. RF is an ensemble technique that builds a population of tree classifiers. The importance of each feature can be determined by measuring the increase in node purity, which indicates how much each feature contributes to the separation of samples into BV+ and BV2 categories. This method provides a clear way to rank features by their importance, making it relatively easy to extract and understand the key factors influencing the model's predictions.\n\nGenetic programming (GP), on the other hand, is the least interpretable of the three models. GP uses evolutionary processes to search for highly fit models, often resulting in complex decision trees that are difficult to interpret. The models produced by GP can be very large and intricate, making it challenging to understand how individual features contribute to the final classification. While GP is highly flexible and can capture complex relationships, its stochastic nature and the size of the resulting models make it less transparent compared to LR and RF.",
  "model/output": "The model is a classification model. It is designed to partition samples into two categories: diseased (BV+) and non-diseased (BV-). The accuracy of the models is measured as the percentage of correctly classified samples. The models were built using three different machine learning algorithms: genetic programming (GP), random forests (RF), and logistic regression (LR). These algorithms were applied to classify microbial communities into BV+ and BV- categories based on vaginal microbiome data and associated environmental factors. The performance of the models was evaluated using receiver operator curves (ROCs), which show the trade-off between the true positive rate and the false positive rate. The models demonstrated high accuracy, often exceeding 80%, indicating a strong association between certain features in the dataset and BV status. The top features contributing to the classification were identified and ranked based on their importance to each model. The models were also evaluated for their computational efficiency, with LR and RF being relatively quick, while GP took several hours longer. The study highlights the feasibility of using classification models to identify important microbial community features related to BV, although it also notes the complexities involved in designing future studies.",
  "model/duration": "The execution time varied significantly among the different machine learning techniques used. Logistic regression (LR) and random forests (RF) were relatively quick, typically completing in less than an hour on a single laptop. This efficiency makes them suitable for scenarios where computational resources are limited or time is a critical factor. On the other hand, genetic programming (GP) required several hours longer to complete. This increased computational time is due to the stochastic nature of GP, which involves evolutionary processes to search for highly fit models. The models produced by GP can be very large and complex, contributing to the longer execution time. Despite the longer runtime, GP offers flexibility and the potential for highly accurate models. The choice of technique may depend on the specific needs of the study, balancing the trade-offs between accuracy, interpretability, and computational efficiency.",
  "model/availability": "The source code for the genetic programming classifier was implemented in C++. However, it is not clear if this source code has been publicly released.\n\nThe random forest classifiers were implemented using the R package randomForest. This package is publicly available and can be installed from the Comprehensive R Archive Network (CRAN). The package is licensed under the GNU General Public License (GPL), which allows for free use, modification, and distribution of the software.\n\nNot applicable.",
  "evaluation/method": "To evaluate the classification models, we employed ten-fold cross-validation. This method helps to avoid model overfitting and provides an indication of how well the model is expected to perform with new data. The data was randomly divided into ten parts, with nine parts used for training the model and the remaining part used for testing. This process was repeated ten times, each time using a different part as the testing data. The accuracy of the model in classifying the testing samples was then averaged over the ten datasets to obtain a measure of the accuracy for each machine-learning technique.\n\nWe also used receiver operator curves (ROCs) to evaluate the performance of the models. ROCs show the performance of the model at classifying both BV+ and BV- samples, allowing us to simultaneously compare the type 1 and type 2 errors for each model. A perfect model would have a curve that forms a right angle in the upper left of the ROC. More accurate models have a true positive rate closer to 1 and a false positive rate closer to 0.\n\nAdditionally, we ranked the features by their importance to each model. For the random forest and logistic regression models, feature importance was determined by the increase in node purity and the magnitude of the mean coefficient across cross-validation replicates, respectively. For the genetic programming models, we varied the values for each feature individually in every sample and determined whether varying the feature value changed the classification of the sample. This resulted in a summary value describing the importance of each feature. The top fifteen important features for each classification technique were identified and compared.",
  "evaluation/measure": "In our study, we primarily focused on two key aspects of the classification models: classification accuracy and feature usage. To evaluate the performance of our models, we employed receiver operator curves (ROCs). ROCs are a graphical representation that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. They show the performance of the model at classifying both BV+ and BV- samples, allowing us to simultaneously compare the type 1 and type 2 errors for each model. A perfect model would have a curve that forms a right angle in the upper left of the ROC, indicating a true positive rate of 1 and a false positive rate of 0.\n\nWe measured accuracy as the percentage of correctly classified samples. The accuracy of the models is a measure of how well they partition samples into diseased and non-diseased categories. The results showed that both logistic regression (LR) and random forests (RF) tended to outperform genetic programming (GP). However, the accuracy of all the machine-learning techniques was remarkably similar. RF and LR models obtained accuracies consistently between 90% and 95% when classifying on Nugent score BV. GP models often classified samples with similar accuracies, but high variation between GP models reduced the average GP accuracy. The models performed slightly worse when classifying on Amsel criteria BV, but all three techniques obtained accuracies above 80%.\n\nThe set of metrics used in this study is representative of common practices in the literature for evaluating classification models, particularly in the context of microbial community analysis. ROC curves and accuracy metrics are widely used to assess the performance of binary classifiers, providing a comprehensive view of the model's ability to distinguish between different classes. Additionally, the focus on feature importance aligns with the need to understand which microbial features are most relevant to the classification task, which is a critical aspect of interpretability in biological studies.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating the performance of three specific machine learning algorithms—genetic programming (GP), random forests (RF), and logistic regression (LR)—on two distinct datasets related to bacterial vaginosis (BV). These datasets were drawn from studies published by Ravel et al. and Srinivasan et al., each with its own methodologies and sample characteristics.\n\nWe did not compare our methods to simpler baselines. Our primary goal was to assess the effectiveness of GP, RF, and LR in classifying microbial communities into BV+ and BV- categories. We measured the accuracy of these models using receiver operator curves (ROCs), which allowed us to evaluate the performance of each model in classifying both BV+ and BV- samples. This approach provided a comprehensive view of the models' abilities to handle type 1 and type 2 errors simultaneously.\n\nThe accuracy of the models was remarkably similar across the different techniques, with RF and LR generally outperforming GP. However, GP models showed high variation in accuracy, particularly when classifying based on the Nugent score. This variation is likely due to the stochastic nature of GP and its ability to explore a wide range of possible models.\n\nIn summary, while we did not compare our methods to publicly available benchmarks or simpler baselines, our study demonstrated the feasibility of using GP, RF, and LR for classifying microbial communities related to BV. The high accuracy of these models indicates the presence of significant features in the datasets that are associated with BV.",
  "evaluation/confidence": "The evaluation of our classification models focused on accuracy and feature usage. We measured accuracy as the percentage of correctly classified samples, using receiver operator curves (ROCs) to evaluate performance. These ROCs allowed us to compare both type 1 and type 2 errors for each model.\n\nThe accuracy of our models was remarkably high, often exceeding 80% regardless of the dataset or classification technique used. This high accuracy indicates the presence of some signal of bacterial vaginosis (BV) in the dataset, suggesting that certain features are associated with BV.\n\nWe implemented three different classification techniques: Genetic Programming (GP), Logistic Regression (LR), and Random Forest (RF). While LR and RF tended to outperform GP, the overall accuracy of all techniques was quite similar. For instance, RF and LR models achieved accuracies consistently between 90% and 95% when classifying based on Nugent score BV. GP models also performed well but showed higher variation between different models, which reduced their average accuracy.\n\nTo assess the statistical significance of our results, we repeated the GP analysis ten times due to its high variability. The model with the highest training fitness was selected for evaluation with the testing dataset. This approach helped to ensure that our results were robust and not due to random chance.\n\nWe also identified the top fifteen important features for each classification technique. While there was some overlap in the important features identified by different techniques, the specific features varied. For example, in the Srinivasan et al. dataset, the Nugent score was the only important feature shared by all techniques when classifying based on Amsel criteria BV. When classifying based on Nugent score BV, the whiff test and CG2 were important to all three techniques.\n\nIn summary, our evaluation metrics provide a strong indication of the effectiveness of our classification models. The high accuracy and the identification of important features suggest that our methods are reliable for classifying microbial communities associated with BV. However, further studies and comparisons with different datasets and microbial community characteristics would be beneficial to validate these findings and determine the generalizability of our approach.",
  "evaluation/availability": "Not enough information is available."
}