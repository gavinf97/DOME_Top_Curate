{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BioMed Research International",
  "publication/year": "2014",
  "publication/doi": "10.1155/2014/294279",
  "publication/tags": "- DNA-binding proteins\n- Predictive performance\n- Machine learning algorithms\n- Benchmark dataset\n- Negative samples\n- Evaluation metrics\n- Sensitivity\n- Specificity\n- Accuracy\n- Matthew’s correlation coefficient\n- F1 Measure\n- Independent datasets\n- Training dataset\n- Validation dataset\n- Automated methods\n- Support vector machine\n- Random Tree\n- Artificial neural network\n- Web-server guide\n- Experimental methods",
  "dataset/provenance": "The dataset used in our study is derived from multiple sources to ensure a comprehensive and diverse set of samples. The benchmark dataset, which is the foundation of our analysis, includes 146 DNA-binding proteins and 250 non-DNA-binding proteins. These proteins were obtained from the work of Kumar et al. and Stawiski et al. The expanded benchmark dataset, used to analyze the impact of negative samples, includes the same 146 DNA-binding proteins but is augmented with 2125 non-DNA-binding proteins. These additional non-DNA-binding proteins were extracted from the latest release of the Protein Data Bank (PDB) and processed to ensure a pairwise sequence identity cutoff of 25%.\n\nTwo independent datasets were also utilized to evaluate the performance of our method. Independent dataset1, obtained from Wang and Brown, consists of 82 DNA-binding proteins and 100 non-DNA-binding proteins. Independent dataset2 was constructed from the work of Kumar et al., containing 770 DNA-binding proteins and 815 non-DNA-binding proteins. Both independent datasets underwent a filtering process to remove sequences with ≥40% pairwise sequence identity to any sequence in the benchmark or expanded benchmark datasets, ensuring the robustness of our evaluations.\n\nThe datasets used in this study have been previously utilized in the community, with the benchmark dataset being a standard in the field. The expanded benchmark dataset and the independent datasets were constructed to provide a more rigorous evaluation of our method, enDNA-Prot, and to demonstrate its superiority over existing methods. The complete lists of codes and sequences for all datasets are available in the supplementary materials, ensuring transparency and reproducibility of our results.",
  "dataset/splits": "There are four distinct datasets used in the study. The first dataset is the benchmark dataset, which consists of 146 DNA-binding proteins and 250 non-DNA-binding proteins. This dataset is used to train the method.\n\nThe second dataset is the expanded benchmark dataset, which includes the same 146 DNA-binding proteins but is augmented with 2125 non-DNA-binding proteins. This expanded dataset is used to analyze the impact of the number of negative samples on the performance of the method.\n\nThe third dataset is the independent dataset 1, which contains 82 DNA-binding proteins and 100 non-DNA-binding proteins. This dataset is used to evaluate the performance of the method on an independent set of data.\n\nThe fourth dataset is the independent dataset 2, which comprises 770 DNA-binding proteins and 815 non-DNA-binding proteins. This dataset is also used for independent evaluation of the method's performance.\n\nThe distribution of data points in each dataset is as follows:\n\n* Benchmark dataset: 146 DNA-binding proteins and 250 non-DNA-binding proteins.\n* Expanded benchmark dataset: 146 DNA-binding proteins and 2125 non-DNA-binding proteins.\n* Independent dataset 1: 82 DNA-binding proteins and 100 non-DNA-binding proteins.\n* Independent dataset 2: 770 DNA-binding proteins and 815 non-DNA-binding proteins.",
  "dataset/redundancy": "The datasets used in this study were carefully constructed to ensure independence and to avoid overestimating the predictive performance of the methods. The datasets were split into several subsets, including a benchmark dataset, an expanded benchmark dataset, and two independent datasets.\n\nThe benchmark dataset was used to train the model and consists of 146 DNA-binding proteins and 250 non-DNA-binding proteins. This dataset was obtained from previous work and has a pairwise sequence identity cutoff of 25%.\n\nTo analyze the influence of the number of negative samples, an expanded benchmark dataset was created by adding more non-DNA-binding proteins to the benchmark dataset. This expanded dataset contains 146 DNA-binding proteins and 2125 non-DNA-binding proteins.\n\nTwo independent datasets were also constructed to evaluate the performance of the methods. The first independent dataset, obtained from a previous study, initially contained 92 DNA-binding proteins and 100 non-DNA-binding proteins. To ensure independence, sequences with 40% or greater pairwise sequence identity to any sequence in the benchmark or expanded benchmark datasets were removed. The final independent dataset 1 consists of 82 DNA-binding proteins and 100 non-DNA-binding proteins.\n\nThe second independent dataset was constructed by collecting 823 DNA-binding domains and 823 non-DNA-binding domains from a previous study. The DNA-binding domains were extracted from a specific dataset using keywords and a 25% pairwise sequence identity cutoff. The non-DNA-binding domains were randomly selected with the same cutoff. Sequences with 40% or greater pairwise sequence identity to any sequence in the benchmark or expanded benchmark datasets were removed. The final independent dataset 2 consists of 770 DNA-binding proteins and 815 non-DNA-binding proteins.\n\nThe distribution of these datasets compares favorably to previously published machine learning datasets in the field, ensuring a robust evaluation of the methods. The independence of the training and test sets was enforced by removing sequences with significant similarity to those in the training datasets, thereby preventing data leakage and ensuring an unbiased assessment of the model's performance.",
  "dataset/availability": "The datasets used in this study are made available to the public. The complete list of all the codes and sequences for the benchmark dataset, expanded benchmark dataset, independent dataset 1, and independent dataset 2 can be found in Supplementary Material S1, S2, S3, and S4, respectively. These supplementary materials can be downloaded from a specific URL. The datasets include DNA-binding proteins and non-DNA-binding proteins, with detailed information on their sequences and codes. The datasets are constructed with a pairwise sequence identity cutoff of 25% to ensure diversity. Additionally, sequences with 40% or more pairwise sequence identity to any sequence in the benchmark or expanded benchmark datasets were removed to avoid overestimating the method's performance. The datasets are designed to evaluate the predictive performance of enDNA-Prot and other methods, ensuring transparency and reproducibility in the research.",
  "optimization/algorithm": "The machine-learning algorithm class used is ensemble learning, specifically a variant of boosting known as AdaBoost. The algorithm employed is not a standard AdaBoost but an improved version called Unbalanced-AdaBoost. This variant was developed to address the imbalance in the dataset, which contains a sufficient number of negative samples and a small amount of positive samples.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of the study is on its application to a specific problem in bioinformatics, particularly the prediction of DNA-binding proteins. The development and optimization of Unbalanced-AdaBoost were tailored to enhance the performance of the enDNA-Prot predictor, which is the primary subject of the research. The improvements made to AdaBoost were necessary to handle the unique challenges posed by the imbalanced dataset in this biological context.",
  "optimization/meta": "The model employs an ensemble learning approach, specifically using a meta-predictor strategy known as stacking. This method involves training multiple base learners using different learning algorithms and then combining their outputs with a second-level learner, referred to as a metalearner.\n\nThe base learners utilized in this ensemble include a variety of machine-learning algorithms, such as classifiers based on trees, k-nearest neighbors (KNN), rules, and functions. Specifically, the base learners consist of algorithms like IB1, IB5, IB15, J48graft, JRip, J48, NNge, PART, RandomForest, RandomTree, REPTree, Ridor, SimpleCart, SMO, and others. These diverse algorithms contribute to the robustness and accuracy of the ensemble classifier by leveraging their individual strengths.\n\nThe training data for the base learners is constructed by combining positive samples with negative samples that are sampled with different weight distributions. This process ensures that the base learners are trained on varied datasets, enhancing their ability to generalize and improve predictive performance.\n\nThe metalearner, which combines the outputs of the base learners, uses a weighted vote rule to make the final prediction. This approach helps to mitigate the risk of overfitting and ensures that the model benefits from the collective wisdom of the diverse base learners.\n\nThe independence of the training data is maintained by using different subsets of the negative samples for each base learner. This subsampling with replacement ensures that the training data for each base learner is independent, which is crucial for the effectiveness of the ensemble learning approach.",
  "optimization/encoding": "In our study, each protein sequence was encoded into a feature vector with a dimension of 188. This encoding process involved extracting features solely from the protein sequence. The feature vector was constructed by calculating a feature vector with a dimension of 21 for each physicochemical property, resulting in a comprehensive feature set that captures the essential characteristics of the protein sequences.\n\nThe encoded data was then pre-processed and fed into an ensemble classifier. This classifier was constructed using 20 different machine learning algorithms, leveraging the strengths of multiple learning methods to enhance predictive performance. The ensemble approach allowed us to effectively utilize the large number of negative samples in the dataset, ensuring that the model could generalize well to both balanced and unbalanced datasets.",
  "optimization/parameters": "In the optimization process of our method, the number of parameters, denoted as p, is determined by the structure of our model and the specific learning algorithms employed. Our model utilizes a combination of positive and negative training datasets, with the negative samples being weighted and sampled iteratively. The base learning algorithm consists of 20 different learners, and the number of learning rounds is set to 20. This iterative process involves adjusting weights based on the performance of each base learner, which indirectly influences the effective number of parameters considered during training.\n\nThe selection of p is not explicitly stated as a fixed number but is rather a result of the iterative boosting process. The Unbalanced-AdaBoost algorithm, which is central to our method, dynamically adjusts the weights of the negative samples and combines multiple weak learners to form a strong classifier. This process inherently determines the effective number of parameters used in the model. The voting weights of the base learners are calculated based on their error rates, and these weights are updated in each learning round, contributing to the overall complexity and parameter count of the model.\n\nIn summary, while the exact number of parameters p is not explicitly defined, it is influenced by the number of base learners (20) and the number of learning rounds (20). The dynamic nature of the Unbalanced-AdaBoost algorithm means that the effective number of parameters is adaptively determined during the training process.",
  "optimization/features": "The input features for the enDNA-Prot method are derived from the sequences of proteins, focusing on the composition, distribution, and physicochemical properties of the amino acids. Specifically, the feature vector includes the composition part, which represents the occurrences of the 20 standard amino acids normalized by the sequence length. Additionally, the properties such as content, distribution, and dipeptide composition are considered. Each physicochemical property contributes to a feature vector with a dimension of 21, resulting in a final feature vector with a dimension of 188 after calculating all properties.\n\nFeature selection was not explicitly mentioned as a separate step in the process. The features were constructed based on established methods and properties, ensuring that the relevant information from the protein sequences is captured. The construction of the feature vector was inspired by previous works and designed to maximize the predictive performance of the model. The features were derived from the sequences in a way that avoids overfitting and ensures that the model can generalize well to independent datasets.",
  "optimization/fitting": "Not applicable.",
  "optimization/regularization": "In our study, we employed a regularization method to prevent overfitting. Specifically, we utilized the Unbalanced-AdaBoost algorithm, which is designed to handle imbalanced datasets effectively. This algorithm incorporates a weight distribution mechanism that focuses more on the minority class (positive samples) during training, thereby reducing the risk of overfitting to the majority class (negative samples).\n\nThe Unbalanced-AdaBoost algorithm works by iteratively training base learners on weighted samples, where the weights are adjusted based on the performance of previous learners. This process ensures that the model pays more attention to the samples that are harder to classify, leading to a more robust and generalizable model.\n\nAdditionally, we conducted experiments to analyze the impact of the number of negative samples on the performance of our method. By varying the number of negative samples in the training dataset, we observed that the predictive performance improved up to a certain point and then stabilized. This finding suggests that an optimal number of negative samples can help in achieving the best predictive performance while preventing overfitting.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model enDNA-Prot is not explicitly described as a blackbox or transparent model. However, the evaluation metrics and performance data suggest a focus on quantitative assessment rather than interpretability. The model's performance is evaluated using metrics such as sensitivity, specificity, accuracy, Matthew’s correlation coefficient, and F1 Measure. These metrics indicate a strong emphasis on predictive accuracy and reliability.\n\nThe experiments conducted to evaluate enDNA-Prot involved testing its predictive performance on independent datasets and analyzing the impact of the number of negative samples in the benchmark dataset. The results are presented in tables and figures, showing improvements in accuracy and other metrics when the training dataset is expanded with more negative samples. This approach highlights the model's effectiveness in handling unbalanced datasets but does not provide insights into how the model makes its predictions.\n\nThe use of ensemble methods and the comparison with other methods like DNAbinder, DNA-Prot, and iDNA-Prot further suggest a focus on performance optimization rather than interpretability. The detailed step-by-step guide for using the web-server indicates that the model is designed for practical application, but it does not offer explanations for the model's decisions.\n\nIn summary, while enDNA-Prot demonstrates strong predictive performance, it does not provide clear examples or explanations of how it arrives at its predictions. The model appears to be more of a blackbox, prioritizing accuracy and reliability over interpretability.",
  "model/output": "The model, enDNA-Prot, is a classification model. It is designed to predict whether a given protein sequence is a DNA-binding protein or a non-DNA-binding protein. The model's output is a binary classification, indicating the predicted class of the input protein sequence.\n\nThe performance of enDNA-Prot is evaluated using several metrics, including sensitivity (SE), specificity (SP), accuracy (ACC), Matthew’s correlation coefficient (MCC), and F1 Measure (F1 M). These metrics provide a comprehensive evaluation of the model's predictive performance.\n\nThe model employs an ensemble classification strategy, specifically an AdaBoost ensemble strategy, which involves training multiple base classifiers and combining their outputs to make a final prediction. This approach helps to improve the overall predictive performance and robustness of the model.\n\nThe model's performance was tested on two independent datasets, and the results demonstrated that enDNA-Prot outperforms other existing methods in terms of accuracy and MCC. This indicates that enDNA-Prot is effective in handling unbalanced datasets and provides reliable predictions for DNA-binding proteins.\n\nFor practical use, a web-server guide is provided to help users input their protein sequences in FASTA format and obtain the prediction results. The web-server is designed to be user-friendly, allowing experimental scientists to easily utilize the model for their research.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code of enDNA-Prot is available for download. Users can access it through the homepage of the web-server. Additionally, a detailed step-by-step guide is provided to help users utilize the web-server effectively. The web-server can be accessed at http://bioinformatics.hitsz.edu.cn/Ensemble-DNA-Prot/. This platform allows users to input query protein sequences in FASTA format, with a limit of no more than 50 sequences at a time. The server processes these sequences and provides results indicating whether the inputted query protein sequences are DNA-binding proteins or non-DNA-binding proteins. For those who need further assistance or have questions regarding the predictor or the web-server, there is a contact option available on the homepage to reach out for support.",
  "evaluation/method": "The evaluation of the method involved a series of experiments designed to assess its predictive performance. Initially, the method was tested on two independent datasets to evaluate its predictive performance when trained with a benchmark dataset. This step was crucial for understanding how well the method generalizes to unseen data.\n\nTo ensure a fair and objective evaluation, the method was further tested on another independent dataset. This additional testing helped to confirm the method's robustness and reliability.\n\nThe evaluation metrics used included sensitivity (SE), specificity (SP), accuracy (ACC), Matthew’s correlation coefficient (MCC), and F1 Measure (F1-M). These metrics provided a comprehensive assessment of the method's performance, covering various aspects of its predictive capabilities.\n\nAdditionally, the impact of the number of negative samples in the benchmark dataset on the method's performance was analyzed. This involved constructing multiple training datasets with varying numbers of negative samples and evaluating the method's performance on a validation dataset. The results showed that increasing the number of negative samples up to a certain point improved the method's performance, indicating the importance of a balanced dataset for optimal results.\n\nThe method's performance was also compared with other existing methods using the same datasets. The results demonstrated that the method outperformed other methods in terms of accuracy and MCC, highlighting its effectiveness in DNA-binding protein identification.",
  "evaluation/measure": "In our evaluation of predictive performance, we employed several key metrics that are widely recognized and used in the field. These metrics provide a comprehensive assessment of the model's effectiveness in identifying DNA-binding proteins.\n\nThe top five commonly used evaluation metrics in this regard are Sensitivity (SE), Specificity (SP), Accuracy (ACC), Matthew’s correlation coefficient (MCC), and F1 Measure (F1 M). These metrics are crucial for understanding the model's performance from different perspectives.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). High sensitivity indicates that the model is good at identifying positive samples.\n\nSpecificity measures the proportion of actual negatives that are correctly identified. It is calculated as the ratio of true negatives (TN) to the sum of true negatives and false positives (FP). High specificity indicates that the model is good at identifying negative samples.\n\nAccuracy provides an overall measure of the model's performance by calculating the proportion of correctly identified samples (both positive and negative) out of the total number of samples. It is calculated as the ratio of the sum of true positives and true negatives to the total number of samples.\n\nMatthew’s correlation coefficient (MCC) is a balanced measure that takes into account true and false positives and negatives. It returns a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation.\n\nThe F1 Measure is the harmonic mean of precision and recall. Precision is the ratio of true positives to the sum of true positives and false positives, while recall is the same as sensitivity. The F1 Measure provides a single metric that balances both concerns.\n\nThese metrics collectively offer a robust evaluation framework, ensuring that the model's performance is assessed from multiple angles. This approach is representative of the literature, as these metrics are standard in the field of bioinformatics and machine learning for evaluating classification models.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on independent datasets, not benchmark datasets. The methods compared included DNAbinder with two vectorization methods (P21 and P400), DNA-Prot, and iDNA-Prot. The performance was evaluated using metrics such as accuracy (ACC), Matthew's correlation coefficient (MCC), sensitivity (SE), specificity (SP), and F1-Measure (F1-M).\n\nThe comparison showed that enDNA-Prot outperformed the other methods on both independent datasets. For instance, on independent dataset1, enDNA-Prot achieved an accuracy of 89.56% and an MCC of 0.79, which was significantly higher than the other methods. Similarly, on independent dataset2, enDNA-Prot achieved an accuracy of 83.48% and an MCC of 0.67, again outperforming the other methods.\n\nRegarding simpler baselines, the information provided does not specify if such a comparison was performed. Therefore, it is not applicable to comment on this aspect.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. However, the complete list of all the codes and sequences for the benchmark dataset and the expanded benchmark dataset can be found in Supplementary Material S1 and S2, respectively. These materials are available online, but specific details about the license or access permissions are not provided. For further information, users can refer to the supplementary materials linked in the publication."
}