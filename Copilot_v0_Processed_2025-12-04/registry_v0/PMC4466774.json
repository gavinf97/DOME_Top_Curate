{
  "publication/title": "A Ranking Approach to Genomic Selection",
  "publication/authors": "The authors who contributed to the article are Mathieu Blondel, Akio Onogi, Hiroyoshi Iwata, and Naonori Ueda.\n\nMathieu Blondel was funded by the FIRST program and contributed significantly to the development of the ranking approach to genomic selection. Akio Onogi received a grant-in-aid from the Japan Society for the Promotion of Science and played a crucial role in the implementation and testing of the methods discussed. Hiroyoshi Iwata and Naonori Ueda did not receive specific funding for this work but provided essential expertise and guidance throughout the research process. NTT Communication Science Laboratories supported Blondel and Ueda by providing salaries, but they did not influence the study design, data collection, analysis, decision to publish, or manuscript preparation.",
  "publication/journal": "PLoS ONE",
  "publication/year": "2015",
  "publication/doi": "10.1371/journal.pone.0128570",
  "publication/tags": "- Genomic Selection\n- Machine Learning\n- Ranking Methods\n- Breeding Value\n- Predictive Models\n- Molecular Markers\n- Information Retrieval\n- Normalized Discounted Cumulative Gain\n- Regression Methods\n- Selective Breeding",
  "dataset/provenance": "The datasets used in our study are sourced from various public repositories and previous research studies. For the barley dataset, the Barley-CAP project evaluated the grain yield of 432 barley lines in Aberdeen, Idaho. Among these, 381 lines were genotyped using 3072 DArT markers. The dataset is available at the Triticeae Toolbox.\n\nThe maize dataset comprises 264 maize lines genotyped using 1076 SNP markers. This dataset was used in a previous study and is provided as example data in SelectionTools. It is available at the University of Giessen's population genetics downloads page.\n\nThe rice dataset consists of 395 lines genotyped with 1311 SNPs. Phenotypic values for 14 traits were available for 335 lines without missing records. This dataset is available at the Rice Diversity Project.\n\nThe wheat datasets include two distinct sets. The first, developed by the CIMMYT Global Wheat Breeding program, comprises 599 wheat lines genotyped using 1279 DArT markers after filtering. This dataset is part of the R package BLR. The second wheat dataset, used in a study by Pérez-Rodríguez, consists of 306 lines genotyped with 1695 DArT markers. This dataset is available at the same URL as the maize data.\n\nThe Arabidopsis dataset comprises 422 lines of Arabidopsis thaliana developed by INRA. This dataset is available at the Public Lines website.",
  "dataset/splits": "In our study, we employed a randomized cross-validation scheme to estimate the generalization performance of different models. For each cross-validation iteration, the dataset was split into two parts: 80% for model estimation (training) and 20% for evaluation (testing). This process was repeated for 10 cross-validation iterations to ensure robust and reliable results. The evaluation scores were then averaged across these iterations. This approach ensures that all methods use the same splits, providing a fair comparison. Additionally, for models that require hyper-parameter tuning, we used 5-fold cross-validation within the training split to optimize performance. This nested cross-validation strategy helps in selecting the best hyper-parameters while maintaining an unbiased estimate of model performance.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in our study are publicly available, ensuring transparency and reproducibility of our results. For the Triticeae dataset, which includes 3945 SNPs, the data can be accessed at http://triticeaetoolbox.org. The maize dataset, comprising 264 lines genotyped with 1076 SNP markers, is provided as example data in SelectionTools, available at http://www.uni-giessen.de/cms/fbz/fb09/institute/pflbz2/population-genetics/downloads. The rice dataset, consisting of 395 lines genotyped with 1311 SNPs, is available at http://www.ricediversity.org/data/index.cfm. The wheat dataset developed by the CIMMYT Global Wheat Breeding program is included as part of the R package BLR. The wheat dataset used in the study of Pérez-Rodríguez, which includes 306 lines genotyped with 1695 DArT markers, is available at the same URL as the maize data. The Arabidopsis dataset, comprising 422 lines of Arabidopsis thaliana, is available at http://publiclines.versailles.inra.fr/page/33. The barley dataset, evaluated by the Barley-CAP project, is available upon request from the Barley-CAP project. The datasets are released under licenses that allow for academic use and further research, ensuring that other researchers can build upon our work. To enforce the use of the same data splits, we employed a randomized cross-validation scheme where the dataset was split into 80% for model estimation and 20% for evaluation. This process was repeated for 10 cross-validation iterations, and the evaluation scores were averaged to ensure fair comparison across different methods.",
  "optimization/algorithm": "The optimization algorithm discussed in this subsection is not a new machine-learning algorithm. It is a well-established method known as LambdaMART, which is a listwise approach for learning to rank. LambdaMART builds upon Gradient Boosted Regression Trees (GBRT) to optimize the Normalized Discounted Cumulative Gain (NDCG) metric. This method was developed to handle the discontinuity of the NDCG objective by using an approximation of the negative gradient called the λ-gradient.\n\nLambdaMART was not published in a machine-learning journal because it was developed and presented in technical reports and conference proceedings. Specifically, it was introduced in a technical report by Burges in 2010, which provided an overview from RankNet to LambdaRank to LambdaMART. This method has since been widely recognized and used in the field of information retrieval and ranking.\n\nThe choice of LambdaMART for this study is motivated by its effectiveness in optimizing ranking accuracy, particularly in the context of the Yahoo! Learning to Rank Challenge, where it achieved leading results. The method's ability to handle the complexities of ranking problems makes it a suitable choice for the genomic selection tasks discussed in the publication.",
  "optimization/meta": "The subsection \"Meta-predictor\" is not applicable.",
  "optimization/encoding": "In our study, we employed a discretization technique to encode the continuous trait values for the McRank algorithm. This involved dividing the training trait values into equal-width bins and computing their means. Each training trait value was then replaced by the mean value of the bin it belonged to. This discretization process, although it might seem like a loss of information, can be beneficial when the goal is to maximize Normalized Discounted Cumulative Gain (NDCG) on unseen data. For McRank, we varied the number of bins to observe its effect on performance. Additionally, for tree-based ensemble methods like random forests, gradient boosting regression trees (GBRT), McRank, and LambdaMART, we used 300 trees in the ensemble. The parameter max_features was set to 0.6, meaning that only 60% of the features were considered when searching for the best split during tree induction. This approach both speeds up tree induction and reduces overfitting. The maximum tree depth was chosen from a set of predefined values. For GBRT and LambdaMART, we selected the learning rate parameter from a range of values to prevent overfitting. For ridge and RKHS regression, we used the R package rrBLUP, which estimates the regularization and kernel parameters automatically by (restricted) maximum likelihood, avoiding the need for cross-validation. For RankSVM, we set the regularization parameter based on the number of preference pairs and chose the kernel parameter from a range of values. This comprehensive approach to data encoding and preprocessing ensured that our machine-learning algorithms were optimized for performance and robustness.",
  "optimization/parameters": "In our study, the number of parameters, denoted as p, corresponds to the number of molecular markers used for genotyping. These markers can include various types such as DArT, SNP, etc. The value of p is typically large, as genome-wide genotyping involves a substantial number of markers. This leads to the well-known n << p problem, where the number of individuals (n) in the reference population is much smaller than the number of markers (p).\n\nThe selection of p is not explicitly detailed in our optimization process, as it is determined by the genotyping technology and the specific dataset used. Instead, our focus is on optimizing other hyperparameters and model configurations to improve predictive performance. For instance, we tested various values for parameters like the learning rate in gradient boosting regression trees (GBRT) and the number of bins in McRank to find the optimal settings.\n\nFor tree-based ensemble methods, such as random forests (RF) and GBRT, we used a fixed number of 300 trees in the ensemble. The parameter max_features was set to 0.6, meaning that only 60% of the features are considered when searching for the best split during tree induction. This setting helps to speed up the tree induction process and reduces overfitting. The maximum tree depth (max_depth) was chosen from a set of values {3, 5, 10}.\n\nFor Bayesian regression methods, parameters were estimated using variational Bayesian approaches. The specific values tested for each method varied. For example, for BL, we tested five values for the parameter ϕ and six log-spaced values for ω. For EBL, we used the same values for ψ and θ and tested three additional values. The values for ν and π were fixed for some methods, while others tested multiple values to find the optimal configuration.\n\nIn summary, while the number of markers (p) is determined by the genotyping process and dataset, our optimization efforts focused on tuning other hyperparameters to enhance model performance.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we encountered the common scenario in genomic selection where the number of molecular markers, or features, is significantly larger than the number of individuals, or training points. This is known as the n << p problem, where n represents the number of individuals and p represents the number of markers.\n\nTo address the risk of overfitting, we employed several strategies. For tree-based ensemble methods, such as random forests (RF) and gradient boosting regression trees (GBRT), we used a relatively large number of trees (300) in the ensemble. Additionally, we set the parameter max_features to 0.6, meaning that only 60% of the features were considered when searching for the best split during tree induction. This approach not only speeds up the tree induction process but also helps to reduce overfitting. Furthermore, we chose the maximum tree depth from a set of values {3, 5, 10} to control the complexity of the individual trees.\n\nFor GBRT, we also tuned the learning rate parameter, which controls the contribution of each tree in the ensemble. We tested learning rates of {0.001, 0.01, 0.1, 1.0} and found that a learning rate of 0.1 generally provided the best performance, balancing the trade-off between bias and variance.\n\nTo prevent underfitting, we ensured that our models had sufficient capacity to capture the underlying patterns in the data. For example, in GBRT, we avoided setting the learning rate too small, as this can lead to underfitting. Instead, we selected a learning rate that allowed the model to learn from the data effectively.\n\nFor Bayesian regression methods, we used variational Bayesian approaches to estimate the parameters, which helped to prevent both overfitting and underfitting by incorporating prior distributions and optimizing the evidence lower bound.\n\nIn summary, we addressed the n << p problem and the associated risks of overfitting and underfitting by using ensemble methods, regularization techniques, and appropriate model selection strategies. These approaches allowed us to build robust and generalizable models for genomic selection.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting, particularly in the context of tree-based ensemble methods. For random forests (RF) and gradient boosting regression trees (GBRT), we utilized two key heuristics. The first involved setting the parameter `max_features` to 0.6, which means that only 60% of the features were considered when searching for the best split during tree induction. This approach not only speeds up the tree induction process but also helps in reducing overfitting by limiting the complexity of the trees.\n\nThe second heuristic involved setting the maximum tree depth (`max_depth`) from a predefined set of values {3, 5, 10}. Limiting the depth of the trees ensures that they do not become too complex, which can lead to overfitting. For GBRT and LambdaMART, we also chose the learning rate parameter from a set of values {0.001, 0.01, 0.1, 1.0}. A smaller learning rate helps in preventing the model from fitting the training data too closely, thereby reducing overfitting.\n\nAdditionally, for ridge and RKHS regression, we used the R package `rrBLUP`, which estimates the regularization and kernel parameters automatically by (restricted) maximum likelihood. This approach is closely related to Gaussian processes in the machine learning community and helps in regularizing the model to prevent overfitting.\n\nFor RankSVM, we set the regularization parameter `l` to `jPj`, where `jPj` is the number of preference pairs, and chose `~l` from 15 log-spaced values between 10−6 and 106. This careful selection of hyperparameters ensures that the model is regularized appropriately, reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for various methods are detailed within the publication. For Bayesian regression methods, specific values tested for parameters like ϕ, ω, ψ, θ, ν, π, and S2 are provided, along with the rationale behind these choices. For tree-based ensemble methods such as random forests and gradient boosting regression trees, parameters like the number of trees, max_features, and max_depth are specified. Additionally, the learning rate for gradient boosting and the number of bins for McRank are discussed.\n\nThe implementation details for some methods, such as SSVS and MIX, are noted to be published elsewhere, indicating that further specifics on their configurations may be available in separate publications.\n\nRegarding model files and optimization parameters, the publication mentions the use of specific packages and source codes. For instance, implementations for random forests and gradient boosting were obtained from the scikit-learn Python package, while ridge and RKHS regression used the R package rrBLUP. The source code for McRank and LambdaMART was accessed from a GitHub repository. For RankSVM, the objective function was solved using the L-BFGS method with a specified maximum number of iterations.\n\nThe publication does not explicitly state the availability of model files or optimization parameters in a downloadable format or under a specific license. However, the use of open-source packages and publicly available repositories suggests that the implementations and configurations discussed are accessible to the research community. For precise details on licensing and accessibility, referring to the original sources of the implementations would be necessary.",
  "model/interpretability": "The models discussed in our study vary in their interpretability, ranging from transparent to black-box approaches. Tree-based ensemble methods, such as Random Forests (RF) and Gradient Boosting Regression Trees (GBRT), are generally considered more interpretable than some other models. These methods can provide insights into variable importance and interactions, making them less of a black-box compared to methods like Ridge Regression or RKHS Regression.\n\nFor instance, in RF and GBRT, the importance of each feature can be quantified, showing which variables contribute most to the predictions. This is particularly useful in genomic selection, where understanding which genetic markers are most influential can guide further research and breeding programs. Additionally, these models can handle categorical variables and missing values without prior imputation, adding to their practical utility.\n\nOn the other hand, methods like RankSVM and LambdaMART are more opaque. These models focus on optimizing ranking accuracy through complex loss functions, which do not inherently provide clear insights into the underlying data patterns. While they can achieve high performance in ranking tasks, their internal workings are less transparent, making it difficult to interpret the contributions of individual features.\n\nIn summary, while some models in our study offer clear interpretability through feature importance and interactions, others prioritize predictive performance at the cost of transparency. This trade-off is an important consideration when choosing a model for genomic selection, depending on whether interpretability or pure predictive accuracy is more critical for the specific application.",
  "model/output": "The model discussed in this publication primarily focuses on regression techniques, particularly in the context of genomic selection. Various regression models are explored, including ridge regression, RKHS regression, Bayesian lasso, extended Bayesian lasso, weighted Bayesian shrinkage regression, BayesC, stochastic search variable selection, Bayesian mixture regression model, random forests, and gradient boosted regression trees. These models are used to predict trait values based on genomic data.\n\nIn addition to traditional regression, ranking methods are also considered. These methods, such as McRank, RankSVM, and Lambda-MART, are used to rank candidates based on their predicted trait values. The choice between regression and ranking depends on the specific goals of the selection process. For instance, when both ranking and regression accuracies are important, such as in determining a selling price based on predicted trait values, regression methods like random forests and RKHS regression are preferred. However, when the goal is to select candidates with an appropriate trait value rather than the highest or lowest, model evaluation based on mean squared error (MSE) may be more suitable.\n\nThe output of these models can vary. Regression models provide predicted trait values, which can be used for further analysis or decision-making. Ranking models, on the other hand, output a ranked list of candidates based on their predicted trait values. The choice of model and output type depends on the specific requirements of the genomic selection task at hand.",
  "model/duration": "The execution time of the models varied depending on the specific algorithm and the dataset used. For kernel-based methods like RKHS regression and kernel RankSVM, the computational cost is primarily determined by the pre-computation of the kernel matrix, which takes O(n^2p) time. Once the kernel matrix is computed, solving the system of linear equations for RKHS regression takes O(n^3) time, while the gradient method for kernel RankSVM takes O(n^2) time per iteration.\n\nTree-based ensemble methods, including random forests, gradient boosting regression trees (GBRT), McRank, and LambdaMART, have a computational cost proportional to the number of trees in the ensemble. Tree induction for these methods takes O(p0n log2 n) time on average and O(p0n^2 log n) time in the worst case, where p0 is the number of markers considered for node splitting. For LambdaMART, each tree needs to be fitted against the λ-gradient, which takes O(kn) time, where k is the parameter used for NDCG@k.\n\nIn practice, the number of samples n is usually much smaller than the number of markers p, which is typical in genomic selection. This n << p problem influences the computational efficiency of the models. For instance, the learning rate parameter in GBRT is crucial for preventing overfitting, and optimal accuracy is achieved when the learning rate is neither too large nor too small. The best learning rate was found to be 0.1 for most datasets, making it a good rule of thumb for practical applications.\n\nThe number of bins in McRank also affects its performance. Since McRank assumes that the trait values are discrete, the continuous trait values need to be discretized. The optimal number of bins was determined experimentally, and the results showed that the performance of McRank improved with an appropriate number of bins.\n\nOverall, the execution time of the models depends on the specific algorithm, the size of the dataset, and the parameters used. Kernel-based methods are more computationally intensive due to the kernel matrix computation, while tree-based methods are more efficient but require careful tuning of parameters to achieve optimal performance.",
  "model/availability": "The source code for some of the algorithms used in this study is publicly available. For instance, the implementations of random forests and gradient boosting regression trees (GBRT) were obtained from the scikit-learn Python package. Additionally, the source code for McRank and LambdaMART is available on GitHub. For ridge and RKHS regression, the R package rrBLUP was used. The algorithms for SSVS and MIX were implemented by one of the authors and will be published elsewhere. However, specific details about the licensing terms for these implementations are not provided. For Bayesian regression methods, a program written in C was used, but it is not clear if this code is publicly available.",
  "evaluation/method": "In our study, we employed cross-validation to evaluate the performance of the methods. Specifically, we used six datasets, each comprising different plant species and traits. For datasets with multiple traits, we reported average scores to maintain clarity.\n\nWe assessed the methods using several evaluation measures, including Pearson correlation, Kendall’s τ, and normalized discounted cumulative gain (NDCG) at different levels (NDCG@1, NDCG@5, NDCG@10, and Mean NDCG@10). These measures allowed us to quantify the ranking accuracy of the models, with NDCG being particularly relevant as it rewards models that assign high ranks to individuals with high breeding values.\n\nTo compare the evaluation measures, we computed Spearman’s correlation coefficient (Spearman’s ρ) to assess how well the rankings agreed across different measures. This analysis revealed interesting trends, such as the poor correlation between Pearson correlation and Kendall’s τ with NDCG@k when k is small, highlighting the importance of choosing the right evaluation measure for specific objectives, such as ranking accuracy at the top.\n\nAdditionally, we conducted a detailed comparison of RKHS regression and RankSVM by varying their regularization and RBF kernel hyper-parameters. Heatmaps indicating Mean NDCG@10 for various hyper-parameter combinations showed that RankSVM tends to achieve better Mean NDCG@10 than RKHS regression across many different settings, suggesting that RankSVM is more robust to hyper-parameter choices.",
  "evaluation/measure": "In our study, we employed a variety of performance metrics to evaluate the ranking methods used for genomic selection. These metrics can be broadly categorized into those for global ranking evaluation and those for top-k ranking evaluation.\n\nFor global ranking evaluation, we used pairwise accuracy and Kendall’s τ. Pairwise accuracy measures the proportion of concordant pairs, where a pair is concordant if the predicted order agrees with the true order. Kendall’s τ, on the other hand, is the difference between the ratio of concordant pairs and the ratio of discordant pairs. Both metrics are designed to assess the overall agreement between the predicted and true rankings.\n\nAdditionally, we reported Pearson correlation, which is a commonly used measure in the genomic selection literature. It assesses the linear relationship between the observed and predicted trait values.\n\nFor top-k ranking evaluation, we introduced discounted cumulative gain (DCG) and its normalized version (NDCG). These metrics are particularly useful for evaluating the ability of models to rank the top-k individuals with the highest breeding value. DCG assigns higher weights to correct rankings at the top positions, making it suitable for scenarios where the top rankings are of primary interest. NDCG normalizes DCG to provide a bounded score between 0 and 1, making it easier to interpret.\n\nWe also computed the Spearman’s correlation coefficient (Spearman’s ρ) to quantify the similarity between different evaluation measures. This helped us understand how well the rankings produced by different metrics agree with each other.\n\nThe set of metrics we used is representative of the literature, covering both traditional measures like Pearson correlation and more specialized ranking metrics like NDCG. This comprehensive approach allows us to provide a thorough evaluation of the ranking methods, ensuring that our conclusions are robust and reliable.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various methods to evaluate their performance in genomic selection. We compared ten existing regression methods and three new ranking methods across six datasets, which included four plant species and twenty-five traits. This comparison was performed using several evaluation measures, including Pearson correlation, Kendall’s τ, and normalized discounted cumulative gain (NDCG) at different levels (NDCG@1, NDCG@5, NDCG@10, and Mean NDCG@10).\n\nThe datasets used in our study were diverse, encompassing different plant species and traits, which allowed us to assess the generalizability of the methods. For each dataset, we reported the average scores to provide a clear overview of the methods' performance. The evaluation measures were chosen to reflect different aspects of ranking accuracy, with NDCG being particularly relevant for selective breeding as it rewards models that assign high ranks to individuals with high breeding values.\n\nIn addition to comparing the methods on benchmark datasets, we also performed a detailed analysis of the top-performing methods. For instance, we compared RKHS regression and RankSVM by varying their regularization and RBF kernel hyper-parameters. This analysis revealed that RankSVM was more robust to hyper-parameter choices and often achieved better Mean NDCG@10 scores than RKHS regression.\n\nWe also included simpler baselines in our comparison, such as traditional regression methods like Bayesian lasso and wBSR. These methods were found to be less suitable for ranking tasks compared to more advanced techniques like tree-based ensemble methods (e.g., Random Forests, Gradient Boosting Regression Trees) and ranking-specific methods (e.g., McRank, RankSVM).\n\nOverall, our comparison showed that tree-based ensemble methods and ranking-specific methods generally achieved excellent ranking accuracy. This suggests that these methods are promising for genomic selection and that NDCG can be a useful evaluation measure in this context.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not applicable."
}