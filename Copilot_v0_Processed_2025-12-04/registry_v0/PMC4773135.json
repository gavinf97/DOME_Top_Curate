{
  "publication/title": "Reconstructing Causal Biological Networks through Active Learning",
  "publication/authors": "The authors who contributed to the article are:\n\n- HC, who conceived and designed the experiments, performed the experiments, analyzed the data, and wrote the paper.\n- BB, who contributed to the conception and design of the experiments and wrote the paper.\n- JP, who contributed to the conception and design of the experiments and wrote the paper.",
  "publication/journal": "PLOS ONE",
  "publication/year": "2016",
  "publication/doi": "10.1371/journal.pone.0150611",
  "publication/tags": "- Bayesian networks\n- Gaussian Bayesian networks\n- Active learning\n- Causal biological networks\n- Gene regulatory networks\n- Network reconstruction\n- Intervention data\n- Systems biology\n- Computational biology\n- Machine learning",
  "dataset/provenance": "The dataset used in our study was sourced from the DREAM4 10-node in-silico network reconstruction challenge. This dataset is a commonly used benchmark for network inference algorithms and includes five networks with different structures. These networks were designed to reflect common topological properties of real gene regulatory networks in E. coli or S. cerevisiae, including feedback loops. The expression data for each network was generated using stochastic differential equations and a realistic noise model of microarray data sets.\n\nThe initial observational data set consisted of 11 instances, which included both wild type and 10 multifactorial perturbation data. Additionally, we ran active and random learners to prioritize 20 intervention samples, each consisting of one knockout and one knockdown per gene. We made a simplifying assumption that the learner knows the resulting expression level of the target gene in a knockdown experiment. This dataset has been used in previous research and by the community for evaluating network inference algorithms.\n\nFor our simulations, we also generated a Gaussian Bayesian Network (GBN) with 10 nodes as ground truth. This network was used to create a collection of observational and intervention samples. The parameters of the ground truth GBN were generated by uniformly sampling edge weights from specific ranges, setting base levels from a normal distribution, and fixing the noise level for all nodes. We sampled 10 observational instances to serve as the initial data set and ran both active and random learners until they selected 20 additional intervention experiments.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm presented is an active learning approach for Gaussian Bayesian networks (GBNs). This method falls under the class of Bayesian learning algorithms, which are used for network inference based on both observational and intervention data.\n\nThe algorithm is not entirely new; it builds upon existing frameworks, particularly the information-theoretic approach developed by Murphy. However, it introduces unique optimizations based on linear-algebraic insights specific to GBNs. These optimizations aim to improve the overall complexity and efficiency of the algorithm compared to naive implementations.\n\nThe reason this algorithm was not published in a machine-learning journal is likely due to its specific application in biological network reconstruction. The focus of the publication is on demonstrating the effectiveness of this active learning approach in inferring causal relationships in biological data, particularly in the context of gene expression and signaling pathways. The improvements in runtime and accuracy are showcased through experiments on real biological datasets, such as the DREAM4 data sets and gene expression data from Sachs et al. This context makes it more suitable for publication in a journal focused on computational biology or bioinformatics rather than a general machine-learning journal.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it focuses on Gaussian Bayesian Networks (GBNs) and employs an active learning framework to infer the structure of these networks. The approach involves using mutual information and Kullback-Leibler divergence to prioritize intervention experiments, aiming to improve the quality of learned networks.\n\nThe core of the method involves a Bayesian structure learning algorithm for GBNs, which is used to recover causal links in biological data. The algorithm leverages the Metropolis-Hastings algorithm to explore the posterior distribution over candidate graph structures. This process does not rely on the outputs of other machine-learning models but rather on the analytical expression for marginal likelihood and the design of search space, prior over graphs, and proposal distribution.\n\nThe evaluation of the network reconstruction performance is conducted using metrics such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). These evaluations are based on a ranked list of edges and the expected maximum a posteriori (MAP) edge weight. The method has been tested on various datasets, including gene expression data and simulated data from GBNs, demonstrating its effectiveness in capturing causal relationships in biological data.\n\nThe active learning algorithm is designed to prioritize interventions via active learning, allowing for more efficient design of intervention experiments when subjected to time or resource constraints. This approach does not involve combining predictions from multiple machine-learning models but rather focuses on optimizing the selection of informative experiments to improve the accuracy of the learned network structure.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were tailored to facilitate the application of our active learning algorithm for Gaussian Bayesian networks. We primarily worked with gene expression data, which was collected and preprocessed to ensure compatibility with our model assumptions.\n\nFor the gene expression data, such as that collected by Sachs et al., we dealt with continuous measurements of phosphorylated proteins involved in signaling pathways. The data consisted of single-cell expression profiles, which were directly used in our Bayesian structure learning algorithm. No discretization was applied, adhering to our goal of using continuous Bayesian networks to avoid the need for careful tuning of input data.\n\nThe data was structured to include both observational and intervention samples. Observational data represented the natural state of the system, while intervention data included measurements taken from cells under perturbation induced by different reagents. These perturbations were designed to activate or inhibit specific proteins in the pathway, providing valuable information for inferring causal relationships.\n\nTo handle the computational efficiency of our algorithm, we employed a strategy that involved saving the inverse and determinant of the precision matrix (Î›j) for each candidate graph structure (Gs). This approach reduced the compute time for the posterior probability calculation from O(md^2) to O(d^2), where m is the number of samples and d is the upper bound on the number of parents each node can have. This optimization was crucial for managing the super-exponential growth of candidate graphs with respect to the number of nodes, especially in small-scale networks.\n\nAdditionally, we imposed a limit on the in-degree of nodes, setting it to five, which is a commonly used heuristic in the literature. This constraint helped in managing the complexity of the network inference problem.\n\nIn summary, our data encoding and preprocessing steps focused on maintaining the continuity of the data, efficiently handling computational demands, and applying reasonable constraints to manage the complexity of network inference.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in our study leverages Gaussian Bayesian Networks (GBNs) to capture causal relationships in biological data. The model assumptions of GBNs, including acyclicity and Gaussianity, are applied to real biological data, demonstrating reasonable accuracy in predicting causal links among proteins.\n\nTo address the potential issue of overfitting, given that the number of parameters can be large relative to the number of training points, we utilized a Bayesian approach. This approach inherently incorporates regularization through the prior distribution over the model parameters, which helps to prevent overfitting by penalizing complex models. Additionally, the use of Metropolis-Hastings algorithm for sampling from the posterior distribution ensures that the model is not overly sensitive to the specific data points, further mitigating the risk of overfitting.\n\nUnderfitting was addressed by ensuring that the model was sufficiently complex to capture the underlying structure of the data. The Bayesian framework allows for the exploration of a wide range of candidate graph structures, which helps to avoid underfitting by not imposing overly simplistic assumptions on the data. The active learning algorithm further enhances the model's ability to capture the true network structure by prioritizing informative intervention experiments, thereby improving the accuracy of the learned network.\n\nThe evaluation of the model's performance on various datasets, including the DREAM4 benchmark data, provides evidence of its effectiveness in avoiding both overfitting and underfitting. The consistent improvement in accuracy metrics, such as AUROC and AUPRC, across different datasets and the faster convergence rate of the active learning algorithm support the robustness of the fitting method.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model presented in our work is not a black box but rather a transparent approach to reconstructing causal biological networks. Our method leverages Gaussian Bayesian Networks (GBNs), which are inherently interpretable due to their graphical structure. Each node in the network represents a gene or protein, and directed edges indicate causal relationships, making it straightforward to understand the interactions within the biological system.\n\nThe transparency of our model is further enhanced by the use of active learning, which allows us to prioritize intervention experiments based on their informativeness. This process is driven by mutual information and Kullback-Leibler divergence, providing a clear metric for selecting the most informative interventions. The output of our algorithm is a set of sampled graph structures drawn from the posterior distribution, which intuitively represents the confidence in each candidate graph structure. This output can be summarized using Bayesian model averaging, where features of interest, such as the presence of edges, are averaged over all graph samples.\n\nMoreover, the analytical expressions derived for the marginal likelihood and the posterior distribution over candidate graph structures add to the interpretability. These expressions allow us to explore the space of candidate graph structures using the Metropolis-Hastings algorithm, providing a probabilistic framework for understanding the underlying network structure.\n\nIn summary, our model's transparency is evident in its use of interpretable graphical structures, clear metrics for intervention selection, and probabilistic frameworks for exploring network structures. This makes it a valuable tool for understanding complex biological systems and their quantitative properties.",
  "model/output": "The model discussed is focused on reconstructing causal biological networks through active learning, specifically using Gaussian Bayesian Networks (GBNs). This approach is primarily a classification task, aiming to infer the structure of causal relationships among variables, such as genes or proteins, rather than predicting a continuous outcome. The output of the model is a set of sampled graph structures drawn from the posterior distribution, which represents the belief in each candidate graph structure being the underlying model for the given data. These graph structures can be summarized using methods like Bayesian model averaging to determine the presence of edges or other features of interest. The evaluation of the model's performance involves metrics like AUROC and AUPRC, which are commonly used in classification tasks to assess the accuracy of the learned causal structure. Additionally, the model can prioritize interventions via active learning, selecting the most informative experiments to improve the quality of the learned networks.",
  "model/duration": "The execution time of our algorithm was significantly improved through optimization techniques. Specifically, we implemented rank-one updates to the matrix inverse and determinant, which led to a 30% reduction in runtime. This improvement was demonstrated on simulated data, and it is expected to be even more substantial on larger datasets. The cumulative runtime of the iterative learning procedure was evaluated using a single 3.47 GHz Intel Xeon X5690 CPU to ensure fairness in comparison. Despite this optimization, our method is currently best suited for small-scale networks, typically with fewer than 30 nodes, due to the super-exponential growth of candidate graphs with the number of nodes. The algorithm's design allows for parallelism, which could further enhance runtime performance with multiple CPUs.",
  "model/availability": "The source code for the algorithm described in the publication is publicly available. A MATLAB implementation of the algorithm is provided as supplementary material, specifically labeled as S1 Code. This allows users to access and run the algorithm using MATLAB, facilitating reproducibility and further development. The details regarding the license under which the code is released are not specified, but it is intended for use by the research community.",
  "evaluation/method": "The evaluation of our learning algorithm was conducted through several methods to assess its performance in reconstructing causal networks. We calculated the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC) based on a ranked list of edges. The absolute value of the expected maximum a posteriori (MAP) edge weight, approximated using graph samples from the posterior, was used as the score for each edge. Additionally, we calculated the mean-squared error (MSE) of the expected MAP edge weights over all possible edges, as we had access to the true parameters in our simulated data.\n\nWe also analyzed the trajectory of different accuracy measures over the course of the iterative learning procedure, where one intervention experiment was added at a time. Furthermore, we evaluated how close we were to the final belief over candidate graph structures using a subset of the data, measuring this by calculating the KL divergence of the final belief from the current belief over randomly chosen candidate graphs. This metric is agnostic to whether we have access to the ground truth network and evaluates how much information is lost if only a small subset of intervention experiments is performed.",
  "evaluation/measure": "To evaluate the performance of our learning algorithm, we employed several metrics that are widely recognized in the field of network reconstruction. These metrics include the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). These metrics were calculated based on a ranked list of edges, using the absolute value of the expected maximum a posteriori (MAP) edge weight as the score for each edge. This approach is consistent with the evaluation scheme used in the DREAM4 challenge, ensuring that our results are comparable to other studies in the literature.\n\nIn addition to these metrics, we also calculated the mean-squared error (MSE) of the expected MAP edge weights. This metric is particularly useful for simulated data where the true parameters are known, allowing us to assess the accuracy of our edge weight estimates.\n\nFurthermore, we introduced a metric that is agnostic to whether we have access to the ground truth network. This metric measures the KL divergence of the final belief from the current belief over candidate graph structures. By calculating this divergence, we can evaluate how much information is lost if only a subset of the intervention experiments is performed. This metric is particularly useful for assessing the convergence rate of our algorithm and its ability to efficiently utilize the available data.\n\nOverall, the set of metrics we reported is representative of the literature and provides a comprehensive evaluation of our algorithm's performance. The use of AUROC, AUPRC, and MSE ensures that we can assess both the accuracy and the precision of our edge predictions, while the KL divergence metric provides insights into the algorithm's convergence behavior.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our learning algorithm with publicly available methods using benchmark datasets. Specifically, we assessed our approach against the PC algorithm and GIES, a state-of-the-art non-Bayesian method for learning Gaussian Bayesian Networks (GBNs). The PC algorithm was evaluated using different significance levels, while GIES was applied using both observational and intervention data.\n\nOur performance was gauged on the DREAM4 data sets, which are widely used benchmarks for network inference algorithms. These data sets simulate biological networks with various topological properties, including feedback loops, and were generated using stochastic differential equations and a realistic noise model. We focused on data sets 4 and 5, which contain fewer and weaker cycles, making them more suitable for our acyclic graph-based method.\n\nThe results demonstrated that our active learning algorithm outperformed both PC and GIES in terms of precision-recall curves, indicating higher accuracy in identifying edges in the undirected skeleton of the ground truth network. This suggests that our Bayesian learning approach is more effective in uncovering the true graph structure compared to these established methods.\n\nAdditionally, we compared our method to a simpler baseline, a random learner, which selects intervention experiments uniformly at random. The active learning algorithm consistently achieved higher accuracy and faster convergence across various metrics, including mean-squared error (MSE), area under the precision-recall curve (AUPRC), and area under the receiver operating characteristic curve (AUROC). This comparison underscores the efficiency and effectiveness of our approach in accelerating network reconstruction.",
  "evaluation/confidence": "The evaluation of our learning algorithm involved several performance metrics, including the area under the receiver operating characteristic curve (AUROC), the area under the precision-recall curve (AUPRC), and the mean-squared error (MSE) of the expected maximum a posteriori (MAP) edge weights. These metrics were used to assess the accuracy of the learned causal structures.\n\nTo provide confidence in our results, we conducted multiple trials for each evaluation. For instance, when comparing the active learner with the random learner on simulated data, the results were summarized over five trials. Similarly, the performance on the DREAM4 benchmark data was also evaluated over five trials. This repetition helps to ensure that the observed improvements are not due to random chance.\n\nIn addition to multiple trials, we also reported standard deviations for our metrics. For example, in the figures comparing the active learner with the random learner, dotted lines are drawn at one standard deviation from the mean in each direction. This visual representation allows for an assessment of the variability in our results and provides a sense of the confidence intervals around our performance metrics.\n\nStatistical significance was implicitly addressed through the consistent performance improvements observed across multiple trials and datasets. The active learning algorithm demonstrated higher accuracy and faster convergence than the random learner across all evaluated metrics. This consistent superiority suggests that the observed differences are statistically significant and not merely due to random variation.\n\nFurthermore, when comparing our approach with other methods like PC and GIES, we evaluated the final prediction accuracy in identifying edges in the undirected skeleton of the ground truth network. The precision-recall curves for our approach generally dominated those of PC and GIES, indicating better performance. This comparison was also conducted over multiple trials, reinforcing the statistical significance of our findings.\n\nIn summary, the evaluation of our learning algorithm included multiple trials, reporting of standard deviations, and consistent performance improvements across different datasets and metrics. These elements together provide a strong basis for claiming that our method is superior to others and baselines.",
  "evaluation/availability": "Not enough information is available."
}