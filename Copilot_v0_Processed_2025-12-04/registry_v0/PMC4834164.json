{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Molecular Informatics",
  "publication/year": "2015",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Computational Biology\n- Machine Learning\n- Virtual Screening\n- Ensemble Classifiers\n- Protein Folds\n- Drug-Target Interaction\n- ROC Curves\n- AUC Values\n- SVM\n- Adaboost\n- Random Forest\n- Chemical Descriptors\n- Pharmacophore-Based Interaction\n- Protein Structure Prediction\n- Data Mining\n- Bioinformatics\n- Molecular Informatics\n- Chemogenomics\n- Structural Polymorphism\n- Ligand-Induced Structural Changes",
  "dataset/provenance": "The dataset used in this study is sourced from several well-known databases. The crystal structures for the experiments were obtained from the Protein Data Bank (PDB), which is a freely accessible crystallographic database containing three-dimensional structural data of large biological molecules. The training set decoys were derived from the PubChem database, another freely available resource that provides information on chemical molecules and their activities against biological assays. Additionally, the StARLITe database was utilized, which contains biological activity and binding affinity data between various compounds and proteins, making it suitable for data mining purposes.\n\nThe dataset consists of a training set and a test set for two target proteins. The training set includes 100 positive samples, which are experimentally determined complex structures of the target proteins from the PDB, and 2000 negative samples, resulting in a total of 2100 data points. The test set is more extensive, comprising 500 positive samples and 10,000 negative samples, totaling 10,500 data points. The positive samples in the test set were selected from the StARLITe database, ensuring a diverse range of active compounds. The negative samples were randomly selected from the decoys generated for the training set.\n\nThe dataset preparation involved several steps to ensure its quality and relevance. For the training set, active compounds without determined crystal structures were randomly selected and docked using the GLIDE software to generate five docking poses per compound. These poses were then mixed into the training set. The test set was prepared by clustering active compounds from the StARLITe database and selecting the most inhibitory compound from each cluster. The docking process for the test set followed the same procedure as the training set.\n\nThe dataset's imbalance, with a ratio of 20 negative samples to 1 positive sample, was addressed through the use of ensemble learning methods and iterative weighting to enhance the classifier's performance. This approach aimed to mitigate the impact of the sample set quality on the virtual screening results.",
  "dataset/splits": "Two data splits were used in the experiments: a training set and a test set. The training set consists of 100 positive samples and 2000 negative samples, totaling 2100 data points. The test set is significantly larger, comprising 500 positive samples and 10000 negative samples, totaling 10500 data points. The positive samples in the test set were obtained by selecting the most active compound from each of 100 clusters of active compounds, with each compound contributing five docking poses. The negative samples in both sets were decoy compounds selected from the PubChem database, ensuring a 20:1 ratio of negative to positive samples, which addresses the imbalanced-data problem.",
  "dataset/redundancy": "In our study, we constructed training and test sets for two target proteins to facilitate machine learning. The training set included experimentally determined complex structures from the Protein Data Bank (PDB) as positive samples. To expand the training set, we randomly selected active compounds from known active compounds for which crystal structures with their targets were not determined. This process was repeated 10 times, and for each selected compound, five docking poses were generated using GLIDE and mixed into the training set.\n\nThe test set was prepared by choosing active compounds of the target proteins from StARLITe, dividing them into 100 clusters using hierarchical clustering with the Ward method based on the Euclidean distance between their 2D structure fingerprints. The compound with the highest inhibitory activity was selected from each cluster. These 100 active compounds were docked to their target proteins, and five docking poses for each active compound were used as positive samples for the test set.\n\nTo ensure the independence of the training and test sets, we used different sources for the positive samples. The training set positive samples came from the PDB, while the test set positive samples came from StARLITe. This approach helps to avoid data leakage and ensures that the model's performance is evaluated on truly independent data.\n\nThe distribution of our datasets is characterized by a significant imbalance between negative and positive samples, with a ratio of 20:1. This imbalance is a common challenge in bioinformatics studies and is addressed in our work by improving the machine learning algorithm using ensemble learning methods.\n\nNot sure how this distribution compares to previously published machine learning datasets, as it is not explicitly mentioned in the provided context. However, the imbalance issue is widely recognized and addressed in various bioinformatics and machine learning studies.",
  "dataset/availability": "The data used in this study is available from several public databases. The crystal structures utilized in the experiments are sourced from the Protein Data Bank (PDB), which is freely accessible via the Internet at http://www.rcsb.org/. The training set decoys are derived from the PubChem dataset, also available for free online at http://pubchem.ncbi.nlm.nih.gov/. Additionally, the StARLITe data, which contains biological activity and binding affinity information, was provided by laboratory colleagues. The data from these sources is used to construct training and test sets for machine learning, ensuring that the methods employed are reproducible and accessible to the scientific community. The datasets are structured to include both positive and negative samples, with a focus on addressing the imbalanced-data problem commonly encountered in bioinformatics studies. The specific details of the dataset structure, including the number of positive and negative samples, are provided in the experimental data tables.",
  "optimization/algorithm": "The optimization algorithm employed in this work is an ensemble learning method, specifically Adaboost-SVM. This approach combines multiple classifiers to improve the overall performance and generalization of the model. The core idea is to integrate several weak classifiers, each focusing on different aspects of the data, to create a strong classifier that can handle complex patterns and reduce errors.\n\nThe Adaboost-SVM algorithm is not entirely new; it builds upon existing machine learning techniques. Adaboost is a well-known boosting algorithm that aims to improve the performance of weak classifiers by iteratively adjusting the weights of training samples. SVM, or Support Vector Machine, is a powerful supervised learning algorithm used for classification and regression tasks. By combining these two methods, Adaboost-SVM leverages the strengths of both to enhance the accuracy and robustness of the model.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this work is on its application in virtual screening for drug discovery. The primary contribution lies in demonstrating the effectiveness of ensemble learning methods in improving the quality of virtual screening processes, particularly when dealing with limited and noisy data. The algorithm's application in this specific domain is what makes it novel and relevant to the field of computational biology and drug discovery, rather than the algorithm itself. The emphasis is on how the algorithm addresses the challenges in virtual screening, such as handling low-quality training sets and reducing false positives, rather than the algorithm's theoretical foundations.",
  "optimization/meta": "The model presented in this publication employs an ensemble learning approach, specifically Adaboost-SVM, which integrates multiple machine learning algorithms to enhance the overall performance of virtual screening.\n\nThe ensemble learning method used here combines several weak classifiers, each of which is an SVM (Support Vector Machine). The Adaboost algorithm is utilized to boost the performance of these SVMs by iteratively adjusting the weights of the training samples based on their classification accuracy. This process ensures that the weak classifiers focus more on the misclassified samples in subsequent iterations, thereby improving the overall robustness and accuracy of the model.\n\nThe ensemble learning approach is designed to handle the limitations of single machine learning algorithms by combining their strengths. In this case, the Adaboost-SVM method leverages the capabilities of SVMs while addressing the issue of sample quality in the training set. The independence of the base classifiers is crucial for the effectiveness of the ensemble learning method. Each SVM classifier is trained independently, and their outputs are combined to make the final prediction. This independence helps in reducing the error rate and improving the generalization of the model.\n\nThe experimental results demonstrate that the Adaboost-SVM method outperforms individual SVM and Random Forest algorithms in terms of both 10% Enrichment Factor (EF) and Area Under the Curve (AUC). This indicates that the ensemble learning approach is effective in handling the challenges posed by the quality of the training set and the limited number of structural samples. The method shows a notable improvement in the accuracy and reliability of virtual screening, making it a valuable tool for drug discovery and protein-ligand interaction studies.",
  "optimization/encoding": "In our study, we employed the Pharm-IF method to encode protein-ligand interactions into a binary format. This encoding process was crucial for transforming the complex interactions into a structured format that could be effectively utilized by our machine learning algorithms. The encoded data served as the input for our improved SVM algorithm, Adaboost-SVM, and Random Forest classifiers. This approach allowed us to leverage the strengths of ensemble learning, where multiple weak classifiers are combined to enhance the overall performance and robustness of the model. The encoded data was then used to train and evaluate our models, demonstrating significant improvements in virtual screening efficiency, particularly in handling imbalanced datasets and noise resistance.",
  "optimization/parameters": "In our study, we employed several key parameters to optimize the performance of our models. For the Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel, we utilized parameters such as the kernel function type, degree, sigma, cost factor, cache size, tolerance, penalty factor weight, and cross-validation folds. Specifically, we set the SVM type to C-SVM, the class number to 2, the kernel function to RBF, the degree to 3, sigma to 0.001, the cost factor to 5, the cache size to 500MB, the tolerance to 0.001, the penalty factor weight to 1, and the cross-validation to 5-fold.\n\nFor the Adaboost-SVM ensemble learning method, additional parameters were considered, including the ensemble learning type, the basic classifier type, the number of classifiers per layer, the maximum false alarm rate, the minimum hit rate, the number of iterations, the weight trim rate, and the cache size. We set the ensemble learning type to Adaboost, the basic classifier type to C-SVM, the number of classifiers per layer to 100, the maximum false alarm rate to 0.5, the minimum hit rate to 0.9, the number of iterations to 5, the weight trim rate to 0.9, and the cache size to 500MB.\n\nThe selection of these parameters was guided by the need to balance the complexity of the model with its ability to generalize well to unseen data. The RBF kernel was chosen for its effectiveness in handling non-linear relationships in the data. The sigma value, which controls the width of the RBF kernel, was set based on the standard deviation of the sample set to ensure optimal classification accuracy. The cost factor and tolerance were adjusted to manage the trade-off between minimizing classification errors and preventing overfitting. The number of iterations and the weight trim rate in the Adaboost-SVM were selected to ensure that the ensemble of weak classifiers could effectively learn from the training data and improve the overall classification performance.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our work, we employed an ensemble learning method, specifically Adaboost combined with Support Vector Machines (SVM), to address the challenges of virtual screening in drug discovery. This approach helps to mitigate issues related to the quality of the training set and the complexity of the data.\n\nThe Adaboost-SVM method involves multiple weak classifiers, each based on an SVM with a Radial Basis Function (RBF) kernel. The RBF kernel's mathematical description is given by the formula \\( K(x_i, x_j) = e^{-\\|x_i - x_j\\|^2 / 2\\sigma^2} \\). The parameter \\( \\sigma \\) is crucial as it controls the width of the Gaussian function. If \\( \\sigma \\) is too large, the classifiers become too weak, leading to poor classification performance. Conversely, if \\( \\sigma \\) is too small, the classifiers become too robust, leading to overfitting and reduced generalization.\n\nTo ensure that the model does not overfit, we carefully selected the \\( \\sigma \\) value for each component classifier. We used the standard deviation of the sample set for each component classifier as the \\( \\sigma \\) value. This approach helps to control the classification accuracy of the component classifiers, ensuring that the ensemble learning method remains effective.\n\nThe ensemble learning method also addresses the issue of underfitting by combining multiple weak classifiers. Each weak classifier contributes to the final decision, and the weights of these classifiers are adjusted based on their performance. This iterative process ensures that the model captures the underlying patterns in the data without being too simplistic.\n\nThe number of parameters in our model is managed through the use of cross-validation and the adjustment of sample weights. Cross-validation helps to ensure that the model generalizes well to unseen data, while the adjustment of sample weights ensures that the model focuses on the most informative samples.\n\nIn summary, our approach to fitting the model involves a careful balance between avoiding overfitting and underfitting. By using an ensemble learning method with carefully selected parameters, we ensure that the model is robust and generalizes well to new data.",
  "optimization/regularization": "In our work, we employed several techniques to prevent overfitting and ensure the robustness of our classification models. One of the key methods used was the incorporation of a regularization term in the Support Vector Machine (SVM) optimization process. This term, represented as (1/2)‚Äñùë§‚Äñ2, controls the complexity of the model by penalizing large weights, thereby helping to prevent overfitting to the training data.\n\nAdditionally, we utilized the Adaboost ensemble learning method, which combines multiple weak classifiers to form a strong classifier. This approach not only improves the overall performance but also helps in reducing overfitting by ensuring that each base classifier contributes independently to the final decision. The Adaboost algorithm adjusts the weights of the training samples based on the performance of the previous classifiers, focusing more on the misclassified samples in subsequent iterations. This iterative process enhances the model's ability to generalize to new, unseen data.\n\nFurthermore, we carefully selected the parameters for our SVM and Adaboost-SVM models, including the cost factor and the Gaussian width (ùúé) for the Radial Basis Function (RBF) kernel. The cost factor (ùê∂) in the SVM controls the trade-off between achieving a low training error and a low testing error, while the ùúé value in the RBF kernel influences the smoothness of the decision boundary. By tuning these parameters, we aimed to strike a balance that minimizes overfitting while maximizing the model's performance.\n\nIn summary, our regularization methods included the use of a regularization term in the SVM optimization, the Adaboost ensemble learning technique, and careful parameter tuning. These strategies collectively helped in preventing overfitting and ensuring that our models generalized well to new data.",
  "optimization/config": "The hyper-parameter configurations for the machine learning models used in this study are explicitly detailed in the publication. For the Support Vector Machine (SVM), parameters such as the type of SVM, class number, kernel function, degree in kernel function, sigma in kernel function, cost factor, cache size, tolerance in termination criteria, weight value of penalty factor, and cross-validation settings are provided. Similarly, for the Adaboost-SVM, parameters including the ensemble learning type, class number, basic classifier type, number of classifiers per layer, maximum false alarm rate, minimum hit rate, number of iterations, weight trim rate, and cache size are specified. Additionally, the parameters for the Random Forest algorithm, such as the number of trees, node size, and the number of different descriptors tried at each split, are also reported.\n\nThe optimization schedule and model files are not explicitly mentioned as being available for download. However, the methods and parameters used for optimization are thoroughly described, allowing for reproducibility. The data used in this study, including the crystal structures from the PDB database and the decoys from the PubChem dataset, are available free of charge via the Internet. The StARLITe data was provided by laboratory colleagues.\n\nRegarding the license, the publication does not specify the licensing terms for the data or the models. However, it is implied that the data from PDB and PubChem are freely accessible, suggesting an open-access approach. For the StARLITe data, specific licensing details are not provided, but it is mentioned that the data was shared by colleagues, which may indicate institutional or collaborative sharing agreements.",
  "model/interpretability": "The models employed in this study, including Support Vector Machine (SVM), Adaboost-SVM, and Random Forest, are generally considered to be black-box models. This means that while they are effective in making predictions, the internal workings and decision-making processes are not easily interpretable.\n\nSVM, for instance, operates by finding a hyperplane that best separates the data into different classes. The decision boundaries created by SVM are not straightforward to interpret, especially when using non-linear kernels like the Radial Basis Function (RBF) kernel. The parameters and the support vectors that define the hyperplane are not easily translatable into human-understandable terms.\n\nAdaboost-SVM combines multiple SVM classifiers to improve performance. While the individual SVMs are already complex, the ensemble nature of Adaboost-SVM adds another layer of complexity. The boosting process adjusts the weights of misclassified samples, making the final model even more opaque.\n\nRandom Forest, on the other hand, is an ensemble of decision trees. While individual decision trees can be interpreted by tracing the path from the root to a leaf, the aggregation of many trees in a Random Forest makes it difficult to understand the overall decision-making process. The feature importance scores provided by Random Forest can give some insight into which features are most influential, but they do not provide a clear, step-by-step explanation of how predictions are made.\n\nIn summary, the models used in this study are powerful tools for classification tasks but lack transparency in their decision-making processes. This is a common trade-off in machine learning, where increased predictive power often comes at the cost of interpretability.",
  "model/output": "The model presented in this work is a classification model. It employs Support Vector Machines (SVM) with a Radial Basis Function (RBF) kernel, integrated with the AdaBoost ensemble learning method. The primary goal of this model is to improve the classification of data, particularly in scenarios where the dataset may have imbalances or high complexity. The model's performance is evaluated using metrics such as the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) value, which are standard for assessing classification models. The output of the model is a classification result, indicating whether a given sample belongs to a particular class or not. This is evident from the use of terms like \"true positive rate\" and \"false positive rate,\" which are specific to classification tasks. Additionally, the model's parameters and the description of its training process further confirm that it is designed for classification purposes.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our study, we employed several evaluation methods to assess the performance of our machine learning models in the context of virtual screening. We utilized the receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC) value to evaluate the models' effectiveness. The ROC curve plots the true positive rate against the false positive rate, providing a visual representation of the trade-off between these two metrics. The AUC value quantifies this performance, with higher values indicating better model performance.\n\nAdditionally, we used the enrichment factor (EF) to evaluate the early recognition properties of our screening methods. The EF measures the ratio of the number of active compounds obtained by the screening method to the number that would be obtained by random selection at a predefined sampling percentage. This metric is crucial for assessing the efficiency of our models in identifying active compounds early in the screening process.\n\nTo ensure the robustness of our evaluation, we conducted comparative experiments using three different classification methods: Support Vector Machine (SVM), Adaboost-SVM, and Random Forest. These methods were applied to two types of target proteins, and their performance was compared based on the ROC curves, AUC values, and EF at 10% sampling. The results demonstrated that ensemble learning methods, particularly Adaboost-SVM, outperformed the baseline SVM in terms of both EF and AUC, indicating their superior ability to handle noise and improve screening results, especially when the amount of structural samples is limited.\n\nFurthermore, we addressed the issue of imbalanced data, which is common in virtual screening datasets. Our models were evaluated on datasets where the proportion of negative samples to positive samples was as high as 20:1. Despite this imbalance, our ensemble learning approaches showed significant improvements in performance metrics, highlighting their robustness and effectiveness in handling real-world data challenges.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models in virtual screening. The primary metrics reported include the Enrichment Factor (EF) and the Area Under the Receiver Operating Characteristic Curve (AUC).\n\nThe EF is a crucial measure for assessing the early recognition capabilities of our screening methods. It indicates the ratio of the number of active compounds obtained by virtual screening to those generated by random selection at a predefined sampling percentage. This metric is particularly important in drug discovery, where only a small fraction of compounds from a large database is typically selected for further testing.\n\nThe AUC, on the other hand, provides a comprehensive evaluation of the model's performance across all classification thresholds. It represents the area under the ROC curve, which plots the true positive rate against the false positive rate. An AUC value of 1 indicates a perfect model, while a value of 0.5 suggests a model that performs no better than random guessing. Higher AUC values indicate better model performance.\n\nThese metrics are widely used in the literature for evaluating virtual screening methods and are representative of the standard practices in the field. They provide a clear and quantitative assessment of our models' ability to identify active compounds efficiently and accurately. By reporting both EF and AUC, we ensure a thorough evaluation that considers both the early recognition capabilities and the overall performance of our models.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of different machine learning methods to assess their performance in virtual screening experiments. We employed three distinct classification algorithms: Support Vector Machine (SVM), Adaboost-SVM, and Random Forest. These methods were evaluated on two types of target proteins, SRC and Cathepsin K, to compare their effectiveness in virtual screening.\n\nTo ensure a fair and robust comparison, we used well-established benchmark datasets. The crystal structures for the target proteins were obtained from the Protein Data Bank (PDB), while the decoys and known active compounds were sourced from the PubChem database. Additionally, we utilized the StARLITe database, which contains biological activity and binding affinity data between various compounds and proteins.\n\nThe performance of these methods was evaluated using several metrics, including the enrichment factor (EF) at 10% and the area under the receiver operating characteristic curve (AUC). The ROC curves and AUC values provided a clear visualization and quantitative measure of the trade-off between the true positive rate and the false positive rate for each classifier.\n\nOur results demonstrated that the ensemble learning methods, particularly Adaboost-SVM, outperformed the baseline SVM and Random Forest algorithms. For the SRC model, the 10% EF increased from 4.7 to 5.5, and the AUC value improved from 0.734 to 0.821 when using Adaboost-SVM. Similarly, for the Cathepsin K model, the 10% EF rose from 3.9 to 4.8, and the AUC value enhanced from 0.683 to 0.802. These improvements highlight the superior noise resistance and screening accuracy of the ensemble learning approach, especially when dealing with limited structural samples.\n\nIn summary, our comparison to publicly available methods and simpler baselines on benchmark datasets showed that ensemble learning, particularly Adaboost-SVM, provides a more effective and robust solution for virtual screening experiments. This method significantly enhances the accuracy and reliability of identifying active compounds, addressing the challenges posed by the quality of the training set and the imbalance in data distribution.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The evaluation metrics used in this study include the enrichment factor (EF) and the area under the receiver operating characteristic curve (AUC). These metrics were calculated based on the docking results and the performance of the machine learning models. The EF was used to evaluate the early recognition properties of the screening method, while the AUC provided a more comprehensive assessment of the model's performance across the entire range of sampling percentages. The specific details and results of these evaluations are presented in the publication, but the raw data files themselves are not released to the public."
}