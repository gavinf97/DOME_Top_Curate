{
  "publication/title": "DFLpred: High-throughput prediction of disordered flexible linker regions in protein sequences",
  "publication/authors": "The authors who contributed to this article are Fanchi Meng and Lukasz Kurgan. Both authors played significant roles in the conceptualization, design, testing, and deployment of the DFLpred method for predicting disordered flexible linkers from protein sequences. Their combined efforts led to the development of a novel computational method that integrates multiple sequence-derived markers to accurately predict DFLs.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2016",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Protein sequences\n- Disordered regions\n- Prediction methods\n- Computational biology\n- Bioinformatics\n- Proteome analysis\n- Sequence representation\n- Feature selection\n- Machine learning\n- DFLpred",
  "dataset/provenance": "The dataset used in this study was collected and divided into subsets for a 4-fold cross-validation protocol. This protocol was employed to design and parameterize the predictive model, with four subsets used for training and one subset reserved for independent testing. The training dataset consists of 144 sequences, while the test dataset comprises 60 sequences. These datasets are available for public access at a specified URL.\n\nThe sequences in the test dataset were chosen to have low similarity with those in the training dataset, ensuring that the model's performance could be evaluated on novel data. Similarly, the sequences within individual folds of the training dataset also share low similarity with sequences in other folds. This design helps in assessing the model's generalization capability.\n\nThe dataset was annotated using Interpro, a tool that integrates multiple databases to predict protein families and domains. This annotation process was crucial for investigating the localization of DFLs (disordered flexible linkers) within or between domains. The ratio of intra-domain to inter-domain DFL residues in the training dataset was found to be approximately 0.82, indicating a higher prevalence of DFLs within domains.",
  "dataset/splits": "The dataset was divided into five subsets. Four of these subsets were used in a 4-fold cross-validation protocol to design the predictor, including conceptualizing and selecting inputs for the predictive model, and selecting and parameterizing this model. These data constitute the training dataset, which contains 144 sequences. The remaining fifth subset was used as an independent test dataset, containing 60 sequences. This approach ensures that sequences in the test dataset share low similarity with sequences in the training dataset, and sequences in individual folds of the training dataset share low similarity with sequences in the other folds.",
  "dataset/redundancy": "The datasets were split using a 4-fold cross-validation protocol, where four subsets were used for training and the remaining subset was used for independent testing. This approach ensures that the sequences in the test dataset share low similarity with those in the training dataset, and sequences in individual folds of the training dataset also share low similarity with sequences in other folds. The training dataset consists of 144 sequences, while the test dataset contains 60 sequences. This method of splitting helps to maintain the independence of the training and test sets, which is crucial for evaluating the predictive model's performance accurately. The distribution of sequences in the training and test datasets is designed to reflect a low similarity, which is a common practice in machine learning datasets to prevent data leakage and overfitting.",
  "dataset/availability": "The datasets used in our study are publicly available. The training and test datasets, which consist of 144 and 60 sequences respectively, can be accessed at http://biomine.ece.ualberta.ca/DFLpred/. These datasets were carefully curated to ensure that sequences in the test dataset share low similarity with those in the training dataset, and similarly, sequences in individual folds of the training dataset share low similarity with sequences in other folds. This approach helps in evaluating the generalization capability of our predictive model. The datasets are provided without any specific license restrictions, allowing researchers to freely use them for further studies or comparisons. The availability of these datasets ensures reproducibility and transparency in our research, enabling other scientists to validate our findings and build upon our work.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages a combination of feature selection and model parameter tuning techniques to enhance predictive performance. We utilized logistic regression, naive Bayes, and k-nearest neighbor classifiers, which are well-established machine-learning algorithms. These algorithms were chosen for their robustness and widespread use in various predictive modeling tasks.\n\nThe machine-learning algorithms used are not new; they are standard classifiers in the field of machine learning. Logistic regression is a linear model for binary classification, naive Bayes is a probabilistic classifier based on Bayes' theorem, and k-nearest neighbor is an instance-based learning algorithm. These algorithms were selected for their ability to handle different types of data and their effectiveness in previous studies.\n\nThe focus of our publication is on the application of these algorithms to the specific problem of predicting disordered flexible linkers (DFLs) in protein sequences. The optimization process involves a two-step feature selection method that normalizes and ranks features based on their correlation with the target variable. This is followed by eliminating mutually correlated features using the Pearson correlation coefficient. The selected features are then used to train and parameterize the classifiers through a 4-fold cross-validation process.\n\nThe reason these algorithms were not published in a machine-learning journal is that the innovation lies in their application to a specific biological problem rather than in the development of new machine-learning techniques. The primary contribution of our work is the development of a predictive model for DFLs, which includes the selection of relevant features and the optimization of classifier parameters to achieve high predictive accuracy. This application-driven approach is more suited to a bioinformatics or computational biology journal, where the focus is on solving biological problems using existing machine-learning methods.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a linear function to combine values of four empirically selected features to generate output propensities. These features are computed from the sequence using sliding windows on putative annotations generated with IUPred and two amino acid indices.\n\nThe model was designed using a 4-fold cross-validation protocol on a training dataset, which consisted of four subsets of the data. The remaining fifth subset was used as an independent test dataset, ensuring that sequences in the test dataset shared low similarity with sequences in the training dataset. This approach guarantees that the training data is independent and that the model's performance can be reliably assessed on unseen data.\n\nThe training and test datasets were carefully constructed to include sequences with low similarity to each other, ensuring that the model's performance is not biased by overfitting to similar sequences. The training dataset consisted of 144 sequences, while the test dataset consisted of 60 sequences. This division of data ensures that the model's performance can be evaluated on independent data, providing a robust assessment of its predictive quality.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the input sequences for the machine-learning algorithm. Initially, each residue in the input sequence was represented by its amino acid type and various predicted properties. These properties included the propensity to form structured regions, intrinsically disordered regions, and helical and coil conformations. Additionally, physicochemical properties were estimated using the AAindex database, secondary structure was predicted with PSIPRED, and sequence complexity was computed with SEG.\n\nThe next step involved converting this representation into a set of numeric features using sliding windows. The sliding window aggregated structural and sequence-based information by considering the characteristics of amino acids adjacent in the sequence. The length of the sliding window was set to 17, which is the median value of the length of the longest per-protein DFLs in our dataset. This window size ensures that it covers the full length of at least half of the DFLs without introducing much potential noise from non-DFL residues when predicting shorter regions. For residues at the termini of the sequence, the window size was reduced accordingly, and feature values were normalized by the size of the window.\n\nIn total, 2236 features were considered, including 40 features derived directly from the sequence, 2124 features derived from physicochemical properties of amino acids, 22 features generated from putative secondary structure, 40 features from putative intrinsic disorder and structured regions, and 10 features from sequence complexity. These features quantified the composition of amino acids, counts and lengths of various structural elements, and physicochemical properties within the sliding window.\n\nTo handle the large number of features, a two-step empirical feature selection process was employed. In the first step, low-quality features with low correlation to the annotation of DFLs were removed. This involved calculating the point-biserial correlation coefficient for real-valued features and the u coefficient for binary features. Features with absolute normalized values below a certain threshold were discarded. The remaining features were then ranked by their absolute normalized values.\n\nIn the second step, mutually correlated features were eliminated using the Pearson correlation coefficient. This process involved initializing a set of selected features with the top-ranked feature from the first step and then iteratively adding features that had a low Pearson correlation with the already selected features. This procedure was repeated for the entire list of ranked features, resulting in a subset of features characterized by high predictive value and low mutual correlations.\n\nThe final set of selected features was then used as input for the predictive model, which included logistic regression, naive Bayes, and k-nearest neighbor classifiers. The parameters for logistic regression and k-nearest neighbor were optimized to achieve the highest AUC value in 4-fold cross-validation on the training dataset. The naive Bayes classifier, which has no parameters, was used as is. The selected features and optimized parameters were then evaluated on the test dataset to assess their predictive performance.",
  "optimization/parameters": "In our model, we utilized a combination of thresholds and classifier parameters to optimize performance. Specifically, we varied two thresholds, Tstep1 and Tstep2, between 0.1 and 0.9 with a step of 0.05, resulting in 289 different feature sets. Each feature set was then evaluated using three classifiers: logistic regression, naive Bayes, and k-nearest neighbor. For logistic regression, we considered ridge values ranging from 10^-4 to 10^4 with a step of 1. For the k-nearest neighbor classifier, we varied the number of neighbors (k) from 50 to 800 with a step of 50. The naive Bayes classifier did not require parameter tuning as it has no parameters. The optimal parameters were selected based on the highest AUC value achieved in 4-fold cross-validation on the training dataset. This comprehensive approach ensured that the model was thoroughly optimized for predictive performance.",
  "optimization/features": "In our study, we initially considered a comprehensive set of 2236 features for each residue in the input amino acid sequence. These features encompassed a wide range of properties, including amino acid types, physicochemical properties derived from the AAindex database, putative secondary structures, intrinsically disordered and structured regions, and sequence complexity.\n\nTo ensure that only the most relevant and non-redundant features were used, we performed a two-step empirical feature selection process. This process was crucial for enhancing the predictive power of our model. The first step involved removing low-quality features that had weak correlations with the annotation of DFLs. We utilized the point-biserial correlation coefficient for real-valued features and the u coefficient for binary features. Features with absolute normalized values below a specified threshold were eliminated. The remaining features were then ranked by their absolute normalized correlation values.\n\nIn the second step, we eliminated mutually correlated features using the Pearson correlation coefficient. This step ensured that the selected features were not only predictive but also independent of each other, thereby reducing redundancy. The feature selection process was conducted exclusively using the training dataset, ensuring that the model's performance on the test dataset was not influenced by data leakage.\n\nAs a result of this rigorous feature selection process, we identified a subset of four features that provided the highest predictive value. These features were then used in our final model, which combines their values using a linear function to generate output propensities. This approach allowed us to achieve statistically significant improvements in predictive quality compared to other classifiers.",
  "optimization/fitting": "The fitting method employed in this study involved a systematic approach to feature selection and model optimization, which helped to mitigate both overfitting and underfitting.\n\nTo address the potential issue of overfitting, given the large number of features initially considered, a two-step feature selection process was implemented. First, features were normalized and ranked based on their correlation with the target variable. Then, mutually correlated features were eliminated using the Pearson correlation coefficient. This process ensured that only the most relevant and non-redundant features were retained, reducing the risk of overfitting.\n\nAdditionally, the model selection process involved cross-validation, specifically 4-fold cross-validation, which is a robust technique for assessing model performance and generalizability. This method helps to ensure that the model performs well on unseen data, further mitigating overfitting.\n\nTo avoid underfitting, the study considered a wide range of thresholds and parameters for feature selection and model training. For instance, the thresholds for feature selection were varied between 0.1 and 0.9, and the parameters for the classifiers were optimized based on their performance in cross-validation. This extensive parameter tuning helped to ensure that the model was complex enough to capture the underlying patterns in the data.\n\nFurthermore, the use of multiple classifiers, including logistic regression, naive Bayes, and k-nearest neighbor, provided a comprehensive evaluation of the model's performance. The final model, logistic regression with four features, was selected based on its superior performance in terms of AUC, AUC_lowFPR, and ratio, indicating that it effectively balanced model complexity and performance.\n\nIn summary, the fitting method employed in this study carefully addressed the risks of overfitting and underfitting through feature selection, cross-validation, and extensive parameter tuning, resulting in a robust and generalizable model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive model. One of the key methods used was regularization, specifically for the logistic regression classifier. We applied ridge regularization, which adds a penalty term to the loss function to constrain the coefficients of the model. This helps to prevent the model from fitting the noise in the training data. The ridge parameter, denoted as λ, was varied from 10^-4 to 10^4 with a step of 1, and the optimal value was selected based on the highest area under the curve (AUC) in a 4-fold cross-validation on the training dataset.\n\nAdditionally, we used feature selection techniques to reduce the dimensionality of the data and eliminate irrelevant or redundant features. This was done in two steps. First, we normalized the values of the average point-biserial (rpb) and u correlations to a 0 to 1 range using min–max normalization and removed features with absolute normalized rpb or u values below a certain threshold. The remaining features were then ranked by their absolute normalized rpb or u values. In the second step, we eliminated mutually correlated features using the Pearson correlation coefficient (rpc). Features with an absolute rpc value below a certain threshold were added to the set of selected features, while others were discarded. This process was repeated for the entire list of ranked features.\n\nWe also varied the thresholds for feature selection and the parameters of the classifiers to obtain different feature sets and models. The final model was selected based on the highest AUC value in the cross-validation, ensuring that it generalizes well to unseen data. Furthermore, we used an independent test dataset that was not used in the training or validation process to evaluate the performance of the final model. This dataset consisted of sequences that shared low similarity with those in the training dataset, providing an unbiased assessment of the model's predictive accuracy.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we varied thresholds Tstep1 and Tstep2 between 0.1 and 0.9 with a step of 0.05, resulting in 289 different feature sets. These configurations were tested with three classifiers: logistic regression, naive Bayes, and k-nearest neighbor, using 4-fold cross-validation on the training dataset. The parameters for logistic regression and k-nearest neighbor were selected based on the highest AUC values obtained from the cross-validation. For logistic regression, the ridge parameter ranged from 10^-4 to 10^4, and for k-nearest neighbor, the number of neighbors ranged from 50 to 800. The best-performing model, logistic regression with four features, is highlighted for its statistically significant performance metrics.\n\nThe model files and specific optimization schedules are not explicitly provided in the publication. However, the methodology and parameters used to achieve the reported results are thoroughly documented, allowing for replication of the experiments. The webserver implementation of DFLpred, available at http://biomine.ece.ualberta.ca/DFLpred, provides a practical tool for users to apply the model to their own data. The webserver requires input protein sequences in FASTA format and an email address for result notification. This implementation ensures that the model's predictive capabilities are accessible to the scientific community.",
  "model/interpretability": "The DFLpred model is not a blackbox model. It combines values of four empirically selected features using a linear function to generate the output propensities. These features were computed from the sequence using sliding windows on putative annotations generated with IUPred and two amino acid indices. The model's architecture includes three layers:\n\n1. Represent every residue of the input sequence with its amino acid type and information predicted directly from the sequence, including propensity to form structured regions, intrinsically disordered regions, and helical and coil conformations.\n2. Convert this representation into an empirically selected set of numeric features that are computed using sliding windows.\n3. Input the selected features into an empirically selected and parameterized predictive model to generate propensity scores.\n\nThe features used in the model are ranked by their absolute normalized point-biserial correlation coefficient values, which measure the correlation of a given feature with the annotation of DFLs in the training dataset. The model eliminates mutually correlated features using the Pearson correlation coefficient to ensure that the selected features are not redundant.\n\nThe model's transparency is further enhanced by the use of a linear function, which makes it easier to interpret the contribution of each feature to the final prediction. The model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC), AUC at low false positive rates, and the ratio of true positive rates to false positive rates. The model's predictive quality is statistically significant compared to other classifiers.\n\nIn summary, the DFLpred model is transparent and interpretable, with a clear architecture and empirically selected features that are computed using sliding windows. The model's use of a linear function and evaluation metrics further enhance its interpretability.",
  "model/output": "The model is a classification model. It predicts whether a residue in a protein sequence is a disorder-promoting loop (DFL) or not (NDFL). The output of the model is a propensity score, which indicates the likelihood of a residue being a DFL. This propensity score is then converted into a binary prediction using a cutoff value of 0.18. Residues with a propensity score greater than 0.18 are classified as DFLs, while those with a score of 0.18 or less are classified as NDFLs. The model uses a linear function to combine the values of four empirically selected features to generate these propensity scores. The final output includes both the propensity scores and the binary predictions, which are made available for download via a web server. The server also delivers a notification email with the results and the URL for download. The web server supports batch predictions for datasets containing up to 5000 proteins.",
  "model/duration": "The execution time of our model, DFLpred, is significantly faster compared to other methods. The runtime of DFLpred, along with other methods like Espritz_NMR, PredyFlexy, UMA, FlexPred, and PredBF, was measured and compared. The runtime of these methods spans from 10^2 to 10^6 milliseconds, depending on the length of the input protein sequences. DFLpred is up to 4 orders of magnitude faster than the alternatives, making it a highly efficient tool.\n\nTo provide a practical example, if used to predict the complete reviewed human proteome from UniProt, which consists of 20,193 sequences with an average length of 561 residues, DFLpred would take approximately 40 minutes. In contrast, other methods like ESpritz, PredyFlexy, UMA, FlexPred, and PredBF would take significantly longer, ranging from 80 minutes to over 231 days. This demonstrates the substantial advantage of DFLpred in terms of speed, allowing it to handle large-scale predictions efficiently.\n\nThe measured runtime of DFLpred on the human proteome was 38 minutes, which closely matches the estimated 40 minutes, validating the accuracy of our runtime estimates. This efficiency makes DFLpred capable of providing predictions for the complete human proteome, or any other proteome, using a modern personal computer in under an hour.",
  "model/availability": "The source code for DFLpred is not explicitly mentioned as being released. However, the method is freely available as a web server. Users can access it at http://biomine.ece.ualberta.ca/DFLpred. The web server requires users to provide input protein sequences in FASTA format and an email address. The email is used to deliver a notification of the finished prediction and a URL where the results can be downloaded. The same information is available in the browser window until the prediction is finished. The server automatically generates the corresponding propensities and binary predictions, with residues having a propensity greater than 0.18 assumed to form DFLs. The web server supports batch predictions of datasets with up to 5000 proteins.",
  "evaluation/method": "The evaluation of the method involved a rigorous process to ensure its predictive quality. A 4-fold cross-validation protocol was employed to design the predictor, which included conceptualizing and selecting inputs for the predictive model, as well as selecting and parameterizing the model. This process utilized four subsets of the data for training, while the remaining fifth subset served as an independent test dataset. This approach ensured that sequences in the test dataset shared low similarity with those in the training dataset, and similarly, sequences in individual folds of the training dataset shared low similarity with sequences in other folds. The training dataset consisted of 144 sequences, and the test dataset had 60 sequences.\n\nThe predictive quality was assessed using several metrics, including the Area Under the Curve (AUC), AUClowFPR, and ratio values. These metrics were calculated over the combined test folds in the cross-validation, representing results on the entire training dataset. The evaluation also included statistical significance tests to compare the predictive quality of different classifiers, with a p-value threshold of less than 0.01 indicating significant differences.\n\nThe method was further evaluated on residues localized within and between domains. Supplementary Table S2 provides a comparison of predictive quality for various methods, including DFLpred, UMA, Predyflexy, FlexPred, PredBF, PROFbval, Dynamine, Espritz NMR, IUPred_short, and MFDp. The methods were ranked by their AUC values in each category, with p-values quantifying the significance of differences in predictive quality when compared to DFLpred.\n\nThe evaluation demonstrated that the method provides relatively accurate predictions even for proteins that share low sequence identity. The AUC value of 0.72 secured by DFLpred reflects the challenging nature of the dataset and the potential for incomplete annotations of DFLs. The method's performance was also characterized by a low runtime, with the prediction of the entire proteome taking less than one hour on a modern desktop computer. Additionally, the analysis of putative DFLs in the human proteome generated with DFLpred indicated that DFLs are likely present in many human proteins, with about 10% of human proteins having a significant content of DFL residues.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the predictive quality of our model. The primary metric reported is the Area Under the Receiver Operating Characteristic Curve (AUC), which provides a single scalar value that summarizes the performance of the classifier across all classification thresholds. This metric is widely used in the literature and offers a robust measure of a model's ability to discriminate between positive and negative classes.\n\nAdditionally, we calculated the AUC for the low range of False Positive Rates (AUC_lowFPR), specifically between 0 and 0.1. This metric is crucial for evaluating the model's performance in predicting a small number of high-quality positive residues, where the majority of predictions are true positives. This is particularly important in biological contexts where false positives can be costly.\n\nWe also computed the Ratio, which is the AUC_lowFPR divided by the AUC of a random predictor. This ratio indicates how much better our predictor performs compared to a random guess, especially when the false positive rate is low. A higher ratio signifies a more effective model in identifying true positives with minimal false positives.\n\nTo ensure the statistical significance of our results, we compared the predictive performance of our model with other methods using paired t-tests or Wilcoxon signed-rank tests, depending on the normality of the data. This approach helps in verifying that the observed improvements are not due to chance or biased by specific subsets of the dataset.\n\nThese metrics collectively provide a thorough evaluation of our model's performance, ensuring that it is both effective and reliable in predicting the desired outcomes. The use of AUC, AUC_lowFPR, and the Ratio aligns with established practices in the field, making our evaluation representative and comparable to other studies.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of DFLpred by comparing its predictive performance with several publicly available methods on benchmark datasets. These methods included UMA, which predicts flexible linkers, and various predictors of flexible residues and disordered residues. Additionally, we considered a domain predictor, given that classical linkers are typically found between domains.\n\nTo ensure a comprehensive comparison, we also combined the results of UMA with those of disorder predictors and flexibility predictors with disorder predictors. This combination was motivated by the idea that these methods could potentially identify flexible linkers or flexible residues located in disordered regions, which is a characteristic feature of DFLs. We employed two different approaches to combine these predictions: multiplying the scores predicted by UMA and flexibility predictors by the binary disorder predictions, and scaling these scores by the predicted propensity for disorder.\n\nWe utilized a diverse set of predictors for flexible residues, including PROFbval, FlexPred, PredBF, PredyFlexy, and Dynamine. For disordered residues, we considered two versions of IUPred (short and long), MFDp, and three versions of Espritz (NMR, X-Ray, and DisProt). To predict domains, we applied ThreaDom, which is known for its strong predictive performance and the availability of a webserver.\n\nThe results of these comparisons are summarized in Table 2, which shows the performance of DFLpred and the other methods on the test dataset. We evaluated DFLpred, UMA, five methods for predicting flexible residues, three methods for predicting disordered residues, and ThreaDom for domain prediction. Additionally, we included the results for the two ways of combining these methods, which secured the highest AUC value. The evaluation metrics used were AUC, AUC_lowFPR, and Ratio.\n\nOur findings demonstrated that DFLpred achieved the highest AUC, AUC_lowFPR, and Ratio values, indicating its superior performance compared to the other methods. The improvement offered by DFLpred was statistically significant at a p-value of less than 0.01 when compared with all considered methods. Furthermore, the Ratio indicated that DFLpred is 3.3 times better than a random predictor in AUC for low values of FPR/C20 0.1. This difference is visualized in the insert of Figure 3, which compares the AUC of a random predictor with that of DFLpred.\n\nIn summary, our comparison with publicly available methods and simpler baselines on benchmark datasets confirmed the robustness and accuracy of DFLpred in predicting DFLs.",
  "evaluation/confidence": "The evaluation of our method, DFLpred, includes several performance metrics such as AUC, AUClowFPR, and Ratio, which are calculated over combined test folds in cross-validation, representing results on the entire training dataset. To assess the statistical significance of the differences in predictive quality between DFLpred and other classifiers, we employed a rigorous testing procedure. This involved randomly selecting half of the proteins from the test dataset multiple times and measuring the predictive performance of all considered methods on these subsets. The statistical significance of the differences was determined using paired t-tests or Wilcoxon signed-rank tests, depending on the normality of the measurements. A difference was considered significant if the p-value was less than 0.01. This approach ensures that the results are not biased by any subset of proteins and that the predictive performance is consistent across different subsets of the dataset. The use of statistical tests and the reporting of p-values provide confidence in the superiority of DFLpred over other methods and baselines.",
  "evaluation/availability": "Not enough information is available."
}