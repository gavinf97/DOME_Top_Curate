{
  "publication/title": "Deep Neural Networks for Predicting Human Age and Sex from Blood Biochemistry Data",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "AGING",
  "publication/year": "2016",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Deep learning\n- Deep neural networks\n- Biomarker development\n- Aging biomarkers\n- Human aging\n- Machine learning\n- Permutation feature importance\n- Random forests\n- Gradient boosting machine\n- k-nearest neighbors",
  "dataset/provenance": "The dataset used in this study was provided by an independent laboratory. The data consists of anonymized statistical records from human blood tests, with no patient records included in the study. In total, the dataset contains 62,419 records. Each record includes a person's age and 46 standardized blood markers, such as Glucose, Cholesterol, and Alpha-1-globulins. This dataset has not been used in previous papers by the community. The data was processed to exclude certain markers and normalize the remaining ones for use in training deep neural networks. The processed data was presented in a tabular format with 62,419 rows and 42 columns, including age, sex, and 41 blood markers.",
  "dataset/splits": "The dataset used in the study was split into two main parts: a training set and a test set. The split ratio was 90/10, resulting in 56,177 samples for training and 6,242 samples for testing. This division was done to train and evaluate the performance of the deep neural networks (DNNs) effectively. The training set was used to adjust the hyperparameters of the DNNs, while the test set was used to measure the performance of the trained models. This approach ensures that the models are trained on a large enough dataset to learn patterns and are evaluated on a separate dataset to assess their generalization capability.",
  "dataset/redundancy": "The dataset used in this study consisted of 62,419 anonymized records of human blood tests, each containing a person's age and 46 standardized blood markers. To prepare the data for training deep neural networks, the blood markers were normalized to a 0-1 range. The dataset was then split into training and test sets with a 90/10 ratio, resulting in 56,177 samples for training and 6,242 samples for testing. This split ensures that the training and test sets are independent, which is crucial for evaluating the performance of the models.\n\nThe distribution of the data reflects real-world scenarios where individuals tend to seek medical tests only when they experience complex health issues. This results in a dataset where the minimum and maximum values of each marker are far from their normal range. Notably, there were no patients with blood test values within the reference range, indicating that the dataset is composed of individuals with varying health conditions.\n\nThe most frequently abnormal markers in the distribution were white blood cell count markers, such as basophils, eosinophils, lymphocytes, monocytes, and neutrophils. These markers were excluded from the analysis to prevent the deep neural network predictions from being biased by abnormal ranges. The processed data was presented in a tabular format with 62,419 rows and 42 columns, including age, sex, and 41 blood markers.\n\nThe dataset's distribution differs from previously published machine learning datasets in that it focuses on individuals with health issues, rather than a balanced mix of healthy and unhealthy individuals. This makes the dataset more representative of real-world medical testing scenarios, where individuals often seek tests due to symptoms or concerns about their health. The independence of the training and test sets was enforced by the 90/10 split, ensuring that the models were trained and evaluated on distinct data samples.",
  "dataset/availability": "The data used in this study is not publicly available. The dataset consists of anonymized statistical data of human blood tests provided by an independent laboratory. No patient records were used in the study. The data contains 62,419 records, each consisting of a person's age and 46 standardized blood markers. The data was split into training and test sets with a 90/10 ratio, resulting in 56,177 samples for training and 6,242 samples for testing. The dataset was processed and normalized for training deep neural networks, but it has not been released in a public forum. The study does not provide details on the enforcement of data availability or licensing.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is deep learning, specifically deep neural networks (DNNs). These are a type of artificial neural network with multiple layers between the input and output layers. The DNNs employed were simple feed-forward neural networks trained with the standard backpropagation algorithm.\n\nThe optimization algorithm used for training these DNNs was AdaGrad. AdaGrad is an adaptive learning rate method that adjusts the learning rate for each parameter based on the historical gradients. This approach helps in improving the convergence speed and the stability of the training process.\n\nThe choice of AdaGrad as the optimizer was driven by its effectiveness in handling sparse data and its ability to adapt the learning rate for each parameter individually. This adaptability is particularly useful in the context of training deep neural networks, where different parameters may require different learning rates to converge efficiently.\n\nThe use of DNNs and the AdaGrad optimizer is not new in the field of machine learning. However, the specific application of these techniques to predict human chronological age using blood biochemistry markers is novel. The focus of this study is on the biological and medical implications of the findings rather than the development of new machine-learning algorithms. Therefore, it was published in a journal focused on aging research rather than a machine-learning journal.",
  "optimization/meta": "The meta-predictor employed in this study utilizes a stacking ensemble technique, which indeed uses data from other machine-learning algorithms as input. Specifically, the stacking ensemble model integrates the predictions of multiple deep neural networks (DNNs). These DNNs were trained with varying hyperparameters to ensure diversity in their predictions, which is crucial for the effectiveness of the stacking ensemble.\n\nThe stacking ensemble method combines the predictions of these DNNs using a meta-model. Through experiments, it was determined that the ElasticNet model performed best as the meta-model for this ensemble. This meta-model was selected based on its superior performance in terms of both the coefficient of determination (R²) and epsilon-prediction accuracy.\n\nTo ensure the robustness of the meta-predictor, model selection was performed using 10-fold cross-validation. Additionally, a random search strategy was employed to find the optimal hyperparameters for the considered models. This approach helps in validating that the training data is independent and that the meta-predictor generalizes well to unseen data.\n\nThe process involved training 40 different DNNs on a large dataset of blood test samples. The predictions from these DNNs were then used to construct the stacking ensemble. Two iterative strategies were used to determine the optimal number of DNNs to include in the ensemble: one based on adding DNNs with decreasing R² and another based on increasing the correlation between DNNs. Both strategies converged on the conclusion that no more than 21 DNNs were necessary to achieve optimal performance.\n\nThe final stacking ensemble model, which included 21 DNNs, demonstrated an R² of 0.82 and an epsilon-prediction accuracy of 83.5% within a 10-year frame. This indicates that the meta-predictor effectively leverages the strengths of multiple DNNs to improve prediction accuracy.",
  "optimization/encoding": "The data used in this study consisted of anonymized statistical records from human blood tests, provided by an independent laboratory. Each record included a person's age and 46 standardized blood markers. To prepare this data for the deep neural network (DNN), several preprocessing steps were undertaken.\n\nInitially, five blood markers—basophils, eosinophils, lymphocytes, monocytes, and neutrophils—were excluded due to their high variability and potential to bias the DNN predictions. This exclusion was based on the observation that these markers are frequently abnormal and influenced by various factors such as immune function, infections, allergies, smoking, and metabolic diseases.\n\nThe remaining 41 blood markers, along with age and sex, were then normalized to a 0-1 range using the formula:\n\n\\[ \\text{Normalized Value} = \\frac{\\text{Original Value} - \\text{Minimum Value}}{\\text{Maximum Value} - \\text{Minimum Value}} \\]\n\nThis normalization process ensured that all features were on a comparable scale, which is crucial for the effective training of neural networks.\n\nThe dataset was subsequently split into training and test sets with a 90/10 ratio, resulting in 56,177 samples for training and 6,242 samples for testing. This split allowed for the evaluation of the DNN's performance on unseen data, providing a more reliable assessment of its generalization capabilities.\n\nIn summary, the data encoding involved the exclusion of highly variable blood markers, normalization of the remaining features to a 0-1 range, and splitting the dataset into training and test sets. These steps were essential for preparing the data for the machine-learning algorithm and ensuring robust and unbiased model performance.",
  "optimization/parameters": "In our study, we utilized a deep neural network (DNN) ensemble for predicting human age using blood test data. The model was trained with a set of 41 parameters, which are the features derived from the blood biochemistry reports. These parameters were selected based on their availability in large numbers of blood test results.\n\nThe selection of these parameters was influenced by the need to build a robust and reliable model. However, due to the constraints of the available data, some features that might have been useful were not included. For instance, certain globulin fractions are no longer frequently measured in standard blood tests, limiting their inclusion in our model.\n\nTo handle missing values, which were common as users often did not specify all 41 parameters, we implemented a median filling strategy. This approach was chosen after evaluating several strategies, including zero, mean, mode, and median filling, and was found to have the best performance in terms of both R² and epsilon-prediction accuracy.\n\nThe model's architecture and hyperparameters were carefully tuned to optimize performance. We experimented with various configurations, including the number of hidden layers, the number of neurons in each layer, the choice of activation function, the optimization method, and regularization techniques. The best-performing DNN in the ensemble had 5 hidden layers with 2000, 1500, 1000, 500, and 1 neurons respectively. This configuration, along with the use of the PReLU activation function, AdaGrad optimizer, Dropout regularization, and batch normalization, contributed to the model's high accuracy and robustness.",
  "optimization/features": "The input features for our model consist of 41 standardized blood markers. These markers include various biochemical measurements such as Glucose, Cholesterol, and Alpha-1-globulins, among others. Initially, the dataset contained 46 markers, but five were excluded due to their high variability and potential bias in the predictions. These excluded markers were related to white blood cell counts, which can be influenced by various factors such as immune function, infections, allergies, and metabolic diseases.\n\nFeature selection was performed using the Permutation Feature Importance (PFI) method. This method involves shuffling each feature multiple times and computing the average PFI score to determine the significance of each feature. The top features identified through this process were used to train the deep neural network (DNN). Specifically, the top 10 features by PFI were found to be robust and reliable for predicting age, with the DNN achieving an R² of 0.63 and 70% epsilon-prediction accuracy within a 10-year frame.\n\nThe feature selection process was conducted using the training set only, ensuring that the model's performance was evaluated on an independent test set. This approach helps to prevent overfitting and ensures that the selected features are generalizable to new data. The final model was trained on a dataset split into 90% training and 10% testing, with the training set consisting of 56,177 samples and the test set consisting of 6,242 samples.",
  "optimization/fitting": "In our study, we employed deep neural networks (DNNs) with a significant number of parameters, which indeed exceeded the number of training points. To address the risk of overfitting, we implemented several strategies.\n\nFirstly, we used an ensemble of DNNs, each with different hyperparameters, including varying numbers of hidden layers, neurons, activation functions, and regularization techniques. This approach helped to ensure that the models did not memorize the training data but rather generalized well to unseen data.\n\nSecondly, we incorporated regularization techniques such as Dropout with a probability of 0.2 after each layer and l2 weight decay. These methods helped to prevent the model from becoming too complex and overfitting the training data.\n\nAdditionally, we used Batch Normalization after the first two layers to stabilize the learning process and improve the model's generalization performance.\n\nTo further mitigate overfitting, we performed extensive hyperparameter tuning and selected the best-performing models based on their performance on a validation set. This ensured that the models were not only fitting the training data well but also generalizing to new data.\n\nRegarding underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. The use of multiple hidden layers and a large number of neurons allowed the models to learn intricate relationships. Moreover, the comparison of different architectures and hyperparameters helped us to find a balance between model complexity and performance.\n\nThe final ensemble model, consisting of 21 DNNs, demonstrated strong performance metrics, including an R2 value of 0.82 and 83.5% epsilon-prediction accuracy within a 10-year frame. This indicates that our models were neither overfitting nor underfitting the data but rather achieving a good balance between bias and variance.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and improve the generalization of our deep neural networks (DNNs). One of the key methods used was Dropout, which involves randomly setting a fraction of input units to zero at each update during training time. This helps to prevent units from co-adapting too much, thereby improving the model's ability to generalize to unseen data. We applied Dropout with a probability of 0.2 after each layer.\n\nAdditionally, we utilized l2 weight decay, also known as Ridge regularization. This technique adds a penalty equal to the square of the magnitude of coefficients to the loss function, which helps to keep the weights small and reduces the risk of overfitting.\n\nTo further stabilize the training process and mitigate overfitting, we implemented Batch Normalization after the first two layers. This technique normalizes the inputs of each layer, which helps to accelerate training and provides some regularizing effect.\n\nThese regularization methods, combined with careful tuning of hyperparameters, contributed to the robustness and effectiveness of our DNN models in predicting human age.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are reported in detail. Specifically, we provided a table of experiments with different hyperparameters for the deep neural networks (DNNs) in the supplementary material. This table includes variations in the number of hidden layers, the number of neurons in each layer, the choice of activation function, the optimization method, and regularization techniques.\n\nThe best-performing DNN in our ensemble had five hidden layers with 2000, 1500, 1000, 500, and 1 neurons respectively. This model used the PReLU activation function, the AdaGrad optimizer, Dropout with a probability of 0.2 after each layer, and l2 weight decay. Batch normalization was applied after the first two layers to further stabilize the model's convergence and mitigate overfitting.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the main text or supplementary materials. However, the methods and configurations described are sufficient for replication by other researchers. The supplementary data, including the table of hyperparameters and descriptive statistics, is available for reference and can be used to reproduce the experiments.\n\nThe publication does not specify the licensing terms for the supplementary data or model configurations. However, academic publications typically allow for the reuse of methods and configurations for research purposes, provided that proper citation is given. For specific licensing details, one would need to refer to the journal's policies or contact the authors directly.",
  "model/interpretability": "The model employed in this study is not entirely a black box. To enhance interpretability, a Permutation Feature Importance (PFI) method was utilized. This technique assesses the significance of each feature by measuring the impact of permuting its values on the model's performance. The underlying assumption is that permuting important features will significantly reduce the model's accuracy, providing insights into which features are crucial for predictions.\n\nThe PFI method was applied to analyze the importance of blood test markers. This approach yielded a sorted list of markers based on their importance, revealing that markers such as albumin, glucose, alkaline phosphatase, urea, and erythrocytes are particularly significant. This method is native to the model and straightforward to interpret, making it a valuable tool for understanding the contributions of different features.\n\nAdditionally, a top features analysis was conducted to evaluate how the performance of a single deep neural network (DNN) decreases as the number of markers used in the model is reduced. The results indicated that using the top 10 features identified by PFI resulted in only a minor drop in performance, suggesting that these features are robust and reliable for age prediction. This analysis further supports the interpretability of the model by highlighting the most influential markers.\n\nThe use of PFI and top features analysis demonstrates that, while the DNN itself may be complex, the methods employed to interpret its predictions provide clear insights into the importance of various features. This approach helps to demystify the model and makes it more transparent, allowing for a better understanding of how different markers contribute to the predictions.",
  "model/output": "The model employed in this study is a regression model. The decision to treat human age prediction as a regression problem was driven by two primary factors. Firstly, age is an ordered variable, meaning it has a natural sequence. Secondly, regression allows for the analysis of differences in marker values across different ages, which is a more intuitive way to assess the influence of these markers. This approach was deemed more suitable than classification methods for the given task.\n\nThe performance of the model was evaluated using four key metrics. The first metric is the Pearson’s correlation coefficient, which measures the linear relationship between the predicted and actual ages. The second metric is the coefficient of determination (R²), which indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. The third metric is the Mean Absolute Error (MAE), which quantifies the average magnitude of the errors in a set of predictions, without considering their direction. The fourth metric is the epsilon-prediction accuracy, which assesses the proportion of predictions that fall within a specified range of the true age. In this study, an epsilon value of 10 was used, meaning a prediction was considered correct if it fell within a 10-year range of the true age.\n\nThe best-performing single Deep Neural Network (DNN) achieved an R² value of 0.80 and an epsilon-prediction accuracy of 82% within a 10-year frame. This model outperformed other machine learning techniques such as k-Nearest Neighbors, Support Vector Machine, Random Forests, and Gradient Boosting Machine. To further enhance the model's performance, an ensemble approach based on the stacking technique was employed. This method involves combining the predictions of multiple DNNs using a meta-model. The experiments showed that the ElasticNet model was the best performer among the considered models for this ensemble approach.\n\nThe model's performance was also evaluated using a feature importance method inspired by the feature randomization technique used in Random Forest. This method computes significance scores for all features by determining the impact of random permutations of feature values on the model's performance. The top 10 features identified by this method were found to be robust and reliable for age prediction, as using only these features resulted in a minimal drop in performance. The model's predictions were validated using real-world data collected from an online platform, where users could input their blood test data to predict their age and sex. The median filling strategy for handling missing values was found to be the most effective in terms of both R² and epsilon-prediction accuracy.",
  "model/duration": "All experiments were conducted on an Nvidia Tesla K80 graphics processing unit. However, the exact execution time for the model to run is not specified. The focus was on the performance metrics and the strategies used for model training and evaluation, rather than the computational time. The iterative process of adding DNNs to the ensemble and the evaluation of different filling strategies were prioritized in the analysis.",
  "model/availability": "The source code for the deep neural network ensemble used in this study is not publicly released. However, the model is made accessible to the public through an online system. This system, available at www.Aging.AI, allows users to input their blood test data to predict their age and sex. The online platform was developed to validate the approach and evaluate its real-life performance. It collects blood biochemistry reports from users, enabling them to predict their chronological age based on the markers analyzed by the deep neural network ensemble. The system also provides an option for users to enter only the 10 most important markers, making it more convenient for those who do not wish to specify all 41 parameters of the blood test.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to ensure the robustness and accuracy of the predictions. The performance of the trained neural networks was measured on a test set, distinct from the training set, to assess their generalization capabilities. This approach helps in understanding how well the models perform on unseen data.\n\nFour primary metrics were used to evaluate the models:\n\n1. **Pearson’s correlation coefficient (r)**: This metric measures the linear relationship between the predicted and actual ages. It is defined using the formula involving the sum of products of deviations from the mean for both predicted and actual values, normalized by the product of the standard deviations.\n\n2. **Coefficient of determination (R²)**: This standard metric indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. It is calculated as the ratio of the sum of squares of residuals to the total sum of squares.\n\n3. **Mean absolute error (MAE)**: This metric provides the average magnitude of the errors in a set of predictions, without considering their direction. It is defined as the average of the absolute differences between predicted and actual values.\n\n4. **Epsilon-prediction accuracy (ε)**: This metric considers a prediction correct if the predicted age falls within a specified range around the true age. For example, with ε set to 10, a prediction is considered correct if it falls within ±10 years of the true age. This metric is particularly useful for cohort analysis without fixed age ranges.\n\nThe evaluation also involved comparing the performances of different deep neural networks (DNNs) with varying hyperparameters. The best-performing DNNs were selected based on these metrics, and their predictions were combined using a stacking ensemble technique. This ensemble method involved training multiple DNNs with different hyperparameters and combining their predictions to improve overall performance.\n\nAdditionally, feature importance was assessed using a permutation feature importance (PFI) method. This method involves shuffling each feature multiple times and observing the impact on the model's performance. Features that significantly affect the model's accuracy when permuted are considered important. This approach helps in identifying the most influential markers for age prediction.\n\nThe experiments were conducted on Nvidia Tesla K80 graphics processing units, ensuring efficient computation and training of the models. The evaluation process included iterative strategies to determine the optimal number of DNNs in the ensemble, aiming to balance performance and computational efficiency.",
  "evaluation/measure": "In our study, we evaluated the performance of our deep neural networks (DNNs) using four key metrics. These metrics were chosen to provide a comprehensive assessment of the model's predictive accuracy and reliability.\n\nThe first metric is the Pearson’s correlation coefficient, denoted as r. This coefficient measures the linear relationship between the predicted and actual ages, providing a value between -1 and 1. A value closer to 1 indicates a strong positive correlation, which is desirable in our case.\n\nThe second metric is the coefficient of determination, denoted as R². This metric represents the proportion of the variance in the actual ages that is predictable from the predicted ages. An R² value of 1 indicates perfect prediction, while a value of 0 indicates that the model does not explain any of the variance.\n\nThe third metric is the Mean Absolute Error (MAE). MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It provides an intuitive measure of prediction accuracy, with lower values indicating better performance.\n\nThe fourth metric is the epsilon-prediction accuracy. This metric assesses the proportion of predictions that fall within a specified range (epsilon) of the actual values. For example, if epsilon is set to 10, a prediction is considered accurate if it falls within 10 years of the actual age. This metric is particularly useful for understanding the practical applicability of the model's predictions.\n\nThese metrics are widely used in the literature for evaluating regression models, making our evaluation representative and comparable to other studies in the field. The combination of these metrics allows us to assess both the statistical performance and the practical utility of our DNNs in predicting human age from blood biochemistry data.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our deep neural network (DNN) approach with several established machine learning (ML) techniques to evaluate its performance. The ML models we compared against included Gradient Boosting Machine (GBM), Random Forests (RF), Decision Trees (DT), Linear Regression (LR), k-Nearest Neighbors (kNN), ElasticNet, and Support Vector Machines (SVM). This comparison was crucial to demonstrate the superiority of DNNs in predicting biological age and sex from blood biochemistry data.\n\nFor biological age prediction, GBM showed the highest R² value among the ML models, with a value of 0.72. However, our DNN ensemble outperformed all these models, achieving an R² of 0.82. This indicates that DNNs are more effective in capturing the complex relationships within the data. For biological sex prediction, all ML models had comparable high R² values, but again, our DNN approach demonstrated superior performance.\n\nAdditionally, we compared our deep-learned predictor with published epigenetics and transcriptomics markers of human age. Despite using only blood biochemistry data with 41 values per patient, our biomarker outperformed blood transcriptomics biomarkers presented by Peters et al., which had an R² of 0.6 for the best model. While epigenetics markers showed stronger correlations with chronological age, with R² values of 0.93 for Horvath's methylation clock and 0.89 for the Hannum et al. methylation clock, our approach provided a more practical and less invasive method for age prediction using readily available blood test data.\n\nIn summary, our DNN ensemble not only outperformed simpler ML baselines but also showed competitive performance against more complex and invasive biomarkers, highlighting its potential for practical applications in age prediction.",
  "evaluation/confidence": "The evaluation of our deep neural network (DNN) models focused on several key performance metrics, including the Pearson’s correlation coefficient, the coefficient of determination (R²), mean absolute error (MAE), and epsilon-prediction accuracy. These metrics were chosen to provide a comprehensive assessment of the model's predictive performance.\n\nTo ensure the robustness of our results, we conducted experiments on a large dataset consisting of 62,419 blood test records, which were split into training and test sets with a 90/10 ratio. This split allowed us to train our models on a substantial amount of data while reserving a significant portion for unbiased evaluation.\n\nThe performance of the best single DNN was evaluated using R² and epsilon-prediction accuracy. The best model achieved an R² of 0.80 and an epsilon-prediction accuracy of 82% within a 10-year frame. These results indicate a strong predictive capability, as the model's predictions were closely aligned with the actual ages of the individuals in the test set.\n\nTo further validate the superiority of our DNN approach, we compared it with several baseline machine learning models, including Gradient Boosting Machine, Random Forests, Decision Trees, Linear Regression, k-Nearest Neighbors, ElasticNet, and Support Vector Machines. The DNN outperformed all these models in terms of R² for biological age prediction, demonstrating its effectiveness.\n\nAdditionally, we employed stacking techniques to combine multiple DNNs, which further improved the predictive performance. The stacking ensemble, particularly with the ElasticNet model, showed the highest epsilon-prediction accuracy and R² among the stacking models. This approach leveraged the strengths of different models to enhance overall prediction accuracy.\n\nWhile specific confidence intervals for the performance metrics were not explicitly provided, the use of a large dataset and rigorous evaluation methods, including cross-validation, ensures that the results are statistically significant. The consistent outperforming of the DNN models against baseline methods and the improvement seen with stacking techniques provide strong evidence of the method's superiority.",
  "evaluation/availability": "The evaluation availability of our study is designed to be accessible and transparent. We have made our deep network ensemble available to the public through an online platform. This platform, accessible at www.Aging.AI, allows any individual with blood test data to predict their age and sex. To validate our approach, we collected blood biochemistry reports uploaded to the site from January 25 to March 15, 2016. This dataset includes 1,563 samples with indicated real ages, providing a substantial basis for evaluating the model's performance.\n\nThe raw evaluation files themselves are not directly available for download, but the platform's functionality serves as a practical demonstration of the model's capabilities. Users can input their blood test data and receive predictions, effectively using the model in a real-world scenario. This approach ensures that the evaluation is not only theoretical but also applicable to real-life situations.\n\nThe data used for training and evaluation was anonymized and provided by an independent laboratory, Invitro. The dataset consists of 62,419 records, each containing a person's age and 46 standardized blood markers. This data was split into training and test sets with a 90/10 ratio, ensuring a robust evaluation process. The experiments were conducted on an Nvidia Tesla K80 graphics processing unit, and the results were measured using four metrics: Pearson’s correlation coefficient, the coefficient of determination (R²), mean absolute error (MAE), and epsilon-prediction accuracy.\n\nThe platform and the underlying model are made available under the terms of the Creative Commons Attribution License. This license permits unrestricted use, distribution, and reproduction in any medium, provided that the original author and source are credited. This ensures that the evaluation and the model itself are accessible to the scientific community and the public, fostering further research and applications in the field of aging biomarkers."
}