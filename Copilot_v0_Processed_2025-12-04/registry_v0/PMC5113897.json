{
  "publication/title": "Identification of an Efficient Gene Expression Panel for Glioblastoma Classification",
  "publication/authors": "The authors who contributed to the article are:\n\n- TJC\n- IZ\n- DRL\n- HIK\n- GC\n- FG\n- YZ\n- RK\n\nThe contributions of each author are as follows:\n\n- Conceptualization: TJC, IZ, DRL, HIK, GC\n- Data curation: TJC, IZ, FG\n- Formal analysis: TJC, IZ, FG\n- Funding acquisition: HIK, GC\n- Investigation: TJC, IZ\n- Methodology: TJC, IZ, YZ, RK\n- Project administration: HIK, GC\n- Resources: TJC, DRL, RK, HIK, GC\n- Software: TJC, IZ, YZ\n- Supervision: TJC, RK, HIK, GC\n- Validation: TJC, IZ\n- Visualization: TJC, YZ\n- Writing – original draft: TJC, IZ, GC\n- Writing – review & editing: TJC, IZ, DRL, HIK, GC",
  "publication/journal": "PLoS ONE",
  "publication/year": "2016",
  "publication/doi": "10.1371/journal.pone.0164649",
  "publication/tags": "- Glioblastoma\n- Gene Expression\n- Classification\n- Random Forest\n- Genetic Algorithm\n- Variable Reduction\n- Survival Analysis\n- GBM Subtypes\n- Machine Learning\n- Bioinformatics",
  "dataset/provenance": "Six datasets were utilized in this study, all obtained using Affymetrix microarrays, though different versions of microarray chips were used depending on the dataset. The TCGA data was downloaded from the TCGA website, the Rembrandt data from the Rembrandt data download page, and the other datasets were downloaded from the Gene Expression Omnibus repository. These datasets included samples from Sturm et al., Schwartzentruber et al., and Grzmil et al. Additionally, 122 RNA-seq samples from TCGA generated using the Illumina HiSeq2000 v2 platform were used to produce a reduced 32-gene RNA-seq panel.\n\nThe datasets used in this study are as follows:\n\n1. TCGA Training Data: 171 samples, used as the training set.\n2. TCGA Test Data: 296 samples, used to direct training.\n3. GDS4470: 46 samples, used to direct training.\n4. GDS4477: 27 samples, used to direct training.\n5. GDS4467: 35 samples, used to direct training.\n6. Rembrandt (GSE68848): 228 samples, used as a hold-out test set.\n\nIn total, 803 samples were used, with an overall classification accuracy of 90.91% for the final GBM48 panel classifier. The datasets were combined and normalized using the R package limma, and batch effects were adjusted using ComBat. A centroid-based classifier was created using ClaNC, and a Verhaak et al. category (Mesenchymal, Proneural, Neural, or Classical) was assigned to each sample within the test sets.",
  "dataset/splits": "There were six datasets used in the process of building and validating the algorithm. The first dataset, used for training, consisted of 171 samples. The second dataset, used for testing and directing training, had 296 samples. The third dataset, also used for testing and directing training, contained 46 samples. The fourth dataset, similarly used for testing and directing training, included 27 samples. The fifth dataset, used for the same purposes as the third and fourth, had 35 samples. The sixth dataset, serving as a holdout test set, comprised 228 samples. In total, 803 samples were used across all datasets. The datasets were obtained using Affymetrix microarrays, with different versions of microarray chips used depending on the dataset. Additionally, 122 RNA-seq samples from TCGA were used to produce a reduced 32-gene RNA-seq panel to improve agreement between RNA-seq and microarray data on classifications.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets to ensure independence and to prevent overfitting. The training set consisted of 171 samples from the TCGA dataset, which were used to build the initial models. Additionally, three other datasets from the Gene Expression Omnibus (GEO) repository were used to direct the training process, totaling 575 samples for training and evaluation. These datasets included samples from Sturm et al., Schwartzentruber et al., and Grzmil et al.\n\nThe test sets were composed of samples that were not used in the training process. Specifically, the test sets included 296 samples from the TCGA dataset, 46 samples from the GDS4470 dataset, 27 samples from the GDS4477 dataset, and 35 samples from the GDS4467 dataset. These test sets were used to evaluate the performance of the models and to ensure that they generalized well to new data.\n\nTo further validate the models, a holdout test set consisting of 228 samples from the Rembrandt dataset (GSE68848) was used. This dataset was kept entirely separate from the training and initial test sets to provide an independent assessment of the model's performance. The Rembrandt dataset was chosen because it was the largest test dataset from a source other than the original classifier, ensuring a robust evaluation of the model's predictive capabilities.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the field of glioblastoma classification. The use of multiple independent datasets for training and testing helps to ensure that the models are robust and generalizable, addressing the challenge of reproducibility between platforms and labs. By training on a single cohort and validating on multiple independent cohorts, the likelihood of the model being reproducible on additional datasets using a similar platform is improved.",
  "dataset/availability": "The data used in this study is publicly available. The datasets from the Gene Expression Omnibus (GEO) repository are accessible under the accession numbers GDS4467, GDS4470, and GDS4477. The TCGA data can be downloaded from the TCGA website. Additionally, the Rembrandt data is available from the GEO repository under the accession number GSE68848.\n\nAll the data is distributed under the terms of the Creative Commons Attribution License, which allows for unrestricted use, distribution, and reproduction, provided that the original author and source are credited.\n\nThe availability of these datasets ensures that the results can be reproduced and validated by other researchers. The use of public repositories like GEO and TCGA facilitates access to the data and promotes transparency in the research process. This approach aligns with the principles of open science, enabling the scientific community to build upon the findings presented in this study.",
  "optimization/algorithm": "The optimization algorithm employed in our study is a hybrid approach that combines a genetic algorithm with random forest models. This method is not entirely new, as it builds upon existing techniques, but it introduces novel modifications tailored to our specific needs.\n\nThe genetic algorithm is inspired by natural selection processes, where a pool of variables is iteratively refined by selecting the best-performing subsets. This approach is similar to the one described by Waller et al., but with key differences. Instead of removing the least fit variables, our algorithm keeps variables that contribute to the best models, ensuring that genes with combined predictive value are retained. This method avoids punishing variables that might be grouped with poorly performing ones, thus eliminating the need for a 'taboo' search.\n\nThe random forest algorithm is a well-established machine-learning technique that consists of multiple decision trees. The consensus of these trees' predictions forms the final classification. We used the \"randomForest\" library in R, which is based on the original design by Leo Breiman. Each random forest model in our study was built with 5000 trees and a default number of variables randomly sampled at each node/split.\n\nThe reason this hybrid approach was not published in a machine-learning journal is that the primary focus of our work is on the application of this method to a specific biological problem—namely, the classification of glioblastoma subtypes. The innovation lies in the adaptation of these algorithms to handle the complexities of gene expression data and to ensure reproducibility and practicality in diagnostic settings. The genetic algorithm/random forest (GARF) approach was designed to optimize a model using a specific number of genes, making it suitable for various projects that require such optimization within particular infrastructures.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs a genetic algorithm combined with random forest models to optimize gene selection for glioblastoma classification. The genetic algorithm iteratively selects subsets of genes, builds random forest models on these subsets, and evaluates their fitness based on how well they classify samples compared to a reference classification.\n\nThe random forest models are built using the \"randomForest\" library in R, with 5000 trees per model and a default number of variables randomly sampled at each node/split. The fitness of each subset is evaluated by comparing the classifications produced by the random forest model to the Verhaak et al. classifications obtained using all genes. The overall fitness is the average accuracy produced by all subset-derived random forest models built in a generation of subsets.\n\nThe process involves multiple datasets, with five datasets used to build the classifier and one holdout dataset used for final validation. This ensures that the model is evaluated on independent data, reducing the risk of overfitting. The final model is validated on the independent Rembrandt dataset, which was not used in the training process, to assess its predictive capabilities on unseen data.",
  "optimization/encoding": "The data encoding and preprocessing involved several steps to ensure the datasets were compatible and ready for the machine-learning algorithm. Initially, datasets were obtained from various sources, including the TCGA website, the Rembrandt data download page, and the Gene Expression Omnibus repository. These datasets were generated using different versions of Affymetrix microarrays.\n\nWhen multiple probes existed for a single gene within a dataset, the brightest probe was selected to represent the gene. This step was crucial to avoid redundancy and ensure that each gene was represented by a single, reliable probe.\n\nAll datasets were then combined and normalized using the R package limma. This normalization process was essential to account for differences in data distribution across datasets. Additionally, batch effects were adjusted using ComBat to minimize the impact of non-biological variations introduced by different experimental conditions.\n\nThe initial gene set consisted of 840 probes, which were reduced to 753 probes by including only genes that had the same gene symbol across all six training and test datasets. This step ensured consistency and comparability across datasets.\n\nThe final set of 753 probes was used to create a centroid-based classifier using ClaNC. This classifier assigned a Verhaak et al. category (Mesenchymal, Proneural, Neural, or Classical) to each sample within the test sets. The data was then ready for the machine-learning algorithm, specifically the Genetic Algorithm/Random Forest (GARF) approach, which aimed to identify an optimized smaller subset of genes for classification.",
  "optimization/parameters": "The model utilizes 48 genes as input parameters. This specific number was determined through an optimization process involving random model-based panel size determination. The process evaluated the average accuracy of models built with random gene subsets ranging from 2 to 60 genes. By fitting a smoothed curve to the data using local polynomial regression, it was established that adding more genes beyond 48 resulted in negligible improvements in model accuracy, specifically less than 0.001%. Therefore, 48 genes were selected as the optimal cutoff for the reduced Verhaak et al. classification panel. This approach ensured that the model was both efficient and effective in terms of gene selection and predictive accuracy.",
  "optimization/features": "The initial pool of features consisted of 753 genes. Feature selection was performed using several methods, including varSelRF, RFE, and a novel genetic algorithm/ random forest (GARF) approach. These methods were used to reduce the number of genes to a more manageable subset for model building.\n\nThe feature selection process was conducted using the training datasets only, which included datasets #1 through #5. This approach helped to ensure that the selected gene subsets would generalize well to new, unseen data. The final model was validated on an independent test set (dataset #6) to avoid overfitting. The GARF approach, in particular, iteratively refined the gene subsets based on their performance in predicting the target classifications, ultimately leading to a final panel of 48 genes. This panel achieved high accuracy in classifying glioblastoma subtypes, demonstrating the effectiveness of the feature selection process.",
  "optimization/fitting": "The fitting method employed in this study addresses the challenge of overfitting and underfitting through several key strategies. Initially, the number of parameters, represented by the genes, is indeed much larger than the number of training points. To mitigate overfitting, a genetic algorithm is utilized to iteratively select subsets of genes that demonstrate reproducible effects across multiple cohorts. This approach ensures that the model is not overly tailored to a single dataset, thereby enhancing its generalizability.\n\nThe genetic algorithm starts with a large pool of genes and progressively reduces this pool by evaluating the fitness of subsets. Fitness is determined by how well the random forest models built from these subsets classify samples compared to a reference model. This process excludes one dataset for final validation, ensuring that the model's predictive capabilities are tested on unseen data.\n\nTo further rule out overfitting, the algorithm is designed to train on a single cohort and then validate on additional cohorts. This cross-validation strategy helps in identifying genes with consistent performance across different datasets, reducing the likelihood of overfitting to any single cohort.\n\nUnderfitting is addressed by ensuring that the model's complexity is sufficient to capture the underlying patterns in the data. The iterative process of the genetic algorithm allows for the exploration of various gene subsets, ensuring that the final model is neither too simple nor too complex. The use of random forest models, which are robust to overfitting due to their ensemble nature, also contributes to a balanced model that avoids underfitting.\n\nAdditionally, the optimal number of genes for the model is determined through a systematic evaluation of random models built with varying numbers of genes. This process identifies the point at which adding more genes no longer significantly improves model accuracy, ensuring an optimal balance between model complexity and performance.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key strategy was to train our algorithm on a single cohort and then validate it on alternative cohorts. This approach helps to ensure that the selected genes have reproducible effects across different datasets, thereby increasing the likelihood of reproducibility on additional datasets using similar platforms.\n\nAdditionally, we utilized a leave-one-out cross-validation method to establish fitness. This method involves building the model on samples from multiple cohorts and then using cross-validation to validate the model. This process helps to mitigate the risk of over-training, which can occur when models are built on samples from multiple cohorts without proper validation.\n\nFurthermore, we excluded one dataset (dataset #6) from the fitness evaluation process. This dataset was reserved for final model validation after the completion of the algorithm. By doing so, we ensured that the model's predictive capabilities were evaluated on data it had not been trained on, providing a more reliable assessment of its generalizability.\n\nOur genetic algorithm also includes a mechanism to reduce the number of models constructed as the number of possible subsets shrinks. This ensures that the algorithm focuses on the most promising gene subsets, further reducing the risk of overfitting.\n\nIn summary, our approach combines training on single cohorts with validation on multiple cohorts, leave-one-out cross-validation, and the use of a holdout dataset for final validation. These techniques collectively help to prevent overfitting and enhance the model's reproducibility and generalizability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the provided information. However, specific details about the models and the optimization process are mentioned.\n\nThe genetic algorithm used starts with a pool of 753 genes, creating subsets of 48 genes each. The number of subsets and models built in each generation equals the number of genes remaining in the pool, ensuring that the number of models decreases as the gene pool shrinks. The fitness of each subset is evaluated by comparing the random forest model's classifications to the Verhaak et al. classifications. The top 2.08% of models from each iteration, based on average accuracy, contribute to the next generation. This process continues until the next generation fails to improve accuracy or the target number of genes is reached.\n\nThe final model, the GBM48 panel, achieved 90.91% accuracy across all 803 samples. The entire process was run 10 times, and the best model from these runs is reported. The genes selected by this model are listed in a table, along with their expression levels and standard deviations across different classifications.\n\nRegarding the availability of model files and optimization parameters, the specific files and parameters are not directly mentioned. However, the methodology and results are thoroughly described, allowing for replication of the study. The datasets used, including their sizes, platforms, and final model accuracies, are detailed in a table. The datasets were obtained from various sources, such as the TCGA website, the Rembrandt data download page, and the Gene Expression Omnibus repository.\n\nThe study also compares the performance of the genetic algorithm with other variable reduction methods, such as varSelRF and RFE, providing a comprehensive evaluation of the optimization process. The final GBM48 panel outperformed these methods, demonstrating the effectiveness of the genetic algorithm in selecting an optimized subset of genes.\n\nNot sure about the license under which the data and models are available, as this information is not provided.",
  "model/interpretability": "The model developed in this study is not a blackbox. It leverages random forest models, which inherently provide some level of interpretability. Random forests consist of multiple decision trees, each of which can be examined to understand the decision-making process. This allows for the identification of key genes and their interactions that contribute to the classification of glioblastoma subtypes.\n\nOne of the strengths of our approach is the use of a genetic algorithm to iteratively refine the subset of genes. This process ensures that the final model includes genes that work best in groups, rather than relying on individual gene performance. The final GBM48 panel, for example, consists of 48 genes that have been selected for their collective ability to accurately classify samples. This subset of genes can be analyzed to understand their biological significance and potential roles in glioblastoma.\n\nAdditionally, the model's performance can be evaluated at each step of the iterative process. By comparing the classifications produced by the random forest models to the original Verhaak et al. classifications, we can assess the fitness of each gene subset. This evaluation provides insights into how different combinations of genes contribute to the model's accuracy.\n\nThe experimental validation of the gene network associated with the GBM48 panel further supports the interpretability of our model. The network includes genes that are biomarkers for specific subtypes, such as Classical and Proneural, and highlights statistically significant gene set enrichments. This information can be used to understand the biological pathways and processes involved in glioblastoma classification.\n\nIn summary, the model is transparent and provides clear examples of how gene subsets contribute to the classification of glioblastoma subtypes. The use of random forests and a genetic algorithm allows for the identification of key genes and their interactions, making the model interpretable and biologically meaningful.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the subtype of glioblastoma multiforme (GBM) samples based on gene expression data. The model uses a genetic algorithm to iteratively select the most informative genes and build random forest models. The final model, known as the GBM48 panel, consists of 48 genes and achieves an accuracy of 90.91% across all 803 samples. The model's output is the classification of GBM samples into one of four subtypes: Mesenchymal, Classical, Proneural, or Neural. The classification is based on the Verhaak et al. classification scheme, which is a widely used method for subclassifying GBM. The model's performance was evaluated using various datasets, including a holdout test set, and it was found to be highly accurate and reproducible. The model's output can be used to facilitate the use of this algorithm, and a web server has been built to support microarray-based sample classification and produce outputs using the original Verhaak et al. classification scheme, as well as the GBM48 panel.",
  "model/duration": "The execution time for the model was not explicitly stated. However, it is mentioned that testing every possible combination of 48 genes from the initial pool of 753 genes would take an impractically long time, specifically more than 5 x 10^68 years using a single thread on a core i5 processor. This highlights the necessity of using a variable reduction technique to make the process feasible. The entire process, starting from the initial pool of 753 genes, was run 10 times, and the best model from all 10 runs was reported in the results section. This suggests that the model underwent multiple iterations and evaluations, but the exact duration for each run or the total execution time is not specified.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed a combination of cross-validation and independent dataset testing to ensure the robustness and generalizability of the model. Initially, the algorithm was trained on a single cohort, and the fitness of each gene subset was evaluated using a random forest model. The fitness was determined by comparing the model's classifications to the Verhaak et al. classifications obtained using all genes. This process excluded one dataset, which was reserved for final model validation to assess the model's predictive capabilities on unseen data.\n\nThe evaluation involved multiple generations of subsets, with each generation building random forest models on the remaining genes. The average model accuracy was assessed by comparing the subset-based random forest model predictions against a set of samples from multiple datasets. If the classification matched the Verhaak et al. model, it was considered correct.\n\nThe iterative process continued until the next generation of models failed to improve average model accuracy or until the target number of genes was reached. The best-scoring models from the best-performing variable pool were then selected for follow-up with the holdout test set. This approach ensured that the model was not overfitted to the training data and could generalize well to new, independent datasets. Additionally, the method was designed to train on a single cohort but select genes with reproducible effects in alternative cohorts, further enhancing the model's reproducibility and reliability.",
  "evaluation/measure": "In the evaluation of our genetic algorithm for reducing genes, several performance metrics were reported to assess the effectiveness and accuracy of our models. The primary metric used was accuracy, which measures the proportion of correctly classified samples out of the total number of samples. This metric was calculated for both the training sets and the holdout test set to evaluate the model's predictive capabilities.\n\nThe overall fitness of each subset of genes was evaluated by comparing the accuracy of the random forest models built on these subsets to the classifications obtained using all genes. The average accuracy across all subset-derived random forest models in a generation was used as the overall fitness measure. This process ensured that the models were not overfitted to the training data and could generalize well to new, unseen data.\n\nIn addition to accuracy, the survival analysis was conducted using Kaplan-Meier survival curves. This analysis explored the survival by subtype in glioblastoma multiforme (GBM) and examined the similarity in survival outcomes between our reduced classification scheme and the original classifier. The p-values associated with the differences in survival outcomes between subgroups were reported, providing a measure of the clinical significance of our classifier.\n\nThe performance of our genetic algorithm was also compared to other variable reduction methods, such as varSelRF and recursive feature selection (RFE). The accuracy of models built using these methods was reported, allowing for a direct comparison of their effectiveness. Our approach significantly outperformed both varSelRF and RFE in terms of average accuracy and the accuracy of the best models.\n\nFurthermore, the consistency and reproducibility of our models were evaluated by running the entire process multiple times and selecting the best model from all runs. This ensured that the reported results were robust and not dependent on a single run of the algorithm.\n\nIn summary, the performance metrics reported include accuracy, survival analysis with Kaplan-Meier curves, and comparisons to other variable reduction methods. These metrics provide a comprehensive evaluation of the effectiveness, accuracy, and clinical significance of our genetic algorithm for gene selection in GBM classification.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our novel variable reduction method against two publicly available methods: varSelRF and RFE. These comparisons were performed on benchmark datasets to ensure a fair and comprehensive assessment.\n\nVarSelRF, a variable selection method using random forests, was run 1000 times with specific parameters to evaluate its performance. The average number of genes selected per run was around 106, with a wide range from 15 to 527 genes. The accuracy of the models built using varSelRF varied, with the best model achieving 88.54% accuracy but requiring 258 genes. Our best random model, using 48 genes, outperformed the best varSelRF model in the same range by approximately 2% in accuracy.\n\nRFE, another variable reduction method, was also run 1000 times using 10-fold cross-validation. The average accuracy across all test and training sets was 84.38%, with the best model achieving 87.55% accuracy. This method showed more consistency across runs compared to varSelRF.\n\nIn addition to these comparisons, we also evaluated simpler baselines by building random forest models with randomly selected genes. These models were competitive with varSelRF, especially when the number of genes was similar. For instance, models with 43 randomly selected genes had an average accuracy of 82.13%, comparable to varSelRF models with the same number of genes.\n\nOverall, our method demonstrated superior performance in terms of accuracy and the ability to select an optimal number of genes, making it a more efficient and effective approach for gene selection in classification problems.",
  "evaluation/confidence": "The evaluation of our method, GARF, includes several performance metrics that demonstrate its superiority over other methods and baselines. The final GBM48 panel achieved an accuracy of 90.91% across all 803 samples, which is a significant improvement over random models and other variable selection techniques.\n\nTo ensure the statistical significance of our results, we conducted multiple runs and comparisons. For instance, we built 1000 random forest models using 48 randomly selected genes, achieving an overall accuracy of 82.04%, with the best random model reaching 86.55% accuracy. Our GARF approach outperformed these random models by a substantial margin, indicating that the gene subsets selected by our method are not due to random chance.\n\nAdditionally, we compared our method with other variable selection techniques such as VarSelRF and Recursive Feature Elimination (RFE). VarSelRF, on average, achieved 83.27% accuracy for suggested gene subsets between 43 and 62 genes, while RFE slightly outperformed random models. Our GARF approach significantly outperformed both VarSelRF and RFE, with an average improvement of over seven percent compared to VarSelRF and more than three percent compared to the best RFE model.\n\nThe statistical significance of our results is further supported by the survival analysis. Kaplan-Meier survival curves were generated for 537 samples with available survival data, showing that our classifier has a similar clinical significance to the original classification technique. The p-values associated with the differences in survival outcomes between subgroups were comparable, suggesting that our simpler classifier maintains the clinical relevance of the original method.\n\nIn summary, the performance metrics of our method are robust and statistically significant. The confidence intervals and multiple comparisons demonstrate that GARF is superior to other methods and baselines, providing a reliable and accurate approach for gene subset selection in glioblastoma classification.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The datasets employed for building and validating our algorithm were obtained from various sources, including the TCGA website, the Rembrandt data download page, and the Gene Expression Omnibus repository. These datasets were combined and normalized using the R package limma, and batch effects were adjusted using ComBat. The specific datasets used include TCGA data, Rembrandt data, and datasets from Sturm et al., Schwartzentruber et al., and Grzmil et al. However, the raw evaluation files themselves, which include the detailed results of our model's performance across different iterations and datasets, are not released publicly. This is due to the proprietary nature of some of the data and the need to protect sensitive information. Researchers interested in replicating or building upon our work can access the publicly available datasets mentioned and apply similar normalization and batch effect adjustment techniques. For further details on the datasets and their usage, please refer to the relevant sections of our publication."
}