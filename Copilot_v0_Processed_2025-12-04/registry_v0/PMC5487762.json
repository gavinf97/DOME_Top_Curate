{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "J Comput Aided Mol Des",
  "publication/year": "2017",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Machine Learning\n- Molecular Docking\n- Chemical Space Network\n- Functional Group Analysis\n- Protein-Ligand Interaction\n- Inhibitor Prediction\n- Classification Models\n- Drug Design\n- Virtual Screening\n- Structural Diversity",
  "dataset/provenance": "The dataset used in this study was sourced from two primary in vitro inhibition studies on human BSEP. The first set, used as the training set, consists of 408 compounds, with 113 identified as strong inhibitors (mean IC50 ≤ 10 μM) and 295 as non-inhibitors (mean IC50 > 300 μM). This dataset was originally compiled by Warner et al. The second set, used as an external test set, contains 166 compounds from Pedersen et al., with 44 strong inhibitors and 122 non-inhibitors. This dataset is based on inhibition of BSEP-mediated taurocholate transport in inverted membrane vesicles.\n\nAdditionally, a dataset provided by AstraZeneca within the IMI project eTOX was used as a second external test set. This dataset includes 1092 compounds with BSEP inhibitory potencies measured in a [3H]-taurocholate transport assay. After removing overlapping compounds from the first two datasets, 638 compounds remained, consisting of 248 inhibitors and 390 non-inhibitors.\n\nThe datasets used in this study have been previously utilized in the community for similar research purposes, providing a robust foundation for our analysis. The Warner et al. dataset has been particularly influential in defining strong inhibitors based on a stringent IC50 threshold, ensuring the reliability of our training set. The Pedersen et al. dataset and the AstraZeneca dataset further validate our models by providing diverse and extensive external test sets.",
  "dataset/splits": "In our study, we utilized three distinct data splits to evaluate the performance of our models. The primary split was the training set, which comprised 408 compounds, with 113 identified as strong inhibitors and 295 as non-inhibitors. This set was derived from the work of Warner et al., focusing on in vitro inhibition data for human BSEP, with a stringent threshold of mean IC50 ≤ 10 μM for strong inhibitors.\n\nThe first external test set consisted of 166 compounds, sourced from Pedersen et al. This set included 44 strong inhibitors and 122 non-inhibitors, based on inhibition of BSEP-mediated taurocholate transport in inverted membrane vesicles. Overlapping compounds with the training set were removed to ensure the integrity of the test data.\n\nThe second external test set was provided by AstraZeneca as part of the IMI project eTOX. This set contained 638 compounds, with 248 inhibitors and 390 non-inhibitors. The data was measured using a [3H]-taurocholate transport assay in Sf21 membrane vesicles, following the protocol described by Dawson et al. Again, overlapping compounds were excluded to maintain the independence of the test set.",
  "dataset/redundancy": "The datasets used in this study were carefully curated to ensure independence between the training and test sets. The training set consisted of 408 compounds, with 113 identified as inhibitors and 295 as non-inhibitors. This set was derived from the work of Warner et al., focusing on strong inhibitors with a mean IC50 ≤ 10 μM.\n\nTwo external test sets were employed to evaluate the models. The first test set, from Pedersen et al., initially contained 166 compounds but was reduced to 166 after removing overlaps with the training set, resulting in 44 inhibitors and 122 non-inhibitors. The second test set, provided by AstraZeneca within the IMI project eTOX, included 1092 compounds, which was reduced to 638 after removing overlaps, yielding 248 inhibitors and 390 non-inhibitors.\n\nTo enforce independence, overlapping compounds between the training set and each test set were removed. This process ensured that the test sets contained compounds not seen during the training phase, providing an unbiased evaluation of the models' performance.\n\nThe distribution of inhibitors and non-inhibitors in these datasets is designed to reflect a realistic scenario where strong inhibitors are less common than non-inhibitors. This distribution is comparable to previously published machine learning datasets in the field, which often face similar challenges in balancing the number of positive and negative examples. The focus on strong inhibitors with a low IC50 threshold ensures that the models are trained to identify compounds with significant inhibitory potential, which is crucial for drug discovery and development.",
  "dataset/availability": "The datasets used in this study are not publicly available. The training set consists of 408 compounds, with 113 identified as strong inhibitors and 295 as non-inhibitors, sourced from the work of Warner et al. The external test sets include 166 compounds from Pedersen et al., with 44 inhibitors and 122 non-inhibitors, and an additional 638 compounds from AstraZeneca, comprising 248 inhibitors and 390 non-inhibitors. These datasets were curated based on specific inhibition criteria and do not overlap with each other. The data from Pedersen et al. and AstraZeneca were used to evaluate the performance of the models developed using the training set. The datasets were not released in a public forum, and no specific license information is provided.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established classifiers from the WEKA software suite. The specific algorithms employed include J48, Random Forest, REP-Tree, LibSVM, and Naive Bayes. These algorithms are not new but are widely recognized and used in the field of machine learning for their robustness and effectiveness in various classification tasks.\n\nThe choice to use these established algorithms rather than developing a new one is driven by the need for reliability and comparability. These algorithms have been extensively validated and are known to perform well across a range of datasets, making them suitable for the classification tasks in this study. Additionally, using well-known algorithms allows for easier reproducibility and comparison with other studies in the field.\n\nThe focus of this study is on the application of these machine-learning algorithms to the specific problem of classifying inhibitors and non-inhibitors of the BSEP protein, rather than on the development of new machine-learning techniques. Therefore, the algorithms were applied using their default parameters, and the results were analyzed to gain insights into the binding modes and structural similarities of the compounds.",
  "optimization/meta": "The optimization process involved evaluating various machine learning classifiers, including J48, Random Forest, REP-Tree, LibSVM, and Naive Bayes, using the WEKA software. These classifiers were applied with default parameters and tenfold internal cross-validation to build binary classification models.\n\nThe meta-predictor approach was not explicitly detailed in the optimization subsection. However, the study did explore the combination of ligand-based and structure-based models in a sequential manner. This sequential approach started with a ligand-based method and proceeded with screening the positives using structure-based models. This method improved precision and reduced false positives, indicating a form of meta-predictor strategy.\n\nThe training data used for the machine learning models included a dataset of 1865 structures for the training set, 2009 structures for the external test set from Pedersen et al., and 1560 structures for the external test set from AstraZeneca. The independence of the training data is implied by the use of separate external test sets, which were derived from different sources and not used in the training process. This ensures that the models were evaluated on independent data, providing a robust assessment of their performance.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent several preprocessing steps to ensure optimal encoding and representation. Initially, different ionization states were generated for each ligand at a target pH of 7.0 ± 2.0 using Epik. Tautomers were also generated for each ligand to account for various possible forms. Stereoisomers were retained as specified in the input file, preserving the chirality information throughout the calculations.\n\nThe dataset consisted of 1865 structures for the training set, 2009 structures for the external test set from Pedersen et al., and 1560 structures for the external test set from AstraZeneca. These structures were used for docking with the genetic algorithm-based GOLD suite, employing the fitness functions GoldScore and ChemScore. Additionally, GlideXP docking from Maestro was utilized to compare different scoring functions. All poses were rescored using the external scoring function XScore.\n\nTo gain deeper insights into the binding modes of inhibitors and non-inhibitors, protein–ligand interaction fingerprints (PLIF) of the resultant complexes were analyzed. This involved encoding the residues involved in interactions with the ligand, the nature of these interactions, and the functional groups of the ligand that interacted with the residues.\n\nFunctional group analysis was performed in two stages. First, substructure patterns of 100 functional groups in SMARTS notation were extracted. Next, pattern matching was conducted using the SMARTSQueryTool implemented in the Chemistry Development Kit. For each functional group, the occurrences of the fragments in a given set of molecules were calculated.\n\nTanimoto similarities between the inhibitors and non-inhibitors of the training set were calculated using MACCS fingerprints. A chemical space network (CSN) was constructed and analyzed to assess the structural similarity shared by the compounds of both groups. A threshold value of 0.7 was set based on the average of Tanimoto maximum similarity in the dataset to show connections between the compounds.\n\nThe applicability domain (AD) analysis was performed to evaluate if the chemical space covered by the training set was applicable to predict the outcomes of the test sets. The Euclidean distance approach using the KNIME node APD was implemented to assess if the test sets were within the AD of the training set.\n\nIn summary, the data encoding and preprocessing involved generating various states and forms of ligands, performing docking with multiple scoring functions, analyzing protein–ligand interactions, conducting functional group analysis, calculating structural similarities, and assessing the applicability domain. These steps ensured that the data was appropriately encoded and pre-processed for the machine-learning algorithm.",
  "optimization/parameters": "In the optimization process, several parameters were utilized to build and evaluate the models. The primary parameters involved in the docking studies included the fitness functions GoldScore (GS) and ChemScore (CS), which were used to assess the binding affinity of ligands to the protein. Additionally, the external scoring function XScore was employed to rescore all the poses generated during the docking process.\n\nThe dataset consisted of a training set with 1865 structures (318 inhibitors and 1547 non-inhibitors), an external test set from Pedersen et al. with 2009 structures (858 inhibitors and 1151 non-inhibitors), and another external test set from AstraZeneca with 1560 structures (668 inhibitors and 892 non-inhibitors). These datasets were used to train and validate the models, ensuring a comprehensive evaluation of the docking and scoring parameters.\n\nThe selection of parameters was guided by the need to generate a diverse set of ligand structures, including different ionization states, tautomers, and stereoisomers. The target pH for ionization states was set at 7.0 ± 2.0, and the chirality information from the input files was retained throughout the calculations. This approach ensured that the models could accurately capture the structural diversity of the ligands, which is crucial for reliable docking and scoring.\n\nFurthermore, the molecular dynamics simulations were carried out using the GROMOS 54a7 force field, with the protein placed in a rectangular box containing approximately 34,000 simple point charge (SPC) water molecules. Sodium and chloride ions were added to achieve a neutral system, and the simulations were performed at a constant temperature of 300 K for 20 ns. The stability of the protein structure was evaluated using the root-mean-square fluctuation (rmsf) of active site residues and the secondary structure over the simulation time.\n\nIn summary, the optimization process involved a combination of docking and scoring parameters, along with molecular dynamics simulations, to ensure the robustness and accuracy of the models. The selection of parameters was based on the need to capture the structural diversity of the ligands and to evaluate the binding affinity and stability of the protein-ligand complexes.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The optimization process involved a comprehensive approach to ensure that both overfitting and underfitting were mitigated. The dataset consisted of a substantial number of structures, with 1865 structures in the training set, 2009 in one external test set, and 1560 in another. This large dataset helped in reducing the risk of overfitting, as the model had ample data to learn from without memorizing the training examples.\n\nTo further prevent overfitting, a tenfold internal cross-validation was employed. This technique involves dividing the training data into ten subsets, training the model on nine subsets, and validating it on the remaining one. This process is repeated ten times, with each subset serving as the validation set once. This method ensures that the model generalizes well to unseen data.\n\nSeveral machine learning classifiers were used, including J48, Random Forest, REP-Tree, LibSVM, and Naive Bayes, all with default parameters. The use of multiple classifiers helps in identifying the most robust model and reduces the risk of overfitting to a specific algorithm.\n\nAdditionally, the performance of the models was evaluated using standard metrics such as sensitivity, specificity, accuracy, G-mean, and Matthews’s correlation coefficient (MCC). These metrics provide a comprehensive evaluation of the model's performance and help in identifying any signs of overfitting or underfitting.\n\nThe applicability domain (AD) assessment was also performed to ensure that the test sets were within the chemical space of the training set. This step is crucial in confirming that the model's predictions are reliable and not due to extrapolation beyond the training data.\n\nIn summary, the optimization process included a large and diverse dataset, cross-validation, multiple classifiers, and thorough performance evaluation, all of which contributed to mitigating the risks of overfitting and underfitting.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we utilized the OPLS_2005 force field for structure minimization and Epik version 3.1 for generating different ionization states of ligands at a target pH of 7.0 ± 2.0. Tautomers and stereoisomers were also generated for each ligand, retaining chirality information from the input files.\n\nFor docking studies, we employed the GOLD suite (version 5.2.0) in high-throughput mode, using the fitness functions GoldScore and ChemScore. Additionally, GlideXP docking from Maestro was used to compare different scoring functions, and all poses were rescored using the external scoring function XScore.\n\nThe machine learning models were built using WEKA (version 3.7.10) with classifiers such as J48, Random Forest, REP-Tree, LibSVM, and Naive Bayes, all with default parameters and tenfold internal cross-validation.\n\nThe chemical space network was constructed using MACCS fingerprints to calculate Tanimoto similarities, with a threshold value of 0.7 set based on the average of Tanimoto maximum similarity in the dataset.\n\nThe specific model files and optimization schedules are not explicitly detailed in the publication, but the methods and parameters used for each step of the process are thoroughly described. The software tools mentioned, such as GOLD, Maestro, and WEKA, are commercially available or open-source, with their respective licenses governing their use. The datasets used for training and testing, including the number of inhibitors and non-inhibitors, are also specified.\n\nNot applicable",
  "model/interpretability": "The models developed in this study are not entirely black-box, as several efforts were made to enhance their interpretability. The use of protein-ligand interaction fingerprints (PLIFs) provides a transparent way to understand the interactions between ligands and the protein. Three types of PLIFs were generated, encoding the residues involved in interactions, the nature of these interactions, and the functional groups of the ligand that interact with the residues. This approach allows for a detailed analysis of how specific functional groups in ligands interact with particular residues in the protein, offering insights into the molecular basis of inhibition.\n\nAdditionally, the analysis of functional groups in inhibitors and non-inhibitors helps identify structural features responsible for differences in activity. For instance, groups such as halide/halogen, ether, carbonyl, vinyl carbons, and amide were more frequently found in inhibitors, suggesting more hydrophobic-driven interactions. The heat map generated from PLIF analysis visually detects significant interactions between specific residues and functional groups, further aiding in the interpretability of the model.\n\nThe docking results were retrospectively assessed to confirm the presence of these interactions, providing a clear example of how the model's predictions can be interpreted at a molecular level. For example, the docking pose of Glimepiride showed interactions between its carbonyl groups and specific residues, as well as hydrophobic interactions involving its arene moiety. This detailed interaction analysis helps in understanding why certain compounds are predicted as inhibitors, making the model more transparent and interpretable.",
  "model/output": "The model developed is a classification model. It is designed to distinguish between inhibitors and non-inhibitors of the bile salt export pump (BSEP). The classification is based on docking scores and other performance measures such as the area under the ROC curve (AUC) and Matthews's correlation coefficient (MCC). The model accurately predicted a significant percentage of compounds in both the training set and external test sets, indicating its effectiveness in classifying inhibitors and non-inhibitors.\n\nThe performance of the model was evaluated using standard parameters such as true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). Sensitivity, specificity, and accuracy values were calculated to estimate the model's performance in classifying inhibitors and non-inhibitors. The G-mean and Matthews’s correlation coefficient (MCC) were also calculated to measure the overall quality of the model.\n\nThe ChemScore docking run using Xscore as a rescoring function retrieved the best-performing model with an AUC of 0.918 and an MCC of 0.689. This model accurately predicted 88% of the training set compounds and 72% of the external test set compounds derived from Pedersen et al., as well as 77% of a set of AstraZeneca internal compounds. The high AUC values observed indicate a high capacity of the model in ranking compounds by their probability of being inhibitors of BSEP.\n\nThe probability of prediction was calculated by examining the distribution of docking scores for the training set molecules. Based on the minimum and maximum score values, the scores were binned in different intervals. Each bin is characterized by the corresponding number of inhibitors and non-inhibitors. A p-value (Chi square test) was calculated for each bin to identify the best scoring range that can be used to separate inhibitors from non-inhibitors.\n\nThe model's performance was further improved by including logP and molecular weight as additional layers of information besides the scoring function. This inclusion increased the performance of the models, indicating that considering physicochemical properties of molecules that influence their activity significantly improves the performance of structure-based prediction models.",
  "model/duration": "The execution time for the model involved several stages, each contributing to the overall duration. The homology modeling process, which included constructing multiple models using Modeller and the Prime module in Maestro, was followed by energy minimization and evaluation using various scoring methods. This process was computationally intensive and required significant time.\n\nMolecular dynamics simulations were carried out using Gromacs, with energy minimization, NVT equilibration, NPT equilibration, and a production simulation lasting 20 nanoseconds. These simulations were essential for evaluating the stability of the protein structure and required substantial computational resources and time.\n\nDocking studies were performed in high-throughput mode using the GOLD suite, with fitness functions GoldScore and ChemScore. Additionally, GlideXP docking from Maestro was used to compare different scoring functions. All poses were rescored using the external scoring function XScore. The docking runs and rescoring processes added to the overall execution time.\n\nMachine learning-based model building was conducted using the open-source software WEKA, with classifiers such as J48, Random Forest, REP-Tree, LibSVM, and Naive Bayes. The process included tenfold internal cross-validation, which further contributed to the execution time.\n\nIn summary, the model's execution time was influenced by the complexity of the homology modeling, molecular dynamics simulations, docking studies, and machine learning processes. Each of these stages required significant computational effort and time to ensure accurate and reliable results.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to ensure the robustness and accuracy of the classification models developed. Standard parameters such as true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) were used to assess the performance of the models. Sensitivity, specificity, and accuracy were calculated for each model to estimate its effectiveness in classifying inhibitors and non-inhibitors. Sensitivity measures the proportion of actual inhibitors correctly identified, while specificity measures the proportion of actual non-inhibitors correctly identified. Accuracy provides an overall measure of the correct predictions made by the model.\n\nTo provide a more comprehensive evaluation, the G-mean and Matthews’s correlation coefficient (MCC) were also calculated. The G-mean takes into account both sensitivity and specificity, offering a balanced view of the model's performance. The MCC is particularly useful for evaluating the quality of binary classifications, as it considers all four categories of the confusion matrix.\n\nThe models were evaluated using a training set comprising 408 compounds, with 113 inhibitors and 295 non-inhibitors. Two external test sets were used to further validate the models: one containing 166 compounds from Pedersen et al., and another with 638 compounds provided by AstraZeneca. The performance of the models was assessed using the area under the ROC curve (AUC), which is independent of class distribution and provides a good metric for evaluating virtual screening approaches. High AUC values, particularly above 0.8, indicate a strong capacity of the model to rank compounds by their probability of being inhibitors.\n\nThe ChemScore docking run using Xscore as a rescoring function retrieved the best-performing model, with an AUC of 0.918 and an MCC of 0.689. This model accurately predicted 88% of the training set compounds, 72% of the external test set compounds from Pedersen et al., and 77% of the AstraZeneca internal compounds. The results from the applicability domain (AD) assessment showed that all compounds from both test sets were within the chemical domain of the training compounds, ensuring the reliability of the predictions.\n\nAdditionally, the probability of prediction was examined by analyzing the distribution of docking scores for the training set molecules. The scores were binned into different intervals, and a p-value (Chi square test) was calculated for each bin to identify the best scoring range that could distinguish between inhibitors and non-inhibitors. For the training set using ChemScore scoring, bin 35–40 gave the maximum number of inhibitors, with 88% of inhibitors and 12% of non-inhibitors falling within this range. This range was also effective for the test sets, with at least 75% of the inhibitors found within this scoring interval.",
  "evaluation/measure": "In the evaluation of our classification models, we employed several standard performance metrics to assess their quality. These metrics include the count of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). From these, we calculated sensitivity, specificity, and accuracy for each model. Sensitivity measures the proportion of actual inhibitors correctly identified, while specificity measures the proportion of actual non-inhibitors correctly identified. Accuracy provides an overall measure of the model's correctness.\n\nTo provide a more comprehensive evaluation, we also calculated the G-mean, which balances sensitivity and specificity, and the Matthews's correlation coefficient (MCC). The MCC is particularly useful as it takes into account all four categories of the confusion matrix and is considered a balanced measure even if the classes are of very different sizes.\n\nAdditionally, we reported the area under the ROC curve (AUC), which is a robust metric for evaluating the performance of virtual screening approaches. High AUC values, particularly above 0.8, indicate a strong capacity of the model to rank compounds by their probability of being inhibitors.\n\nThe reported metrics are representative of those commonly used in the literature for evaluating classification models in computational chemistry and drug discovery. The use of AUC, MCC, and other confusion matrix parameters ensures that our evaluation is thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of various methods to evaluate their performance in classifying inhibitors and non-inhibitors of the BSEP transporter. We utilized multiple scoring functions, including ChemScore, GoldScore, and GlideXP, and employed XScore for rescoring the docking poses. This approach allowed us to assess the effectiveness of different scoring strategies in distinguishing between inhibitors and non-inhibitors.\n\nWe also compared our structure-based models with ligand-based classification methods. The ligand-based classification, following the workflow from Montanari et al., demonstrated a precision of 0.77. When combined with our structure-based model using ChemScore and rescoring with XScore, the precision improved to 0.83, significantly reducing the number of false positives. This sequential approach not only enhanced precision but also reduced computational time, making it a versatile strategy for large-scale in silico screening.\n\nAdditionally, we evaluated the performance of our models on external test sets, including datasets from Pedersen et al. and AstraZeneca. The models showed high accuracy in predicting the inhibitory properties of compounds, with the ChemScore docking run using XScore as the rescoring function achieving the best performance. The area under the ROC curve (AUC) measures indicated a high capacity for ranking compounds by their probability of being BSEP inhibitors.\n\nWe also explored the use of a consensus approach, combining different scoring functions. However, the accuracy of predictions did not improve when using a consensus of multiple scoring functions. This suggests that the individual performance of well-chosen scoring functions, such as ChemScore with XScore rescoring, is sufficient for accurate classification.\n\nIn summary, our comparison of methods involved evaluating different scoring functions, combining ligand- and structure-based approaches, and assessing performance on external datasets. The results highlight the effectiveness of our structure-based models, particularly when enhanced with rescoring techniques, in accurately classifying BSEP inhibitors and non-inhibitors.",
  "evaluation/confidence": "The evaluation of our models involved several performance metrics, including sensitivity, specificity, accuracy, G-mean, and Matthews's correlation coefficient (MCC). These metrics were calculated for each model to assess their performance in classifying inhibitors and non-inhibitors. The area under the ROC curve (AUC) was also used as a key metric, with high AUC values (above 0.8) indicating a strong capacity of the models to rank compounds by their probability of being inhibitors.\n\nTo determine the best scoring range for separating inhibitors from non-inhibitors, a p-value (Chi square test) was calculated for each bin. This statistical test helped identify the scoring ranges that significantly distinguished between inhibitors and non-inhibitors.\n\nThe ChemScore docking run, when rescored using Xscore, retrieved a model with an AUC of 0.918 and an MCC of 0.689. These values are comparable to models developed by other researchers, indicating the robustness of our approach. The model accurately predicted 88% of the training set compounds and 72% of the external test set compounds, demonstrating its reliability.\n\nThe results from the applicability domain (AD) assessment showed that all compounds from both test sets were within the chemical domain of the training compounds. This suggests that the model's predictions are reliable within the chemical space covered by the training set.\n\nIn summary, the performance metrics and statistical significance of our models provide confidence in their ability to classify inhibitors and non-inhibitors effectively. The use of multiple scoring functions and the application of statistical tests ensure that the results are robust and reliable.",
  "evaluation/availability": "Not enough information is available."
}