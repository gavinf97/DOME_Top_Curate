{
  "publication/title": "Classification and analysis of a large collection of in vivo bioassay descriptions",
  "publication/authors": "The authors who contributed to the article are:\n\n- JPO\n- MZ\n\nJPO contributed to the article with conceptualization, data curation, formal analysis, funding acquisition, investigation, methodology, project administration, resources, supervision, validation, and writing – review & editing.\n\nMZ contributed to the article with data curation, formal analysis, investigation, methodology, resources, software, validation, visualization, and writing – original draft.",
  "publication/journal": "PLOS Computational Biology",
  "publication/year": "2017",
  "publication/doi": "https://doi.org/10.1371/journal.pcbi.1005641",
  "publication/tags": "- Named Entity Recognition\n- Noun Phrase Extraction\n- Bioassay Descriptions\n- Machine Learning\n- Word2Vec\n- Random Forest Classifiers\n- Text Preprocessing\n- Semantic Similarity\n- Experimental Procedures\n- Biomedical Text Analysis",
  "dataset/provenance": "The dataset used in this study was sourced from ChEMBL, specifically version 21. This version was downloaded from the ChEMBL database's FTP website and subsequently loaded into a local MySQL database for analysis. The dataset comprises a substantial number of functional whole organism-based assays conducted on mouse and rat models. Specifically, there are 100,250 such assays, each accompanied by a concise description and a set of curated annotations. These annotations include details such as the species name, structures and ATC classification codes of the compounds tested, and information about the publications in which the assays were reported.\n\nThis dataset has been utilized in various analyses and has been made available for further research. The data points within this dataset have been indexed using Elasticsearch, a distributed full-text search engine built on Apache Lucene, to facilitate easy querying and manipulation. The SQL queries used to retrieve the data and calculate relevant statistics are documented and available for reference. This approach ensures that the dataset is not only comprehensive but also accessible for further exploration and analysis by the scientific community.",
  "dataset/splits": "In our study, we employed two distinct methods for splitting the data into subsets used for training and testing in a 10-fold cross-validation procedure. The first method involved randomly splitting all assays into equally sized subsets, resulting in ten subsets with an approximately equal number of data points in each. The second method focused on partitioning the assays by randomly splitting the documents from which the assay data were curated. This approach ensured that assays described in the same document, which often pertain to similar experiments differing only in details like dose or timing, were not used for both training and testing. This method helped prevent overly optimistic performance and poor generalization to new data. The distribution of data points in each split was designed to be as balanced as possible, with each subset containing a similar number of assays. This careful splitting strategy was crucial for evaluating the performance and generalization capabilities of our models.",
  "dataset/redundancy": "Two different methods were employed to split the data into ten subsets for training and testing in the 10-fold cross-validation procedure. The first method involved randomly splitting all assays into equally sized subsets. The second method, however, partitioned the assays by randomly splitting the documents from which the assay data were curated. This approach was crucial because descriptions of assays reported in the same document often share similarities, frequently corresponding to the same experiments that differ only in details like dose or timing. By ensuring that assays from the same document were not used for both training and testing, the second method prevented classifiers from achieving overly optimistic performance and poor generalization to new data. This method is particularly important in the context of machine learning datasets, as it helps to maintain the independence of training and test sets, a critical factor for the reliability of model evaluation. The second splitting method is designed to address potential redundancy in the dataset, ensuring that the model's performance is a true reflection of its ability to generalize to unseen data.",
  "dataset/availability": "The data used in our study is publicly available. We utilized ChEMBL, specifically release 21, to generate our corpus of experimental descriptions of in vivo assays involving animal models of human disease and physiology. The dataset consists of 100,250 functional whole organism-based assays in mouse and rat, each with a concise description and curated annotations.\n\nTo facilitate easy querying and manipulation, we indexed the textual descriptions and associated structured data using Elasticsearch, a distributed full-text search engine built on Apache Lucene. This indexing allows for efficient retrieval and analysis of the data.\n\nAll SQL queries used to retrieve the data and calculate statistics are provided in the supplementary information, specifically in S2 Text. This ensures transparency and reproducibility of our methods. The datasets themselves are available in S1 and S2 Datasets, which can be accessed through the supplementary materials accompanying our publication.\n\nThe data includes detailed information such as species name, structures and ATC classification codes of compounds tested in the assays, and publication details. This comprehensive dataset enables researchers to replicate our findings and build upon our work.\n\nThe licensing details for the use of ChEMBL data can be found on the ChEMBL website. Generally, ChEMBL data is available under the Creative Commons Attribution-ShareAlike 4.0 International License, which allows for free use, distribution, and adaptation of the data, provided that appropriate credit is given and any derivative works are shared under the same license.",
  "optimization/algorithm": "The optimization algorithm employed in this study is based on the random forest (RF) machine-learning algorithm class. Random forests are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe random forest algorithm is not new; it has been widely used and studied in the machine-learning community for many years. The decision to use this well-established algorithm in this context is due to its robustness, ability to handle high-dimensional data, and effectiveness in dealing with both classification and regression tasks. Random forests are known for their ability to capture complex interactions between features and provide reliable performance even when the dataset is imbalanced.\n\nThe focus of this work is on the application of text processing techniques and neural language models to mine and classify in vivo assay descriptions, rather than the development of a new machine-learning algorithm. Therefore, the use of a proven and widely-accepted algorithm like random forests is appropriate for achieving the study's objectives. The algorithm's implementation was done using the Python scikit-learn library, which is a standard tool in the machine-learning community.",
  "optimization/meta": "The models described in the publication do not function as meta-predictors. Instead, they are individual random forest classifiers trained on assay descriptions. These classifiers predict various aspects of assays, such as whether they involve cidal/cytotoxic drugs, drugs acting on the nervous system, specific ATC code combinations, or specific drug classes within the nervous system category.\n\nEach random forest classifier is an ensemble of decision tree estimators. The training process involves calculating vectors for assay descriptions as the average of their corresponding word embeddings. These vectors are then used to train the classifiers. The final classification is determined by averaging the probabilistic predictions of individual decision trees.\n\nThe models do not use data from other machine-learning algorithms as input. They rely solely on the textual information from assay descriptions. The training and testing data are split using two methods: random splitting of all assays and partitioning by splitting the documents from which the assay data were curated. The second method ensures that assays from the same document are not used for both training and testing, which helps to prevent overly optimistic performance and poor generalization to new data. This approach assures that the training data is independent, as assays from the same document are not used in both the training and testing sets.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the text corpus for machine learning algorithms. Initially, a text corpus of experimental descriptions of in vivo assays involving animal models of human disease and physiology was generated. This corpus was created by selecting functional whole organism-based assays in mouse and rat from a specific database release, resulting in a dataset of over 100,000 assays.\n\nThe textual descriptions of these assays were indexed using Elasticsearch, a distributed full-text search engine, to facilitate easy querying and manipulation. This indexing process allowed for efficient retrieval and statistical analysis of the data.\n\nThe preprocessing of the assay descriptions involved shallow grammatical analysis using the GENIA tagger, which annotated each word with its corresponding part-of-speech category. This step was crucial for identifying longer chunks of text corresponding to noun phrases, which were then simplified using custom tags and chunking rules.\n\nNamed entity recognition (NER) methods were employed to identify strains, experimental animal models, and phenotypic terms within the text. These methods combined dictionary and rule-based approaches to ensure accurate extraction of relevant entities.\n\nTo convert the text into numerical vectors, an unsupervised neural network model, Word2Vec, was used. The assay descriptions were tokenized into words and noun phrases, and standard English stop words and numbers were removed to improve the model's performance. The Word2Vec model parameters were set to standard values, including a window size of 5, a minimum word frequency of 30, and a dimensionality of 250 for the resulting word embeddings.\n\nThe output of the Word2Vec model was a set of 250-dimensional numerical vectors, each corresponding to a single word or phrase from the input corpus. These vectors were used to find semantic similarities between concepts and to train classification models. Cosine distances between the vector embeddings were calculated to determine semantic similarity values for pairs of terms and entire assay descriptions.\n\nAdditionally, the names of genetic strains mentioned in the descriptions were normalized to reduce the impact of synonyms and improve the robustness of the model. This multistep text processing workflow, including shallow parsing and custom grammar patterns, enhanced the performance of the classifiers trained on the assay descriptions.",
  "optimization/parameters": "In our study, we utilized Word2Vec to convert words and phrases from assay descriptions into numerical vectors. The model parameters were set to standard values, with the dimensionality of the resulting word embeddings set to 250. This means that each word or phrase from the input corpus is represented by a 250-dimensional numerical vector. The other parameters included a window size of 5, which is the maximum distance between the current and predicted word, and a minimum word frequency count of 30. These settings were chosen to balance the trade-off between capturing semantic meaning and computational efficiency. The choice of 250 dimensions was based on empirical evidence suggesting that this dimensionality provides a good balance between capturing semantic information and computational feasibility.",
  "optimization/features": "The input features for our models are derived from the assay descriptions, which are converted into numerical vectors using the Word2Vec model. Each word or phrase from the assay descriptions is represented by a 250-dimensional vector. Therefore, the number of features (f) used as input is 250.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the features were generated through an unsupervised learning process using Word2Vec, which creates vector representations based on the semantic context of words and phrases in the assay descriptions. This approach inherently selects relevant features by capturing the most meaningful patterns in the text data.\n\nThe process of generating these features involved preprocessing the assay descriptions, including tokenization, normalization of genetic strain names, and removal of numbers and standard stop words. The Word2Vec model was then trained on this preprocessed data to produce the 250-dimensional vectors. This training process was conducted using the entire dataset, ensuring that the features are robust and generalizable.",
  "optimization/fitting": "The fitting method employed in this study utilized random forest classifiers, which are ensemble learning methods that consist of multiple decision trees. Each classifier was trained on a dataset of assay descriptions, converted into numerical vectors using Word2Vec. The random forest approach inherently helps to mitigate overfitting by averaging the predictions of individual decision trees, which are trained on different subsets of the data.\n\nThe number of parameters in the random forest models is indeed large, as each tree in the forest can have a significant number of nodes and splits. However, overfitting was addressed through several strategies. Firstly, the use of 10-fold cross-validation ensured that the model's performance was evaluated on multiple subsets of the data, providing a robust estimate of its generalization capability. Secondly, the adjusted class weights in the random forest models helped to balance the impact of dataset imbalance, reducing the risk of the model becoming biased towards the majority class. Additionally, the out-of-bag (OOB) estimate provided an internal estimate of the model's error, further helping to assess overfitting.\n\nTo avoid underfitting, the models were trained with a sufficient number of decision trees (200 estimators), which allowed the ensemble to capture complex patterns in the data. The use of word embeddings to convert assay descriptions into numerical vectors also provided rich, high-dimensional features that enabled the models to learn intricate relationships within the data. The performance measures, including precision, recall, accuracy, and F1-score, were calculated to ensure that the models were not too simplistic and could effectively classify the assays into their respective categories.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method involved splitting the data into training and testing subsets using two different approaches. The first method randomly divided all assays into equally sized subsets. The second method, which we found to be more effective, partitioned the assays by randomly splitting the scientific publications from which the assay data were curated. This second approach ensured that assays described in the same document were not used for both training and testing, thereby preventing the models from learning document-specific nuances that might not generalize to new data.\n\nAdditionally, we used random forest classifiers, which are inherently robust to overfitting due to their ensemble nature. Each random forest classifier consisted of an ensemble of decision tree estimators trained on equally sized datasets sampled with replacement. This technique, known as bagging, helps to reduce variance and improve the model's generalization ability.\n\nWe also set the number of tree estimators to 200, which provided a good balance between computational efficiency and model performance. Furthermore, we used adjusted class weights to handle dataset imbalance, ensuring that the models did not become biased towards the majority classes. This was achieved by setting the class_weight parameter to \"balanced,\" which automatically adjusts weights inversely proportional to class frequencies in the input data.\n\nTo evaluate the performance of our models, we followed a 10-fold cross-validation procedure. This method involved splitting the data into ten subsets, training the model on nine subsets, and testing it on the remaining one. This process was repeated ten times, with each subset serving as the test set once. This rigorous evaluation technique helped us to assess the models' performance more accurately and to ensure that they generalized well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in detail within the publication. Specifically, we utilized random forest classifiers with a set number of tree estimators (200) and adjusted class weights to handle dataset imbalance. The class_weight parameter was set to \"balanced,\" and other parameters were kept at their default values as implemented in the Python scikit-learn library.\n\nThe optimization schedule involved a 10-fold cross-validation procedure, where the data was split into ten subsets for training and testing. Two methods were employed for splitting the data: one involved random splitting of all assays, while the other partitioned the assays by randomly splitting the documents from which the assay data were curated. This second method ensured that assays described in the same document were not used for both training and testing, thereby preventing overly optimistic performance and poor generalization.\n\nThe performance measures calculated included precision, recall, accuracy, F1-score, confusion matrix, and out-of-bag estimate (OOB). These metrics were derived following the 10-fold cross-validation procedure.\n\nModel files and specific optimization parameters are not explicitly provided in the publication, but the methods and configurations are thoroughly described, allowing for replication of the experiments. The data used in this study, including the assay descriptions and associated structured data, are available in supplementary datasets (S1 and S2 Datasets). The SQL queries used to retrieve the data and calculate statistics are also provided in supplementary text (S2 Text).\n\nFor those interested in replicating or building upon our work, the detailed descriptions of the methods, parameters, and data availability should facilitate the process. The publication is licensed under terms that allow for academic use and further research, ensuring that the findings and methods can be utilized by the scientific community.",
  "model/interpretability": "The models employed in this study, particularly the random forest classifiers, offer a degree of interpretability that sets them apart from many other machine learning approaches. Random forests are ensemble methods composed of multiple decision trees, each providing a simple, rule-based structure that is relatively easy to interpret. This transparency is crucial for understanding how predictions are made and for validating the model's decisions.\n\nEach decision tree in the random forest operates by splitting the data based on the most informative features, creating a series of if-then rules. These rules can be traced to understand the decision-making process at each node. For instance, a decision tree might first split the data based on whether an assay description contains the term \"antiepileptic,\" and then further split based on the presence of specific animal models or phenotypic terms. This hierarchical structure allows for a clear understanding of which features are most influential in classifying the assays.\n\nMoreover, the use of word embeddings generated by Word2Vec adds another layer of interpretability. Word2Vec converts words and phrases into numerical vectors that capture semantic similarities. By visualizing these embeddings using techniques like t-SNE, we can observe how different assay descriptions cluster together based on their semantic content. This visualization helps in identifying patterns and relationships within the data, making it easier to interpret the model's outputs.\n\nFor example, assays involving drugs acting on the nervous system might cluster together because their descriptions share common terms related to neurological effects. Similarly, assays testing antidiabetic drugs might cluster separately due to the presence of terms like \"blood glucose level\" and \"body weight.\" These clusters provide insights into the underlying structure of the data and help in validating the model's predictions.\n\nAdditionally, the random forest models provide feature importance scores, which indicate the relative significance of each feature in making predictions. This information can be used to identify the most critical terms and phrases in the assay descriptions, further enhancing the interpretability of the model.\n\nIn summary, the combination of decision trees in the random forest and the semantic analysis provided by Word2Vec embeddings makes the models used in this study relatively transparent. This transparency is essential for understanding the decision-making process, validating the model's predictions, and gaining insights into the data.",
  "model/output": "The model is a classification model. We employed random forest classifiers to predict assay class membership based solely on the textual information from the descriptions. Specifically, we built four distinct random forest models to address different classification problems. These models predict whether an assay involves cidal/cytotoxic drugs, drugs acting on the nervous system, specific drug classes for assays involving drugs acting on the nervous system, and the five most common ATC code combinations, which serve as proxies for the most common disease areas. The models were trained using vectors calculated for assay descriptions as the average of their corresponding word embeddings. The final classification is given by averaging the probabilistic predictions of individual decision trees within the ensemble. We evaluated the models using standard performance measures such as precision, recall, accuracy, and F1-score, along with a confusion matrix and out-of-bag estimate. The models demonstrated high overall performance in predicting ATC classes, even without using chemical structure information or explicit drug information in the assay descriptions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a rigorous 10-fold cross-validation procedure. This procedure was used to calculate the overall prediction accuracy and per-class performance measures for each classification problem. The results were detailed in supplementary text, with a classification report and confusion matrix provided for an example classifier.\n\nTwo different methods were applied for splitting the data into ten subsets used for training and testing. The first method involved randomly splitting all assays into equally sized subsets. The second method partitioned the assays by randomly splitting the documents from which the assay data were curated. This second method was crucial because descriptions of assays reported in the same document are often very similar, corresponding to the same experiments that differ only in details like dose or timing. This approach ensured that assays used for both model training and testing were not overly similar, preventing overly optimistic performance and poor generalization to new data.\n\nThe classifiers were trained using random forest models, which consisted of an ensemble of decision tree estimators. Each model was trained on an equally sized dataset sample drawn with replacement. The final classification was determined by averaging the probabilistic predictions of individual decision trees. The number of tree estimators was set to 200, and adjusted class weights were used to mitigate the impact of dataset imbalance.\n\nStandard performance measures, including precision, recall, accuracy, and F1-score, were calculated for each model. Additionally, a confusion matrix and out-of-bag estimate were generated. The performance of the classifiers was compared with other text-based methods, such as paragraph2vec and bag-of-words with TF-IDF weighting, with the assay vector-based classifiers generally outperforming these alternatives.",
  "evaluation/measure": "In the evaluation of our classifiers, we reported several key performance metrics to provide a comprehensive assessment of their effectiveness. These metrics include overall prediction accuracy, which indicates the proportion of correctly classified assays out of the total number of assays. Additionally, we calculated per-class performance measures, such as precision, recall, and F1-score, to evaluate the classifiers' performance on individual assay classes. These metrics are crucial for understanding how well the classifiers perform across different types of assays.\n\nWe also presented confusion matrices to visualize the performance of the classifiers, showing the true positive, true negative, false positive, and false negative rates for each class. This allows for a detailed inspection of where the classifiers might be making errors and which classes are more challenging to predict accurately.\n\nThe reported metrics are standard in the field of machine learning and are widely used in the literature for evaluating classification models. This ensures that our results are comparable with other studies and provides a clear benchmark for the performance of our classifiers. The use of these metrics allows us to demonstrate the robustness and reliability of our models in predicting assay classes based on textual descriptions.",
  "evaluation/comparison": "In our evaluation, we conducted a comprehensive comparison of our assay vector-based classifiers with other text-based approaches to assess their relative performance. Specifically, we compared our method with paragraph2vec and bag-of-words with TF-IDF weighting. These comparisons were crucial to demonstrate the effectiveness of our approach, which leverages assay vectors derived from word embeddings.\n\nThe assay vector-based classifiers consistently outperformed both paragraph2vec and bag-of-words with TF-IDF weighting. This superior performance was evident across various classification tasks, including distinguishing between assays involving cidal/cytotoxic drugs and non-cytotoxic drugs, as well as predicting whether an assay involved drugs acting on the nervous system.\n\nOur evaluation also included a detailed analysis of performance measures such as precision, recall, accuracy, and F1-score, which were calculated following a 10-fold cross-validation procedure. This rigorous evaluation ensured that our results were robust and generalizable. Additionally, we provided a confusion matrix and out-of-bag estimates to offer a comprehensive view of the classifiers' performance.\n\nThe comparisons were not limited to overall performance metrics. We also examined the performance of our classifiers on specific classes within each classification problem. For instance, we noted that the classifiers performed well even when dealing with imbalanced datasets, thanks to the use of adjusted class weights. This aspect was particularly important for classes with fewer assays, such as those involving anesthetics, where the recall value was lower due to the imbalance in the training set.\n\nIn summary, our method demonstrated significant advantages over simpler baselines and publicly available methods, highlighting the effectiveness of using assay vectors for classifying in vivo bioassay descriptions. The detailed performance analysis and comparisons provided in the supplementary materials further support the robustness and reliability of our approach.",
  "evaluation/confidence": "The evaluation of our classifiers involved a rigorous 10-fold cross-validation procedure, which helps in assessing the performance metrics' reliability. However, specific details about confidence intervals for the performance metrics are not explicitly mentioned. The results indicate high overall accuracy for the classifiers, suggesting statistical significance in their performance. For instance, one classifier achieved an overall accuracy of 0.97 for distinguishing between assays involving cidal/cytotoxic drugs and non-cytotoxic drugs, which is notably high. Another classifier predicting assays involving drugs acting on the nervous system achieved an overall accuracy of 0.93. These high accuracies imply that the results are statistically significant and that the method is superior to other text-based approaches like paragraph2vec and bag-of-words with TF-IDF weighting. The use of random forest models with adjusted class weights further ensures that the performance metrics are robust and not overly influenced by dataset imbalances. Additionally, the application of two different splitting methods for the data—random splitting of assays and document-based splitting—provides a comprehensive evaluation of the classifiers' generalization capabilities. The document-based splitting method, in particular, helps in avoiding overly optimistic performance estimates by ensuring that similar assays are not used for both training and testing. Overall, the evaluation suggests that the classifiers are reliable and statistically significant in their predictions.",
  "evaluation/availability": "The raw evaluation files are not directly available for public download. However, the data used in the analyses and the statistics reported in the dataset description section were retrieved using specific SQL queries from a local MySQL database loaded with ChEMBL version 21. These queries can be found in the supplementary information provided with the publication. Additionally, the data itself is available in supplementary datasets (S1 and S2 Datasets) accompanying the publication. For those interested in replicating the analyses, the SQL queries and datasets offer a comprehensive starting point. The publication also includes details on the preprocessing steps, model training, and evaluation methods, which can be followed to reproduce the results."
}