{
  "publication/title": "Hidden Markov Modeling of Frequency-Following Responses to Mandarin Lexical Tones",
  "publication/authors": "Fernando Llanos, Zilong Xie, and Bharath Chandrasekaran contributed to the article.\n\nFernando Llanos and Zilong Xie are affiliated with the Department of Communication Sciences & Disorders, Moody College of Communication, The University of Texas at Austin.\n\nBharath Chandrasekaran is affiliated with the Department of Communication Sciences & Disorders, Moody College of Communication, The University of Texas at Austin, the Department of Psychology, College of Liberal Arts, The University of Texas at Austin, and the Institute for Neuroscience, College of Liberal Arts, The University of Texas at Austin.\n\nThe specific contributions of each author to the paper are not detailed.",
  "publication/journal": "Journal of Neuroscience Methods",
  "publication/year": "2017",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Frequency-Following Response (FFR)\n- Mandarin Lexical Tones\n- Hidden Markov Modeling (HMM)\n- Machine Learning\n- Auditory Function\n- Neural Pitch Processing\n- Language Experience\n- Electrophysiological Activity\n- Subcortical Encoding\n- Speech Categorization\n- Signal-to-Noise Ratio (SNR)\n- Cross-language Differences\n- Auditory Plasticity\n- Bioacoustics\n- Neurophysiology",
  "dataset/provenance": "The dataset used in this study consists of frequency-following responses (FFRs) elicited from Mandarin tones. The tones are categorized into four distinct classes, each representing a different pitch contour. The FFRs were recorded from participants listening to complete stimulus waveforms, with the data divided into disjoint training and testing subsets to avoid stimulus artifact bias and sample selection bias. The number of data points varies depending on the specific experimental run, with training sizes ranging from 100 to 900 trials per tone in steps of 50. The testing size is typically set to be double the averaging size, which itself varies from 50 trials to half the training size in increments of 25. This design ensures a comprehensive exploration of different training, testing, and averaging sizes to optimize the performance of the hidden Markov model (HMM) classifier.\n\nThe FFRs have been previously used in studies involving Mandarin tones, often requiring a large number of stimulus repetitions, typically around 1500. However, our machine learning approach demonstrates that reliable decoding can be achieved with significantly fewer trials. For instance, when considering all tones, the language experience effect becomes statistically reliable at around 550 trials. When focusing on a specific tone, such as T3, this threshold can be as low as 300 trials. This reduction in the number of required trials is a notable advancement, making the experimental design more efficient and accessible, particularly for difficult-to-test populations like infants, older adults, and hearing-impaired listeners.\n\nThe dataset has been utilized to train HMMs, which are then combined in a parallel architecture to classify novel F0 sequences from any of the four tone classes. The accuracy of the classifier is computed from the confusion matrix, providing both tone-specific and general accuracy scores. The use of cross-validation with a K-fold partition ensures that the model's performance is robust and generalizable. The dataset's structure and the methods employed highlight its potential for further research in auditory processing and the development of more efficient experimental designs.",
  "dataset/splits": "The dataset was divided into disjoint training and testing subsets for each tone class. The number of data splits, or K-fold partitions, varied depending on the size of the tone dataset. For instance, if the testing size was 100 trials and the total number of frequency-following responses (FFRs) for a tone dataset was 500, there would be 5 possible disjoint testing selections. This means that the dataset was split into 5 different testing subsets, each containing 100 trials, and the remaining trials were used for training.\n\nThe distribution of data points in each split was designed to avoid stimulus artifact bias and sample selection bias. To achieve this, training and testing subsets alternated an equal number of trials with opposite stimulus polarity. This ensured that each split had a balanced representation of stimulus polarities, helping to mitigate any potential biases.\n\nIn addition to the training and testing splits, there was also a cross-validation process. The hidden Markov model (HMM) classifier was trained and tested K times, using different cross-validated selections of testing and training datasets each time. This approach helped to ensure that the classifier's performance was robust and not dependent on a particular split of the data.\n\nThe size of the training and testing subsets varied across different runs. The training size ranged from 100 to 900 trials per tone in steps of 50. The testing size was set to be double the averaging size to minimize the risk of underfitting while maintaining a sufficient number of trials for testing. Combinations of training and testing sizes that resulted in a total sample size larger than 1000 were excluded. This careful manipulation of dataset splits and sizes helped to optimize the classifier's performance and ensure reliable results.",
  "dataset/redundancy": "The datasets used in our study were divided into disjoint training and testing subsets to ensure independence between the two sets. This division was crucial to avoid stimulus artifact bias and sample selection bias. To mitigate stimulus artifact bias, the training and testing subsets alternated an equal number of trials with opposite stimulus polarity. This approach helped to control for any potential artifacts that might arise from the stimulus presentation.\n\nTo enforce independence and avoid sample selection bias, we employed a K-fold cross-validation strategy. The value of K was determined by the number of possible disjoint testing selections available in the corresponding tone dataset. For instance, if the testing size was 100 trials and the total number of frequency-following responses (FFRs) to a tone dataset was 500, then K would be 5. This means that the dataset was split into 5 folds, and the model was trained and tested K times using different cross-validated selections of testing and training data sets at each step.\n\nThe distribution of our datasets differs from some previously published machine learning datasets in that we specifically aimed to minimize redundancy and underfitting. We conducted an exploratory series of hidden Markov model (HMM) runs across different training and moving average window (MAW) sizes. This series helped us identify MAW sizes for which the accuracy of the classifier did not show significant improvement. By excluding these MAW sizes in future runs, we aimed to minimize the risk of underfitting without compromising the accuracy of the classifier.\n\nIn summary, our datasets were carefully split to ensure independence between training and testing sets, and we employed a rigorous cross-validation strategy to avoid biases. The distribution of our datasets was designed to optimize the performance of our machine learning models while minimizing redundancy and underfitting.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Hidden Markov Model (HMM). This class of algorithms is well-suited for decoding time-varying sequences, such as the fundamental frequency (F0) contours of lexical tones. HMMs are designed to model probabilistic sequences of states that are not directly observable, making them ideal for capturing the variability in F0 sequences associated with different tone categories.\n\nThe use of HMMs in our research is not entirely novel, as they have been previously applied in various domains, including speech recognition and bioinformatics. However, our application of HMMs to decode frequency-following responses (FFRs) to lexical tones is innovative. To the best of our knowledge, this is the first study utilizing HMMs for this specific purpose. The decision to use HMMs was driven by their ability to model the temporal dynamics of lexical tones, which are characterized by distinct F0 contours over time.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus of our study is on auditory neuroscience and the neural encoding of pitch, rather than the development of new machine-learning algorithms. Our goal was to demonstrate the feasibility and effectiveness of using HMMs to decode FFRs and capture language experience-dependent plasticity in auditory processing. The methodological innovations in our study are secondary to the scientific questions we aimed to address. Therefore, the appropriate venue for our work is a neuroscience methods journal, where the application of machine learning to neural data is of particular interest.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It relies solely on the hidden Markov model (HMM) for decoding Mandarin tones from frequency-following responses (FFRs). The HMM is trained using Viterbi to learn fundamental frequency (F0) sequences from the same tone class. This training process does not incorporate data from other machine-learning algorithms as input.\n\nThe HMM classifier is designed to handle the time-varying profile of lexical tones by assuming that the emissions (e.g., F0 contours) in these series are produced by a series of states that cannot be directly observed (e.g., target lexical tone categories). The hidden states follow each other with certain transition probabilities established from the distribution of the emissions across sequences.\n\nThe training and testing subsets are divided to avoid stimulus artifact bias and sample selection bias. The selection of testing and training subsets is cross-validated using a K-fold partition, ensuring that the data used for training and testing are independent. This cross-validation process helps in estimating the accuracy of the classifier more reliably.\n\nIn summary, the model does not use a meta-predictor approach. It exclusively uses the HMM for decoding, and the training data is independently partitioned to ensure unbiased results.",
  "optimization/encoding": "The data encoding process involved using the fundamental frequency (F0) contours derived from the frequency-following responses (FFRs) to Mandarin tones. These F0 contours were used as the input for the hidden Markov model (HMM), which is well-suited for decoding time-varying profiles like those of lexical tones. The HMM was trained to learn the sequences of F0 contours from the same tone class, assuming that these emissions are produced by a series of hidden states that represent the target lexical tone categories.\n\nTo prepare the data, FFRs were averaged using a moving average window (MAW) to increase the signal-to-noise ratio (SNR) while maintaining an adequate number of trials for training and testing. The length of the MAW was varied to minimize the risk of underfitting, with sizes ranging from 50 trials to half the training size in increments of 25. This averaging method helped to reduce noise and improve the reliability of the F0 contours used in the HMM.\n\nThe training and testing subsets were created from the FFR data, ensuring that they were disjoint and alternated stimulus polarity to avoid artifact bias. Cross-validation was employed using a K-fold partition, where K was equal to the number of possible disjoint testing selections available in the tone dataset. This approach helped to avoid sample selection bias and ensured that the model's performance was robust across different subsets of the data.\n\nThe accuracy of the HMM was evaluated using confusion matrices, which provided tone-specific accuracy scores (Acc1, Acc2, Acc3, and Acc4) and a general accuracy score across all tones (Acc0). These scores were computed from the number of true positives, true negatives, false positives, and false negatives resulting from the classification of novel F0 sequences. The model's performance was assessed over time to determine the amount of information required for accurate tone decoding, with the HMM trained on complete F0 sequences and tested on gradually increasing portions of the FFR.",
  "optimization/parameters": "The model utilized in our study employs a Hidden Markov Model (HMM) classifier consisting of four independent HMMs, each defined as a stochastic chain of three self-connected hidden states. These states are feedforward connected to the next two states, creating a structured topology for decoding the fundamental frequency (F0) contours of Mandarin tones.\n\nThe F0 contours are quantized into sequences of 22 discrete F0 intervals, or codewords, using a codebook of 50 centroids. This codebook is generated by clustering all available F0 data points for training into 50 Voronoi cells using the Linde-Buzo-Gray algorithm. The choice of parameters, including the HMM architecture and codebook length, was derived from a previous study that focused on recognizing Mandarin tone production.\n\nThe selection of the moving average window (MAW) size was carefully considered to minimize the risk of underfitting. Exploratory runs were conducted across different training and MAW sizes to identify the optimal MAW sizes that did not significantly improve classifier accuracy. This approach ensured that the model maintained high accuracy while reducing the risk of underfitting.\n\nThe training size ranged from 100 to 900 trials per tone in steps of 50 across runs. The averaging size varied from 50 trials to half the corresponding training size in increments of 25. The testing size was set to be double the averaging size to balance the need for a sufficient number of trials while minimizing underfitting.\n\nIn summary, the model parameters were selected based on a combination of theoretical considerations and empirical evidence from exploratory runs. This approach ensured that the model was optimized for accuracy and robustness in decoding Mandarin tones from frequency-following responses (FFRs).",
  "optimization/features": "The input features for the classifier are the F0 contours extracted from the frequency-following responses (FFRs). These contours are quantized into sequences of discrete F0 intervals, or codewords, using a codebook of 50 centroids. This means that the number of features (f) used as input is 22, as the F0 contours are sliced into 22 consecutive time frames.\n\nFeature selection was not explicitly performed in the traditional sense, as the features used are based on the F0 contours, which are fundamental to the analysis of pitch patterns in the FFRs. The choice of parameters, such as the HMM architecture and codebook length, was derived from previous studies focused on recognizing Mandarin tone production. The F0 contours were estimated from averaged FFRs using a specific method and quantized into sequences of discrete F0 intervals. This process ensures that the most relevant features for decoding the Mandarin tones are used.\n\nThe creation of the codebook involved clustering all the F0 data points available for training into 50 Voronoi cells using the Linde-Buzo-Gray algorithm. This step was performed using the training set only, ensuring that the feature extraction process is independent of the testing data. This approach helps to avoid overfitting and ensures that the model generalizes well to new data.",
  "optimization/fitting": "The fitting method employed in this study involved the use of Hidden Markov Models (HMMs) to decode frequency-following responses (FFRs) into Mandarin tones. The HMM classifier consisted of four independent HMMs, each trained to decode one of the four Mandarin tones. The number of parameters in the HMMs was managed carefully to avoid both overfitting and underfitting.\n\nTo address the potential issue of overfitting, where the model might fit the noise in the training data rather than the underlying pattern, several strategies were implemented. Firstly, the training and testing subsets were carefully partitioned using a K-fold cross-validation approach. This ensured that the model was trained and tested on different subsets of the data, reducing the risk of overfitting to any particular subset. Additionally, the averaging size of the FFRs was manipulated as a function of the training size to minimize the risk of overfitting. Specifically, the averaging size varied from 50 trials to half the corresponding training size in increments of 25. This helped to increase the signal-to-noise ratio (SNR) of each FFR while keeping an adequate number of trials for training and testing.\n\nTo rule out underfitting, where the model might be too simple to capture the underlying patterns in the data, the training size ranged from 100 to 900 trials per tone in steps of 50. This range allowed for a sufficient number of trials to be used in training, ensuring that the model could capture the necessary patterns. Furthermore, the testing size was set to be double the corresponding averaging size, which helped to minimize the risk of underfitting while keeping a reduced but decent number of trials per run. Combinations of training and testing sizes that led to sample sizes larger than 1000 were excluded to maintain a balance between model complexity and the risk of underfitting.\n\nThe accuracy of the classifier was estimated for each tone, resulting in four tone-specific accuracy scores. Additionally, a general accuracy score across all tones was calculated. This comprehensive approach to accuracy estimation helped to ensure that the model was not underfitting the data. The exploratory series of HMM runs across different training and moving average window (MAW) sizes also played a crucial role in identifying the optimal MAW sizes for which the accuracy of the classifier did not grant much improvement. By excluding these MAW sizes in future runs, the risk of underfitting was minimized without decreasing the accuracy of the classifier.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and underfitting. To mitigate the risk of underfitting, we used a moving average window (MAW) to increase the signal-to-noise ratio (SNR) of each frequency-following response (FFR). However, this method also introduced redundancy across trials, which could lead to underfitting. To address this, we conducted an exploratory series of hidden Markov model (HMM) runs with varying training and MAW sizes. Our goal was to identify MAW sizes that did not significantly improve classifier accuracy, as these sizes could be excluded in future runs to minimize underfitting risk without compromising accuracy.\n\nAdditionally, we implemented a K-fold cross-validation approach to divide FFRs into disjoint training and testing subsets. This method helped to avoid sample selection bias and ensured that our model's performance was robust and generalizable. Furthermore, we alternated stimulus polarity in training and testing subsets to prevent stimulus artifact bias.\n\nTo manage the risk of overfitting, we manipulated the training, testing, and averaging sizes across different runs. We ensured that the testing size was double the averaging size to maintain a decent number of trials while minimizing underfitting. Combinations of sizes that resulted in sample sizes larger than 1000 were excluded to prevent overfitting. By carefully controlling these parameters, we aimed to achieve a balanced model that generalizes well to new data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model employed in this study is a Hidden Markov Model (HMM), which is inherently more interpretable than many other machine learning models. Unlike black-box models, the HMM provides a structured way to understand the underlying processes generating the observed data. The HMM consists of a series of hidden states that emit observable outputs, such as F0 contours, with certain probabilities. These states transition from one to another according to predefined transition probabilities.\n\nOne of the key aspects of the HMM's interpretability is its ability to model time-varying patterns, such as the pitch contours of Mandarin tones. Each HMM in the classifier is designed as a stochastic chain of hidden states, where each state represents a segment of the pitch contour. The transitions between these states capture the temporal dynamics of the pitch patterns, making it possible to understand how different segments of the pitch contour contribute to the classification of tones.\n\nThe F0 contours are quantized into sequences of discrete intervals, or codewords, which are then used to train the HMM. This quantization process allows for a clear mapping between the observed data and the hidden states, providing insights into how the model interprets the input signals. For example, a rising pitch contour in a Mandarin tone would be represented by a sequence of states where lower F0 values transition to higher F0 values with higher probabilities.\n\nAdditionally, the HMM's architecture, which includes self-connected and feedforward-connected states, allows for the modeling of both short-term and long-term dependencies in the pitch contours. This structure enables the model to capture the nuances of the pitch patterns, making it more interpretable in the context of tone classification.\n\nIn summary, the HMM used in this study is not a black-box model. Its structure and the way it processes input data provide a transparent and interpretable framework for understanding the neural encoding of pitch patterns in Mandarin tones. The model's ability to capture temporal dynamics and the clear mapping between observed data and hidden states contribute to its interpretability.",
  "model/output": "The model is a classification model. It uses Hidden Markov Models (HMMs) to decode fundamental frequency (F0) contours into four distinct tone categories. Each HMM is trained to learn F0 sequences from the same tone class, and then these trained HMMs are combined in a parallel architecture to classify novel F0 sequences. The classification is based on the HMM that provides the highest likelihood log-probability for a given F0 sequence. The accuracy of the classifier is computed from the confusion matrix, which includes true positives, true negatives, false positives, and false negatives. This results in both tone-specific accuracy scores and a general accuracy score across all tones. The model's performance is evaluated using cross-validation, with training and testing subsets alternating stimulus polarity to avoid bias. The accuracy scores are computed at each cross-validation step, providing a robust measure of the classifier's performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the implemented MATLAB scripts used in the HMM classifier is available. These scripts were developed in MATLAB R2015b and are provided in the supplementary methods. The specific details on how to access these scripts are not provided here, but they are intended to be shared with the research community. The exact licensing terms for these scripts are not specified, but they are likely to be available under standard academic sharing practices, which typically allow for non-commercial use and modification with proper attribution.",
  "evaluation/method": "The evaluation method involved a rigorous process to ensure the robustness and accuracy of the classifier. Each hidden Markov model (HMM) in the classifier was trained using the Viterbi algorithm to learn fundamental frequency (F0) sequences from the same tone class. These trained HMMs were then combined in a parallel architecture to classify novel F0 sequences from any of the four tone classes. The accuracy of the classifier was determined by decoding each novel F0 sequence to the tone category of the HMM that provided the highest likelihood log-probability.\n\nThe accuracy was computed directly from the confusion matrix, which resulted from this classification method. This involved calculating the number of true positives and true negatives divided by the total number of true positives, true negatives, false positives, and false negatives. Accuracy scores were first estimated for each tone, resulting in four tone-specific accuracy scores. Additionally, a general accuracy score was calculated across all tones.\n\nTo avoid stimulus artifact bias, training and testing subsets alternated the same number of trials with opposite stimulus polarity. Cross-validation using a K-fold partition was employed to avoid sample selection bias. The value of K was equal to the possible number of disjoint testing selections available in the corresponding tone dataset. For example, if the testing size was 100 trials and the number of frequency-following responses (FFRs) to the tone dataset was 500, the possible number of disjoint testing selections was 5. The HMM classifier was trained and tested K times, using different cross-validated selections of testing and training datasets at each step. Accuracy scores were computed from the confusion matrix resulting from the classification of all F0 sequences at each cross-validation step.\n\nFFRs in the training and testing subsets were averaged using a moving average window (MAW) of varying lengths. This averaging method increased the signal-to-noise ratio (SNR) of each FFR while maintaining an adequate number of trials for training and testing. However, it also increased the level of redundancy across trials, posing a risk of under-fitting. To mitigate this risk, an exploratory series of HMM runs was conducted across different training and MAW sizes. The goal was to identify MAW sizes for which the classifier's accuracy did not show significant improvement, thereby minimizing the risk of under-fitting without decreasing the classifier's accuracy. Training sizes were fixed at small, medium, and large sizes, and the MAW size was manipulated from 50 trials to the corresponding training size.",
  "evaluation/measure": "In the evaluation of our classifier's performance, several key metrics were reported to assess the accuracy and reliability of tone decoding in Mandarin tones. The primary performance metrics included tone-specific accuracy scores (Acc1, Acc2, Acc3, and Acc4) and a general accuracy score (Acc0). These metrics were derived from the confusion matrix resulting from the classification of fundamental frequency (F0) sequences.\n\nThe tone-specific accuracy scores ranged from 0 to 1, indicating the proportion of correctly classified tones for each of the four tone categories. The general accuracy score (Acc0) provided an overall measure of classification performance across all tones. These accuracy scores were computed for various combinations of training, testing, and averaging sizes, allowing us to evaluate the classifier's performance under different conditions.\n\nAdditionally, we validated our frequency-following response (FFR) dataset using established metrics such as the F0 error metric, stimulus-to-response correlation, and peak autocorrelation. These metrics have been widely used in the literature to evaluate the effects of long-term and short-term auditory experience on the subcortical encoding of Mandarin tones. The Chinese group demonstrated more optimal neural encoding of pitch compared to the English group, as evidenced by lower F0 errors and higher stimulus-to-response and peak autocorrelation coefficients.\n\nThe reported metrics are representative of those commonly used in the field. The accuracy scores provide a direct measure of classification performance, while the FFR metrics offer insights into the neural encoding of pitch, which is crucial for tone decoding. Together, these metrics offer a comprehensive evaluation of the classifier's performance and its ability to decode Mandarin tones accurately.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of our method's performance includes a thorough statistical analysis to ensure the reliability and significance of our results. We employed two-sample t-tests and Mann-Whitney tests to assess language group differences in decoding accuracy, accounting for non-normal and heteroscedastic distributions. These tests provided p-values that indicate the statistical significance of the observed differences.\n\nThe results are visually represented using color-shape scales and dotted lines marking the significance level (0.05). This approach allows for a clear interpretation of where the differences between language groups are statistically significant. For instance, in the optimal condition, we observed significant differences in general accuracy across almost all combinations of training, testing, and averaging sizes, with only a few exceptions.\n\nMoreover, we evaluated the evolution of tone-specific accuracy scores over time, using statistical tests at each time point. This temporal analysis helps to identify at which specific moments the differences between language groups become significant. The use of confidence intervals and statistical significance tests throughout our evaluation ensures that our claims about the method's superiority are robust and reliable.",
  "evaluation/availability": "Not enough information is available."
}