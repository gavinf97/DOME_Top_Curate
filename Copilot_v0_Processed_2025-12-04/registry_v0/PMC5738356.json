{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Wilten Nicola and Claudia Clopath. Nicola was responsible for writing the software and performing simulations. Both Nicola and Clopath contributed to the investigation and analysis, as well as writing the manuscript.",
  "publication/journal": "Nature Communications",
  "publication/year": "2017",
  "publication/doi": "10.1038/s41467-017-01827-3",
  "publication/tags": "- Spiking Neural Networks\n- FORCE Learning\n- Neural Computation\n- Computational Neuroscience\n- Izhikevich Model\n- High-Dimensional Temporal Signals\n- Songbird Circuit\n- Theta Oscillations\n- Neural Signal Processing\n- Machine Learning in Neuroscience",
  "dataset/provenance": "Not enough information is available.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data splits used in our study are not explicitly detailed as being released in a public forum. However, we have made significant efforts to ensure that our work is reproducible and accessible to the scientific community.\n\nWe have uploaded the source code for training the examples to an online repository, specifically modelDB, under accession number 190565. This code is currently accessible to referees and editors with the password rv15556 and will be made public after the publication of the manuscript. This step ensures that other researchers can replicate our findings and build upon our work.\n\nRegarding the data itself, while specific datasets may not be publicly available, the methods and models we used are thoroughly documented. This includes detailed descriptions of the simulations and the analysis performed. Additionally, supplementary materials such as audio clips and movies are provided to illustrate the results, further aiding in the reproducibility of our study.\n\nThe license under which our article is published is the Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source is provided. This ensures that our work can be freely used and built upon by others, fostering collaboration and advancement in the field.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is supervised learning, specifically leveraging the FORCE (First-order Reduced and Controlled Error) training method. This approach is not entirely new; it has been previously applied to spiking neural networks. However, our implementation and application of FORCE training are unique in several ways.\n\nThe FORCE training method we employ is adapted to work with spiking neurons, which introduces additional complexities compared to rate-based models. This adaptation allows us to explore the dynamics of spiking neural networks in a more biologically plausible manner. The key insight that enabled successful learning in our implementation is the expansion of the state space due to the inclusion of slow time constants in the single neuron dynamics, such as spike frequency adaptation in the Izhikevich model.\n\nThe reason this work is published in a neuroscience journal rather than a machine-learning journal is that our primary focus is on the biological implications and applications of the FORCE training method. We aim to demonstrate how this algorithm can be used to model and understand neural dynamics in biological systems, such as the songbird circuit and hippocampal functioning. While the machine-learning aspects are crucial, they serve as a tool to achieve our broader neuroscience goals.\n\nOur approach allows for greater flexibility in modeling neuron and synapse dynamics, potentially resulting in more detailed and biologically motivated networks. This distinction is important because it highlights the unique contributions our work makes to the field of computational neuroscience, even though the underlying optimization principles are shared with other machine-learning techniques.",
  "optimization/meta": "The model described does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on specific teaching signals and high-dimensional temporal signals (HDTS) for training.\n\nThe core of the model involves FORCE training, which is applied to networks of neurons to generate desired outputs. For instance, in one example, the teaching signal is the spectrogram of a zebra finch's song, obtained from a data repository. The network is trained to reproduce this spectrogram without external inputs, demonstrating the generation of an internal HDTS.\n\nIn another example, the teaching signal is an 8-second movie clip, downsampled to form the supervisor. The network is trained to replay the movie while simultaneously generating an internal HDTS. This approach does not involve meta-learning or the integration of predictions from other machine-learning methods.\n\nThe training data for these examples is independent and specific to the tasks at hand. For the songbird example, the data consists of the spectrogram of a zebra finch's song. For the movie example, the data is a sequence of movie frames. The independence of the training data is clear, as each example is trained on its own specific teaching signal.\n\nThe model's focus is on the ability of neural networks to learn and reproduce complex temporal patterns through FORCE training, rather than on combining the outputs of multiple machine-learning algorithms.",
  "optimization/encoding": "The data encoding process varied depending on the specific application. For the songbird example, the teaching signal was derived from a 5-second recording of a male zebra finch's singing behavior, converted into a spectrogram using a 22.7 ms window. This spectrogram served as the supervisor signal for the network. The HVC input was modeled as a sequence of 500 pulses, constructed from the positive component of a sinusoidal oscillation with a 20 ms period. These pulses were multiplied by a static feedforward weight matrix, with weights drawn uniformly from a specified range.\n\nFor the movie clip example, the teaching signal consisted of an 8-second movie clip. The frames were smoothed and interpolated to allow for the application of the Recursive Least Squares (RLS) algorithm at any time point, rather than just at the fixed frame rates of the original clip. The movie was downsampled to 1920 pixels, forming the supervisor signal.\n\nIn the case of the high-dimensional temporal signal (HDTS), the signal consisted of 32 pulses, each 250 ms in duration. This HDTS was multiplied by a static feedforward weight matrix, with weights also drawn uniformly from a specified range. The network was then trained using the FORCE training method for a duration of 74 seconds.\n\nThe network architecture consisted of 1000 neurons for most examples, with specific parameters such as G, Q, λ−1, and Δt set accordingly. The average firing rate for the network varied depending on the example, with values such as 34 Hz and 52.93 Hz reported. The integration time step was consistently set at 0.04 ms.\n\nPost-training, the weight matrix could be manipulated using a parameter α to control the balance between excitation and inhibition. This allowed for the amplification or diminution of excitatory connections. The network's performance was tested after turning off the RLS to ensure that learning was successful and that the network could reproduce the desired output autonomously.",
  "optimization/parameters": "In our study, we utilized four key parameters in our model: Q, G, ν, and τD. These parameters were chosen based on their critical roles in the dynamics and learning capabilities of the neural networks we investigated.\n\nThe parameter Q is a scaling factor for the feedback weight matrix, which is essential for stabilizing the dynamics during training. It was introduced to address the issue of low target function amplitude, which can hinder convergence in the FORCE learning algorithm. The parameter G represents the overall synaptic strength, influencing the network's responsiveness to inputs. The parameter ν is related to the frequency of the input signals, which is crucial for understanding how the network responds to different temporal patterns. Lastly, τD, the synaptic decay time constant, is particularly important as it determines the temporal integration window of the network, affecting how quickly or slowly the network responds to changes in input.\n\nThe selection of these parameters was guided by both theoretical considerations and empirical observations. We observed that the synaptic decay time constant τD is particularly important, with faster supervisors requiring faster time constants and slower supervisors needing slower time constants. This trend was explicitly stated and supported by supplementary figures that illustrate the convergence behavior of the networks under different parameter regimes.\n\nAdditionally, we found that the Izhikevich model was the most robust among the three neuronal models tested (Theta, Leaky Integrate and Fire, and Izhikevich), showing convergent parameter regimes for a majority of the sinusoids tested. This robustness is likely due to the model's ability to capture a wide range of neuronal dynamics, making it more adaptable to different input conditions.\n\nIn summary, the parameters Q, G, ν, and τD were selected for their significant impact on the network's learning and dynamic properties. The choice of these parameters was informed by both theoretical insights and empirical evidence, ensuring that our model could effectively capture the complex behaviors observed in neural systems.",
  "optimization/features": "The input features used in the network varied depending on the specific task and model. For the songbird example, the teaching signal consisted of the spectrogram for a 5-second recording of a male zebra finch's singing behavior. This spectrogram was generated with a 22.7 ms window. The network consisted of 1000 neurons and was trained for 50 seconds with specific parameters.\n\nIn another example, the teaching signal was an 8-second movie clip. The frames were smoothed and interpolated to allow for training at any time point. The movie was downsampled to 1920 pixels (30 × 64), which formed the supervisor for the network.\n\nFor classification tasks, the inputs were uniformly distributed within a specific range, and the classification boundary varied depending on whether the task was linear or nonlinear. The inputs were multiplied by an input weight matrix to yield the input current.\n\nFeature selection was not explicitly mentioned as a separate process. Instead, the features were directly derived from the teaching signals, which were designed to match the desired output dynamics. The parameters and initial conditions of the network were carefully chosen to ensure effective learning and generalization.\n\nThe training process involved feeding the teaching signals into the network via feedforward weights, allowing the network to learn the desired dynamics. The recurrent nature of the network enabled it to classify inputs based on both the current input and the network's internal state, which depended on the initial conditions.\n\nIn summary, the input features were derived from the teaching signals, which were designed to match the desired output dynamics. The network parameters and initial conditions were chosen to ensure effective learning and generalization. Feature selection was not performed as a separate process.",
  "optimization/fitting": "The fitting method employed in our study involves training networks of neurons to learn specific dynamics using the FORCE training method. The number of parameters in our networks is indeed much larger than the number of training points. To address potential overfitting, we utilized a regularization technique known as Recursive Least Squares (RLS) during the training process. This method helps to prevent the network from memorizing the training data by penalizing large weights, thereby promoting generalization to new data.\n\nTo ensure that underfitting was not an issue, we conducted extensive simulations across a wide range of parameter spaces. For instance, networks of 2000 neurons were run over a 16 × 17 point mesh in the (G, Q) parameter space for various oscillators. The synaptic decay time constants and rise times were also varied to observe their effects on network performance. Additionally, we used different training durations for different oscillators, such as 5 seconds for sinusoidal oscillators and 85 seconds for more complex oscillators like \"Ode to Joy.\" This thorough exploration of the parameter space helped us identify regions where the networks could accurately learn the target dynamics without being overly simplified.\n\nThe performance of the networks was evaluated using the L2 error, which measures the difference between the network output and the desired output. Darker color schemes in our figures indicate greater accuracy, providing a visual representation of the network's performance across different parameter settings. By analyzing these results, we could ensure that the networks were neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting during the training process. Specifically, we used the FORCE method, which inherently includes regularization mechanisms to stabilize the network dynamics. The FORCE method involves a feedback term that helps to control the fluctuations generated by the balanced network. This feedback term is scaled appropriately to ensure that the network remains stable and does not overfit to the training data.\n\nAdditionally, we varied the parameter G, which controls the strength of the connections in the network. By adjusting G, we could observe how the coefficients of variation changed pre- and post-training. This allowed us to fine-tune the network to achieve the desired balance between learning the target dynamics and avoiding overfitting.\n\nFurthermore, we introduced a parameter α to control the balance between excitation and inhibition in the weight matrix post-training. This parameter allowed us to amplify or diminish the excitatory connections, providing an additional layer of regularization. By carefully tuning α, we could ensure that the network maintained a stable and generalizable performance across different tasks.\n\nIn summary, our regularization methods included the use of the FORCE method's inherent feedback mechanisms, the adjustment of the parameter G, and the introduction of the parameter α to control the balance between excitation and inhibition. These techniques collectively helped to prevent overfitting and ensure that our network models generalized well to new data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available and have been made accessible to the community. We have uploaded the relevant code to modelDB, an online repository for sharing computational models. The accession number for the uploaded code is 190565. Currently, the code is accessible to referees and editors with the password rv15556, and it will be made public after the publication of the manuscript. This ensures that other researchers can replicate our results and build upon our work.\n\nAdditionally, we have included a line in the introductory paragraph of the results section containing the modelDB accession number and a modelDB citation, providing clear guidance on how to access the materials. This step aligns with our commitment to transparency and reproducibility in scientific research. The code is licensed under terms that permit use, sharing, adaptation, distribution, and reproduction, as long as appropriate credit is given to the original authors and the source is cited. This licensing approach facilitates the broader dissemination and application of our methods and findings.",
  "model/interpretability": "The model presented in this work is not a black box. It is designed to be interpretable, with a focus on understanding how high-dimensional temporal signals (HDTS) function within the network. The model's transparency is demonstrated through several key aspects:\n\nFirstly, the model's ability to generate HDTS autonomously, without external inputs, is clearly illustrated. This is shown in examples such as the training of an Izhikevich network to reproduce the long form of \"Ode to Joy,\" where the network generates its own internal HDTS simultaneously with the production of the song. This is visually represented in supplementary figures, providing a clear example of the model's interpretability.\n\nSecondly, the model's capacity to handle complex tasks is demonstrated through examples like training a network to simultaneously generate an internal HDTS and replay a movie. This is achieved without any inputs into the network, further emphasizing the model's transparency. The supplementary materials include figures that show all neurons simultaneously encoding the HDTS and the movie scene, offering a concrete example of the model's interpretability.\n\nAdditionally, the model's design and functionality are elaborated in the materials and methods section, specifically in the added section titled \"High Dimensional Temporal Signals.\" This section provides detailed information on how the HDTS functions within the network, reinforcing the model's transparency.\n\nThe code used for training these examples is also made available in an online repository, allowing for further inspection and verification of the model's interpretability. This code is accessible through modelDB under a specific accession number, ensuring that the community can review and understand the model's inner workings.",
  "model/output": "The model is designed for classification tasks. Specifically, it can classify inputs into two distinct classes. The network of Izhikevich neurons is trained using the FORCE method to distinguish between positive and negative pulses, which correspond to different classes. The classification boundaries can be either linear or nonlinear, demonstrating the model's versatility in handling various types of classification problems. The network's performance is evaluated based on its ability to correctly classify inputs, with accuracy rates reported for both linear and nonlinear classification tasks. Additionally, the model's recurrent nature allows it to classify the same input differently based on initial conditions, particularly near the classification boundary. This behavior is analyzed using peri-stimulus time histograms and voltage variance measurements, providing insights into the network's decision-making process.",
  "model/duration": "The execution time for the model varied depending on the specific parameters and the type of input signal used. For sinusoidal oscillators, the model underwent 5 seconds of FORCE training followed by 5 seconds of testing. In the case of more complex signals, such as the Ode to Joy oscillator, the training period was extended to 85 seconds, with an additional 35 seconds allocated for testing. These durations were chosen to ensure that the model could effectively learn and reproduce the target signals. The specific execution times were determined through a systematic exploration of the parameter space, which involved running the networks over a 16 × 17 point mesh in the (G, Q) parameter space. This process allowed for the identification of optimal parameters that minimized the L2 error, thereby enhancing the model's accuracy and robustness. The synaptic decay time constants and rise times were also varied to further optimize the model's performance.",
  "model/availability": "The source code for training the examples presented in this manuscript has been released. It is available in an online repository, specifically on modelDB. The code can be accessed under the accession number 190565. Currently, the code is accessible to referees and editors with the password rv15556. However, it will be made public after the publication of the manuscript. This release is intended to contribute to the community by providing the necessary tools to replicate and build upon the work presented. The code is licensed under terms that allow for use, sharing, adaptation, distribution, and reproduction, provided that appropriate credit is given to the original authors and the source. The specific license details can be found at the provided link.",
  "evaluation/method": "The evaluation of our method involved several key steps and considerations. Initially, we implemented changes to address specific concerns raised by reviewers. These changes included removing references to biological systems and explicitly stating that examples, such as the songbird circuit, should be viewed as potential future applications of FORCE training. We also removed biological labels for specific frequency ranges and instead referred to numerical values. Additionally, we rewrote large parts of the results and discussion to reduce unnecessary references to experimental results for either the songbird circuit or hippocampal functioning.\n\nWe reinforced the results on the High Dimensional Temporal Signals (HDTS) and elaborated on how it functions by adding a section to the materials and methods. To further demonstrate the capabilities of our method, we trained a network to simultaneously generate its own internal HDTS along with producing a song. This was shown in Supplementary Figure S18, where the supervisor consisted of 69 components, with the first 5 containing the supervisor and the last 64 containing the HDTS. There were no inputs into the network, and it had to generate its own internal HDTS simultaneously with the production of the song. This was also demonstrated in Figure 1(b) for a shorter example of \"Ode to Joy.\"\n\nMoreover, we trained a network to simultaneously generate its own internal HDTS while replaying a movie. The supervisor in this case consisted of 1984 dimensions (1920 pixels for the movie and 64-dimensional HDTS). This figure has been added to the supplementary materials, showing that all neurons simultaneously encode the HDTS and the movie scene. This approach is different from previous examples, as it involves a single network performing both tasks.\n\nWe also addressed the suggestion to release the source code for training these examples in an online repository. Initially, we included a note to the editor with the modelDB accession number and the referee access password. However, it appears that this was not communicated in the review process. We have since uploaded the code to modelDB under accession number 190565. The code is currently accessible to referees and editors with the password rv15556 and will be made public after the publication of the manuscript. We have added a line in the introductory paragraph of the results section containing the modelDB accession number and a modelDB citation.\n\nIn response to specific comments, we removed designations of autonomous/non-autonomous and instead rewrote the relevant section to describe the process as occurring \"without external inputs.\" We also provided two copies of the revised manuscript: one with tracked changes and one without, to address all concerns raised by the reviewers.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our methods. Primarily, we measured the accuracy of our models in generating desired outputs, which is a standard metric in the literature for evaluating the performance of neural networks. This involved comparing the model's output to the target signals, such as the songbird spectrogram, to ensure that the trained network could replicate complex temporal patterns.\n\nAdditionally, we analyzed the temporal dynamics of the network using Singular Value Decomposition (SVD). This method allowed us to gain insights into the high-dimensional temporal signals (HDTS) and understand how they contribute to the network's performance. While we acknowledge the importance of this result, space constraints in the final manuscript prevented us from elaborating on it extensively in the main text.\n\nWe also conducted perturbation studies to assess the robustness of our models. These studies involved introducing perturbations to the network and observing how the dynamics changed. This approach is crucial for understanding the stability and reliability of the network under varying conditions.\n\nFurthermore, we evaluated the biological relevance of our results by comparing the model's dynamics to known physiological processes. Although the biological relevance remains limited, we made significant efforts to link our findings to established physiology, providing a foundation for future studies.\n\nOverall, our set of performance metrics is representative of the literature, focusing on accuracy, temporal dynamics, robustness, and biological relevance. These metrics collectively provide a comprehensive evaluation of our models' capabilities and their potential applications in both computational and biological contexts.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with publicly available approaches and simpler baselines to ensure the robustness and novelty of our findings. We specifically compared our FORCE-trained spiking neural networks with other established methods, such as the Neural Engineering Framework (NEF) and Deneve’s work. These comparisons were framed as optimization problems, focusing on minimizing the L2 reconstruction error for a linear decoder. Each method has its own advantages and disadvantages, but our approach allows for greater flexibility in modeling neuron and synapse dynamics, potentially resulting in more biologically detailed networks.\n\nWe also tested our methods against simpler baselines, including different neuron models like the theta neuron and the Leaky Integrate-and-Fire (LIF) neuron. These tests involved training the networks on both simple sine wave oscillation tasks and more complex tasks like reproducing a short segment of \"Ode to Joy.\" The results showed that while all models could achieve the tasks, the Izhikevich model required the least parameter tuning and performed the best. This is likely due to the mixture of long and short time constants in the Izhikevich model, which provides a richer dynamical range.\n\nAdditionally, we addressed the question of whether the spiking nature of our representation makes a difference. While the addition of slow time constants in rate models could mimic some aspects of spiking networks, our approach leverages the unique properties of spiking neurons, such as spike frequency adaptation, to expand the state space and achieve more complex dynamics. This comparison highlights the importance of the spiking nature in our models and underscores the novelty of our approach.\n\nWe also provided detailed histograms of successful training attempts for different models, given random initializations. This data demonstrates the reliability and effectiveness of our FORCE learning method when applied to spiking networks, addressing concerns about the success rate of previous attempts in the community. The histograms show that our method \"just works\" for a variety of tasks, reinforcing the robustness of our approach.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "The source code for training the examples presented in this manuscript has been made available to facilitate further research and reproducibility. The code is hosted on the ModelDB repository, which is accessible to the scientific community. The specific accession number for the code is 190565. Currently, the code is accessible to referees and editors with the password rv15556, but it will be made public upon the publication of the manuscript. This decision aligns with the recommendation from the reviewers to release the source code, ensuring that other researchers can build upon our work and contribute to the field.\n\nIn addition to the code, supplementary materials such as audio clips and movies are provided to illustrate the performance of the spiking neural networks. These materials include real-time demonstrations of the network's ability to reproduce complex signals, such as the short Ode to Joy example. The supplementary materials are included to provide a comprehensive understanding of the network's capabilities and to support the findings presented in the manuscript."
}