{
  "publication/title": "PredRBR: A Gradient Tree Boosting-Based Method for RNA-Binding Residue Prediction",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BMC Bioinformatics",
  "publication/year": "2017",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- RNA-binding residue\n- Gradient tree boosting\n- Structural neighborhood features\n- Protein-RNA interaction\n- Machine learning\n- Bioinformatics\n- Computational biology\n- Feature selection\n- Prediction model\n- Molecular mechanism",
  "dataset/provenance": "The dataset used in this work is named RBP170, previously known as RBP199. The proteins in this dataset were obtained from protein-RNA complexes available in the Protein Data Bank (PDB) as of May 2010. To ensure the quality and non-redundancy of the dataset, several filters were applied. Proteins with less than 30% sequence identity or structures with a resolution worse than 3.5Å were removed using PISCES. Additionally, proteins with fewer than 40 residues, fewer than 3 RNA-binding residues, or binding RNA with fewer than 5 nucleotides were excluded. After removing 9 obsolete complexes from the PDB, a total of 170 protein sequences were generated.\n\nAnother independent dataset, BPP101, was collected from the PDB with deposition dates ranging from June 2010 to May 2014. Similar to RBP170, this dataset includes non-redundant and high-quality RNA-binding proteins, with sequence identity less than 30% and resolution better than 3.5 Å. Proteins with sequence similarity greater than 40% to all proteins in RBP170 were removed using CD-HIT, resulting in 101 protein sequences obtained from 90 RNA-binding complexes.\n\nA residue is defined as an RNA-binding site if at least one atom in the protein is within a distance cutoff of 5.0Å from an atom of the binding RNA. The RBP170 dataset includes a significant number of non-binding sites, which are about six times more than the RNA-binding sites. This imbalance is addressed using a random under-sampling strategy to create balanced datasets for training.",
  "dataset/splits": "Two data splits were used in this study. The first dataset, RBP170, was used for training and consists of 170 protein sequences. These sequences were derived from protein-RNA complexes in the Protein Data Bank (PDB) as of May 2010, with proteins filtered to have less than 30% sequence identity and resolution better than 3.5Å. Additionally, proteins with fewer than 40 residues or fewer than 3 RNA-binding residues were excluded.\n\nThe second dataset, BPP101, was used for independent testing and contains 101 protein sequences. This dataset was collected from PDB with deposition dates ranging from June 2010 to May 2014. Similar to RBP170, the proteins in BPP101 were selected to be non-redundant and of high quality, with sequence identity less than 30% and resolution better than 3.5 Å. Furthermore, proteins with sequence similarity greater than 40% to any protein in RBP170 were removed.\n\nIn both datasets, a residue is defined as an RNA-binding site if at least one atom in the protein is within a distance cutoff of 5.0Å from an atom of the binding RNA. The RBP170 dataset includes a higher number of non-binding sites compared to RNA-binding sites, which presents an imbalance problem addressed in the study.",
  "dataset/redundancy": "In our study, we utilized two datasets to train and validate our RNA-binding residue prediction method, PredRBR. The primary dataset, RBP170, was used for training. This dataset was derived from protein-RNA complexes available in the Protein Data Bank (PDB) as of May 2010. To ensure the quality and non-redundancy of the dataset, we applied several filters. Proteins with less than 30% sequence identity or structures with a resolution worse than 3.5Å were removed using PISCES. Additionally, proteins with fewer than 40 residues, fewer than 3 RNA-binding residues, or binding RNA with fewer than 5 nucleotides were excluded. This process resulted in 170 protein sequences.\n\nFor independent testing, we collected another dataset, BPP101, from PDB with deposition dates ranging from June 2010 to May 2014. Similar to RBP170, we ensured that the proteins in BPP101 were non-redundant and of high quality, with sequence identity less than 30% and resolution better than 3.5 Å. Furthermore, we used CD-HIT to remove proteins with sequence similarity greater than 40% to all proteins in RBP170. This resulted in 101 protein sequences from 90 RNA-binding complexes.\n\nThe training and test sets are independent, as the BPP101 dataset was collected from a different time period and underwent additional filtering to ensure no overlap with the RBP170 dataset. This independence is crucial for evaluating the generalizability of our model. The distribution of binding and non-binding sites in these datasets is comparable to previously published machine learning datasets in the field, with a notable imbalance where non-binding sites outnumber binding sites. This imbalance was addressed using a random under-sampling strategy to create balanced datasets for training.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is Gradient Tree Boosting (GTB). This is an effective ensemble method for both regression and classification tasks. It is not a new algorithm; it has been previously established and applied in various domains.\n\nThe GTB algorithm is implemented using scikit-learn, a widely-used machine learning library in Python. The choice of GTB for this work is driven by its proven effectiveness in handling complex prediction tasks, particularly in the context of RNA-binding site prediction. The algorithm iteratively builds multiple classification trees, each correcting the errors of the previous ones, thereby improving the overall predictive performance.\n\nThe decision to use GTB in this specific context, rather than publishing it in a machine-learning journal, is likely due to the focus of the current work on biological applications. The primary goal is to enhance the prediction of RNA-binding residues, leveraging the strengths of GTB within this specialized domain. The algorithm's implementation and application are tailored to address the unique challenges and requirements of RNA-binding site prediction, making it a suitable choice for this biological research.",
  "optimization/meta": "The PredRBR model is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it employs a Gradient Tree Boosting (GTB) algorithm to predict RNA-binding residues. The model utilizes a comprehensive set of features, including sequence and structural site features, as well as neighborhood attributes, which are selected using the mRMR-IFS method. This approach ensures that the most informative features are used to enhance prediction performance.\n\nThe training data for PredRBR is independent and consists of a dataset called RBP170, which is used for 10-fold cross-validation. This dataset is distinct from the independent test dataset RBP101, which is used to validate the model's usability. The independence of the training data is crucial for evaluating the model's generalizability and performance.",
  "optimization/encoding": "In our study, we utilized a comprehensive set of features to encode the data for the machine-learning algorithm. Initially, a wide range of sequence and structural site features were computed, totaling 189 characteristics. These included site features, Euclidean neighborhood features, and Voronoi neighborhood features. To handle the imbalance problem, where non-binding sites outnumbered RNA-binding sites by approximately six times, we employed a random under-sampling strategy. This involved randomly selecting negative samples (non-binding sites) to create a balanced 1:1 dataset with positive samples (RNA-binding sites) for training.\n\nTo ensure optimal feature selection, we applied the Maximum Relevance Minimum Redundancy (mRMR) method. This approach prioritizes features that have the highest correlation with the target attribute while minimizing redundancy with already chosen characteristics. The mRMR method was measured using mutual information, which quantifies the dependency between variables. Following mRMR, we used Incremental Feature Selection (IFS) to determine the optimal feature set. This involved generating multiple feature subsets based on the mRMR results and evaluating their performance using the Gradient Tree Boosting (GTB) algorithm with 10-fold cross-validation. The feature subset that achieved the highest overall performance, measured by the combination of AUC and MCC, was selected as the optimal set.\n\nThe GTB algorithm was then applied to predict RNA-binding residues. For the input feature vectors, labels were assigned where \"-1\" denoted non-binding residues and \"+1\" represented RNA-binding sites. The algorithm iteratively built multiple classification trees, adjusting the model to minimize the loss function and improve predictive accuracy. This process resulted in a robust GTB model that effectively identified RNA-binding residues.",
  "optimization/parameters": "In our study, we utilized a comprehensive set of features to predict RNA-binding residues. Initially, we computed a total of 189 sequence and structure-based features, which included site features, Euclidean neighborhood features, and Voronoi neighborhood features. To optimize the model, we employed the mRMR-IFS approach to select the most relevant and non-redundant features. This process resulted in an optimal set of 177 features that significantly contributed to the classification performance. The selection of these features was crucial in enhancing the model's accuracy and efficiency. The Gradient Tree Boosting algorithm was then applied using these selected features to build the final prediction model. This approach ensured that the model was trained on the most informative parameters, leading to improved prediction performance.",
  "optimization/features": "In the optimization process of our model, we initially calculated a total of 189 features. These features encompassed both sequence-based and structure-based characteristics, including Euclidean and Voronoi neighborhood features. To enhance the prediction performance, we employed a feature selection method known as mRMR-IFS. This method involves two main steps: first, using the mRMR (maximum Relevance Minimum Redundancy) approach to rank the features based on their relevance to the target attribute and their redundancy with already selected features. Second, applying the IFS (Incremental Feature Selection) to determine the optimal subset of features. The feature selection was performed using the training set only, ensuring that the evaluation was unbiased. Through this process, we identified the top 177 features that achieved the highest performance metrics, including AUC and MCC. These selected features were then used to build the final RNA-binding site prediction model.",
  "optimization/fitting": "The fitting method employed in this work is the Gradient Tree Boosting (GTB) algorithm, which is an effective ensemble method for both regression and classification tasks. The GTB algorithm iteratively builds multiple classification trees, which helps in capturing complex patterns in the data.\n\nThe number of features initially considered is quite large, with a total of 189 site, Euclidean, and Voronoi characteristics. However, to mitigate the risk of overfitting due to the high dimensionality of the feature space, a feature selection method called Maximum Relevance Minimum Redundancy and Incremental Feature Selection (mRMR-IFS) is used. This method selects a smaller subset of optimal features that contribute the most to the classification task, reducing the dimensionality of the feature space and thus helping to prevent overfitting.\n\nAdditionally, to address the imbalance problem in the dataset, where the number of non-binding sites is significantly higher than that of RNA-binding sites, a random under-sampling strategy is employed. This strategy generates balanced datasets by randomly selecting negative samples (non-binding sites) and combining them with the positive samples (RNA-binding sites) to create a 1:1 balanced dataset. This helps in ensuring that the model does not become biased towards the majority class and improves the overall performance of the prediction model.\n\nTo evaluate the performance of the model and ensure that it is neither overfitting nor underfitting, several widely used evaluation metrics are adopted, including sensitivity (recall), specificity, precision, accuracy, F-measure, and Matthews Correlation Coefficient (MCC). The model is also validated using 10-fold cross-validation, which helps in assessing the generalization performance of the model on unseen data.\n\nThe results demonstrate that the proposed GTB-based PredRBR model achieves high performance metrics, indicating that the model is neither overfitting nor underfitting. The model's performance is further validated on an independent test dataset, confirming its effectiveness and robustness.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was the Gradient Tree Boosting (GTB) algorithm, which inherently includes regularization mechanisms. This algorithm builds multiple decision trees in a sequential manner, where each new tree corrects the errors of the previous ones. By doing so, it reduces the variance and helps in preventing overfitting.\n\nAdditionally, we utilized a feature selection approach called Maximum Relevance Minimum Redundancy and Incremental Feature Selection (mRMR-IFS). This method helps in selecting a subset of optimal features that contribute the most to the classification task, thereby reducing the dimensionality of the data and mitigating the risk of overfitting.\n\nFurthermore, we implemented a balanced under-sampling technique to handle the class imbalance in our dataset. This technique ensures that the model does not become biased towards the majority class, which is crucial for maintaining the model's generalizability and performance on unseen data.\n\nThese regularization techniques collectively contributed to the development of a robust and reliable RNA-binding site prediction model.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The PredRBR model, which employs the Gradient Tree Boosting (GTB) algorithm, is not inherently a black-box model. GTB is an ensemble learning method that builds multiple decision trees sequentially, each correcting the errors of the previous ones. This process allows for a certain level of interpretability.\n\nOne of the key aspects of GTB's interpretability is the ability to trace the contributions of individual features to the final prediction. Each decision tree in the ensemble can be examined to understand how different features influence the outcome. For instance, by analyzing the splits in the trees, one can identify which features are most important in distinguishing between RNA-binding and non-binding residues.\n\nMoreover, the use of the Maximum Relevance Minimum Redundancy (mRMR) and Incremental Feature Selection (IFS) approach adds another layer of interpretability. mRMR helps in selecting features that have the highest correlation with the target variable (RNA-binding residues) while minimizing redundancy among the selected features. This ensures that the most relevant features are used in the model, making it easier to understand which characteristics of the residues are crucial for prediction.\n\nFor example, if a particular feature related to the interaction propensity of a residue triplet with a specific nucleotide is frequently used in the decision trees, it indicates that this feature is important for predicting RNA-binding sites. Similarly, structural neighborhood features that capture the local environment of a residue can provide insights into how the spatial arrangement of amino acids influences RNA binding.\n\nIn summary, while the PredRBR model is complex due to its ensemble nature, the use of decision trees and feature selection methods like mRMR-IFS provides a pathway to interpret the model's decisions. This interpretability is crucial for understanding the biological significance of the features and for gaining insights into the mechanisms of protein-RNA interactions.",
  "model/output": "The model is a classification model. It is designed to predict RNA-binding residues, which involves categorizing each residue as either a binding site or a non-binding site. The Gradient Tree Boosting (GTB) algorithm used in the PredRBR framework is an effective ensemble method for classification issues. The model takes input feature vectors and labels, where the labels indicate whether a residue is a binding site (\"+1\") or a non-binding site (\"-1\"). The output of the model is a decision function that classifies the residues accordingly. The performance of the model is evaluated using metrics such as accuracy, sensitivity, specificity, precision, F-measure, Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic (ROC) curve (AUC), which are all relevant to classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the proposed PredRBR model involved several rigorous steps to ensure its effectiveness and robustness. Initially, a cross-validation approach was employed using the RBP170 dataset. This dataset was subjected to a 10-fold cross-validation process, where the data was divided into 10 subsets, and the model was trained and tested 10 times, each time using a different subset as the test set and the remaining nine as the training set. This method helps in assessing the model's performance and generalizability.\n\nAdditionally, the model was validated on an independent test dataset, specifically the RBP101 dataset, which includes 101 non-homologous proteins. This dataset was used to evaluate the model's performance in a real-world scenario, ensuring that it can generalize well to unseen data.\n\nTo address the imbalance between positive and negative samples in the dataset, a random under-sampling strategy was applied. This involved randomly selecting negative samples (non-binding sites) to create a balanced dataset with an equal number of positive and negative samples. This approach helps in mitigating the bias towards the majority class and ensures that the model performs well on both classes.\n\nThe performance of the PredRBR model was measured using several widely used metrics, including sensitivity (recall), specificity, precision, accuracy, F-measure, and Matthews Correlation Coefficient (MCC). These metrics provide a comprehensive evaluation of the model's performance, covering aspects such as the true positive rate, true negative rate, and the balance between precision and recall.\n\nThe model's performance was also compared with several existing state-of-the-art machine learning methods, including Support Vector Machine (SVM), Random Forest (RF), and Adaboost. The comparison was based on the same feature set and training dataset, ensuring a fair evaluation. The results showed that PredRBR outperformed these methods in terms of sensitivity, MCC, F-measure, and AUC score, demonstrating its superior performance in predicting RNA-binding residues.",
  "evaluation/measure": "To evaluate the performance of our proposed model, we utilized several widely adopted metrics. These include sensitivity (also known as recall), specificity, precision, accuracy, F-measure, and the Matthews Correlation Coefficient (MCC). Sensitivity measures the proportion of true positive predictions among all actual positives, while specificity assesses the proportion of true negative predictions among all actual negatives. Precision indicates the proportion of true positive predictions among all positive predictions made. Accuracy provides the overall correctness of the model by considering both true positives and true negatives. The F-measure is the harmonic mean of precision and recall, offering a single metric that balances both concerns. The MCC is a balanced measure that considers all four outcomes of the confusion matrix, providing a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates random prediction, and -1 indicates total disagreement between prediction and observation.\n\nThese metrics are commonly used in the literature for evaluating predictive models, particularly in the context of imbalanced datasets, which is relevant to our work. The use of these metrics ensures that our evaluation is comprehensive and representative of the model's performance across various aspects. Additionally, we employed the area under the receiver operating characteristic curve (AUC-ROC) to evaluate the overall performance, especially in the context of imbalanced datasets. The AUC-ROC provides a single scalar value that summarizes the model's ability to discriminate between positive and negative classes across all possible classification thresholds. This metric is particularly useful for comparing the performance of different models and understanding their effectiveness in handling imbalanced data.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed PredRBR model against several state-of-the-art machine learning methods and existing RNA-binding residue prediction approaches. We conducted a comprehensive comparison using benchmark datasets to ensure a fair and rigorous assessment.\n\nWe compared PredRBR with established machine learning algorithms, including Support Vector Machine (SVM), Random Forest (RF), and Adaboost. These methods were evaluated using the same feature set on the training dataset (RBP170) with 10-fold cross-validation. The results showed that PredRBR outperformed these methods in terms of sensitivity, specificity, F-measure, and AUC score. For instance, PredRBR achieved a sensitivity of 0.85, a specificity of 0.84, a F-measure of 0.60, and an AUC score of 0.92, which were superior to the best-performing method, Random Forest, which had a sensitivity of 0.81, a specificity of 0.83, a F-measure of 0.57, and an AUC score in the range of 0.87-0.90.\n\nAdditionally, we compared PredRBR with other existing RNA-binding residue prediction approaches, such as BindN, PPRint, Liu-2010, BindN+, RNABindR2.0, RNABindRPlus, and SNBRFinder, on an independent test dataset (RBP101). PredRBR demonstrated the best predictive performance with an accuracy of 0.83, a sensitivity of 0.59, a specificity of 0.85, a precision of 0.28, a F-measure of 0.38, and a MCC of 0.32. These results indicate that PredRBR effectively identifies real RNA-binding residues and non-RNA-binding residues, reducing experimental costs.\n\nThe ROC curves and AUC scores further validated the superior performance of PredRBR. On the RBP101 dataset, PredRBR achieved an AUC score of 0.82, which was higher than that of other methods, ranging from 0.64 to 0.80. This improvement highlights the effectiveness of our proposed GTB-based PredRBR model, which integrates the Gradient Tree Boosting algorithm and optimal selected features, particularly structural neighborhood properties.\n\nIn summary, the comparison with publicly available methods and simpler baselines on benchmark datasets confirmed the robustness and superiority of the PredRBR model in predicting RNA-binding residues.",
  "evaluation/confidence": "The evaluation of the PredRBR model includes confidence intervals for the performance metrics, providing a measure of the variability and reliability of the results. These intervals are presented for accuracy, sensitivity, specificity, precision, F-measure, MCC, and AUC across different datasets and comparisons.\n\nFor instance, when comparing PredRBR with other machine learning methods on the RBP170 dataset, the performance metrics such as accuracy, sensitivity, specificity, precision, F-measure, MCC, and AUC are reported with their respective confidence intervals. This allows for a clear understanding of the model's performance and its statistical significance.\n\nThe results indicate that PredRBR achieves superior performance metrics compared to other methods, with statistically significant improvements in sensitivity, MCC, F-measure, and AUC. For example, PredRBR shows at least a 2% increase in sensitivity, a 7% increase in MCC value, and a 5% increase in F-measure compared to Random Forest. Additionally, PredRBR achieves a higher AUC score of 0.92, whereas the other methods have AUC scores ranging from 0.87 to 0.90.\n\nThese statistically significant improvements suggest that the PredRBR model is indeed superior to the compared machine learning approaches. The use of confidence intervals and the consistent outperforming of PredRBR across multiple metrics and datasets provide strong evidence of its effectiveness and reliability.",
  "evaluation/availability": "Not enough information is available."
}