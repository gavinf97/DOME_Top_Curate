{
  "publication/title": "Detecting pairwise interactive effects of continuous random variables for biomarker identification with small sample size",
  "publication/authors": "The authors who contributed to this article are Amin Ahmadi Adl, Hye-Seung Lee, and Xiaoning Qian.\n\nAmin Ahmadi Adl is the corresponding author and has contributed significantly to the research and writing of the paper. He is affiliated with the Department of Computer Science and Engineering at the University of South Florida.\n\nHye-Seung Lee has also made substantial contributions to the work presented in this paper. She is associated with the Health Informatics Institute, Department of Pediatrics, at the University of South Florida.\n\nXiaoning Qian has contributed to the research and is affiliated with both the Department of Computer Science and Engineering at the University of South Florida and the Department of Electrical and Computer Engineering at Texas A&M University.",
  "publication/journal": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
  "publication/year": "2017",
  "publication/doi": "10.1109/TCBB.2016.2586042",
  "publication/tags": "- Synergy\n- Interactions\n- Biomarker Identification\n- Continuous Random Variables\n- Pairwise Interactive Effects\n- Small Sample Size\n- Information Theoretic Methods\n- Classification Based Methods\n- Association Based Methods\n- Gene Expression Analysis",
  "dataset/provenance": "The datasets used in our study are publicly available breast cancer gene expression datasets. Specifically, we utilized datasets from the \"USA\" and the \"Netherlands.\" These datasets have been previously used in the community for various studies related to breast cancer research. The \"USA\" dataset was used to identify biomarkers, and the \"Netherlands\" dataset served as an independent validation set to assess the performance of these biomarkers. The exact number of data points in each dataset is not specified here, but they are sufficient to perform network-based ranking and individual-based ranking for predicting breast cancer metastasis. The datasets contain gene expression profiles, which are crucial for identifying interactive effects among genes and for improving the accuracy of breast cancer metastasis prognosis.",
  "dataset/splits": "In our study, we simulated datasets to evaluate the performance of various interaction detection methods. We created 1,000 datasets for each of the following sample sizes: 20, 40, 60, 80, 100, 120, and 140. This resulted in a total of seven different data splits, each containing 1,000 datasets.\n\nFor each sample size, the datasets were generated using a case-control model with specific distributions for variables and their interactions. The variables were assigned values based on Gaussian distributions and mixtures of Gaussians (MoG), with means and standard deviations predefined. Noise was introduced by flipping the simulated outcome with a 0.1 probability.\n\nThe performance of the interaction detection methods was evaluated using the area under the Receiver Operating Characteristic (ROC) curve (AUC) values. The variable pairs were sorted based on their estimated interactive effects, and true positive and false positive rates were calculated by comparing the top-ranked pairs with the actual interacting pairs. The ROC curves were then approximated based on these rates, and AUC values were calculated to assess the methods' performance.\n\nTo further verify the consistency of the results, additional simulations were conducted with 200 variables, and the performance was compared with the previous results obtained from datasets with 50 variables. The simulation results for both scenarios were visualized using bar plots with standard deviations for small (20 samples) and large (140 samples) sample sizes.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm discussed in this subsection is not a standalone machine-learning algorithm but rather a method for estimating interactive effects between variables using existing machine-learning techniques. The primary classes of machine-learning algorithms used here are classification algorithms, specifically Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and K-Nearest Neighbors (KNN).\n\nThese methods are not entirely new; they build upon established classification techniques. However, the way they are applied to estimate interactive effects between variables is novel. The focus is on using supervised learning to improve the quantification of interactive effects by incorporating outcome information into the quantization process.\n\nThe reason these methods were not published in a machine-learning journal is that the primary application and innovation lie in the field of bioinformatics and computational biology. The techniques are tailored to address specific challenges in analyzing gene interactions and other biomedical data, rather than being general-purpose machine-learning algorithms. The publication aims to contribute to the understanding of interactive effects in biological systems, leveraging existing machine-learning tools in a novel context.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We employed various quantization schemes to transform continuous variables into discrete ones, which is essential for estimating interactive effects.\n\nOne simple method involved dichotomizing the original measurements using sample mean estimates as thresholds, converting them into Bernoulli random variables. Alternatively, we used quantiles for quantization. For pairs of variables, we simultaneously quantized them by categorizing observed values into groups based on a separating line, such as \\( x_i - x_j = 0 \\). This approach is fundamental to Relative Expression Analysis (REA), where data points in the \\((x_i, x_j)\\) plane are divided into those with \\( x_i \\geq x_j \\) and those with \\( x_i < x_j \\).\n\nTo address the challenge of unknown appropriate thresholds, we explored more sophisticated clustering methods. For instance, we utilized the UPGMA hierarchical clustering method to quantize continuous variables automatically. By taking clustering results at different hierarchical levels, we obtained robust interactive effect estimates. Other clustering methods were also considered for more complex quantization of continuous variables.\n\nIn addition to quantization, we estimated conditional entropies by assuming that input variables follow simple distributions, such as Gaussian distributions. This allowed us to compute entropies directly using the maximum likelihood estimates of distribution parameters. We tested the performance of these methods in our simulation experiments.\n\nFor classification-based methods, we used logistic regression to relate input variables to a binary outcome. The interactive effect of two variables on the outcome was computed using the p-value of the interaction term in the logistic regression model. We also evaluated the Relative Expression Analysis (REA) method, which measures the interactive effect based on the relative expression of variables.\n\nFurthermore, we proposed new supervised quantization approaches that consider the outcome information \\( y \\) to improve the estimation of interactive effects. These methods involved learning classifiers to predict \\( y \\) from \\( x \\) and using the separating boundaries detected by the classifiers for quantization. Different classifiers, such as Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and K-Nearest Neighbors (KNN), were used to explore various separating boundaries.\n\nOverall, our data encoding and preprocessing strategies aimed to capture the underlying structure of the data and enhance the accuracy of interactive effect estimates.",
  "optimization/parameters": "In our study, we utilized two different sets of input variables for our simulations. Initially, we worked with 50 input variables to evaluate the performance of various methods under different sample sizes. This setup allowed us to thoroughly assess how each method handled a moderate number of variables.\n\nAdditionally, to further validate our findings and ensure the robustness of our proposed methods, we conducted another set of simulations with 200 input variables. This increase in the number of variables helped us understand how the methods perform with a larger and more complex dataset.\n\nThe selection of these parameter values was driven by the need to cover a range of scenarios that are commonly encountered in biomedical applications. By testing with both 50 and 200 variables, we aimed to provide a comprehensive evaluation that could be applicable to various real-world situations.",
  "optimization/features": "In the optimization process, we utilized gene expression profiles from two micro-array datasets focused on breast cancer metastasis. The first dataset, referred to as the \"USA\" dataset, includes 286 samples, with 107 of these samples eventually developing metastasis. The second dataset, known as the \"Netherlands\" dataset, comprises 295 samples, of which 79 developed metastasis. Both datasets measure the expression of 6,168 genes. These genes were used as input variables for our analysis. For genes measured by more than one probe within a dataset, the average of the corresponding expression values was used.\n\nFeature selection was indeed performed using a network-based ranking method. This method takes into account both individual and interactive effects among variables to identify accurate biomarkers. The network-based ranking was applied to the \"USA\" dataset to identify biomarkers for predicting breast cancer metastasis. The feature selection process was conducted using the training set only, ensuring that the evaluation remained unbiased. This approach helps in reducing model complexity and improving the identification of biomarkers with significant predictive power.",
  "optimization/fitting": "In our study, we addressed the challenge of having a potentially large number of parameters relative to the number of training points, particularly when dealing with interactive effects in datasets with a limited number of samples. To mitigate overfitting, we employed several strategies.\n\nFor our KNN-based methods, we implemented a technique similar to SynDichoUPGMA, where we estimated interactive effects by averaging over multiple KNN classifiers with different values of K. This approach helps to avoid overfitting by reducing the variance associated with any single KNN model. Additionally, we ensured that our models were robust by evaluating their performance across a wide range of sample sizes, from as few as 20 samples to as many as 140 samples. This comprehensive evaluation allowed us to observe how our methods performed under different conditions and to identify scenarios where overfitting might occur.\n\nTo further validate our methods, we compared them with LASSO-based approaches, which are known for their ability to handle high-dimensional data by imposing L1 regularization. By including a large number of interaction terms in the LASSO model, we were able to assess the performance of our methods in a statistically rigorous manner. The results showed that our classification accuracy-based measures, such as AccQDA and AccKNN, outperformed other methods, including LASSO, in both small and large sample size scenarios.\n\nIn terms of underfitting, we ensured that our models were sufficiently complex to capture the interactive effects present in the data. For example, our SynPosteriorQDA and SynPosteriorKNN methods, which model posterior probabilities, demonstrated superior performance compared to simpler, linear models. This indicates that our methods were capable of capturing the necessary complexity in the data without being overly simplistic.\n\nOverall, our approach involved a careful balance between model complexity and regularization to avoid both overfitting and underfitting. By evaluating our methods across a range of sample sizes and comparing them with established techniques, we were able to demonstrate their effectiveness and robustness.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting, particularly when dealing with small sample sizes. One of the key methods we used is the LASSO (Least Absolute Shrinkage and Selection Operator) logistic regression, which incorporates L1 penalty. This technique helps in feature selection by shrinking some coefficients to zero, effectively reducing the model complexity and preventing overfitting. We also utilized 10-fold cross-validation to select the optimal L1-norm regularization coefficient, ensuring that our model generalizes well to unseen data. Additionally, we compared our methods with LASSO-based approaches to evaluate their performance, especially in scenarios with limited samples. This comparison highlighted the effectiveness of our proposed measures in handling small sample sizes without compromising on the detection of interactive effects.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models we have developed and discussed in this publication are designed to be transparent and interpretable, rather than black-box systems. This transparency is crucial for understanding the interactive effects between variables and their impact on the outcome.\n\nOne of the key aspects of our models is their reliance on well-established statistical and machine learning techniques, such as Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and k-Nearest Neighbors (KNN). These methods are widely understood and have clear mathematical foundations, making it easier to interpret their results.\n\nFor instance, our classification accuracy-based methods, such as AccLDA, AccQDA, and AccKNN, measure the interactive effect between variables by evaluating the training accuracy of these classifiers. The accuracy is weighed using posterior probabilities, providing a clear and interpretable metric. This approach allows us to understand how the combination of variables influences the outcome, rather than relying on individual variables alone.\n\nAdditionally, our information theoretic measures, like SynPosteriorQDA and SynPosteriorKNN, model the posterior probabilities to improve the detection of interactive effects. This method provides insights into how the variables interact and contribute to the outcome, making the model more interpretable.\n\nFurthermore, our association analysis methods compare the statistical association between variables given the outcome and without it. This comparison is intuitive and provides a clear understanding of how the variables' relationship changes with respect to the outcome.\n\nIn summary, the models presented in this publication are designed to be transparent and interpretable. They use established statistical and machine learning techniques, providing clear metrics and insights into the interactive effects between variables and their impact on the outcome. This transparency is essential for practical applications, especially in biomedical research, where understanding the underlying mechanisms is crucial.",
  "model/output": "The model discussed in this publication primarily focuses on classification tasks. It introduces methods for estimating interactive effects based on classification accuracy. Specifically, it uses various classification techniques such as Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and K-Nearest Neighbors (KNN) to measure the interactive effects between variables. The output of these models is the classification accuracy, which is used to estimate how two continuous variables interact with respect to an outcome. The accuracy measure is defined in a way that it can take values between -0.5 and 0.5, where -0.5 indicates all inverse predictions, 0.5 indicates all correct predictions, and 0.0 indicates random predictions. Consequently, the interactive effect measure can range from -1.5 to 1.5. The methods discussed, such as AccLDA, AccQDA, and AccKNN, utilize the training accuracy of these classification methods to estimate interactive effects. Additionally, the model considers the posterior probability to weigh correct and incorrect predictions, enhancing the reliability of the accuracy estimates. The results indicate that these classification accuracy-based methods outperform other information theoretic-based methods, especially in scenarios with varying sample sizes. For small sample sizes, AccQDA tends to perform better due to its controlled model complexity, while AccKNN is more effective for larger sample sizes. Overall, the model's output is centered around classification accuracy and the interactive effects estimated through these classification methods.",
  "model/duration": "The execution time of the methods discussed in this paper varies depending on the complexity of the modeling approach and the size of the dataset. We have compared the methods based on the running time required to calculate all the interactive effects in a set of simulated datasets. Generally, simpler quadratic modelings tend to be faster and are better suited for smaller sample sizes as they have a lower chance of overfitting the data. On the other hand, complex non-linear modeling methods, while often more accurate, can be computationally intensive and are better suited for larger sample sizes.\n\nFor instance, methods based on linear models, such as SynDichoLDA, SynPosteriorLDA, and AccLDA, are relatively quick but may struggle to capture interactive effects in datasets with more general interacting relationships. Among these, AccLDA is the best performing in terms of both accuracy and speed.\n\nMore computationally demanding methods, like those involving LASSO logistic regression with L1 penalty, can become statistically and computationally prohibitive, especially with a limited number of samples. This is because the number of terms in the model grows quadratically with respect to the number of variables, leading to a significant increase in running time.\n\nTo provide a concrete example, when evaluating the performance of the LASSO model with 50 original variables and 1,225 interaction terms, the running time was notably higher compared to other methods. This highlights the trade-off between model complexity and execution time.\n\nFor more detailed information on the running times of each method, please refer to Section 8 in the supplementary materials.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our methods involved several rigorous steps to ensure their effectiveness and robustness. We primarily used a 10-fold cross-validation approach to assess performance. This involved dividing the dataset into nine training folds and one test fold, repeating this process ten times to ensure comprehensive evaluation. For each training set, we constructed a \"synergy network\" and ranked variables based on their interactive effects. The top T variables from this ranking were then used to train an SVM classifier with an RBF kernel. The classifier's performance was evaluated on the test fold using the AUC metric. This procedure was repeated for different values of T, ranging from 1 to 50, to observe how the inclusion of more variables affected performance.\n\nTo demonstrate the importance of interactive effects, we compared the network-based ranking method with an individual-based ranking method, which considered only the individual effects of variables. The results showed that the network-based approach, which accounts for interactive effects, significantly outperformed the individual-based approach, particularly when considering a range of 15 to 30 top genes. This was further validated through statistical tests, which showed significant p-values for these ranges.\n\nAdditionally, we tested the identified biomarkers on an independent dataset, the \"Netherlands\" dataset, to ensure their generalizability and stability. We trained an SVM classifier using the biomarkers identified from the \"USA\" dataset and performed 100 repeated 10-fold cross-validation tests. The results confirmed that the network-based ranking method not only performed better within the original dataset but also across different datasets, highlighting the importance of considering interactions among biomarkers for accurate and stable predictions.\n\nWe also evaluated the performance of different methods based on their complexity, classifying them into linear, quadratic, and complex non-linear modelings. Our proposed classification accuracy-based measures, AccQDA and AccKNN, consistently outperformed other methods within their respective complexity classes. This suggests that our measures are effective regardless of the modeling complexity. Furthermore, we compared the running times of different methods to ensure practical applicability, especially in scenarios with limited sample sizes.\n\nIn summary, our evaluation methods included cross-validation, comparison with individual-based ranking, testing on independent datasets, and performance assessment based on model complexity. These steps collectively demonstrated the superiority and robustness of our proposed methods in capturing interactive effects and improving biomarker identification.",
  "evaluation/measure": "In our evaluation, we primarily focus on performance metrics that assess the accuracy and effectiveness of our proposed methods in capturing interactive effects between variables. The key performance metric reported is the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. This metric is widely used in the literature for evaluating the performance of classification models, particularly in the context of biomarker identification and phenotype prediction.\n\nWe employ a standard 10-fold cross-validation procedure to evaluate the performance of our methods. This involves constructing a \"synergy network\" and ranking variables based on their interactive effects using nine folds of the data (training set). The top-ranked variables are then used to train an SVM classifier, and its performance is evaluated on the remaining one fold (test set). This process is repeated ten times, and the average AUC is reported as the estimated performance.\n\nIn addition to AUC, we also consider the complexity of the models, classifying them into linear, quadratic, and complex non-linear categories. This classification helps in understanding how different types of models perform across various sample sizes. For instance, complex non-linear models tend to perform better with larger sample sizes, while simpler quadratic models are more effective with smaller sample sizes due to their lower risk of overfitting.\n\nFurthermore, we compare our methods with existing information theoretic and classification-based approaches. This comparison includes evaluating the running time of each method to calculate interactive effects, which is crucial for practical applications, especially in biomedical research where computational efficiency is often a concern.\n\nTo ensure the robustness of our findings, we also evaluate the performance of our methods using a LASSO model with L1-norm regularization. This comparison helps in understanding how our proposed measures fare against more computationally intensive methods that simultaneously model all variables and their interactions.\n\nOverall, the set of metrics reported in our evaluation is representative of the current literature in the field. We focus on metrics that are widely accepted and used, such as AUC, and supplement them with additional evaluations that provide a comprehensive understanding of the performance and practicality of our methods.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed methods against both existing and simpler baseline methods. We focused on benchmark datasets generated through simulations, which allowed us to control various factors such as sample size and the complexity of interactions.\n\nWe compared our methods to publicly available techniques, categorizing them into three main groups: information theoretic methods, classification-based methods, and association-based methods. For information theoretic methods, we evaluated approaches that estimate marginal and conditional probability distributions to compute synergy as an interactive effect. In the classification-based category, we included methods that use models like logistic regression or relative expression analysis to measure interactive effects through model parameters or classification accuracy. Additionally, we considered association-based methods that assess the increase in association between variables after observing the outcome.\n\nOur proposed methods, such as SynPosteriorQDA and SynPosteriorKNN, were designed to improve detection accuracy by modeling posterior probabilities. We demonstrated that these methods outperform traditional information theoretic approaches, especially in scenarios with varying sample sizes. Furthermore, our classification accuracy-based measures, AccQDA and AccKNN, showed superior performance for small and large sample sizes, respectively. AccQDA achieved higher AUC values for limited samples, likely due to its controlled model complexity.\n\nWe also compared our methods to simpler baselines, including linear models like SynDichoLDA and SynPosteriorLDA, which failed to capture interactive effects in datasets with complex relationships. Our proposed AccLDA, however, proved to be the best-performing method among linear models. Additionally, we evaluated the running time of each method to ensure practical applicability, especially for datasets with a large number of variables.\n\nTo address the issue of multiple hypothesis testing and potential false positives, we explored penalized feature selection methods like LASSO logistic regression. We included pairwise interaction terms in the logistic regression model and compared the performance of our methods with LASSO-based approaches using simulated datasets. This comprehensive evaluation underscores the effectiveness and robustness of our proposed methods across different scenarios and sample sizes.",
  "evaluation/confidence": "The evaluation of our methods includes a thorough assessment of their statistical significance. We have employed various techniques to ensure that the performance metrics are reliable and that the results are statistically significant.\n\nTo evaluate the performance of our methods, we used the Area Under the Receiver Operating Characteristic Curve (AUC) as a primary metric. The AUC values were computed for different sample sizes, and we observed that our proposed methods, particularly those based on classification accuracy, consistently outperformed other information theoretic based methods. This performance was evaluated across a range of sample sizes, demonstrating the robustness of our approaches.\n\nWe also conducted simulations with different numbers of input variables to verify the consistency of our results. For instance, we performed additional simulations with 200 input variables and compared the results with those obtained from simulations with 50 input variables. The consistency of the results across different simulation settings further supports the reliability of our methods.\n\nIn addition to AUC values, we performed statistical tests to assess the significance of the improvements observed. For example, we conducted two-sample t-tests to compare the performance of our methods with baselines. The p-values obtained from these tests were generally very low (less than 1e âˆ’ 5), indicating that the improvements achieved by our methods are statistically significant. Specifically, for certain thresholds (T = 1 and T = 3), the p-values were 0.24 and 0.02, respectively, which still demonstrate significant improvement, especially when considering independent datasets.\n\nFurthermore, we ensured that the null hypothesis distributions for our statistics were well-understood to avoid false discoveries. For discrete random variables, we referenced existing studies that derived distributions for mutual information and interactions among quantized variables. For other methods, we used permutation-based approaches to empirically estimate the null hypothesis distribution, which is a computationally demanding but reliable method.\n\nOverall, the evaluation of our methods includes confidence intervals for performance metrics and rigorous statistical tests to ensure that the observed improvements are not due to chance. The consistent performance across different simulation settings and the statistically significant results provide strong evidence that our proposed methods are superior to existing baselines.",
  "evaluation/availability": "Not enough information is available."
}