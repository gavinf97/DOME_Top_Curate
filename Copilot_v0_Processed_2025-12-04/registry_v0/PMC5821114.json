{
  "publication/title": "Neural circuit inference from function to structure",
  "publication/authors": "The authors who contributed to this article are Esteban Real, Hiroki Asari, Tim Gollisch, and Markus Meister.\n\nEsteban Real performed the extracellular array recordings that constitute the main data set. He also designed the models, coded them, ran the simulations, and analyzed the results. Additionally, he contributed to writing the manuscript.\n\nHiroki Asari performed the simultaneous intracellular and extracellular recordings used to test the models. He also analyzed the results and contributed to writing the manuscript.\n\nTim Gollisch designed the models and contributed to writing the manuscript.\n\nMarkus Meister designed the models, contributed to writing the manuscript, and is the lead contact for the article.",
  "publication/journal": "Curr Biol.",
  "publication/year": "2017",
  "publication/doi": "10.1016/j.cub.2016.11.040",
  "publication/tags": "- Neural circuits\n- Retina\n- Ganglion cells\n- Bipolar cells\n- Circuit inference\n- Modeling approach\n- Functional dynamics\n- Structural connectivity\n- Visual responses\n- Neurobiology\n- Physiological recordings\n- Circuit models\n- Receptive fields\n- Neural function\n- Circuit structure\n- Experimental tests\n- Predictive modeling\n- Neuroscience\n- Big data integration\n- Biological realism",
  "dataset/provenance": "The dataset used in our study was sourced from recordings of the isolated salamander retina. We recorded the spike trains of approximately 200 ganglion cells while stimulating the photoreceptor layer with a spatially and temporally rich display consisting of an array of vertical bars that flickered randomly and independently at 60 Hz. This stimulus was designed to drive a wide range of spatio-temporal computations in the retina while limiting the complexity of analysis and modeling by restricting it to one spatial dimension.\n\nThe dataset included repeated presentations of the same flicker sequence, which reliably evoked very similar spike trains, suggesting that essential features of the retinaâ€™s light response could be captured by a deterministic model of the ganglion cell and its input circuitry. Additionally, we presented a long non-repeating flicker sequence to explore as many spatio-temporal patterns as possible. Thirty ganglion cells were selected for quantitative modeling based on the stability of their responses throughout the extended recording period.\n\nThe data used in this study is novel and has not been previously published or used by the community. It was specifically collected for the purpose of testing our modeling approach for inferring the structure of neural circuits from sparse physiological recordings. The dataset provides a comprehensive set of recordings that allowed us to develop and validate our models, making specific predictions for the response properties and connectivity of bipolar cells, which were subsequently confirmed by direct physiological recordings.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The training set consisted of approximately 80% of the data, which was used to optimize the parameters of the models. The test set, comprising the remaining 20% of the data, was used to evaluate the performance of the models. This split allowed for a comprehensive assessment of the models' ability to generalize from the training data to unseen data.\n\nThe training set included a long stretch of non-repeating flicker sequences, which provided a rich variety of spatio-temporal patterns. This diversity was crucial for capturing the essential features of the retina's light response. The test set, on the other hand, consisted of repeated presentations of the same flicker sequence. This repetition ensured that the models could reliably predict the responses of ganglion cells to familiar stimuli, thereby validating their performance.\n\nThe distribution of data points in each split was designed to ensure that the models were trained on a wide range of conditions and could be tested under consistent and controlled circumstances. This approach helped in achieving a robust and reliable evaluation of the models' predictive capabilities.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study falls under the class of high-dimensional parameter search methods. This approach is not novel in the field of machine learning but is particularly well-suited for our specific application in neural circuit modeling. The algorithm's primary function is to optimize the parameters of our cascade models to best fit the experimental data obtained from ganglion cell recordings.\n\nThe choice of this algorithm is driven by the need to handle a large number of parameters efficiently. Our models, particularly the more complex ones like the linear-nonlinear-sum-nonlinear (LNSN) model, involve numerous parameters that describe the spatio-temporal filters, nonlinearities, and pooling weights. The algorithm systematically adjusts these parameters to maximize the explained variance in the ganglion cell firing rates, ensuring that the models accurately predict the visual responses.\n\nThe reason this algorithm was not published in a machine-learning journal is that its development and application are deeply rooted in the biological context of retinal circuitry. The algorithm's effectiveness is demonstrated through its ability to infer biologically plausible structures and dynamics within the retina, rather than through abstract performance metrics. The focus of our work is on the biological insights gained from the modeling process, which are subsequently validated through experimental measurements. This interdisciplinary approach highlights the algorithm's practical utility in neuroscience rather than its theoretical novelty in machine learning.",
  "optimization/meta": "Not applicable. The models described do not use data from other machine-learning algorithms as input. The modeling approach focuses on predicting the firing rate of ganglion cells using cascade models, which are networks of simple elements involving linear filtering or static nonlinear transforms. These models are constructed to take the time course of the flicker stimulus as input and produce a time course of the firing rate at the output. The parameters of the model are optimized to fit the data, with a focus on maximizing the fraction of variance in the firing rate that the model explains. The performance of the models is evaluated on a separate test set, ensuring that the training and testing data are independent. The models include variations such as the LN model, LNSN model, and more complex models like LNFDSNF, but they do not incorporate outputs from other machine-learning algorithms as part of their input.",
  "optimization/encoding": "The data encoding process focused on predicting the firing rate of ganglion cells, specifically the expected number of spikes fired in any given 1/60 second interval. The input to the models was the time course of the flicker stimulus, which was used to produce a time course of the firing rate as output. The data was divided into a training set, consisting of approximately 80% of the data with non-repeating flicker, and a test set, consisting of the remaining 20% with repeated flicker. This division allowed for the optimization of model parameters on the training set and the evaluation of model performance on the test set.\n\nThe models used were cascade models, which are networks of simple elements involving either linear filtering or static nonlinear transforms. These models map naturally onto neural circuitry and can be adjusted from coarse-grained to fine-grained versions. The initial model used was the linear-nonlinear (LN) model, which consists of a single linear-nonlinear cascade. This model was able to approximate the ganglion cell output but had limitations in accurately predicting firing events.\n\nTo improve the model, a sequence of four cascade models was created by systematically adding components to the circuits. Each model derived its name from the cascade of components, with the final model being the linear-nonlinear-feedback-delayed-sum-nonlinear-feedback (LNFDSNF) model. For each model class, the components of the circuit were parameterized, and the fitting algorithm found the optimal parameter values for each ganglion cell. The improvement in model performance was not due to overfitting but rather to the models capturing additional aspects of the computations carried out by the retina. The models were evaluated using separate training and testing data, achieving equivalent values in the explained variance, which implies that each model truly captures additional aspects of the computations carried out by the retina.",
  "optimization/parameters": "In our study, the number of parameters used in the models varied depending on the specific model structure. The simplest model, the LN model, had the most free parameters, totaling 186. This included 185 parameters in the linear filter, with 31 spatial positions, each having a 6-parameter temporal filter, and 1 parameter in the nonlinearity.\n\nFor the more complex models, such as the LNSN, LNSNF, and LNFSNF models, the number of free parameters was reduced to 68. This reduction was achieved by imposing a structure guided by known retinal anatomy, which provided a constraint that regularized the optimization process.\n\nThe LNFDSNF model, the most complex model we tested, had 108 free parameters. This model included delays inserted between each BCM and the GCM, which were allowed to vary independently for each BCM.\n\nThe selection of the number of parameters was guided by the goal of balancing model complexity with biological realism. We aimed to create models that were complex enough to capture the essential features of the retinal circuitry but not so complex that they were computationally intractable or overfitted to the data. The models were systematically refined by adding components to the circuits, and the performance of each model was evaluated to ensure that it significantly outperformed the previous one. This approach allowed us to determine which plausible neural circuit best explained the functional data.",
  "optimization/features": "The input features for our models are derived from the time course of the flicker stimulus. Specifically, the models take the temporal dynamics of the flicker stimulus as input to predict the firing rate of ganglion cells. The stimulus is presented in intervals of 1/60 seconds, which means the input features are essentially the stimulus intensities at these discrete time points.\n\nFeature selection, in the traditional sense, was not performed. Instead, the models are designed to capture the relevant temporal patterns directly from the stimulus data. The models are structured to include various components that process these input features, such as linear filters and nonlinear transformations, which together allow the models to extract meaningful patterns from the stimulus.\n\nThe optimization process involved using a training set, which constituted approximately 80% of the data, to fit the models. This training set was used to maximize the fraction of variance in the firing rate that the models explain. The remaining 20% of the data, referred to as the test set, was used to evaluate the model performance. This separation ensures that the models are not overfitted to the training data and can generalize well to new, unseen data.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance to avoid both over-fitting and under-fitting. The models constructed were cascade models, which are networks of simple elements involving linear filtering and static nonlinear transforms. These models were designed to map the time course of the flicker stimulus to the firing rate of ganglion cells.\n\nTo address the concern of over-fitting, we used separate training and testing datasets. The training set consisted of approximately 80% of the data, which was non-repeating flicker, while the test set consisted of the remaining 20%, which was repeated flicker. The model parameters were optimized to fit the training set, and the performance was then evaluated on the test set. This approach ensured that the model's performance was not merely a result of memorizing the training data but rather reflected its ability to generalize to new data.\n\nAdditionally, the models were systematically constructed by adding components guided by known retinal anatomy. This structured approach provided constraints that regularized the optimization process, helping to avoid the \"curse of dimensionality\" often associated with high-dimensional parameter spaces. For instance, the linear-nonlinear-sum-nonlinear (LNSN) model, which included multiple bipolar cell modules, achieved better performance despite having fewer free parameters than the simpler linear-nonlinear (LN) model. This indicates that the added complexity was justified and did not lead to over-fitting.\n\nTo rule out under-fitting, we tracked the performance metric across successive changes in the model structure. Each new model was more general than the previous one and significantly outperformed it in predicting the visual responses of certain ganglion cells. This progressive improvement in performance across models of increasing complexity suggests that the models were capturing additional aspects of the computations carried out by the retina, rather than being too simplistic to capture the underlying processes.\n\nIn summary, the fitting method involved using separate training and testing datasets, adding components guided by known anatomy, and tracking performance metrics across model structures. These steps ensured that the models were neither over-fitted nor under-fitted, providing a robust and biologically plausible representation of the retinal circuitry.",
  "optimization/regularization": "In our modeling approach, we employed several techniques to prevent over-fitting and ensure that our models generalized well to new data. One key method involved imposing a structure guided by known anatomy of the retina. By using repeating identical subunits from bipolar cells, we provided a constraint that regularized the optimization process. This anatomical guidance helped to circumvent the \"curse of dimensionality\" in model fitting, which is a common issue when dealing with high-dimensional data.\n\nAdditionally, we used separate training and testing datasets. The models were optimized on a long stretch of non-repeating flicker data (the training set, comprising about 80% of the data), and their performance was evaluated on the remaining data examined with repeated flicker (the test set, about 20%). This approach ensured that the models' performance metrics were tracked across successive changes in the model structure, providing a robust evaluation of their predictive power.\n\nFurthermore, we systematically added components to the circuit models, creating a sequence of increasingly complex models. Each model was more general than the previous one and significantly outperformed it in predicting the visual responses of certain ganglion cells. Importantly, the improvement in performance was not simply due to over-fitting after the addition of more free parameters. In fact, the linear-nonlinear (LN) model, which had the most free parameters among the models tested, did not always perform the best. This indicates that each model truly captured additional aspects of the computations carried out by the retina, rather than just fitting noise in the data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models we developed are not black boxes; instead, they are designed to be interpretable and biologically plausible. We employed cascade models, which are networks of simple elements involving either linear filtering or static nonlinear transforms. These models map naturally onto neural circuitry, allowing us to draw direct parallels between the model components and known biological structures.\n\nOne of the key aspects of our models is their alignment with retinal anatomy. For instance, the linear-nonlinear-sum-nonlinear (LNSN) model consists of multiple \"bipolar cell-like\" modules, each representing a miniature linear-nonlinear (LN) model. These modules are identical but placed at different spatial locations in the retina. Their outputs are weighted, pooled together, and rectified by a ganglion cell module (GCM). This structure provides a clear interpretation: the bipolar cell modules (BCMs) correspond to real biological bipolar cells, and the GCM represents the ganglion cell's pooling and rectification processes.\n\nThe spatio-temporal filters of the BCMs match existing measurements of bipolar cell receptive fields, exhibiting a \"Mexican hat\" shape in the spatial domain and specific kinetics in the time domain. This alignment with experimental data indicates that the BCMs are not arbitrary components but reflect actual biological properties. Similarly, the pooling weights of the GCM attain a center-surround structure at a larger scale, comparable to the spatial extent of ganglion cell dendritic fields. This distinction between the spatial pooling properties of the outer retina (BCM component) and the inner retina (GCM) is another example of how our models provide interpretable insights into retinal processing.\n\nMoreover, the models' performance improvements with added complexity are not due to overfitting. Each model captures additional aspects of retinal computations, as evidenced by their better prediction of visual responses and the biological realism of their components. For example, the introduction of feedback in the linear-nonlinear-feedback-sum-nonlinear-feedback (LNFSNF) model significantly improves performance, indicating the importance of feedback mechanisms in retinal processing.\n\nIn summary, our models are transparent and interpretable, with components that correspond to known biological structures and functions. This interpretability allows us to gain insights into the computations carried out by the retina and to make testable predictions about bipolar cell physiology.",
  "model/output": "The model developed in our study is primarily a regression model. It is designed to predict the firing rate of ganglion cells (GCs), which is the expected number of spikes fired in any given time interval, typically 1/60 seconds. The model takes the time course of a flicker stimulus as input and produces a time course of the firing rate as output. The parameters of the model are optimized to fit the data, specifically by maximizing the fraction of variance in the firing rate that the model explains. This approach is characteristic of regression models, where the goal is to predict a continuous output variable based on input data.\n\nThe models we constructed are cascade models, which involve a sequence of linear filtering and nonlinear transformations. These models map naturally onto neural circuitry and can be adjusted to different levels of detail, from coarse-grained representations to fine-grained, multi-compartment models. The cascade structure allows for the incorporation of biological details such as neurons, axons, synapses, and dendrites, making the model both biologically plausible and mathematically tractable.\n\nWe started with a simple linear-nonlinear (LN) model and systematically added components to create more complex models, such as the linear-nonlinear-feedback-delayed-sum-nonlinear-feedback (LNFDSNF) model. Each model was optimized to fit the data, and the performance of each model was evaluated on a separate test set to ensure that the improvements were not due to overfitting. The models were able to capture additional aspects of the computations carried out by the retina, providing a more accurate prediction of the visual responses of ganglion cells.\n\nIn summary, the models we developed are regression models that predict the firing rate of ganglion cells based on the input stimulus. The cascade structure of the models allows for the incorporation of biological details and provides a powerful tool for inferring the inner details of a neural circuit from simulation and fitting of its overall performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method involved a progressive extension of the model complexity, starting from a linear-nonlinear (LN) model and advancing to a linear-nonlinear-feedback-delayed-sum-nonlinear-feedback (LNFDSNF) model. Each model's performance was assessed by its ability to predict ganglion cell (GC) firing rates in response to randomly flickering bar stimuli. The models were trained on approximately 80% of the data and tested on the remaining 20%, ensuring that performance metrics were not merely a result of overfitting.\n\nThe explained variance, which measures the fraction of variance in the firing rate that the model explains, was used as the primary performance metric. This metric was tracked across successive changes in the model structure, demonstrating that each model captured additional aspects of the computations carried out by the retina.\n\nExperimental tests were conducted to validate the biological realism of the models. These tests involved measuring the receptive and projective fields of real bipolar cells (BCs) and comparing them to their predicted counterparts in the models. The experiments combined sharp electrode recordings from BCs and multi-electrode array recordings from GCs, allowing for the identification of projection patterns from BCs to GCs. The data selection for model fitting was done before fitting to avoid biasing the results.\n\nThe LNFDSNF model, in particular, showed a significant improvement in performance, achieving an 8% increase in explained variance. This model's ability to delay the receptive field surround independently of other circuit elements was crucial in accommodating the delayed receptive field surround observed in GCs. The experimental data supported the model's predictions, indicating that the inferred elements in the fitting process have actual biological counterparts.",
  "evaluation/measure": "In our study, we focused on evaluating the performance of various models in predicting the firing rate of ganglion cells. The primary metric we used was the explained variance (E.V.), which quantifies the fraction of variance in the firing rate that each model explains. This metric was chosen because it provides a clear indication of how well each model approximates the actual neural responses.\n\nWe reported the explained variance for individual cells across different models, presenting both the mean and the interquartile range to show the variability in performance. This allowed us to compare the effectiveness of each model in capturing the neural responses. Additionally, we plotted the variance explained by each model as a ratio to the variance explained by the simplest model, the LN model. This ratio helped to highlight the improvements in performance as more complex features were added to the models.\n\nThe models we evaluated included the LN model, which served as a baseline, and more complex models such as LNSN, LNSNF, LNFSNF, and LNFDSNF. Each of these models introduced additional components to the circuit, such as nonlinearities and feedback mechanisms, which significantly improved the explained variance. For instance, the introduction of a nonlinearity at the bipolar cell output and the addition of feedback mechanisms showed substantial jumps in performance.\n\nOur approach to evaluating model performance is representative of standard practices in sensory neuroscience. The use of explained variance as a performance metric is common in the field, as it provides a straightforward way to assess how well a model captures the variability in neural responses. By comparing the performance of our models to the LN model, we were able to demonstrate the incremental improvements achieved by adding more complex features. This method allows for a clear and comparative evaluation of model performance, making our results interpretable and comparable to other studies in the literature.",
  "evaluation/comparison": "A comparison to simpler baselines was indeed performed. The study began with a reference point model known as the Linear-Nonlinear (LN) model, which is a common starting point in sensory neuroscience. This model consists of a single linear-nonlinear cascade and was used to approximate the output of ganglion cells. However, it was found that the LN model often predicted firing at inappropriate times, resulting in wider and flatter firing events than observed.\n\nTo improve upon this, a sequence of four cascade models was created by systematically adding components to the circuits. Each subsequent model derived its name from the cascade of components added. The most complex model developed was the Linear-Nonlinear-Feedback-Delayed-Sum-Nonlinear-Feedback (LNFDSNF) model. For each model class, the components of the circuit were parameterized, and the fitting algorithm found the optimal parameter values for each ganglion cell.\n\nThe performance of these models was evaluated by tracking a metric across successive changes in the model structure. This metric was the fraction of variance in the firing rate that the model explains. Each model circuit was more general than the previous one and significantly outperformed it in predicting the visual responses of certain ganglion cells. The improvement in performance was not simply due to overfitting after the addition of more free parameters, as the LN model actually had the most free parameters among the models tested. Separate training and testing data were used, achieving equivalent values in the explained variance, which implies that each model truly captures additional aspects of the computations carried out by the retina.\n\nThe study did not explicitly mention a comparison to publicly available methods on benchmark datasets.",
  "evaluation/confidence": "The performance metrics used in our study include explained variance, which is presented with median values and interquartile ranges. This provides a sense of the confidence intervals around the performance of each model. For instance, the explained variance for different models like LN, LNSN, LNSNF, LNFSNF, and LNFDSNF are given with their respective interquartile ranges, indicating the variability and reliability of these metrics.\n\nStatistical significance is a crucial aspect of our analysis. We employed sign tests to compare the performance across different models and corresponding experimental data. The results show that each model significantly outperforms the previous one in predicting the visual responses of certain ganglion cells, with p-values less than 0.001 for every step. This statistical rigor ensures that the improvements in model performance are not due to overfitting but rather capture additional aspects of the computations carried out by the retina.\n\nAdditionally, the experimental tests confirm the biological realism of the models. For example, the spatial characteristics of the bipolar cell module (BCM) filters matched well with the measured bipolar cell receptive fields, and the projective weights quantified from current injections into bipolar cells aligned with the model predictions. These findings further bolster the confidence in the superiority of our method over baselines.",
  "evaluation/availability": "Not enough information is available."
}