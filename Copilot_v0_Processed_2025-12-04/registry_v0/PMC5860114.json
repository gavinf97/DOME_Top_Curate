{
  "publication/title": "NeBcon: protein contact map prediction using neural network training coupled with naïve Bayes classifiers",
  "publication/authors": "The authors who contributed to the article are:\n\n- Baoji He\n- S. M. Mortuza\n- Yanting Wang\n- Hong-Bin Shen\n- Yang Zhang\n\nYang Zhang is the corresponding author.\n\nNot sure about the specific contributions of each author to the paper.",
  "publication/journal": "Bioinformatics",
  "publication/year": "2017",
  "publication/doi": "10.1093/bioinformatics/btx164",
  "publication/tags": "- Protein contact prediction\n- Machine learning\n- Co-evolution methods\n- Naïve Bayes classifier\n- Neural networks\n- Meta-prediction\n- Structural bioinformatics\n- Protein structure prediction\n- Contact map prediction\n- Computational biology",
  "dataset/provenance": "The dataset used in our study was sourced from the Protein Data Bank (PDB). We collected 517 non-redundant proteins for training purposes. These proteins were selected based on specific criteria: they had a pairwise sequence identity of less than 25%, a length between 50 and 300 amino acids, and a resolution better than 3 Å. This training set included a total of 407,036 short-range contacts, 757,315 medium-range contacts, and 5,209,080 long-range contacts. Additionally, we gathered 98 non-redundant proteins for testing, with lengths ranging from 80 to 160 amino acids. These test proteins were categorized into 21 alpha-proteins, 17 beta-proteins, and 60 alpha/beta-proteins. Importantly, none of the test proteins had a sequence identity greater than 25% with any of the training proteins, ensuring the independence of the datasets. The list of training and testing proteins is available for reference.",
  "dataset/splits": "In our study, we utilized two primary datasets for training and testing purposes.\n\nFor training, we collected 517 non-homologous proteins from the PDB. These proteins had a pairwise sequence identity of less than 25%, lengths between 50 and 300 amino acids, and a resolution better than 3 Å. This training set contained a significant number of short, medium, and long-range contacts, with specific counts provided for true contacts within each range.\n\nFor testing, we gathered 98 non-redundant proteins with lengths ranging from 80 to 160 amino acids. This test set included 21 alpha-proteins, 17 beta-proteins, and 60 alpha/beta-proteins. Importantly, none of these test proteins had a sequence identity greater than 25% with any of the training proteins, ensuring a non-homologous test set. The test dataset was further categorized into Easy and Hard targets based on the LOMETS meta-threading method, with 50 proteins classified as Easy targets and 48 as Hard targets.\n\nThe test dataset contained a specified number of true short, medium, and long-range contacts, with a focus on long-range contacts due to their importance in determining protein structure topology.",
  "dataset/redundancy": "To ensure the independence of the training and test sets, we collected 517 non-homologous proteins from the PDB for training. These proteins have a pairwise sequence identity of less than 25%, lengths between 50 and 300 amino acids, and a resolution better than 3 Å. For testing, we gathered 98 non-redundant proteins with lengths ranging from 80 to 160 amino acids, also from the PDB. These test proteins are non-homologous to the training set, meaning none of the test proteins share more than 25% sequence identity with any protein in the training set. This strict criterion ensures that the training and test sets are independent, preventing any data leakage that could artificially inflate the performance metrics.\n\nThe distribution of our dataset is designed to cover a diverse range of protein structures, including alpha, beta, and alpha/beta proteins. This diversity is crucial for evaluating the performance of our contact prediction methods across different types of protein structures. The test set includes 21 alpha proteins, 17 beta proteins, and 60 alpha/beta proteins, providing a comprehensive assessment of our methods' robustness and generalizability.\n\nCompared to previously published machine learning datasets for protein contact prediction, our dataset maintains a similar level of diversity and non-redundancy. The use of non-homologous proteins ensures that our models are trained and tested on independent data, which is essential for reliable performance evaluation. The strict criteria for sequence identity and resolution further enhance the quality of our dataset, making it comparable to or even more rigorous than many existing datasets in the field.",
  "dataset/availability": "The data used in this study is publicly available. A list of the training and testing proteins is provided at http://zhanglab.ccmb.med.umich.edu/NeBcon/benchmark. The dataset consists of 517 non-homologous proteins from the PDB, which have a pairwise sequence identity of less than 25%, a length between 50 and 300 amino acids, and a resolution better than 3 Å. Additionally, 98 non-redundant proteins with lengths ranging from 80 to 160 amino acids were collected from the PDB for testing the performance of NeBcon on different categories of proteins. These proteins are non-homologous to the training set, ensuring that none of the test proteins have a sequence identity greater than 25% to any of the training proteins. The data is made available to ensure reproducibility and to allow other researchers to validate or build upon the findings presented in this study. The specific licensing details for the data are not mentioned, but it is implied that the data is freely accessible for research purposes.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages a combination of machine learning techniques to enhance protein contact map prediction. Specifically, we utilize a neural network (NN) coupled with a naive Bayes classifier (NBC). The NBC model is used to derive posterior probabilities that appropriately account for the average accuracy of each program given a specific confidence score. This approach helps to enhance the efficiency of contact score combinations.\n\nThe neural network component is integrated to address potential limitations in the NBC model. It is trained on intrinsic structural features that may have been missed or distorted in the purely mathematical NBC model. This integration has been found particularly useful in improving contact accuracy for distant-homologous hard targets.\n\nThe use of these machine learning techniques is not new in the field of bioinformatics, but our specific implementation and combination of NBC and NN are novel in the context of protein contact map prediction. The decision to publish in a bioinformatics journal rather than a machine-learning journal is driven by the application focus. Our primary goal is to advance the field of structural bioinformatics by improving the accuracy and reliability of protein contact map predictions, which is crucial for protein structure prediction and related biological research.",
  "optimization/meta": "The model described in this publication is indeed a meta-predictor, specifically designed to integrate multiple contact prediction methods to enhance the accuracy of protein residue-residue contact map predictions. The meta-predictor, named NeBcon, combines outputs from eight different contact prediction algorithms. These algorithms include three machine learning-based methods: BETACON, SVMcon, and SVMSEQ. Additionally, it incorporates three co-evolution-based methods: PSICOV, CCMpred, and FreeContact. Two meta-server-based methods, STRUCTCH and MetaPSICOV, are also included in the combination.\n\nThe integration of these diverse methods allows NeBcon to leverage the strengths of both co-evolution and machine learning approaches. Co-evolution-based methods are particularly effective for targets with a high number of sequence homologs, while machine learning methods are more robust for targets with fewer homologs. By combining these approaches, NeBcon aims to provide reliable and balanced contact map predictions for both easy and hard targets.\n\nRegarding the independence of the training data, it is not explicitly stated whether the training data for each of the component methods is entirely independent. However, the inclusion of methods from different categories (machine learning, co-evolution, and meta-server) suggests an effort to diversify the sources of information, which can help in achieving more robust and accurate predictions. The meta-predictor uses a naive Bayes classifier to combine the predictions from these eight methods, enhancing the efficiency of contact score combinations. Additionally, intrinsic sequence features are integrated through neural network training to further improve the contact accuracy, especially for distant-homologous hard targets.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the input for the machine-learning algorithm. We extracted various features from the query sequence to train our neural network-based contact predictor, NeBcon. These features can be categorized into sequence-based and mutation features.\n\nFor sequence-based features, we considered six types of intrinsic features for each residue in the query sequence. These features include a residence feature that labels whether a residue falls within the query sequence or not. This results in 22 features for each pair of residues, considering a sliding window of 11 neighboring residues.\n\nAdditionally, we included a sequence separation feature that accounts for the distance between pairs of residues and the length of the protein. This results in 2 features.\n\nMutation features were also considered, which involve the probability of a specific amino acid or a gap appearing at a particular position in the PSI-BLAST multiple sequence alignment (MSA). This results in 462 features for a residue pair, considering that the MSA contains 20 amino acids and gaps, and each residue window spans 11 positions.\n\nFurthermore, we utilized posterior probabilities from eight existing contact prediction programs as training inputs. For a pair of residues, we calculated a posterior probability for each pair of two residues from the two 11-residue windows associated with the ith and jth residues. This results in 121 features.\n\nOverall, we designed 717 features for NeBcon, which were used to train the neural network. The neural network training was performed using the Weka data mining package, with 150 hidden units and one output unit. The number of units in the hidden layer was determined based on the optimization of the 10-fold cross-validation on the training proteins.\n\nTo enhance specificity, short-, medium-, and long-range contacts were trained separately. For long-range contacts, due to the large number of residue pairs, we constructed a set of 1 million contacts, consisting of all true contacts and a random selection from the non-contact pool, maintaining a contact and non-contact pairs ratio of 2:23. This approach ensured sufficient and stable training results.",
  "optimization/parameters": "In the optimization process, the model utilizes four types of parameters. These parameters are essential for calculating the posterior probability of residue pairs being in contact. The parameters include the prior probabilities for contacts and non-contacts, as well as the conditional probabilities derived from the contact prediction results of eight different contact predictors. The prior probabilities are calculated based on the number of true contacts and non-contacts in the training set, which consists of 517 non-homologous proteins. The conditional probabilities are determined by the confidence scores provided by the contact predictors for residue pairs in short-, medium-, and long-range contacts.\n\nThe selection of these parameters was guided by the need to enhance the specificity of the model for different contact ranges. Short-, medium-, and long-range contacts are defined based on the sequence separation between residues, with short-range contacts having a separation of 6–11 amino acids, medium-range contacts having a separation of 12–24 amino acids, and long-range contacts having a separation of more than 24 amino acids. This categorization allows the model to better handle the varying complexities associated with different contact ranges. The training process involved using a sliding window of 11 neighboring residues to enhance the stability of feature selection, and the parameters were optimized through a 10-fold cross-validation on the training proteins. This approach ensures that the model is robust and generalizable to new data.",
  "optimization/features": "In the optimization process of our method, a total of 717 features are used as input for the neural network training. These features are categorized into two main groups: sequence-based features and Naïve Bayes classifier scores.\n\nThe sequence-based features consist of six types of intrinsic features extracted from the query sequence. These features are designed to capture various aspects of the residues, such as their residence within the query sequence, secondary structure predictions, solvation properties, Shannon entropy from multiple sequence alignments, sequence separation, and mutation probabilities. Each type of feature contributes a specific number of individual features, resulting in a total of 576 sequence-based features.\n\nAdditionally, the posterior probabilities from a Naïve Bayes classifier, which combines predictions from eight existing contact predictors, contribute 121 features. These features are essential for training the contact prediction model and have been shown to significantly improve the accuracy of the predictions.\n\nFeature selection was performed to enhance the stability of feature selection. A sliding window of 11 neighboring residues is selected for each target residue, which helps in capturing the local context of the residues. This window size was chosen to balance between capturing sufficient local information and keeping the feature space manageable.\n\nThe feature selection process was conducted using the training set only, ensuring that the selected features are generalizable to unseen data. The training set consisted of 517 non-redundant proteins with specific criteria for sequence identity, length, and resolution. This rigorous selection process ensures that the features used in the model are robust and reliable for contact prediction.",
  "optimization/fitting": "The fitting method employed in this study involves a combination of naive Bayes classifiers and neural network training. The naive Bayes classifier (NBC) is used to combine the results from eight existing contact prediction programs, leveraging their posterior probabilities. This approach allows for the appropriate weighting of contact maps based on the relative accuracy of specific residue pairs at each given confidence score.\n\nThe neural network component of NeBcon is trained using a set of 517 non-redundant proteins, which were also used to determine the NBC model. This ensures consistency in the training data across both models. The neural network extracts two categories of features: sequence-based features and NBC scores. Sequence-based features include residence features, secondary structure features, solvent accessibility features, sequence separation features, and mutation features. These features are designed to capture various intrinsic properties of the protein sequences.\n\nTo enhance the specificity, short-, medium-, and long-range contacts are trained separately. For long-range contacts, due to the large number of residue pairs, a subset of 1 million contacts is used for training. This subset includes all true contacts and a randomly selected set of non-contact pairs, maintaining a contact-to-non-contact ratio of 2:23. This approach ensures that the training data is manageable while still being representative of the full dataset.\n\nThe neural network training was performed using the Weka data mining package, with 150 hidden units and one output unit. The number of hidden units was optimized through 10-fold cross-validation on the training proteins. This cross-validation process helps to rule out over-fitting by ensuring that the model generalizes well to unseen data. Additionally, the use of a large and diverse training set helps to mitigate the risk of under-fitting, as the model is exposed to a wide range of protein structures and sequences.\n\nIn summary, the fitting method involves a careful balance of model complexity and data representation. The use of cross-validation and a diverse training set helps to ensure that the model is neither over-fitted nor under-fitted, providing robust and accurate contact predictions.",
  "optimization/regularization": "Not applicable.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The model described in this publication is not a black box. It leverages a combination of well-understood statistical and machine learning techniques to enhance protein contact prediction. The core of the model is the Naïve Bayes Classifier (NBC), which combines the outputs of multiple contact predictors. The NBC model is transparent in the sense that it uses Bayes' theorem to calculate the posterior probability of residue pairs being in contact, based on the confidence scores from different predictors. This process is mathematically well-defined and interpretable.\n\nThe NBC model's parameters, such as the prior probabilities and conditional probabilities, are trained separately for short-, medium-, and long-range contacts. This separation enhances the model's specificity and interpretability, as it tailors the predictions to different types of contacts.\n\nAdditionally, the model integrates neural network training with the NBC results. The neural network uses a set of intrinsic features derived from the query sequence, such as sequence profiles, predicted secondary structures, and mutation features. These features are designed to capture various aspects of the protein's structure and evolution, making the model's predictions more interpretable.\n\nThe neural network's architecture is also transparent, with a fixed number of hidden units and one output unit. The number of units in the hidden layer was determined through cross-validation, ensuring that the model is optimized for performance while remaining interpretable.\n\nIn summary, the model is not a black box but rather a combination of interpretable statistical and machine learning techniques. The use of the NBC model, the separation of contact types, and the design of intrinsic features all contribute to the model's transparency and interpretability.",
  "model/output": "The model described is primarily a classification model. It focuses on predicting whether pairs of residues in a protein are in contact or not. This is achieved through a combination of a naive Bayes classifier and neural networks. The naive Bayes classifier combines the confidence scores from multiple contact predictors to calculate the posterior probability of residue pairs being in contact. This posterior probability is then used as a feature in the neural network training to further refine the contact predictions. The final output of the model is a classification of residue pairs as either in contact or not in contact, making it a classification model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The software associated with our model, NeBcon, is freely available to the public. Both an online server and a standalone program can be accessed at the URL http://zhanglab.ccmb.med.umich.edu/NeBcon/. This allows users to either run the algorithm through a web interface or download the standalone version for local execution. The availability of these resources ensures that researchers and practitioners can easily utilize NeBcon for their protein contact map predictions without any barriers.",
  "evaluation/method": "The evaluation of the method involved a comprehensive assessment using a set of 98 non-redundant proteins. The performance was measured by comparing the top L/5 contact predictions across various contact ranges: short (6–11), medium (12–24), and long (>24). The accuracy of the predictions was evaluated against eight different component methods, including both co-evolution-based and machine learning-based predictors.\n\nThe method was also tested on targets from the CASP10 and CASP11 experiments, focusing on free-modeling (FM) domains to avoid contamination from template-based modeling. The results showed that the method outperformed other predictors in the FM category, with statistically significant improvements in some cases.\n\nAdditionally, the method was compared with other meta-server-based methods, such as PconsC2 and PconsC3, using 38 FM domains from the CASP12 experiment. The comparisons highlighted the importance of combining complementary methods to enhance the accuracy of contact predictions.\n\nThe evaluation demonstrated the efficiency of the method in integrating multiple contact predictions and intrinsic sequence features through neural network training, resulting in statistically significant improvements over individual predictors and other combination methods.",
  "evaluation/measure": "In the evaluation of our method, we primarily focus on the accuracy of contact predictions, which is defined as the ratio of correctly predicted contacts in the top L/5 predictions to the total number of predicted contacts. This metric is equivalent to precision, measuring the true positives among the top predictions. We consider the top L/5 predictions because this cutoff is frequently used in the literature and in CASP assessments. Additionally, we analyze the performance on different contact ranges: short (6–11), medium (12–24), and long (>24) range contacts, with a particular emphasis on long-range contacts due to their importance in determining protein topology.\n\nWe also report P-values from student's t-tests to assess the statistical significance of the improvements observed with our method compared to other predictors. This helps to ensure that the observed differences in performance are not due to random chance.\n\nFor a comprehensive evaluation, we categorize proteins into Easy and Hard targets based on the LOMETS method, which assesses the availability of homologous templates. This allows us to examine how our method performs on different types of structure prediction targets. We provide detailed accuracy summaries for both Easy and Hard cases in supplementary tables.\n\nFurthermore, we compare our method's performance against individual predictors and meta-server predictors, highlighting the advantages of meta-prediction approaches. The reported metrics are representative of the standards used in the field, ensuring that our evaluation is comparable to other studies in the literature.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we thoroughly evaluated the performance of our proposed methods against both publicly available methods and simpler baselines. For benchmark datasets, we utilized a set of 98 non-redundant proteins, ensuring a diverse and challenging testbed for our evaluations. These proteins were categorized into Easy and Hard targets based on the availability of sequence homologs, which significantly impacts the performance of different prediction methods.\n\nWe compared our Naïve Bayes Classifier (NBC) combination method against individual predictors and other naive combination methods, such as voting and weighting. The results demonstrated that the NBC method consistently outperformed individual predictors and simpler baselines. For instance, the highest accuracy of the NBC combinations was 0.528 for Easy targets and 0.235 for Hard targets, which were significantly greater than the best individual predictors. The average accuracies of the NBC combinations were also much higher than those of the individual predictors, highlighting the efficiency of the NBC approach.\n\nAdditionally, we compared our method against meta-server predictors like STRUCTCH and MetaPSICOV, which combine predictions from multiple sources. The results showed that our method, NeBcon, which integrates NBC with neural network training, achieved higher average accuracies across all contact ranges, including short, medium, and long-range contacts. The statistical significance of these improvements was confirmed through student's t-tests, with most P-values being far below 0.05.\n\nWe also examined the contribution of neural network training to the NBC model by comparing NeBcon with NBC predictions. The data showed clear improvements for both Easy and Hard targets, with NeBcon outperforming NBC in a majority of the test cases. The average accuracies indicated a 15% improvement over NBC, demonstrating the usefulness of integrating intrinsic sequence-based features through neural network training.\n\nFurthermore, we assessed the robustness of NeBcon by excluding MetaPSICOV from the NBC combination and retraining the model. The results showed that while the accuracy of NeBcon without MetaPSICOV was lower than the full version, it was still comparable to MetaPSICOV alone, indicating that NeBcon's performance is not solely reliant on any single component method.\n\nIn summary, our evaluations on benchmark datasets and comparisons with publicly available methods and simpler baselines demonstrated the superior performance and robustness of our proposed methods. The consistent improvements across various contact ranges and target types underscore the effectiveness of combining complementary predictors and integrating intrinsic sequence features through neural network training.",
  "evaluation/confidence": "The evaluation of our method, NeBcon, includes a thorough statistical analysis to ensure the robustness and significance of our results. We employed the student's t-test to compare the performance of NeBcon with other predictors and baselines. The P-values obtained from these tests are crucial for determining the statistical significance of the improvements observed.\n\nFor instance, when comparing NeBcon with individual predictors and meta-server predictors like STRUCTCH and MetaPSICOV, most P-values are far below 0.05. This indicates that the improvements achieved by NeBcon are statistically significant. Specifically, NeBcon shows higher average accuracy across all contact ranges, including short, medium, and long-range contacts, with statistically significant P-values.\n\nIn the comparison between NeBcon and NBC, the P-value from the student's t-test is 0.03, suggesting that the improvement by NeBcon over NBC is statistically significant. This is particularly notable for hard targets, where the inclusion of machine learning methods significantly enhances the predictions.\n\nAdditionally, the integration of neural network training with the naive Bayes classifier in NeBcon demonstrates a clear improvement over NBC alone. The average accuracies of NeBcon and NBC are 0.628 and 0.546, respectively, for the 98 test targets, indicating a 15% improvement. The P-value for this comparison is 3.5 x 10^-8, which is highly statistically significant.\n\nFurthermore, the performance of NeBcon was evaluated on the most recent CASP experiments, where it showed higher accuracy than other predictors in the free-modeling (FM) category. Although some P-values were relatively high due to the limited number of test targets, the overall trend indicates the superior performance of NeBcon.\n\nIn summary, the evaluation confidence of our method is high, supported by statistically significant P-values and consistent improvements across various contact ranges and target types. This rigorous statistical analysis underscores the reliability and superiority of NeBcon in protein contact map predictions.",
  "evaluation/availability": "Not enough information is available."
}