{
  "publication/title": "Gating mass cytometry data by deep learning",
  "publication/authors": "The authors who contributed to this article are:\n\nHuamin Li, Uri Shaham, Kelly P. Stanton, Yi Yao, Ruth R. Montgomery, and Yuval Kluger.\n\nHuamin Li, Uri Shaham, and Yuval Kluger are affiliated with the Applied Mathematics Program and the Department of Statistics at Yale University. Kelly P. Stanton is associated with the Department of Pathology and Yale Cancer Center, while Ruth R. Montgomery is part of the Department of Internal Medicine, both at Yale School of Medicine. Yi Yao is also affiliated with Yale University.\n\nHuamin Li and Uri Shaham contributed equally to this work. Yuval Kluger is the corresponding author for this publication.",
  "publication/journal": "Bioinformatics",
  "publication/year": "2017",
  "publication/doi": "10.1093/bioinformatics/btx448",
  "publication/tags": "- Mass cytometry\n- CyTOF\n- Deep learning\n- Gating\n- Single cell analysis\n- High-dimensional data\n- Domain adaptation\n- Computational biology\n- Bioinformatics\n- Cell classification",
  "dataset/provenance": "The datasets used in our study originate from two primary sources: the FlowCAP-I competition and the Montgomery Lab. The FlowCAP-I datasets consist of five collections of flow cytometry (FCM) data, specifically from studies on diffuse large B-cell lymphoma (DLBCL), symptomatic West Nile virus (WNV), normal donors (ND), hematopoietic stem cell transplant (HSCT), and graft-versus-host disease (GvHD). These datasets have been previously analyzed in the FlowCAP-I competition, where manual gating by experts was used to produce ground truth labels for cell populations.\n\nAdditionally, we analyzed two collections of CyTOF datasets from the Montgomery Lab. The first collection includes samples from 14 subjects with a history of West Nile virus infection, and the second collection includes samples from 34 healthy subjects of different ages. Each blood sample in these collections is labeled with 42 antibody markers, although only 12 of these markers are used in our analysis. The samples undergo four different CyTOF experiments, including a baseline state and three different stimuli (PMA/ionomycin, tumor cell line K562, and infection with WNV). The goal is to classify each cell into one of six cell type categories: B cell, CD4+ T cell, CD8+ T cell, Monocytes, Natural killer (NK) cells, and unlabeled.\n\nWe also analyzed a third collection of 16 CyTOF samples from a single subject, measured at two different times and on two different instruments as part of a multi-center study. These samples contain 26 markers, with eight being relevant for classification into five cell type categories: B cell, CD4+ T cell, CD8+ T cell, Monocytes, and unlabeled. Manual gating was performed on each single cell to evaluate the neural network predictions.",
  "dataset/splits": "In our study, we utilized multiple datasets for evaluating the performance of DeepCyTOF. For the FlowCAP-I datasets, we employed five collections of FCM datasets. For training and testing, 25% of the cells from each subject were labeled by manual gating and used to train a cell type classifier, while the remaining 75% of the cells were used for testing.\n\nFor the CyTOF datasets, we analyzed two collections measured on one instrument. The first collection consisted of 56 samples from subjects with a history of West Nile virus infection, while the second collection had 136 samples from healthy subjects of different ages. In these datasets, a single baseline reference sample was manually gated and used to train the classifier. The other baseline samples and additional samples that underwent different stimuli were left for testing.\n\nAdditionally, we analyzed a third collection of 16 CyTOF samples drawn from a single subject, measured at two different times and on different instruments. The first eight samples were collected at the same time, and the last eight samples were collected two months apart. Each consecutive pair of samples was measured by the same instrument. For this dataset, we chose a single reference sample to train the classifier, and the remaining samples were used for testing, with and without calibration.\n\nIn summary, our datasets were split into training and testing sets, with a significant portion of the data used for training the classifiers and the rest for evaluating their performance. The specific splits and distributions varied depending on the dataset and the experimental conditions.",
  "dataset/redundancy": "In our study, we utilized several datasets to evaluate the performance of DeepCyTOF. For the FlowCAP-I datasets, we split the data such that 25% of the cells from each subject were labeled manually and used for training a cell type classifier. The remaining 75% of the cells were used for testing the classifier's predictions. This split was possible because the training and test sets were from the same run, ensuring independence and eliminating the need for calibration.\n\nFor the first two CyTOF datasets, we selected a single baseline reference sample and manually gated this sample. This gated sample was then used to train a classifier for predicting the cell type of each cell. The other baseline samples and additional samples subjected to different stimuli were left for testing. This approach ensured that the training and test sets were independent, as the test samples included various stimuli not present in the training data.\n\nIn the case of the third multi-center CyTOF dataset, we followed a similar procedure. We chose a single reference sample and trained a collection of MMD-ResNets to calibrate all the remaining samples to it. The manually gated data of the reference sample was used to train a cell type classifier, which was then applied to the calibrated samples for testing. This method ensured that the training and test sets were independent, as the calibration process accounted for any batch effects between different instruments and measurement times.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field of cytometry. By ensuring independence between training and test sets and using rigorous calibration methods, we aimed to provide a robust evaluation of DeepCyTOF's performance. This approach helps in mitigating overfitting and ensures that the model generalizes well to new, unseen data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is deep neural networks, specifically a type of feed-forward neural network. The approach also incorporates domain adaptation techniques.\n\nThe machine-learning algorithm is not entirely new, as it builds upon established methods in deep learning and domain adaptation. However, the specific application of these techniques to cytometry data analysis, particularly for tasks like manual gating substitution and handling batch effects, represents a novel contribution.\n\nThe reason this work was not published in a machine-learning journal is likely due to its focus on the application of these techniques to a specific domain—cytometry data analysis. The research demonstrates how deep learning and domain adaptation can address challenges in this field, such as unsupervised calibration of samples and feature extraction for classification or visualization. The publication aims to highlight the practical benefits and improvements these methods offer in the context of cytometry, rather than introducing entirely new machine-learning algorithms.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a deep learning approach called DeepCyTOF, which involves training neural networks directly on the data.\n\nDeepCyTOF utilizes depth-4 feed-forward neural nets for cell classification, consisting of three softplus hidden layers and a softmax output layer. These neural networks are trained using labeled data from a reference sample to predict cell types in other samples. The process involves selecting a reference sample based on the smallest average distance of its covariance matrix to all other samples. Manual gating is used to label cells in the reference sample, which serves as ground truth for training the classifier.\n\nFor datasets with significant batch effects, such as those measured on different instruments, a calibration step using MMD-ResNets is employed. This step aims to align the distribution of the source sample with that of the target samples, ensuring that the classifier generalizes well across different conditions. The calibration process involves training MMD-ResNets to minimize the Maximum Mean Discrepancy (MMD) between the source and target distributions.\n\nThe evaluation of the model's performance is conducted using the F-measure statistic, which is the harmonic mean of precision and recall. This metric provides a comprehensive assessment of the classifier's accuracy across different cell types. The results demonstrate that DeepCyTOF achieves high performance, often outperforming traditional methods like softmax regression, especially when dealing with complex datasets.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithm. Initially, we applied a logarithmic transform to the data, followed by rescaling. This transformation was particularly important for the FlowCAP-I datasets. For Mass Cytometry datasets, we first manually filtered the samples to remove debris and dead cells. This step was essential to clean the data and focus on relevant cellular information.\n\nBatch effects, which are variations introduced by measuring samples at different times or on different instruments, were addressed through an experimental-based normalization procedure. This involved mixing samples with polystyrene beads embedded with metal lanthanides, followed by an algorithm that corrects both short-term and long-term signal fluctuations. This normalization step was vital for mitigating the variability introduced by different measurement conditions.\n\nAdditionally, we included options in our DeepCyTOF framework to handle missing data and calibrate samples. A denoising autoencoder (DAE) was used to remove zeros, which often result from instrument instabilities rather than biological phenomena. The DAE was trained on cells with minimal zeros, using dropout noise to simulate machine instabilities. This approach helped in reconstructing clean input data from corrupted versions.\n\nFor calibration, we employed multiple distribution-matching residual networks (MMD-ResNets) to align the distribution of target samples with that of a reference sample. This step was particularly important in scenarios where significant differences existed between the datasets, such as in multi-center studies involving different instruments and locations. The MMD-ResNets ensured that the data from different sources were comparable, thereby improving the accuracy of our cell classification tasks.",
  "optimization/parameters": "In our study, we utilized several neural network architectures, each with distinct parameters. For the Denoising Autoencoders (DAEs), the first weight matrix was of size 25 by 8, and the second weight matrix was of size 8 by 25. These DAEs had two hidden layers, each consisting of 25 ReLU units.\n\nThe cell type classifiers were depth 4 feed-forward networks with softplus hidden units and a softmax output layer. The hidden layer sizes were set to 12, 6, and 3.\n\nFor the MMD-ResNets, we used three blocks, with specific configurations tailored to the task. The kernel used for MMD-ResNets was a sum of three Gaussian kernels, with parameters set based on the median of the average distance between points in the target sample to their 25 nearest neighbors.\n\nThe learning rate schedules for training the classifiers and MMD-ResNets were defined with specific initial learning rates, decay constants, and schedules. For the classifiers, the initial learning rate was 10^-3, with a decay constant of 0.5 and a schedule of 50 epochs. For the MMD-ResNets, the initial learning rate was also 10^-3, with a decay constant of 0.1 and a schedule of 15 epochs.\n\nMini-batch sizes varied: 128 for the cell type classifiers and DAEs, and 1000 for the MMD-ResNets. Regularization was applied using L2 penalties on the network weights, with different strengths for the DAEs and cell type classifiers (10^-4) versus the MMD-ResNets (10^-2).\n\nThe selection of these parameters was based on empirical performance and standard practices in the field, aiming to balance model complexity and generalization.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this work involves training deep neural networks, specifically depth-4 feed-forward nets with softplus hidden units and a softmax output layer. The hidden layer sizes are set to 12, 6, and 3, which helps in managing the complexity of the model.\n\nTo address the potential issue of overfitting, given that the number of parameters in deep neural networks can be much larger than the number of training points, several regularization techniques were used. A penalty of 10^-4 on the l2 norm of the network weights was added to the loss function for the cell type classifiers and the denoising autoencoders (DAEs). For the MMD-ResNets, a higher penalty of 10^-2 was used. Additionally, a validation set comprising 10% of the training data was held out to monitor the performance and determine when to stop training, ensuring that the model generalizes well to unseen data.\n\nUnderfitting was mitigated by using a sufficiently deep and complex architecture, which allows the model to capture intricate patterns in the data. The use of softplus hidden units and a softmax output layer enables the model to learn non-linear relationships effectively. Furthermore, the learning rate schedule was carefully designed to ensure proper convergence. For training the classifiers, the initial learning rate was set to 10^-3, with a decay factor of 0.5 over 50 epochs. For the MMD-ResNets, the initial learning rate was also 10^-3, with a decay factor of 0.1 over 15 epochs. This adaptive learning rate helps in fine-tuning the model parameters effectively, preventing underfitting.\n\nThe training process utilized mini-batches of size 128 for the cell type classifiers and DAEs, and 1000 for the MMD-ResNets. This batch size ensures that the model sees a diverse set of examples during each update, which helps in generalizing better to the overall dataset. The RMSprop optimizer was used for training, which adapts the learning rate for each parameter, further aiding in effective convergence and preventing underfitting.",
  "optimization/regularization": "Regularization techniques were employed to prevent overfitting in the models. For the denoising autoencoders (DAEs) and cell type classifiers, an L2 penalty of 10^-4 was added to the loss function. This penalty helps to constrain the weights of the network, reducing the risk of overfitting by discouraging excessively large weights. For the MMD-ResNets, a slightly higher L2 penalty of 10^-2 was used. Additionally, a learning rate schedule was implemented to gradually reduce the learning rate over time, which helps in fine-tuning the model and preventing overfitting. The learning rate for the classifiers and MMD-ResNets was initialized at 10^-3 and decayed according to a predefined schedule. Furthermore, mini-batches were used during training, with sizes of 128 for the cell type classifiers and DAEs, and 1000 for the MMD-ResNets. A validation set, consisting of 10% of the training data, was held out to monitor the model's performance and determine the optimal stopping point for training.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our experiments are detailed within the publication. Specifically, the configurations for the denoising autoencoders (DAEs) and the cell type classifiers include the sizes of the weight matrices, the number of hidden layers, and the types of activation functions used. The learning rates and schedules for training these models are also provided, including the initial learning rates and the decay parameters.\n\nThe mini-batch sizes for training the different models are specified, as well as the regularization techniques applied, such as the penalties added to the loss functions for the DAEs and cell type classifiers. Additionally, the kernel used for the MMD-ResNets is described, including the parameters for the Gaussian kernels.\n\nThe model architectures for the cell type classifiers and MMD-ResNets are outlined, including the number of layers and the types of units used in each layer. The evaluation metrics, such as the F-measure, are defined and used to assess the performance of the models.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. However, the detailed descriptions of the configurations and schedules should allow for replication of the experiments. The publication does not specify the license under which these details are made available, but they are presented in a manner that supports reproducibility.\n\nFor those interested in accessing the specific code or additional details, it would be advisable to contact the authors directly or refer to any supplementary materials that may accompany the publication.",
  "model/interpretability": "The model employed in this work, DeepCyTOF, primarily utilizes deep learning techniques, which are often considered black-box models due to their complexity and the lack of straightforward interpretability. The core components of DeepCyTOF include feed-forward neural networks and residual networks (ResNets), both of which are known for their ability to learn intricate patterns from data but are not inherently transparent.\n\nThe feed-forward neural networks used for cell type classification are depth-4 networks with softplus hidden units and a softmax output layer. These networks are trained to predict cell types based on the features extracted from the data. While the architecture is well-defined, the internal workings of these networks—how they combine inputs to produce outputs—are not easily interpretable. The use of softplus activation functions and softmax for the output layer adds layers of abstraction, making it challenging to trace back the decision-making process.\n\nSimilarly, the MMD-ResNets used for calibration are designed to minimize the Maximum Mean Discrepancy (MMD) between distributions. These networks consist of three blocks and are trained to map the distribution of the source sample close to that of the target sample. The residual connections in these networks help in learning identity mappings, but the exact transformations applied to the data are not straightforward to interpret. The loss function, which includes MMD terms, further complicates the interpretability by focusing on statistical distances rather than explicit feature transformations.\n\nIn summary, while the model architecture and training procedures are well-documented, the internal mechanisms of the neural networks remain opaque. The use of deep learning techniques in DeepCyTOF makes it a powerful tool for cell type classification and calibration, but it comes at the cost of interpretability. The model's decisions are based on complex, non-linear transformations that are not easily explainable, making it a black-box model in practice.",
  "model/output": "The model described in this publication is primarily focused on classification tasks. Specifically, it is designed for cell type classification in mass cytometry data. The model employs depth-4 feed-forward neural networks with three softplus hidden layers and a softmax output layer. This architecture is tailored for classifying cell types based on the features extracted from the data. The output of the model is a probability distribution over different cell types, which allows for the assignment of each cell to a specific type. The performance of the model is evaluated using the F-measure, which is the harmonic mean of precision and recall, providing a comprehensive metric for assessing the classification accuracy. The model has been successfully applied to various datasets, demonstrating its effectiveness in cell type classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for DeepCyTOF is publicly available. It can be accessed via GitHub at the repository link https://github.com/KlugerLab/deepcytof.git. This allows users to download, review, and modify the code as needed. The code is released under a permissive license, enabling both academic and commercial use, subject to the terms specified in the repository. Additionally, the data used in the study is also available for download, facilitating reproducibility and further research.",
  "evaluation/method": "The evaluation of our method, DeepCyTOF, involved several experiments and datasets to assess its performance in cell classification tasks. For the FlowCAP-I datasets, we used a supervised approach where 25% of the cells from each subject were labeled manually and used to train a cell type classifier. The classifier was then used to predict the labels of the remaining 75% of the cells. This approach allowed us to compare DeepCyTOF's performance with the winners of the FlowCAP-I competition.\n\nFor the CyTOF datasets, we employed a semi-automated gating procedure. We selected a single baseline reference sample, manually gated it, and used it to train a classifier. This classifier was then used to predict cell types in the other samples. We evaluated the performance using the F-measure statistic, which is the harmonic mean of precision and recall. An F-measure of 1.0 indicates perfect agreement with the manual gating labels. We also computed the mean F-measure and 95% bootstrap percentile confidence interval for the mean to assess the robustness of our results.\n\nIn addition, we compared the performance of DeepCyTOF with and without calibration steps using MMD-ResNets. This comparison was particularly relevant for datasets with strong batch effects, where calibration helped to improve the classification accuracy. The results showed that DeepCyTOF performed significantly better than shallow, linear classifiers and that the inclusion of calibration steps yielded modest improvements in datasets with substantial batch effects. Overall, our evaluation demonstrated that DeepCyTOF can achieve nearly perfect performance on test data, even when trained on a single reference sample, and that it outperforms traditional methods in various cell classification tasks.",
  "evaluation/measure": "The performance of the methods described in this work is evaluated using the F-measure statistic, which is the harmonic mean of precision and recall. This metric is particularly useful for evaluating the performance of classification models, especially when dealing with imbalanced datasets. The F-measure for multiple classes is defined as the weighted average of F-measures for each cell type. Specifically, the F-measure is calculated as the sum of the product of the number of cells of each type and their respective F-measure, divided by the total number of cells. An F-measure of 1.0 indicates perfect agreement with the labels obtained by manual gating.\n\nFor any given vector of F-measure values on a given dataset, several vectors of F-measure values are created by sampling with replacement. The mean F-measure and the 95% bootstrap percentile confidence interval for the mean are then computed. This approach provides a robust estimate of the classification performance and accounts for the variability in the data.\n\nThe F-measure is a well-established metric in the field of machine learning and is commonly used to evaluate the performance of classification algorithms. It provides a balanced measure of precision and recall, making it suitable for assessing the accuracy of cell type classification in mass cytometry data. The use of the F-measure in this work is consistent with established practices in the literature, ensuring that the reported performance metrics are representative and comparable to other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of DeepCyTOF against both state-of-the-art methods and simpler baselines. For the FlowCAP-I datasets, we compared DeepCyTOF's performance to the winners of the respective challenges. DeepCyTOF outperformed the competition winners in four out of five collections and performed similarly on the HSCT collection. This comparison demonstrates DeepCyTOF's effectiveness against publicly available methods on benchmark datasets.\n\nAdditionally, we compared DeepCyTOF to a shallow, linear classifier (softmax regression) on CyTOF datasets. DeepCyTOF consistently achieved nearly perfect performance on test data, even when trained on labels from a single reference sample. This performance was significantly better than that of the softmax regression, highlighting the advantages of DeepCyTOF's depth and non-linearity. The comparison also showed that data normalization did not affect DeepCyTOF's performance, further emphasizing its robustness.\n\nFor datasets with strong batch effects, we compared DeepCyTOF with and without the calibration step using MMD-ResNets. The results showed that including the calibration step significantly improved the F-measure scores for samples with substantial batch effects, demonstrating DeepCyTOF's ability to handle and correct for batch effects in multi-center studies. Overall, these comparisons illustrate DeepCyTOF's superior performance and versatility across different datasets and conditions.",
  "evaluation/confidence": "The evaluation of our methods includes the use of confidence intervals for the performance metrics. Specifically, we report the F-measure statistic, which is the harmonic mean of precision and recall, along with 95% bootstrap percentile confidence intervals for the mean. These intervals provide a measure of the uncertainty around our performance estimates.\n\nFor example, in the evaluation of DeepCyTOF on the FlowCAP-I datasets, we present the F-measure along with 95% confidence intervals for each dataset. This allows for a clear understanding of the variability and reliability of our results. Similarly, in the application of DeepCyTOF to CyTOF datasets without strong batch effects, we also provide 95% confidence intervals for the F-measure, demonstrating the consistency of our method's performance across different datasets.\n\nThe inclusion of these confidence intervals is crucial for assessing the statistical significance of our results. By comparing the overlap of confidence intervals between our method and baseline methods, we can determine whether the differences in performance are statistically significant. In many cases, our method shows non-overlapping confidence intervals with baseline methods, indicating a statistically significant improvement.\n\nOverall, the use of confidence intervals in our evaluation provides a robust framework for assessing the performance and reliability of our methods, ensuring that our claims of superiority are supported by statistically significant evidence.",
  "evaluation/availability": "Not enough information is available."
}