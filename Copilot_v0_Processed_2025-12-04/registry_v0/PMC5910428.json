{
  "publication/title": "Computational Protein Design with Deep Learning Neural Networks",
  "publication/authors": "The authors who contributed to this article are Y.Q., J.Z., J.W., and H.C. Y.Q. and J.Z. were responsible for designing the study and experiments. Y.Q., J.W., and H.C. performed the experiments. Y.Q. and J.Z. prepared the manuscript.",
  "publication/journal": "Scientific Reports",
  "publication/year": "2018",
  "publication/doi": "10.1038/s41598-018-24760-x",
  "publication/tags": "- Computational Protein Design\n- Deep Learning\n- Neural Networks\n- Protein Structure\n- Amino Acid Prediction\n- Machine Learning\n- Protein Engineering\n- Structural Biology\n- Bioinformatics\n- Protein Folding",
  "dataset/provenance": "The dataset used in this study was sourced from the Protein Data Bank (PDB), a well-known repository of protein structures. High-resolution protein structures were collected using specific filtering conditions, including structure determination method, resolution, chain length, and sequence identity. These conditions ensured that the dataset was of high quality and suitable for training the neural network.\n\nThree datasets were prepared based on different sequence identity cutoffs: 30%, 50%, and 90%. These cutoffs were used to remove homologous proteins, ensuring that the datasets were diverse and representative of different protein structures. For each dataset, clusters were created by extracting each residue and its closest neighbor residues based on the Cα-Cα distance. The number of neighbor residues considered varied (N = 10, 15, 20, 25, 30), and these clusters were randomly split into five sets for five-fold cross-validation.\n\nThe datasets were named according to the sequence identity cutoff and the number of neighbor residues. For example, SI30N10 refers to the dataset with a 30% sequence identity cutoff and 10 neighbor residues. Similar naming conventions were applied to other datasets.\n\nThe number of data samples varied across the datasets. For instance, the number of data samples almost doubled from the SI30 dataset to the SI90 dataset. This increase in data samples contributed to the improved accuracy observed in datasets with higher protein identity cutoffs.\n\nThe datasets used in this study were compared with those used in previous work, such as the SPIN method developed by Zhou and coworkers. SPIN was trained on 1532 non-redundant proteins with a sequence identity cutoff of 30%. In contrast, the current study used a significantly larger number of data samples, which allowed for more robust training and better performance.\n\nThe datasets were used to train a neural network for predicting the probability of 20 natural amino acids at each residue in a protein. The neural network architecture included a residue probability network, a weight network, and fully-connected layers leading to a 20-dimensional softmax layer. The input features for the network included basic geometric and structural properties of the residues, such as Cα-Cα distance, backbone dihedrals, secondary structures, hydrogen bonds, and solvent accessible surface area.\n\nIn summary, the dataset was sourced from the PDB, with high-resolution protein structures filtered based on specific conditions. Three datasets were created with different sequence identity cutoffs, and clusters were formed by considering neighbor residues. The datasets were used to train a neural network, and the number of data samples varied across the datasets, contributing to the overall accuracy of the predictions.",
  "dataset/splits": "The dataset was divided into five splits for the purpose of five-fold cross-validation. Each split contained a random selection of clusters, which consisted of a target residue and its neighboring residues. The number of data samples in each split is not explicitly stated, but it is mentioned that the datasets were prepared based on three sequence identity cutoffs: 30%, 50%, and 90%. For each of these datasets, the number of neighbor residues considered was varied (N = 10, 15, 20, 25, 30). The specific naming convention used for the datasets is SI30N10, SI50N15, etc., where the first part indicates the sequence identity cutoff and the second part indicates the number of neighbor residues. The distribution of data points in each split is not detailed, but it is implied that the splits were designed to be representative of the overall dataset.",
  "dataset/redundancy": "The datasets used in our study were prepared based on three sequence identity cutoffs: 30%, 50%, and 90%. These cutoffs were applied to remove homologous proteins and ensure that the datasets were independent. For each sequence identity cutoff, structures were retrieved with the specified sequence identity, and additional filters were applied to remove membrane proteins, structures with D-amino acids, and non-protein residues. The resulting datasets consisted of structures with varying sequence identities.\n\nTo ensure the independence of the training and test sets, the datasets were randomly split into five sets for five-fold cross-validation. This means that each dataset was divided into five parts, and the model was trained and tested five times, each time using a different part as the test set and the remaining four parts as the training set. This approach helps to ensure that the model's performance is evaluated on independent data and that the results are not due to overfitting.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets in the field of protein design. The datasets used in our study are larger and more diverse than many previously published datasets, which helps to improve the model's performance and generalization to new data. The use of high-resolution protein structures and the application of strict filtering criteria also help to ensure the quality and reliability of the datasets.",
  "dataset/availability": "The datasets generated and analyzed during the study are not publicly available. They can be obtained from the corresponding author upon reasonable request. This approach ensures that the data is shared responsibly and that any potential misuse or misinterpretation is minimized. By controlling the distribution, the authors can also provide guidance on the proper use of the datasets, ensuring that the research is conducted ethically and accurately. This method of data sharing is common in scientific research, particularly when the datasets are complex or sensitive. It allows for collaboration and verification of results while maintaining oversight of how the data is used.",
  "optimization/algorithm": "The optimization algorithm employed in our study is a deep learning neural network, specifically constructed using the Keras library. This approach falls under the class of supervised learning algorithms, where the network is trained to predict the probability of 20 natural amino acids for each residue in a protein.\n\nThe neural network utilized is not entirely novel; it leverages established techniques in deep learning. The choice of using a deep learning neural network was driven by its proven effectiveness in handling large datasets and extracting complex features, which is particularly relevant given the increasing number of solved protein structures.\n\nThe decision to publish this work in a scientific journal rather than a machine-learning journal is rooted in the application domain. The primary focus of our research is on computational protein design, which has significant implications in biology, chemistry, and medicine. While the deep learning techniques used are well-known in the machine-learning community, their application to protein design is what sets this work apart. The integration of deep learning with protein design methods represents a novel contribution to the field of computational biology, making it more appropriate for publication in a scientific journal that caters to a broader audience interested in protein engineering and related applications.",
  "optimization/meta": "The model described does not function as a traditional meta-predictor that combines the outputs of multiple independent machine-learning algorithms. Instead, it employs a hierarchical neural network architecture designed to predict the probabilities of amino acid types for a target residue.\n\nThe architecture consists of two main components: the residue probability network and the weight network. The residue probability network considers the target residue and its neighboring residues, using shared parameters to process different target-neighbor residue pairs. This setup is analogous to the application of convolution layers in image recognition, where the same network is applied to different regions of the input image.\n\nTo address the issue of equally weighting the outputs of each target-neighbor residue pair, a weight network is introduced. This network takes the same input as the residue probability network but outputs a single weight for each pair. The outputs of the residue probability network are then multiplied by these weights and concatenated. Several fully-connected layers are constructed on top of the weighted residue probabilities, leading to a 20-dimensional softmax layer that outputs the probabilities of the 20 possible amino acid types for the target residue.\n\nThe training data for this model is derived from high-resolution protein structures obtained from the Protein Data Bank (PDB), with specific filtering conditions applied to ensure data quality and independence. Three datasets are prepared based on sequence identity cutoffs of 30%, 50%, and 90%, referred to as SI30, SI50, and SI90, respectively. These datasets are used to remove homologous proteins and ensure that the training data is independent.\n\nIn summary, while the model utilizes a sophisticated neural network architecture, it does not rely on the outputs of other machine-learning algorithms as input. The training data is carefully curated to ensure independence and quality, supporting the model's ability to make accurate predictions about amino acid types in protein structures.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps to ensure that the input features were suitable for neural network training. The input features for the neural networks were derived from both the central residues and their neighboring residues.\n\nFor the central residues, the features included the cosine and sine values of backbone dihedrals (ϕ, ψ, and ω), the total solvent accessible surface area (SASA) of backbone atoms (Cα, N, C, and O), and the secondary structure type (helix, sheet, or loop). These features were chosen to capture the conformational and structural properties of the central residues.\n\nFor the neighbor residues, the features included the cosine and sine values of backbone dihedrals (ϕ, ψ, and ω), the total SASA of backbone atoms, the Cα-Cα distance to the central residue, unit vectors defining the relative positions and orientations of the neighbor residues with respect to the central residue, the secondary structure type, and the number of backbone-backbone hydrogen bonds between the central residue and the neighbor residue. These features were designed to provide a comprehensive representation of the local environment around the central residue.\n\nThe cosine and sine values of the dihedrals were used because dihedral angles, which range from -180 to 180 degrees, are not continuous at the boundaries. This encoding helps the neural network to better capture the periodic nature of dihedral angles.\n\nThe SASA values were calculated using the Naccess program on the whole protein structure, with sidechain atoms removed. This approach was taken because during protein design, the identity of a residue and thus its sidechain atoms are unknown.\n\nSecondary structure was assigned using the Stride program. All other features were calculated with an in-house program.\n\nTo prepare the data for training, structures that satisfied specific conditions were retrieved, including sequence identity cutoffs of 30%, 50%, and 90%. Membrane proteins and structures containing D-amino acids were removed. The resulting datasets consisted of structures with varying sequence identities. For each structure, non-protein residues such as water, ions, and ligands were removed, and structural clusters were extracted based on the Cα-Cα distance to the central residue. Clusters with incomplete or low-occupancy atoms were discarded. Each cluster was then translated and oriented so that the Cα, N, and C atoms of the target residue were positioned at specific coordinates, facilitating consistent input for the neural network.\n\nThe training samples were weighted to account for the different abundances of each residue type in the training set. This weighting scheme ensured that the neural network learned more from underrepresented residue types, thereby improving the overall prediction accuracy. The output of the neural network was the probability of the 20 amino acids for the central residue of a cluster.",
  "optimization/parameters": "The neural network model utilized in this study incorporates several input parameters derived from both the target residue and its neighboring residues. These parameters include basic geometric and structural properties such as the Cα-Cα distance, cosine and sine values of backbone dihedrals (ϕ, ψ, and ω), relative location of the neighbor residue to the target residue, secondary structure types, number of backbone-backbone hydrogen bonds, and solvent accessible surface area of backbone atoms.\n\nThe selection of these parameters was guided by their relevance to the structural and functional properties of proteins. For instance, the Cα-Cα distance and the unit vectors defining the position and orientation of neighbor residues help capture the spatial arrangement, which is crucial for understanding residue interactions. The cosine and sine values of dihedrals were used to handle the discontinuity in the range of dihedral angles. The solvent accessible surface area and secondary structure types provide insights into the residue's exposure to the solvent and its structural context within the protein.\n\nThe number of layers and nodes in each fully-connected layer were determined through training and testing on the smallest dataset, SI30N10. This iterative process ensured that the model's architecture was optimized for performance. The training was conducted for 1000 epochs to ensure convergence, using categorical cross-entropy as the loss function and stochastic gradient descent for optimization. The learning rate was set to 0.01, with a Nesterov momentum of 0.9 and a batch size of 40,000. Additionally, training samples were weighted to account for the different abundances of each residue type, ensuring that underrepresented residue types were given more attention during training.\n\nThe model's architecture includes a residue probability network and a weight network. The residue probability network considers the target residue and its 10–30 neighbor residues, applying the same parameters repeatedly. The weight network assigns a single weight to each target-neighbor residue pair, which is then used to adjust the output of the residue probability network. Several fully-connected layers are constructed on top of the weighted residue probabilities, leading to a 20-dimensional softmax layer that outputs the probabilities of the 20 residue types for the target residue.\n\nNot sure about the exact number of parameters used in the model, as this detail was not explicitly mentioned. However, the model's design and training process were carefully optimized to ensure robust performance in predicting residue types.",
  "optimization/features": "The input features for the neural networks used in this study are derived from both the central (target) residues and their neighboring residues. For the central residues, the features include the cosine and sine values of backbone dihedrals (ϕ, ψ, and ω), the total solvent accessible surface area (SASA) of backbone atoms (Cα, N, C, and O), and the secondary structure type (helix, sheet, or loop).\n\nFor the neighboring residues, the features include the cosine and sine values of backbone dihedrals (ϕ, ψ, and ω), the total SASA of backbone atoms, the Cα-Cα distance to the central residue, unit vectors defining the relative positions and orientations of the residues, the secondary structure type, and the number of backbone-backbone hydrogen bonds between the central and neighboring residues.\n\nThe specific features used were selected based on their relevance to the structural and geometric properties of the residues, ensuring that the neural network could effectively learn from the data. Feature selection was performed to include only the most informative features, and this process was conducted using the training set to avoid data leakage and ensure the robustness of the model.\n\nThe total number of features (f) used as input varies depending on the number of neighboring residues considered (N = 10, 15, 20, 25, 30). However, the exact count of features can be determined by summing the individual features for the central and neighboring residues. For example, for each central residue, there are 8 features (3 dihedrals × 2 + 1 SASA + 3 secondary structure types + 1 for the residue type). For each neighboring residue, there are additional features including dihedrals, SASA, distances, unit vectors, secondary structure, and hydrogen bonds. The precise count of features would depend on the specific configuration of the network and the number of neighboring residues included.",
  "optimization/fitting": "The neural network architecture employed in this study is designed to handle a substantial number of parameters relative to the training data points. To mitigate the risk of overfitting, several strategies were implemented. Firstly, the training samples were weighted to account for the varying abundance of each residue type, ensuring that the network learns more from underrepresented residue types. This weighting scheme helps in balancing the training process and prevents the model from becoming biased towards more frequent residues.\n\nAdditionally, the network was trained using stochastic gradient descent with a Nesterov momentum of 0.9 and a learning rate of 0.01. This optimization method, combined with a large batch size of 40,000, helps in stabilizing the training process and reducing the likelihood of overfitting. The use of categorical cross-entropy as the loss function further ensures that the model focuses on correctly classifying the residue types.\n\nTo rule out underfitting, the network was trained for 1000 epochs, which is sufficient to ensure convergence. The architecture includes multiple fully-connected layers, allowing the model to capture complex patterns in the data. The input features, which include geometric and structural properties of the residues, provide a rich set of information for the network to learn from. The use of cosine and sine values for backbone dihedrals ensures that the network can handle the periodic nature of these angles effectively.\n\nThe performance of the network was evaluated using five-fold cross-validation on different datasets with varying sequence identity cutoffs and numbers of neighbor residues. This rigorous evaluation process helps in assessing the generalization capability of the model and ensures that it performs well on unseen data. The results show that the network achieves high accuracy, indicating that it has neither overfitted nor underfitted the training data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the training of our neural network. One key method involved weighting the training samples to account for the varying abundance of each residue type. This was achieved using the formula \\( W_i = \\frac{N_{max}}{N_i} \\), where \\( N_{max} \\) is the maximal number of samples of all 20 residue types, and \\( N_i \\) is the number of samples of residue type \\( i \\). This weighting scheme ensured that the neural network focused more on underrepresented residue types, thereby enhancing its ability to generalize across different residue types.\n\nAdditionally, we utilized five-fold cross-validation to assess the performance of our model. The dataset was randomly split into five sets, and the model was trained and validated on different combinations of these sets. This approach helped in evaluating the model's performance more robustly and in reducing the risk of overfitting to a specific subset of the data.\n\nFurthermore, the neural network was trained for 1000 epochs to ensure convergence, which is a standard practice to allow the model to learn the underlying patterns in the data without overfitting to the training set. The use of stochastic gradient descent with a Nesterov momentum of 0.9 and a batch size of 40,000 also contributed to stabilizing the training process and preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. The training process employed categorical cross-entropy as the loss function and utilized the stochastic gradient descent method for optimization. Specific parameters included a learning rate of 0.01, a Nesterov momentum of 0.9, and a batch size of 40,000. To address the varying abundance of each residue type in the training set, samples were weighted using the formula \\( W_i = \\frac{N_{max}}{N_i} \\), where \\( N_{max} \\) is the maximal number of samples of all 20 residue types, and \\( N_i \\) is the number of samples of residue type \\( i \\). This weighting ensures that the neural network learns more from underrepresented residue types.\n\nThe datasets generated and analyzed during this study are available from the corresponding author upon reasonable request. The article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, provided appropriate credit is given to the original authors and the source. This license allows for the reuse of the information with proper attribution.\n\nThe neural network was constructed using the Keras library, and the training process was conducted for 1000 epochs to ensure convergence. The input features for the neural networks included geometric and structural properties of the residues, such as Cα-Cα distance, backbone dihedrals, relative locations, secondary structures, hydrogen bonds, and solvent accessible surface area. These details are provided to ensure reproducibility and transparency in our methodology.",
  "model/interpretability": "The model developed in this study is not entirely a black box, as it incorporates several interpretable components and design choices that provide insights into its functioning. The neural network architecture consists of a residue probability network and a weight network, both of which process features derived from the geometric and structural properties of residues. These features include basic properties such as Cα-Cα distance, backbone dihedrals, secondary structures, and solvent accessible surface area, which are intuitive and directly related to the physical characteristics of proteins.\n\nThe residue probability network considers the target residue and its neighboring residues, with the number of neighbors (N) ranging from 10 to 30. This setup is analogous to convolution layers in image recognition, where the same network is applied to different regions of the input. The weight network assigns different importance to each neighbor residue, addressing the issue of equal weighting in the residue probability network. This design choice reflects the understanding that some neighbor residues have a more significant impact on the target residue than others.\n\nThe final output of the model is a 20-dimensional softmax layer, which can be interpreted as the probabilities of the 20 different residue types for the target residue. This output provides a clear and interpretable measure of the model's predictions. Additionally, the model's performance is evaluated using metrics such as recall and precision for each amino acid, which offer further insights into its behavior and limitations.\n\nThe amino-acid specific accuracy analysis reveals that the model performs well on residues with distinct structural properties, such as Pro and Gly. This indicates that the model can learn and exploit these structural features effectively. However, the model struggles with less abundant amino acids and hydrophobic residues, highlighting areas where improvements can be made.\n\nIn summary, while the model does involve complex neural network architectures, it is designed with interpretable components and features that provide insights into its predictions. The use of physically meaningful features and the clear interpretation of the output probabilities contribute to the model's transparency.",
  "model/output": "The model is a classification model. It predicts the probabilities of 20 different residue types for a target residue in a protein. The final output layer of the model is a 20-dimensional softmax layer, which provides these probabilities. This setup allows the model to classify the target residue into one of the 20 possible amino acid types. The training process uses categorical cross-entropy as the loss function, which is typical for classification tasks. Additionally, the model's performance is evaluated based on accuracy, which is a common metric for classification models. The output is not a continuous value, but rather a probability distribution over the 20 residue types, further confirming that the model is designed for classification rather than regression.",
  "model/duration": "The execution time for our model was significantly supported by the Supercomputer Center of East China Normal University, which provided the necessary computational resources. The neural network training was performed for 1000 epochs to ensure convergence, which is a substantial number of iterations indicating a thorough training process. However, specific details about the exact duration of the training process are not provided. The training involved datasets with varying sequence identity cutoffs and neighbor residues, which were split into five sets for five-fold cross-validation. This rigorous validation process would have contributed to the overall execution time. Additionally, the use of stochastic gradient descent with a batch size of 40,000 suggests that the training was efficient, but the exact time taken for each epoch or the total training time is not specified.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The method was evaluated using a combination of approaches to ensure its robustness and accuracy. A neural network was trained on a large dataset of protein structures, and its performance was assessed through various metrics.\n\nOne key evaluation involved comparing the network's predictions with those of a previous method called SPIN. This comparison was conducted on a set of 50 proteins, ensuring that the training set for our network did not include these proteins to maintain fairness. The results showed that our network achieved approximately 3% higher sequence identity than SPIN, indicating improved accuracy.\n\nAdditionally, the network's performance was tested on a de novo designed protein, Top7, which was not included in the training set. The top predictions from our network demonstrated higher sequence identities compared to SPIN, further validating the network's effectiveness.\n\nThe evaluation also included a comparison with the position-specific scoring matrix (PSSM) from PSI-BLAST. The root mean square error (RMSE) between the predicted matrices and those from PSI-BLAST was calculated, showing very similar RMSE values for both our network and SPIN. This suggests that our network, trained solely on protein sequences, can achieve comparable performance to methods that incorporate PSSMs.\n\nFurthermore, the network's accuracy was assessed using top-K predictions, where K ranges from 1 to 10. The results indicated that the native amino acid is concentrated in the top predictions, with top-5 and top-10 accuracies being 76.3% and 91.7%, respectively. This high concentration of the native amino acid in the top predictions is crucial for identifying the correct amino acid and improving the overall accuracy of the design.\n\nIn summary, the evaluation methods included comparisons with existing techniques, testing on independent datasets, and assessing the network's ability to predict accurate sequence identities. These evaluations collectively demonstrate the effectiveness and reliability of the deep learning neural network approach for computational protein design.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our neural network in computational protein design. The primary metric reported is the average sequence identity, which measures the percentage of amino acids in the predicted sequence that match the native sequence. This metric was calculated for the top 1 to 10 predictions, providing a comprehensive view of the network's performance.\n\nAdditionally, we compared our network's performance with that of SPIN, another method developed by Zhou and coworkers. This comparison included the average sequence identity on a test set of 50 proteins, where our network demonstrated approximately 3% higher identity than SPIN.\n\nWe also evaluated the recall and precision for each amino acid. Recall represents the percentage of native residues correctly predicted, while precision indicates the percentage of predictions that are accurate. Notably, amino acids like Pro and Gly showed higher recall and precision due to their distinct structural properties.\n\nTo further characterize the amino-acid specific accuracy, we calculated the probability of each native amino acid being predicted as one of the 20 possible amino acids. This information was visualized in a 2D heat map, where the diagonal grids showed higher probabilities, as expected. Interestingly, certain groups of amino acids, such as RK, DN, VI, and FYW, exhibited similar prediction patterns.\n\nMoreover, we assessed the root mean square error (RMSE) of the position-specific scoring matrix (PSSM) predictions compared to those generated by PSI-BLAST. Our network and SPIN showed very similar RMSE values, indicating comparable performance in predicting sequence profiles.\n\nThese metrics collectively provide a robust evaluation of our neural network's performance, aligning with standard practices in the literature for assessing computational protein design methods. The use of sequence identity, recall, precision, and RMSE ensures a comprehensive and representative assessment of the network's accuracy and reliability.",
  "evaluation/comparison": "In the evaluation of our neural network for computational protein design, we performed a comparison with publicly available methods to benchmark our approach. Specifically, we compared our network's performance with SPIN, a method developed by Zhou and coworkers. SPIN was trained on a dataset of 1532 non-redundant proteins and achieved a sequence identity of 30.3% on a test set of 500 proteins. For a fair comparison, we re-trained our network on the SI30N15 dataset, excluding the 50 proteins used in the SPIN evaluation. Our network demonstrated approximately 3% higher sequence identity than SPIN when considering the top 1 to 10 predictions.\n\nAdditionally, we evaluated our method on a de novo designed protein, Top7 (PDB ID 1QYS), which was not included in our training set. The top 1 prediction from SPIN showed an identity of 0.250, while our network's top 1 predictions from different datasets (SI30N15, SI50N15, and SI90N15) had identities of 0.283, 0.304, and 0.402, respectively. This comparison highlights the improved accuracy of our network, particularly when trained on larger datasets.\n\nWe also compared our predictions with the position-specific scoring matrix (PSSM) from PSI-BLAST. The root mean square error (RMSE) values for our network and SPIN were very similar (0.139 for our network and 0.141 for SPIN), indicating that both methods perform comparably in this regard. However, it is important to note that SPIN was trained on PSSMs from PSI-BLAST for predicting sequence profiles, whereas our network was trained on protein sequences only.\n\nIn summary, our neural network shows superior performance compared to SPIN, especially when trained on larger datasets. The comparison with simpler baselines, such as PSSM from PSI-BLAST, further validates the effectiveness of our approach in computational protein design.",
  "evaluation/confidence": "The evaluation of our neural network's performance includes confidence intervals for the accuracy metrics. These intervals are represented as standard deviations in the tables presenting the results. For instance, in Table 1, the accuracy values are accompanied by standard deviations in parentheses, indicating the variability and reliability of the measurements.\n\nStatistical significance is considered in our comparisons. For example, when comparing our network's performance with SPIN, we re-trained our network on a dataset excluding the 50 proteins used for comparison to ensure a fair evaluation. The results show that our network achieves approximately 3% higher sequence identity than SPIN, suggesting a statistically significant improvement.\n\nAdditionally, the five-fold cross-validation method used in our study helps to ensure that the results are robust and not dependent on a particular split of the data. This approach provides a more reliable estimate of the network's performance and its generalizability to new, unseen data.\n\nIn summary, the performance metrics include confidence intervals, and the results demonstrate statistical significance, supporting the claim that our method is superior to others and baselines in the context of computational protein design.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. The publication does discuss datasets and methods used for training and evaluating neural networks, but it does not provide direct links or instructions for accessing the raw evaluation files. The supplementary information, which may include additional details or datasets, is available at a specified DOI, but specific details about the availability of raw evaluation files are not provided. For those interested in the datasets and methods, the supplementary information and the described datasets can be referenced, but direct access to raw evaluation files would require further inquiry or clarification from the authors."
}