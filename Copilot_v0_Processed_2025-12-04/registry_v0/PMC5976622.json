{
  "publication/title": "Distinct Epigenomic Patterns Are Associated with Haploinsufficiency and Predict Risk Genes of Developmental Disorders",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Nature Communications",
  "publication/year": "2018",
  "publication/doi": "10.1038/s41467-018-04552-7",
  "publication/tags": "- Epigenomics\n- Haploinsufficiency\n- Machine Learning\n- Enhancer-Promoter Interactions\n- Developmental Disorders\n- Genomics\n- Bioinformatics\n- Variant Prioritization\n- Chromatin Modifications\n- Gene Regulation\n\nNot sure if the tags provided are the ones used in the publication, but they are a good summary of the topics covered in the paper.",
  "dataset/provenance": "The dataset used in this study was sourced from Ensembl release 75 for gene annotation and transcription start site locations. All genomic coordinates are based on the hg19 human genome assembly. Non-hg19 coordinates were converted using the UCSC LiftOver tool. Gene symbols were converted to Ensembl IDs using annotation tables from Ensembl BioMart.\n\nThe positive training set, consisting of curated haploinsufficient genes, was collected from two sources: previously studied haploinsufficient genes and genes with a haploinsufficient score of 3 in the ClinGen Dosage Sensitivity Map. The negative training set included genes deleted in two or more healthy individuals, based on copy number variations detected in 2026 normal individuals. Only genes with at least half of their length covered by any deletion were considered.\n\nThe raw training set underwent pruning to optimize performance. Non-protein-coding genes and those on sex chromosomes were excluded. Genes with contradictory evidence were removed from both the positive and negative training sets. After pruning, the positive training set contained 287 genes, and the negative training set contained 574 genes. The full list of training genes is available in Supplementary Table 1.\n\nEpigenomic feature data were obtained from the uniformly processed peak calling results of the Roadmap and ENCODE projects. Promoter features included H2A.Z, H3K27me3, H3K4me3, and H3K9ac, with \"GappedPeaks\" used to allow for broad domains of ChIP-seq signal. The assignment of a GappedPeak to a gene followed specific steps, using the transcription start sites of Ensembl canonical transcripts and a defined basal cis-regulatory region around the promoter.\n\nThe results of peak width and the number of interacting enhancers were consolidated into a matrix, with each row representing a gene and each column representing a combination of a tissue and a data type. This matrix served as input for machine learning models.",
  "dataset/splits": "In our study, we employed a robust cross-validation strategy to ensure the reliability of our machine learning models. Specifically, we used 100 runs of tenfold cross-validation. In each run, the data was split into 10 folds, where 9 folds were used for training and 1 fold was used for testing. This process was repeated 100 times to ensure comprehensive validation.\n\nFor each machine learning method, we assessed the performance based on these 100 runs. In each run, 10% of the training genes were randomly selected and left out to form a test set for validation. The remaining 90% of the data were used to train the model. After training, the test set was used to calculate model sensitivity and specificity.\n\nThe training set consisted of 287 positive examples (haploinsufficient genes) and 574 negative examples (haplosufficient genes). These genes were carefully curated and preprocessed to ensure high quality and relevance to our study. The test set, comprising 10% of the training genes in each run, varied slightly due to the random selection process, but on average, it included approximately 86 positive examples and 157 negative examples.\n\nThis cross-validation approach allowed us to thoroughly evaluate the performance of our models and ensure that our results were robust and generalizable. The use of multiple runs and folds helped to mitigate the risk of overfitting and provided a more accurate assessment of model performance.",
  "dataset/redundancy": "In our study, we employed a rigorous approach to ensure the independence and representativeness of our training and test sets. For each machine learning method, we assessed performance using 100 runs of tenfold cross-validation. In each run, 10% of the training genes were randomly selected and set aside to form a test set for validation. This process was repeated 30 times, and the arithmetic mean of the 30 sets of probabilities was taken as the final result. This method ensures that each gene has the opportunity to be in both the training and test sets, providing a comprehensive evaluation of model performance.\n\nTo further enforce independence, we pruned the raw training set to remove any genes with contradictory evidence. For the positive training set, we excluded genes with a haploinsufficiency score of 3 in ClinGen Dosage Sensitivity Map and those with sufficient contradictory evidence. For the negative training set, we removed genes with pLI≥ 0.2 and expected loss-of-function variants greater than 10^-11. After pruning, the positive training set consisted of 287 genes, and the negative training set consisted of 574 genes.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. Our approach of using tenfold cross-validation and multiple runs ensures that our results are robust and generalizable. The pruning process helps to mitigate any potential biases or noise in the data, leading to a more accurate and reliable model. Additionally, our use of epigenomic features from various tissues and data types provides a comprehensive view of gene regulation, which is crucial for predicting haploinsufficiency.",
  "dataset/availability": "The data used in this study is not publicly available in a forum. However, the article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source, a link to the Creative Commons license is provided, and any changes made are indicated. This license applies to the article itself and the images or other third-party material included in it, unless otherwise specified.\n\nThe full list of training genes used in the study is available in Supplementary Table 1. Additionally, there are supplementary data files that provide further details:\n\n* Supplementary Data 1 contains the genes used for random forest model training.\n* Supplementary Data 2 lists human diseases and biological pathways implicated with known haploinsufficient (HIS) genes.\n* Supplementary Data 3 provides the predicted Episcore of all protein-coding genes.\n\nThese supplementary files are available alongside the main article.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and belong to the supervised learning class. Specifically, we employed Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection. These algorithms are widely recognized and have been extensively used in various domains, including bioinformatics.\n\nThe algorithms used are not new; they are standard methods in the field of machine learning. Random Forest is implemented using the R package \"randomForest,\" SVM is implemented using the R package \"e1071,\" and LASSO is implemented using the R package \"glmnet.\" These packages are part of the broader machine learning community and are well-documented.\n\nThe reason these algorithms were not published in a machine-learning journal is that the focus of this study is on their application to a specific biological problem rather than the development of new machine-learning techniques. The primary goal was to predict haploinsufficiency using epigenomic features, and the chosen algorithms were selected for their robustness and effectiveness in handling complex datasets. The performance of these algorithms was evaluated using tenfold cross-validation, and their results were compared to assess their suitability for the task at hand. The study's contributions lie in the application of these methods to biological data and the insights gained from their use, rather than the innovation of new machine-learning algorithms.",
  "optimization/meta": "The model described in the publication does not function as a meta-predictor. Instead, it employs multiple machine learning approaches independently to predict haploinsufficiency. The methods used include Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection. Each of these methods was implemented using specific R packages: \"randomForest\" for Random Forest, \"e1071\" for SVM, and \"glmnet\" for LASSO with an alpha value of 1.\n\nThe performance of each machine learning method was assessed using 100 runs of tenfold cross-validation. In each run, 10% of the training genes were randomly selected to form a test set for validation, ensuring that the training data remained independent from the test data. The remaining data were used to train the model, and the test set was used to calculate model sensitivity and specificity. The results were then used to create ROC curves and calculate AUC values using the R package \"ROCR\".\n\nThe final probabilities of being positive (i.e., probabilities of being haploinsufficient) for all genes were estimated by using all training genes to train the model. This process was repeated 30 times, and the arithmetic mean of the 30 sets of probabilities was taken as the final result. This approach ensures that the training data is independent and that the model's performance is robust and reliable.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several steps to ensure the data was in a suitable format for model training. Initially, epigenomic feature data was preprocessed using uniformly processed peak calling results from the Roadmap and ENCODE projects. For promoter features such as H2A.Z, H3K27me3, H3K4me3, and H3K9ac, \"GappedPeaks\" were utilized to account for broad domains of ChIP-seq signals. These peaks were assigned to genes based on their proximity to the transcription start sites (TSS) of Ensembl canonical transcripts, specifically within a region extending 5 kb upstream and 1 kb downstream of the TSS. This definition aligns with the GREAT tool's basal cis-regulatory region around promoters.\n\nIn cases where multiple GappedPeaks were assigned to a TSS, the closest peak was retained. For genes with multiple TSS and thus multiple assigned GappedPeaks, the longest peak was kept. The width of the retained peak was then used as an epigenomic feature in the machine-learning models. If a gene had no associated GappedPeak, the peak width was recorded as zero.\n\nTo calculate the number of interacting enhancers for each gene, two approaches were employed. The naïve approach involved counting peaks of ChIP-seq signals associated with enhancers, using signals such as H3K4me1, H3K27ac, and DNase I hypersensitivity sites. These counts were recorded separately for each signal type. The counting was performed within the surrounding topologically associating domain (TAD) or within a ±20 kb region around each TSS. For genes with multiple TSS, the largest number of interacting enhancers was retained.\n\nA more advanced approach adapted EpiTensor to infer gene-enhancer relationships. This method used normalized coverage of various ChIP-seq signals, including H3K27ac, H3K27me3, H3K36me3, H3K4me1, H3K4me3, H3K9me3, DNase I, and RNA-seq. Enhancer annotations from the 15-state chromHMM were utilized, and the output of EpiTensor provided a more accurate estimation of interacting enhancers.\n\nThe results of peak width and the number of interacting enhancers were consolidated into a matrix. Each row of this matrix represented a gene, and each column represented a combination of a tissue and a data type, referred to as an epigenomic feature. This matrix served as the input for the machine-learning models, which included Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection. The models were trained and validated using tenfold cross-validation, with performance assessed based on sensitivity and specificity. The final probabilities of being haploinsufficient were estimated by repeating the process multiple times and averaging the results.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the machine learning approach employed. For the Random Forest model, the number of parameters was determined by the number of trees and the maximum depth of the trees, among other hyperparameters. For the Support Vector Machine (SVM) models, the parameters included the regularization parameter and the kernel type. When using LASSO feature selection with SVM, an additional parameter, the alpha value, was set to 1.\n\nThe selection of these parameters was guided by a combination of domain knowledge and empirical performance. For instance, the alpha value in the LASSO feature selection was set to 1 to enforce a strong penalty on the absolute size of the regression coefficients, effectively performing variable selection. The other parameters were tuned using cross-validation techniques to optimize model performance. Specifically, we assessed the performance based on 100 runs of tenfold cross-validation, where in each run, 10% of the training genes were randomly selected and left out to form a test set for validation. This process helped in selecting the optimal parameters that balanced model complexity and generalization performance.",
  "optimization/features": "The input features for our machine learning models were derived from epigenomic data, specifically from the Roadmap and ENCODE projects. These features include various histone modifications and chromatin marks, such as H2A.Z, H3K27me3, H3K4me3, H3K9ac, H3K4me1, H3K27ac, and DNase I hypersensitivity sites. The number of features (f) used as input varies depending on the tissue and data type combinations, referred to as epigenomic features. For example, features like \"H3K4me3 peak width in fetal heart\" represent one such combination.\n\nFeature selection was performed using the LASSO (Least Absolute Shrinkage and Selection Operator) method with an alpha value of 1. This was implemented using the R package \"glmnet\". The feature selection process was conducted using the training set only, ensuring that the model's performance on the test set remained unbiased. This approach helped in identifying the most relevant features for predicting haploinsufficiency, enhancing the model's predictive power.",
  "optimization/fitting": "In our study, we employed several machine learning approaches, including Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection, to predict haploinsufficiency. The number of parameters in our models was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, we implemented a rigorous cross-validation strategy. Specifically, we performed 100 runs of tenfold cross-validation. In each run, 10% of the training genes were randomly selected and left out to form a test set for validation. The remaining data were used to train the model, and the test set was used to calculate model sensitivity and specificity. This process ensured that our models were evaluated on unseen data, helping to rule out overfitting.\n\nTo further validate our models, we used the R package \"ROCR\" to generate ROC curves based on the 100 runs and calculated the Area Under the Curve (AUC) values. The consistent performance across multiple runs and the high AUC values indicated that our models generalized well to unseen data, providing confidence that overfitting was not a significant issue.\n\nAdditionally, we repeated the entire process 30 times and took the arithmetic mean of the 30 sets of probabilities as the final results. This ensemble approach helped to stabilize the predictions and reduce the variance, further ensuring that our models were robust and not underfitting the data.\n\nIn summary, our use of cross-validation, ROC analysis, and ensemble methods helped to rule out both overfitting and underfitting, ensuring that our machine learning models were reliable and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting when applying machine learning models. One of the key methods used was LASSO (Least Absolute Shrinkage and Selection Operator) feature selection in conjunction with Support Vector Machine (SVM). LASSO is a regularization technique that adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. This process helps in reducing the complexity of the model by shrinking some of the coefficients to zero, effectively performing feature selection and thus preventing overfitting.\n\nAdditionally, we utilized tenfold cross-validation to assess the performance of our models. In this process, the data was divided into ten subsets, and the model was trained on nine of these subsets while the remaining subset was used for validation. This procedure was repeated ten times, each time with a different subset held out for validation. The results from these runs were averaged to provide a more robust estimate of the model's performance, further helping to mitigate overfitting.\n\nMoreover, we repeated the entire process of training and validation 30 times to ensure the stability and reliability of our results. By averaging the probabilities of being positive (i.e., probabilities of being haploinsufficient) across these 30 sets, we obtained a more accurate and generalizable final result. This extensive cross-validation and repetition helped in ensuring that our models were not overfitted to the training data and could generalize well to new, unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the main text. Specifically, we detailed the implementation of machine learning approaches, including Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection. For Random Forest, we used the R package \"randomForest\". SVM was implemented using the R package \"e1071\", and LASSO was implemented using the R package \"glmnet\" with an alpha value equal to 1. The performance of these models was assessed based on 100 runs of tenfold cross-validation, where 10% of the training genes were randomly selected for validation in each run.\n\nThe optimization schedule involved training the models on the remaining data and then validating them using the test set to calculate sensitivity and specificity. We used the R package \"ROCR\" to generate ROC curves and calculate AUC values. The entire process was repeated 30 times, and the arithmetic mean of the probabilities from these runs was taken as the final result.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the main text. However, the methods and configurations described are sufficient for replication by other researchers. The article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the reuse of the methods and configurations described in the study.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as we have taken steps to ensure interpretability. We utilized machine learning approaches, including Random Forest, which inherently provides insights into feature importance. The Random Forest model allows us to assess the importance of each feature in predicting haploinsufficiency. Specifically, we obtained the importance values, measured by the mean decrease of the Gini index, for each epigenomic feature. This metric helps us understand which features contribute most significantly to the model's predictions.\n\nFor instance, active promoter and enhancer features such as H3K4me3, H3K9ac, H2A.Z, and Enhancer showed higher importance compared to repressive promoter features like H3K27me3. This indicates that the model places greater emphasis on features associated with active regulatory elements, which is biologically plausible and aligns with our understanding of gene regulation.\n\nBy examining the importance of these features, we can gain a clearer understanding of the biological mechanisms underlying haploinsufficiency. This transparency is crucial for validating the model's predictions and ensuring that they are grounded in biological reality. Additionally, the use of cross-validation and the calculation of AUC values provide further insights into the model's performance and reliability.",
  "model/output": "The model employed in this study is a classification model. It is designed to predict haploinsufficiency, which is a binary outcome—either a gene is haploinsufficient or it is not. The model uses various machine learning approaches, including Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection, to classify genes based on their epigenetic features.\n\nThe performance of these models was evaluated using 100 runs of tenfold cross-validation. In each run, 10% of the training genes were randomly selected to form a test set for validation, while the remaining data were used to train the model. The test set was then used to calculate model sensitivity and specificity. The results were visualized using ROC curves, and AUC values were calculated to assess the model's performance.\n\nAdditionally, the model estimates the probabilities of genes being haploinsufficient. This process was repeated 30 times, and the arithmetic mean of the 30 sets of probabilities was taken as the final result. This approach ensures that the model's predictions are robust and reliable.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several rigorous steps to ensure its robustness and accuracy. Multiple machine learning approaches were applied, including Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection. For each method, performance was assessed using 100 runs of tenfold cross-validation. In each run, 10% of the training genes were randomly selected to form a test set for validation, while the remaining data were used to train the model. The test set was then used to calculate model sensitivity and specificity. The Receiver Operating Characteristic (ROC) curve was generated based on these 100 runs, and the Area Under the Curve (AUC) values were calculated to quantify the performance.\n\nAdditionally, the whole process was repeated 30 times, and the arithmetic mean of the 30 sets of probabilities was taken as the final result. This approach ensured that the model's predictions were stable and reliable.\n\nTo further evaluate the utility of Episcore in prioritizing genes with only one Loss of Function (LoF) mutation, two independent Congenital Heart Disease (CHD) cohorts were utilized: the Deciphering Developmental Disorders (DDD) consortium CHD and the Pediatric Cardiac Genomics Consortium (PCGC) CHD. Both studies included trios from an earlier CHD study to increase detection power, with duplicates removed to avoid duplication.\n\nThe evaluation also involved comparing Episcore with other metrics in variant prioritization. This comparison was based on the enrichment of de novo LoF variants, the estimated number of true positives, and precision. The enrichment of de novo LoF variants was calculated for sets of top-ranked genes, and these values were plotted and compared. Similarly, the number of true positives and precision were calculated and plotted for any set of top-ranked genes. The true positive count was used as a proxy for recall, given that the total number of true positives is generally unknown but constant.\n\nOverall, the evaluation method was comprehensive, involving multiple machine learning techniques, extensive cross-validation, and comparisons with other metrics to ensure the reliability and effectiveness of the Episcore method.",
  "evaluation/measure": "In the evaluation of our machine learning models, we employed several performance metrics to comprehensively assess their effectiveness. The primary metrics reported include sensitivity and specificity, which were calculated using a tenfold cross-validation approach repeated 100 times. This method ensured robustness and reliability in our performance estimates.\n\nTo visualize the performance, we generated Receiver Operating Characteristic (ROC) curves using the R package \"ROCR.\" The Area Under the Curve (AUC) values derived from these ROC curves provided a single scalar value that summarized the model's ability to distinguish between positive and negative classes. Both mean and median AUC values were reported to give a clear picture of the model's performance across different runs.\n\nAdditionally, we calculated the probabilities of genes being haploinsufficient (HIS) using all training genes. This process was repeated 30 times, and the arithmetic mean of the probabilities was taken as the final result. This approach helped in understanding the model's confidence in its predictions.\n\nFor variant prioritization, we used metrics such as enrichment of de novo Loss-of-Function (LoF) variants, the number of true positives, and precision. Enrichment was calculated as the ratio of observed to expected de novo LoF variants in a set of genes. The number of true positives was determined by comparing the observed de novo LoF variants to the expected number based on background mutation rates. Precision, or the positive predictive value, was calculated as the ratio of true positives to the total observed de novo LoF variants.\n\nThese metrics are representative of standard practices in the field, ensuring that our evaluation is comparable to other studies. The use of ROC curves and AUC values is a well-established method for assessing binary classifiers. Similarly, enrichment, true positives, and precision are commonly used metrics in genetic studies for evaluating the performance of variant prioritization tools. This set of metrics provides a thorough evaluation of our models' performance and their applicability in real-world scenarios.",
  "evaluation/comparison": "In the evaluation of our methods, we performed a comprehensive comparison with publicly available metrics and simpler baselines to benchmark the performance of our approach. We used several machine learning techniques, including Random Forest, Support Vector Machine (SVM), and SVM with LASSO feature selection, to predict haploinsufficiency. The performance of these methods was assessed using 100 runs of tenfold cross-validation, ensuring a robust evaluation.\n\nTo compare our results with existing metrics, we used Episcore and evaluated it against other metrics such as pLI (Probability of Loss of Function Intolerance) in variant prioritization. We employed two main approaches for this comparison. First, we calculated the enrichment of de novo likely gene-disrupting (LGD) variants for sets of top-ranked genes. This involved determining the number of expected de novo LGD variants for each gene and comparing the observed number in the top-ranked gene sets. Second, we estimated the number of true positives and precision for these top-ranked gene sets. These metrics provided a clear comparison of how well Episcore and other metrics performed in identifying genes with de novo LGD variants.\n\nAdditionally, we benchmarked Episcore against simpler baselines and other publicly available methods. For instance, we compared Episcore with pLI scores, which are widely used in the field. The results showed that Episcore and pLI scores displayed overall concordance, with Episcore achieving high performance metrics. We also compared Episcore with other metrics like Shet and heart expression levels, excluding known haploinsufficient genes used in training. These comparisons demonstrated that Episcore achieved better performance than mutation intolerance-based metrics.\n\nIn summary, our evaluation included a thorough comparison with publicly available methods and simpler baselines, ensuring that Episcore's performance was rigorously benchmarked against established metrics.",
  "evaluation/confidence": "The evaluation of our method, Episcore, includes several performance metrics that are accompanied by confidence intervals to assess their reliability. For instance, the enrichment burden between Episcore and other metrics like pLI is shown with 95% confidence intervals calculated based on the Poisson distribution. This provides a statistical measure of the uncertainty around our estimates.\n\nIn terms of statistical significance, our results demonstrate that Episcore achieves better performance than mutation intolerance-based metrics. This is evident in comparisons where Episcore outperforms other methods in variant prioritization tasks. The use of 100 runs of tenfold cross-validation for each machine learning method ensures that our performance assessments are robust and not due to random chance. The ROC curves generated from these runs show high AUC values, indicating strong discriminative power.\n\nAdditionally, the precision and recall metrics, along with the number of true positives, are calculated and compared for different sets of top-ranked genes. These comparisons are plotted and analyzed to show the superior performance of Episcore. The statistical significance of these results is further supported by the consistent performance across multiple studies and datasets, including whole exome sequencing studies on CHD and various developmental disorders.\n\nOverall, the confidence intervals and statistical analyses provide a solid foundation for claiming that Episcore is a superior method for predicting haploinsufficiency and prioritizing variants.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, supplementary data related to the study is provided. This includes lists of genes used for model training, human diseases and biological pathways implicated with known haploinsufficient genes, and predicted Episcore values for all protein-coding genes. These supplementary files can be accessed and used under a Creative Commons Attribution 4.0 International License, which allows for use, sharing, adaptation, distribution, and reproduction, provided that appropriate credit is given to the original authors and the source. The license also requires a link to the Creative Commons license and an indication if changes were made. For any material not covered by this license, permission must be obtained directly from the copyright holder."
}