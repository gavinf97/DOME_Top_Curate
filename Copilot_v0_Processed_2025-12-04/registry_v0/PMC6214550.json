{
  "publication/title": "Predicting bacterial growth conditions from mRNA and protein abundances",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "PLOS ONE",
  "publication/year": "2018",
  "publication/doi": "10.1371/journal.pone.0206634",
  "publication/tags": "- Machine Learning\n- Gene Expression\n- Bacterial Growth Conditions\n- Environmental Biosensors\n- Support Vector Machines\n- Random Forest Models\n- Multi-class Classification\n- Data Normalization\n- Principal Component Analysis\n- Model Validation",
  "dataset/provenance": "The dataset used in our study was previously generated and consists of whole-genome E. coli (strain REL606) mRNA and protein abundances. These measurements were taken under 34 different conditions, resulting in a total of 155 samples. Among these, mRNA abundances are available for 152 samples, and protein abundances for 105 samples. Notably, 102 samples contain both mRNA and protein abundance data.\n\nThe experimental conditions were created by systematically varying four parameters: carbon source, growth phase, sodium concentration, and magnesium concentration. For simplicity, these conditions were grouped into 16 categories. The carbon sources used were glucose, glycerol, gluconate, and lactate. Sodium concentrations were categorized as base and high, while magnesium concentrations were labeled as low, base, and high. Samples were collected at multiple time points over a two-week interval, covering exponential phase, stationary phase, and late stationary phase.\n\nThe dataset has been previously published and is available for further use by the community. The raw Illumina read data and processed files of read counts per gene can be accessed from the NCBI GEO database under accession numbers GSE67402 and GSE94117. Additionally, mass spectrometry proteomics data are available via PRIDE with accession numbers PXD002140 and PXD005721. This availability ensures that other researchers can replicate and build upon our findings.",
  "dataset/splits": "The dataset was divided into two main subsets: the training/validation set and the test set, using an 80:20 split. This division was done semi-randomly to preserve the ratios of different conditions between the training/validation and the test subsets. The training/validation set was further divided into separate training and validation datasets using a 75:25 split. This procedure was repeated 10 times to generate 10 independent pairs of training and validation datasets. The test set retained the condition labels but discarded the sample labels. The entire procedure, including the data splits and model training, was repeated 60 times for each separate analysis.",
  "dataset/redundancy": "The datasets were split into training/validation and test sets using an 80:20 ratio. This division was done semi-randomly to preserve the ratios of different conditions between the two subsets, ensuring that the training and test sets are independent. This approach helps to maintain the representativeness of each condition in both sets, which is crucial for the robustness of the machine learning models.\n\nTo enforce independence between the training and test sets, condition labels were retained in the training/validation data for supervised learning, while sample labels were discarded for the test set. This method ensures that the models are trained on one set of data and evaluated on a completely separate set, reducing the risk of overfitting and providing a more accurate assessment of model performance.\n\nThe distribution of the datasets in this study is designed to be more representative of real-world scenarios compared to some previously published machine learning datasets. By preserving the ratios of different conditions and ensuring independence between training and test sets, the study aims to create a more reliable and generalizable model. This approach is particularly important in biological studies where the conditions and samples can vary significantly, and ensuring that the model can generalize well to new, unseen data is crucial.",
  "dataset/availability": "The data used in this study is publicly available. All processed data and analysis scripts can be accessed on GitHub. The permanent archived version of the data is available via Zenodo. Additionally, mRNA and protein abundances have been previously published. Raw Illumina read data and processed files of read counts per gene are available from the NCBI GEO database. Mass spectrometry proteomics data can be found via PRIDE. The data splits used for training, validation, and testing were created semi-randomly while preserving the ratios of individual conditions. This ensures that the data splits are reproducible and can be verified by others using the provided scripts and datasets. The data is made available under licenses that allow for public access and reuse, facilitating transparency and reproducibility in research.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the class of supervised learning algorithms. Specifically, we employed Support Vector Machines (SVMs) with three different kernels—linear, radial, and sigmoidal—and random forest models. These algorithms are well-established in the field of machine learning and have been extensively used in various applications.\n\nThe algorithms used are not new; they are standard techniques in the machine learning community. The choice of these algorithms was driven by their proven effectiveness in handling classification tasks and their ability to capture complex patterns in the data. The SVM models, particularly those with radial and sigmoidal kernels, are known for their robustness in high-dimensional spaces, while random forest models are valued for their ability to handle large datasets and provide feature importance rankings.\n\nThe decision to use these established algorithms rather than developing a new one was based on the goal of demonstrating the feasibility and limitations of using machine learning on gene expression data to predict environmental features. The focus was on applying well-understood methods to ensure the reliability and interpretability of the results. Additionally, the use of standard algorithms allows for easier replication and comparison with other studies in the field.\n\nThe algorithms were implemented using widely-used R packages: e1071 for SVM models and randomForest for random forest models. These packages provide efficient and reliable implementations of the algorithms, making them suitable for our analysis. The choice of these packages was driven by their popularity and the extensive documentation and community support available.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a single machine learning algorithm to make predictions. The machine learning algorithms used in this study include Support Vector Machines (SVMs) with different kernels (radial, sigmoidal, and linear) and random forest classification. These algorithms were trained and tuned independently to predict the growth conditions from mRNA and protein abundances.\n\nThe training process involved dividing the data into training and validation datasets multiple times to optimize hyperparameters. This was done to ensure that the model's performance was robust and that the chosen hyperparameters were the best for the given task. The final model was then tested on an independent test dataset to evaluate its predictive accuracy.\n\nThe independence of the training data is maintained through the use of a semi-random splitting method that preserves the ratios of different conditions between the training/validation and test subsets. This ensures that the model is trained on a representative sample of the data and that the test set remains independent for unbiased evaluation. The entire procedure, including data splitting, training, and testing, was repeated multiple times to collect statistics on model performance and to ensure the reliability of the results.",
  "optimization/encoding": "The data encoding and preprocessing involved several steps to prepare the samples for machine learning analysis. Initially, the data was split into training/validation and test datasets using a semi-random approach that preserved class balances. This ensured that the ratios of different conditions were maintained between the training/validation and test subsets.\n\nTechnical replicates were added, and abundances were normalized by size factors calculated via DeSeq2. A variance stabilizing transformation was then applied to stabilize the variance across the mean-abundance levels. This transformation is crucial for making the data more suitable for downstream analyses, such as principal component analysis (PCA).\n\nBatch effects were removed using frozen Surrogate Variable Analysis (fSVA), which can correct for batch effects in both the training and test data without knowing the labels of the test data. This step is essential for ensuring that the results are not confounded by technical variations.\n\nPCA was then used to define the principal axes of the training/validation set. The test dataset was rotated with respect to these axes, and the top 10 most significant axes in the training/validation dataset were selected for learning and prediction. This dimensionality reduction helps in focusing on the most informative features of the data.\n\nThe entire procedure, from data splitting to dimensionality reduction, was repeated 60 times for each separate analysis to ensure the robustness of the findings. This repetition allowed for the collection of statistics on model performance and the assessment of the overall robustness of the results.",
  "optimization/parameters": "In our study, we optimized different sets of parameters depending on the machine learning model used. For the Support Vector Machine (SVM) models, we focused on tuning the \"cost\" parameter for all three kernel types (linear, radial, and sigmoidal). Additionally, for the SVM models with radial and sigmoidal kernels, we also tuned the \"gamma\" parameter. This resulted in two parameters being optimized for these SVM models.\n\nFor the random forest algorithm, we optimized three parameters: \"mtry\", \"ntrees\", and \"nodesize\". The \"mtry\" parameter refers to the number of variables randomly sampled as candidates at each split, \"ntrees\" is the number of trees to grow, and \"nodesize\" is the minimum size of terminal nodes.\n\nThe selection of these parameters was based on their known influence on model performance. The \"cost\" parameter in SVM controls the trade-off between achieving a low training error and a low testing error, while the \"gamma\" parameter defines how far the influence of a single training example reaches. In random forests, \"mtry\" affects the diversity of the trees, \"ntrees\" influences the stability and accuracy of the model, and \"nodesize\" controls the depth of the trees.\n\nWe generated a parameter grid for the tuning process and optimized these parameters to achieve the highest average F1 score across multiple validation datasets. The parameter combination that yielded the best performance was then used for predictions on the test dataset. This approach ensured that our models were well-tuned and capable of generalizing to new, unseen data.",
  "optimization/features": "The input features for our machine learning models were derived from gene expression data, specifically mRNA and protein abundances. We utilized the top 10 principal components from Principal Component Analysis (PCA) for learning and prediction. This dimensionality reduction step helped in identifying the most significant axes in the training/validation dataset, which were then used for model training and tuning.\n\nFeature selection was implicitly performed through the PCA process, as it focuses on the principal components that capture the most variance in the data. This approach ensures that the most informative features are retained while reducing the dimensionality of the dataset. The PCA was applied to the training/validation set, and the test data was rotated with respect to these principal axes, ensuring that the feature selection process was done using the training set only. This method helps in maintaining the integrity of the test set and prevents data leakage, which could otherwise bias the model's performance.",
  "optimization/fitting": "The fitting method involved training and tuning four different machine learning models: SVM models with linear, radial, and sigmoidal kernels, and random forest models. To address the potential issue of overfitting, especially given the complexity of the models and the relatively small number of training points, several strategies were employed.\n\nFirst, the training/validation data was divided into 10 independent pairs of training and validation datasets using a 75:25 split. This process was repeated 10 times to generate multiple pairs, ensuring that the models were trained and validated on different subsets of the data. This approach helped to mitigate overfitting by providing a more robust evaluation of model performance across different data splits.\n\nAdditionally, a parameter grid was generated for the tuning process, and hyperparameters were optimized using a grid search approach. The \"cost\" parameter was tuned for all SVM models, while the \"gamma\" parameter was tuned for the SVM models with radial and sigmoidal kernels. For the random forest algorithm, three parameters—\"mtry\", \"ntrees\", and \"nodesize\"—were optimized. The parameter combination with the highest average F1 score across all validation datasets was selected as the winning combination, ensuring that the models were not overfitting to any single validation set.\n\nTo further rule out overfitting, class weight normalization was applied during training, where class weights were inversely proportional to the corresponding number of training samples. This normalization was calculated independently for each training run, helping to balance the influence of different classes and prevent the model from being biased towards more frequent classes.\n\nThe models were trained on all 10 training datasets and made predictions on the 10 validation datasets. Macro-F1 scores were calculated for each model parameter setting for each validation dataset, and these scores were averaged over all validation datasets to obtain an average performance score for each algorithm and parameter combination. This averaging process helped to ensure that the models were not overfitting to any single validation set.\n\nMoreover, the entire training and prediction process was repeated 60 times to collect statistics on model performance. This extensive repetition helped to provide a comprehensive evaluation of model performance and to ensure that the models were not overfitting to any specific training or validation set.\n\nTo address underfitting, the models were evaluated using a multi-class macro F1 score, which jointly assesses precision and recall. This scoring scheme provided a comprehensive evaluation of model performance and helped to ensure that the models were not underfitting by failing to capture the complexity of the data. The use of different machine learning algorithms, including SVM and random forest models, also helped to ensure that the models were robust and not underfitting due to the choice of a particular algorithm.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method was the use of cross-validation. We divided our training/validation data into 10 independent pairs of training and validation datasets, repeating this procedure 10 times. This approach helped to ensure that our models were not overfitting to any particular subset of the data.\n\nAdditionally, we used class weight normalization during training. This technique adjusts the weights of different classes inversely proportional to their frequency in the training data, which helps to balance the influence of each class and prevents the model from being biased towards the majority class.\n\nWe also applied a grid search for hyperparameter tuning. By optimizing parameters such as \"cost\" and \"gamma\" for SVM models and \"mtry\", \"ntrees\", and \"nodesize\" for random forest models, we aimed to find the best parameter combinations that generalize well to unseen data.\n\nFurthermore, we utilized principal component analysis (PCA) to reduce the dimensionality of our data. By selecting the top 10 most significant principal components, we focused on the most informative features, which helps in reducing noise and preventing overfitting.\n\nAnother important technique we used was frozen Surrogate Variable Analysis (fSVA) to remove batch effects. This method corrects for batch effects in both the training and test data without knowing the labels of the test data, ensuring that our models are not influenced by technical variations.\n\nOverall, these techniques collectively helped to mitigate overfitting and enhance the generalizability of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we optimized the \"cost\" parameter for all three SVM models and the \"gamma\" parameter for the SVM models with radial and sigmoidal kernels. For the random forest algorithm, we optimized three parameters: \"mtry\", \"ntrees\", and \"nodesize\". The tuning process involved generating a parameter grid and repeating the procedure 10 times to generate 10 independent pairs of training and validation datasets. The parameter combination with the highest average F1 score was considered the winning parameter combination and was subsequently used for prediction on the test dataset.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, all processed data and analysis scripts are available on GitHub. The repository can be accessed at https://github.com/umutcaglar/ecoli_multiple_growth_conditions. A permanent archived version is also available via Zenodo with the DOI 10.5281/zenodo.1294110. The data and scripts are made available under a license that allows for reuse and further analysis, ensuring that other researchers can reproduce our findings and build upon our work.",
  "model/interpretability": "The models used in this study include Support Vector Machines (SVMs) with different kernels (linear, radial, and sigmoidal) and random forest models. Among these, the linear kernel SVM models are particularly notable for their interpretability. Unlike black-box models, linear kernel SVMs provide clear and interpretable outputs. This interpretability allows for the identification of the most important features contributing to the model's predictions. The linear kernel SVM models return coefficients that indicate the significance of each feature, making it possible to determine which genes or proteins are most influential in predicting bacterial growth conditions. This transparency is advantageous for future work, as it enables researchers to understand the underlying biological mechanisms driving the predictions. The other models, such as radial and sigmoidal kernel SVMs and random forest models, are more complex and less interpretable due to their non-linear nature and the way they combine features. However, the performance differences between these models and the linear kernel SVM were minor, suggesting that the choice of model does not significantly impact the overall accuracy of the classification task.",
  "model/output": "The model employed in this study is a classification model. Specifically, it is designed to predict a four-dimensional vector of categorical variables defining various growth conditions of bacterial samples. These conditions include growth phase, carbon source, magnesium concentration, and sodium concentration. The model uses machine learning algorithms, such as Support Vector Machines (SVMs) with different kernels (radial, sigmoidal, and linear) and random forest classification, to make these predictions. The performance of the model is evaluated using the multi-class macro-F1 score, which provides a comprehensive measure of prediction accuracy by considering both precision and recall across all classes. The model's output is a set of predictions for the growth conditions based on the input gene expression data, either from mRNA or protein abundances.",
  "model/duration": "The execution time for our model involved several stages, each contributing to the overall duration. Initially, data preparation steps such as normalization and variance stabilizing transformation were performed. Following this, the data was divided into training/validation and test sets using an 80:20 split, ensuring the preservation of condition ratios. Batch effects were removed using frozen Surrogate Variable Analysis (fSVA), and Principal Component Analysis (PCA) was applied to define the principal axes of the training/validation set. The test data was then rotated with respect to these axes.\n\nModel training and tuning were conducted using four different machine learning algorithms: Support Vector Machines (SVM) with linear, radial, and sigmoidal kernels, and random forest models. The tuning process involved optimizing hyperparameters through a grid search approach, repeated 10 times to ensure robustness. This entire procedure, from data preparation to model tuning and prediction, was repeated 60 times to collect comprehensive statistics on model performance.\n\nGiven the complexity and iterative nature of these steps, the total execution time was substantial. However, specific timings were not explicitly measured or reported in the publication. The focus was on ensuring the reliability and accuracy of the models rather than optimizing for speed. The computational resources and time required would depend on the specific hardware and software environment used for the analysis.",
  "model/availability": "The source code for the analysis scripts used in this study is publicly available on GitHub. The repository can be accessed at https://github.com/umutcaglar/ecoli_multiple_growth_conditions. For those who prefer a permanent archived version, it is available via Zenodo with the DOI 10.5281/zenodo.1294110. The code is released under a license that allows for its use and modification, facilitating reproducibility and further research.\n\nAdditionally, the processed data used in the analysis is available through public databases. mRNA and protein abundance data have been previously published and can be accessed from the NCBI GEO database with accession numbers GSE67402 and GSE94117. Mass spectrometry proteomics data are available via the PRIDE database with accession numbers PXD002140 and PXD005721. These resources ensure that the data and methods used in this study are accessible to the scientific community for verification and further exploration.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive approach to ensure the robustness and accuracy of the models. The data was initially divided into training/validation and test sets using an 80:20 split, preserving the ratios of different conditions between these subsets. This division was done semi-randomly to maintain the integrity of the condition labels in the training/validation data, while the sample labels for the test set were discarded.\n\nTo address batch effects, frozen Surrogate Variable Analysis (fSVA) was applied to both the training/validation and test datasets. This step was crucial for correcting batch effects without knowing the labels of the test data. Following fSVA, Principal Component Analysis (PCA) was used to define the principal axes of the training/validation set, and the test dataset was rotated with respect to these axes. The top 10 most significant axes from the training/validation dataset were selected for learning and prediction.\n\nThe study utilized four different machine learning algorithms: Support Vector Machines (SVM) with linear, radial, and sigmoidal kernels, and random forest models. These algorithms were trained and tuned using the dimension-reduced training/validation dataset. The entire procedure, from data preparation to model tuning, was repeated 60 times for each separate analysis to collect statistics on model performance.\n\nFor model tuning, the training/validation dataset was further divided into separate training and validation datasets using a 75:25 split. This process was repeated 10 times to generate 10 independent pairs of training and validation datasets. A parameter grid was generated for the tuning process, optimizing parameters such as \"cost\" for SVM models and \"mtry,\" \"ntrees,\" and \"nodesize\" for random forest models. The parameter combination with the highest average F1 score was selected as the winning combination and used for prediction on the test dataset.\n\nThe performance of the models was evaluated using the multi-class macro-F1 score, which jointly assesses precision and recall. This score was chosen because it provides an intuitive scale ranging from 0 to 1, with 1 representing perfect accuracy. The multi-class macro-F1 score averages individual F1 scores over all conditions, giving each condition equal weight.\n\nAdditionally, the models were validated against independently published external data. This external dataset consisted of measurements for approximately 2,000 proteins, which is substantially less than the 4,196 proteins measured and used to construct the models. The external dataset also used a different bacterial strain, which may have affected the direct comparability of the proteins. Two approaches were tested for applying the model to the external data: filling missing values with median values from the in-house data and restricting the training dataset to include only proteins that appeared in the external validation data. Both approaches yielded comparable results.\n\nIn summary, the evaluation method involved a rigorous process of data splitting, batch effect correction, dimensionality reduction, model training and tuning, and performance assessment using the multi-class macro-F1 score. The models were also validated against external data to ensure their robustness and generalizability.",
  "evaluation/measure": "In our study, we primarily used the multi-class macro F1 score to evaluate the performance of our models. This metric is particularly useful for multi-class classification problems, as it provides an intuitive scale ranging from 0 to 1, with 1 representing perfect accuracy. The macro F1 score averages the F1 scores of individual conditions, giving each condition equal weight rather than each sample. This approach ensures that conditions with fewer samples do not disproportionately influence the overall score.\n\nThe F1 score is a harmonic mean of precision and recall, making it a balanced measure that considers both false positives and false negatives. We chose this metric because it is comprehensive and intuitive, providing a clear indication of model performance across different conditions.\n\nWhile the F1 score is our primary metric, we also recognize the importance of other evaluation methods. Confusion matrices offer a detailed view of model performance, showing the true positives, true negatives, false positives, and false negatives for each class. These matrices provide a rich source of information but are often collapsed into a single number for convenience. We acknowledge that using other scoring schemes, such as multi-class AUROC, could alter model fits and reported accuracies, but we believe the differences would be minor.\n\nOur choice of performance metrics is representative of current practices in the field. The F1 score is widely used in machine learning for its ability to balance precision and recall, making it a reliable indicator of model performance. By focusing on the macro F1 score, we ensure that our evaluation is fair and comprehensive, accounting for all possible errors across different conditions.",
  "evaluation/comparison": "In our study, we evaluated different machine learning models to ensure the robustness of our results and to determine if model choice had a substantial impact on classification accuracy. We compared the performance of three Support Vector Machine (SVM) models—with linear, radial, and sigmoidal kernels—and random forest models. The SVM models with radial and sigmoidal kernels were set to use the c-classification algorithm.\n\nThroughout the tuning stage of our pipeline, we recorded which model and hyperparameter set had the best macro-F1 score for the validation set. We found that the SVM model with a radial kernel outperformed the other models when fit to mRNA data, while the random forest model outperformed the others when fit to protein data. This observation was consistent across 60 independent runs.\n\nWhen comparing the F1 scores for model predictions applied to the test set, we noted that the performance of the SVM models was virtually identical for mRNA data, with the random forest model performing slightly worse. For protein data, the performance was slightly worse overall compared to mRNA data, which may be partially attributed to the fewer samples available in the protein dataset.\n\nThe differences between all models were minor, indicating that the accuracy of our classification task is robust to different assumptions. This finding suggests that while specific models may perform slightly better under certain conditions, the overall performance is consistent across different machine learning approaches. The linear kernel SVM models, in particular, return interpretable output, making them a preferred choice for future work in this space due to their ability to determine the most important features.",
  "evaluation/confidence": "The evaluation of our models focused on the multi-class macro F1 score, which provides an intuitive scale for assessing prediction accuracy. This score ranges from 0 to 1, with 1 representing perfect accuracy. The use of this metric allows for a comprehensive evaluation of our models' performance across multiple conditions simultaneously.\n\nTo ensure the robustness of our results, we repeated the entire procedure 60 times for each separate analysis. This repetition helps in understanding the variability and consistency of our models' performance. However, specific confidence intervals for the performance metrics were not explicitly provided in the main text. The repetition of the procedure across 60 independent runs does imply a form of statistical robustness, as it allows for the observation of performance variability.\n\nThe models were compared based on their average performance over these runs. For instance, the SVM with a radial kernel was the best-performing model for mRNA data, winning 55 out of 60 runs. This indicates a statistically significant advantage over other models in this specific context. Similarly, the SVM with a sigmoidal kernel performed best for protein data, winning 41 out of 60 runs. These results suggest that the choice of model can have a substantial impact on classification accuracy, but the differences were minor overall, indicating that the accuracy of our classification task is robust to different assumptions.\n\nThe use of confusion matrices further enriches the evaluation by providing detailed insights into the specific conditions that were accurately or erroneously predicted. This granularity helps in understanding where the models excel and where they may need improvement. The average accuracy across the diagonal of these matrices provides a clear indication of the models' performance, with the multi-class macro F1 score serving as a summary statistic.\n\nIn summary, while explicit confidence intervals are not provided, the repetition of the procedure and the detailed evaluation through confusion matrices and F1 scores offer a strong basis for claiming the superiority and robustness of our methods. The statistical significance of our findings is supported by the consistent performance of the best-performing models across multiple runs.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, the processed data and analysis scripts used for the evaluation are publicly available on GitHub. The permanent archived version of this data can be accessed via Zenodo. Additionally, the mRNA and protein abundance data used in the study have been previously published and are available from the NCBI GEO database and the PRIDE database, respectively. These datasets can be accessed using the provided accession numbers. The data is made available under the terms specified by these repositories, which typically allow for academic use and sharing with proper citation."
}