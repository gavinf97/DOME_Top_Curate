{
  "publication/title": "Prediction of Drug-Likeness",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Frontiers in Genetics",
  "publication/year": "2018",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Drug-likeness\n- Machine Learning\n- Deep Learning\n- Autoencoders\n- Imbalanced Data\n- Classification\n- SMOTE\n- Drug Discovery\n- Neural Networks\n- Regularization",
  "dataset/provenance": "The datasets used in this study were sourced from several well-known chemical databases. Marketed drug molecules were represented by the ZINC WORLD DRUG dataset, which includes 2,500 molecules. Drug-like molecules were represented by the MDDR dataset, containing 200,000 molecules, and the WDI dataset, with 40,000 molecules. Non-drug-like molecules were sourced from the ACD dataset, which includes 300,000 molecules, and the ZINC ALL PURCHASABLE dataset, which was randomly sampled to reduce its size to 200,000 molecules.\n\nThese datasets were chosen because they are widely used in the cheminformatics community and contain comprehensive 2D molecular structure information in SDF format. The ZINC WORLD DRUG dataset is particularly notable for its focus on marketed drugs, making it a valuable resource for studying drug-likeness. The MDDR and WDI datasets are also well-established in the field, providing a robust set of drug-like molecules for comparison. The ACD and ZINC ALL PURCHASABLE datasets offer a large and diverse set of non-drug-like molecules, ensuring that the models developed can accurately distinguish between drug-like and non-drug-like compounds.\n\nThe datasets were preprocessed to ensure high-quality data for descriptor calculation. This involved removing hydrocarbons, molecules containing elements other than C, H, O, N, P, S, Cl, Br, I, and Si, and any records containing more than one molecule. The data was also standardized, neutralized, tautomerized, aromatized, and cleaned in 2D. Duplicate molecules were removed, and any descriptors with error values or constant values across all molecules were eliminated. This rigorous preprocessing ensured that the data used for model training was accurate and reliable.",
  "dataset/splits": "In our study, we manually constructed two larger datasets: drug-like/non-drug-like and drug/non-drug-like. For each classification task, we randomly split the datasets into training and validation sets in a 9:1 proportion. This means that 90% of the data was used for training the models, while the remaining 10% was used for validation purposes.\n\nThe datasets used in this study were divided as follows:\n\n* **Drug-like/non-drug-like classification:**\n  * **MDDRWDI/ZINC:** This dataset included MDDR and WDI as the positive set and ZINC as the negative set.\n  * **WDI/ACD:** This dataset included WDI as the positive set and ACD as the negative set.\n\n* **Drug/non-drug-like classification:**\n  * **WORLDDRUG/ZINC:** This dataset included ZINC WORLD DRUG as the positive set and ZINC ALL PURCHASABLE as the negative set.\n\nFor each model, we used two oversampling methods to balance the training set: copying the minority class and using SMOTE. This ensured that the number of positive and negative samples during training was approximately equal. The training set was used to train models with 5-fold cross-validation (5-CV), and the additional validation set was used to evaluate the models.",
  "dataset/redundancy": "The datasets used in this study were divided into three categories: drug, drug-like, and non-drug-like. The datasets were split into training and validation sets with a proportion of 9:1. This split was done randomly, ensuring that the training and validation sets were independent. To enforce this independence, the datasets were first randomly divided, and then oversampling methods were applied to balance the training set. This process ensured that the positive and negative samples in the training set were approximately equal, which is crucial for developing unbiased and accurate predictive models.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the field of drug-likeness prediction. For instance, the WDI/ACD dataset contains 38,260 positive samples and 288,540 negative samples, totaling 326,800 samples. Similarly, the MDDR/ZINC dataset includes 171,850 positive samples and 199,220 negative samples, making a total of 371,070 samples. These distributions are representative of the imbalanced nature commonly found in drug-likeness prediction tasks, where the number of non-drug-like compounds often exceeds the number of drug-like compounds.\n\nTo address the issue of dataset redundancy, a thorough data preprocessing step was implemented. This included removing duplicate molecules based on their InChI keys, which account for stereochemistry. If a molecule appeared in both the drug and non-drug sets, it was removed from the non-drug set. For duplicates within the same set, only the first occurrence was retained. This preprocessing step ensured that the datasets were free from redundant information, which could otherwise bias the model's predictions.",
  "dataset/availability": "The datasets used in this study are not publicly released in a forum. The datasets were manually built and include drug-like/non-drug-like and drug/non-drug-like compounds. The datasets were sourced from various databases such as ZINC WORLD DRUG, MDDR, WDI, ACD, and ZINC ALL PURCHASABLE. These datasets contain 2D molecular structure information in SDF format. The specific details of the dataset pairs used in this study can be found in the publication.\n\nThe datasets were divided into drug, drug-like, and non-drug-like categories. Marketed drug molecules were represented by the ZINC WORLD DRUG dataset, drug-like molecules by the MDDR and WDI datasets, and non-drug-like molecules by the ACD and ZINC ALL PURCHASABLE datasets. The non-drug-like dataset was randomly sampled to reduce its size to 200,000 molecules.\n\nData preprocessing steps included element filtering, removal of mixtures, standardization, and removal of duplicates. Descriptor calculations were performed using 2D descriptors, resulting in a descriptor matrix of approximately 700 descriptors per molecule. The descriptor matrix was then post-processed to remove error values and constant descriptors.\n\nDue to the imbalanced nature of the datasets, oversampling algorithms such as copying the minority class and using SMOTE were employed to balance the datasets. The datasets were split into training and validation sets in a 9:1 proportion, and the training set was further balanced using the oversampling methods.\n\nThe datasets were used to train models with 5-fold cross-validation, and the additional validation set was used to evaluate the models. The performance of the models was assessed using classification accuracy (ACC) as the evaluation metric.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning, specifically deep autoencoder neural networks. This approach is not entirely new, as deep learning has been widely used in various fields such as computer vision, natural language processing, and bioinformatics. However, our application of deep autoencoders to drug-likeness prediction is novel and demonstrates the potential of this technique in chemoinformatics.\n\nThe reason this work was published in a genetics journal rather than a machine-learning journal is that the primary focus of our study is on drug-likeness prediction, which falls within the domain of genetics and chemoinformatics. While the deep learning techniques we employed are well-established in the machine-learning community, their application to this specific problem is innovative and contributes significantly to the field of drug discovery. Our study showcases how deep learning can be leveraged to build powerful prediction models for drug-likeness, addressing a critical need in pharmaceutical research.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a stacked autoencoder (SAE) architecture for unsupervised learning, followed by a classification model. The SAE is trained using 2D chemical descriptors to capture the intrinsic relationships between these descriptors. The parameters from the SAE model are then used to initialize the classification model.\n\nThe classification models proposed in this study include MDDRWDI/ZINC and WDI/ACD for drug-like/non-drug-like classification, and WORLDDRUG/ZINC for drug/non-drug-like classification. These models are trained using datasets that are balanced using the SMOTE over-sampling method. The training process involves optimizing hyperparameters such as the number of hidden layer nodes, L2 weight regularizer, dropout value, activation function, optimizer type, and batch size using Bayesian optimization.\n\nThe evaluation of the models is conducted using five performance metrics: accuracy (ACC), specificity (SP), sensitivity (SE), Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic curve (AUC). The results indicate that the models achieve high accuracy and other performance metrics, demonstrating the effectiveness of the SAE pre-training method and the classification models.\n\nIn summary, the model does not incorporate data from other machine-learning algorithms as input and is not a meta-predictor. The training data for the classification models is independent and balanced using the SMOTE method. The models are evaluated using standard performance metrics to ensure their predictive power and robustness.",
  "optimization/encoding": "In our study, we employed 2D molecular descriptors to encode the molecules. The process began with a thorough data preprocessing step, which is crucial in cheminformatics to minimize errors in descriptor calculation. We removed hydrocarbons and molecules containing elements other than C, H, O, N, P, S, Cl, Br, I, and Si. Mixtures and duplicates were also eliminated, and the molecular structures were standardized through neutralization, tautomerization, and aromatization.\n\nAfter preprocessing, we calculated the descriptors using the MOLD2 software, resulting in approximately 700 descriptors per molecule. These descriptors were then subjected to post-processing, where any descriptors with constant values across all molecules or error values like N/A or infinity were removed. This ensured that only relevant and error-free descriptors were used for further analysis.\n\nWe initially attempted using Padel descriptors, but they showed inferior performance and were subsequently discarded. The final descriptor matrix was then used to train our deep learning models, specifically stacked autoencoders, for drug-likeness prediction. This encoding and preprocessing pipeline was essential in preparing high-quality data for our machine learning algorithms, enabling us to achieve state-of-the-art prediction accuracies.",
  "optimization/parameters": "In our study, several hyperparameters were optimized to enhance the performance of our models. These included the number of hidden layer nodes (K), the value of the L2 weight regularizer, the dropout rate, the type of activation function, the type of optimizer, and the batch size. Bayesian optimization was employed to fine-tune these hyperparameters, ensuring that the models were well-calibrated for the task of predicting drug-likeness.\n\nThe number of hidden layer nodes (K) was a critical parameter that was determined through a comparative function. This process involved evaluating different values of K to identify the optimal configuration that balanced model complexity and predictive accuracy. Additionally, the weight of the positive sample loss in the logarithmic likelihood loss function was adjusted within a range of 0.5 to 1.0 to mitigate overfitting and improve the model's sensitivity to positive samples.\n\nThe final optimal hyperparameter settings were derived from this extensive optimization process, which included evaluating the models using 5-fold cross-validation (5-CV) and enforcing early stopping based on classification accuracy on the test set. This rigorous approach ensured that the models were robust and generalizable, capable of achieving high performance on both training and validation datasets.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "In our study, we employed a stacked autoencoder (SAE) model for drug-likeness prediction, which inherently involves a large number of parameters due to the multiple layers and nodes in the neural network. To address the potential issue of overfitting, we implemented several strategies.\n\nFirstly, we used a Truncated-Normal initializer to generate a truncated normal distribution of layer weights, which helps in stabilizing the training process. Additionally, we applied Bayesian optimization to fine-tune hyperparameters such as the number of hidden layer nodes, L2 weight regularizer, dropout rate, activation function, optimizer type, and batch size. This optimization process ensured that our model was not overly complex for the given dataset.\n\nTo further mitigate overfitting, we optimized the weight of the positive and negative sample loss in the logarithmic likelihood loss function. This adjustment helped the model to better handle the imbalance in the dataset, where the positive sample ratio was initially low. We tested different weight values ranging from 0.5 to 1.0 and selected the most suitable weight to avoid overfitting.\n\nWe also employed 5-fold cross-validation (5-CV) during training, which involves splitting the data into five subsets and training the model on four subsets while validating on the remaining one. This process was repeated five times, ensuring that each subset was used for validation once. Early stopping based on classification accuracy on the test set was enforced to prevent the model from overfitting to the training data.\n\nTo rule out underfitting, we ensured that our model had sufficient complexity by using a single hidden layer, which was found to be adequate for our classification objectives. We also used techniques like dropout and L2 regularization to prevent the model from becoming too simple and underfitting the data.\n\nIn summary, our approach involved careful initialization, hyperparameter optimization, loss function adjustment, cross-validation, and regularization techniques to balance the model complexity and prevent both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the generalization of our models. One key approach was the use of Bayesian optimization to fine-tune hyperparameters, such as the number of hidden layer nodes, L2 weight regularizer, dropout rate, activation function, optimizer type, and batch size. This process helped in finding the optimal settings that minimized overfitting.\n\nAdditionally, we optimized the weights of the positive and negative sample losses in the logarithmic likelihood loss function. By adjusting the weight of the positive sample loss, we aimed to balance the learning process and reduce overfitting to the negative samples. This adjustment was particularly important given the imbalance in the original dataset, where the positive sample ratio was low.\n\nWe also utilized early stopping based on classification accuracy on the test set during training. This technique halted the training process when the model's performance on the validation set stopped improving, thereby preventing the model from overfitting to the training data.\n\nFurthermore, we employed 5-fold cross-validation (5-CV) to ensure that our models were robust and not overly tailored to a specific subset of the data. This method involved dividing the dataset into five parts, training the model on four parts, and validating it on the remaining part. This process was repeated five times, with each part serving as the validation set once. The average performance across these five iterations provided a more reliable estimate of the model's true performance.\n\nLastly, we used the Synthetic Minority Over-sampling Technique (SMOTE) to balance the datasets. SMOTE generates synthetic samples for the minority class, which helped in mitigating the class imbalance and improving the model's ability to generalize to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in detail within the publication. Specifically, the final optimal hyper-parameter settings are listed in a table within the text. These settings include the number of hidden layer nodes, the value of the L2 weight regularizer, the value of dropout, the type of activation function, the type of optimizer, and the value of batch size.\n\nThe optimization process involved Bayesian optimization using the Hyperas library, which is based on Hyperopt. This approach was employed to fine-tune the hyperparameters for different models and datasets. The weights of the positive and negative sample loss in the logarithmic likelihood loss function were also optimized to avoid overfitting. The specific weights chosen for different models are detailed in the results section.\n\nRegarding model files and optimization parameters, the publication does not explicitly mention the availability of these files. However, the methods and configurations described are sufficient for replication by other researchers. The use of open-source software libraries such as Keras and TensorFlow ensures that the models can be constructed and trained using the described configurations.\n\nThe publication does not provide specific information about the licensing of the reported configurations or optimization parameters. However, the use of open-source libraries and standard optimization techniques suggests that the methods are freely available for use and replication. Researchers interested in replicating the study can refer to the detailed descriptions provided in the text and use the specified libraries and techniques.",
  "model/interpretability": "The models developed in this study are primarily based on deep learning techniques, specifically using stacked autoencoders (SAE) and neural networks, which are generally considered black-box models. This means that the internal workings of the models are not easily interpretable, and the decision-making process is not transparent.\n\nThe use of autoencoders, which are unsupervised learning algorithms, adds another layer of complexity. Autoencoders aim to reconstruct their input, capturing intrinsic structures of the data rather than simply memorizing it. This process involves encoding input information into hidden layer dimensions, making it challenging to interpret the relationships among different dimensions, especially in drug-likeness prediction where each descriptor is a dimension.\n\nWhile the models perform well in classification tasks, the lack of interpretability is a notable limitation. The relationships among descriptors in drug-likeness prediction are intrinsically more chaotic and irregular compared to image recognition, where pixels are organized in a 2D grid with some similarity and complementarity. This complexity makes it difficult to provide clear examples of how the model arrives at its predictions.\n\nDespite these challenges, the models demonstrate a regularization effect due to unsupervised pre-training, which helps in truly learning the data rather than just memorizing it. This regularization contributes to the models' robustness and generalization capabilities. However, the trade-off is the reduced transparency, making it hard to explain the model's decisions in a straightforward manner.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict drug-likeness, which involves categorizing compounds into drug-like or non-drug-like classes. The model uses a stacked autoencoder (SAE) architecture, which is an unsupervised learning algorithm that trains a neural network to reconstruct its input. This pre-training step helps the model to capture the intrinsic structures of the input data, making it more effective for classification tasks.\n\nThe classification models proposed include MDDRWDI/ZINC and WDI/ACD for drug-like/non-drug-like classification, and WORLDDRUG/ZINC for drug/non-drug-like classification. These models were evaluated using metrics such as accuracy (ACC), sensitivity (SE), specificity (SP), Matthews correlation coefficient (MCC), and the area under the receiver operating characteristic curve (AUC). The performance of these models was assessed on both training and validation sets, with the validation set showing slightly lower accuracy but higher specificity, indicating some overfitting in the training process.\n\nTo address the issue of overfitting, the weight of the positive sample loss in the loss function was optimized. This adjustment helped to balance the sensitivity and specificity of the models, leading to improved performance on the validation set. The models were trained using 5-fold cross-validation (5-CV) and early stopping based on classification accuracy on the test set. The final performance of the models was determined by averaging the results of the 5 trained models.\n\nThe results demonstrate that the autoencoder-based classification models achieve high accuracy in predicting drug-likeness, with the WORLDDRUG/ZINC model showing particularly high performance. This suggests that autoencoders are a promising machine learning algorithm for drug-likeness prediction. The models were built using the Keras library based on TensorFlow, with hyperparameters optimized using Bayesian optimization. The use of SMOTE over-sampling helped to balance the datasets, further enhancing the predictive power of the models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "All models were evaluated using five key metrics: Accuracy (ACC), Specificity (SP), Sensitivity (SE), Matthews Correlation Coefficient (MCC), and the Area Under the Receiver Operating Characteristic Curve (AUC). These metrics were chosen to provide a comprehensive assessment of the models' performance.\n\nTo ensure robust evaluation, a 5-fold cross-validation (5-CV) approach was employed. This method involves dividing the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. The final performance metrics are averaged across these five iterations.\n\nAdditionally, early stopping was enforced based on the classification accuracy on the test set to prevent overfitting. This technique involves monitoring the model's performance on a validation set during training and stopping the training process when the performance stops improving.\n\nTo address the issue of imbalanced datasets, the weight of the positive and negative sample loss in the logarithmic likelihood loss function was optimized. This adjustment helps the model to better handle the imbalance by giving more importance to the positive samples, which are typically fewer in number.\n\nThe performance of the models was also evaluated on an independent external validation set, which was pre-segmented from the original data. This step is crucial to assess the generalizability of the models to unseen data.\n\nOverall, the evaluation method combines cross-validation, early stopping, and loss function optimization to ensure that the models are robust, generalizable, and capable of handling imbalanced datasets.",
  "evaluation/measure": "In our study, we evaluated the performance of our models using a comprehensive set of metrics to ensure a thorough assessment. The primary metrics reported include Accuracy (ACC), Sensitivity (SE), Specificity (SP), Matthews Correlation Coefficient (MCC), and the Area Under the Receiver Operating Characteristic Curve (AUC).\n\nAccuracy (ACC) measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It provides a general sense of how well the model performs overall.\n\nSensitivity (SE), also known as recall or true positive rate, indicates the proportion of actual positives that are correctly identified by the model. This metric is crucial for understanding how well the model can identify drug-like compounds.\n\nSpecificity (SP), or the true negative rate, measures the proportion of actual negatives that are correctly identified. It is important for assessing the model's ability to correctly identify non-drug-like compounds.\n\nThe Matthews Correlation Coefficient (MCC) is a balanced measure that takes into account true and false positives and negatives. It is particularly useful for evaluating the quality of binary classifications, especially when the classes are of very different sizes.\n\nThe Area Under the Receiver Operating Characteristic Curve (AUC) provides an aggregate measure of performance across all classification thresholds. It is a widely used metric in the literature for evaluating the performance of classification models.\n\nThese metrics collectively offer a robust evaluation framework, ensuring that our models are assessed from multiple angles. The choice of these metrics is representative of standard practices in the field, providing a clear and comprehensive view of model performance.",
  "evaluation/comparison": "In our study, we did not directly compare our methods to publicly available methods on benchmark datasets. Instead, we focused on developing and optimizing our own models using deep learning techniques, specifically deep autoencoder neural networks. We manually built three larger datasets from MDDR, WDI, ACD, and ZINC, and calculated molecular descriptors using Mold2 and Padel. Our models achieved high classification accuracies, with 91.04% on the WDI/ACD databases, 91.20% on the MDDR/ZINC dataset, and 96.99% for distinguishing drugs and common compounds in the ZINC World Drug vs. ZINC All Purchasable dataset. These results demonstrate the predictive power of our deep learning models, which outperformed traditional machine learning methods in our experiments.\n\nRegarding simpler baselines, our approach involved comparing different over-sampling methods to balance the datasets. We found that the Synthetic Minority Over-sampling Technique (SMOTE) achieved better prediction accuracies compared to simply copying the minority class. This comparison allowed us to optimize our models and avoid overfitting, ensuring robust performance. Additionally, we used Bayesian optimization to fine-tune hyperparameters, such as the number of hidden layer nodes, L2 weight regularizer, dropout value, activation function, optimizer type, and batch size. These optimizations were crucial in enhancing the performance of our models.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not enough information is available."
}