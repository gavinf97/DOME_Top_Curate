{
  "publication/title": "Machine learning models for early sepsis recognition in the neonatal intensive care unit using readily available electronic health record data.",
  "publication/authors": "The authors who contributed to this article are:\n\n- Aaron J. Masino, who was involved in conceptualization, data curation, formal analysis, funding acquisition, investigation, methodology, software, validation, visualization, and writing the original draft.\n- Mary Catherine Harris, who contributed to conceptualization, funding acquisition, investigation, and writing the original draft.\n- Robert W. Grundmeier, who was involved in conceptualization, funding acquisition, investigation, methodology, project administration, and writing the original draft.\n- Christopher P. Bonafide, who contributed to methodology and writing the review and editing.\n- Fran Balamuth, who contributed to methodology and writing the review and editing.\n- Daniel Forsyth, who was involved in software and writing the review and editing.\n- Svetlana Ostapenko, who contributed to data curation and writing the review and editing.\n- Lakshmi Srinivasan, who was involved in validation and writing the review and editing.\n- Melissa Schmatz, who contributed to data curation and writing the review and editing.\n- Evanette Burrows and Stacey Kleinman were acknowledged for their help in creating the data extraction process and chart review, respectively, but are not listed as authors.",
  "publication/journal": "PLoS ONE",
  "publication/year": "2019",
  "publication/doi": "10.1371/journal.pone.0212665",
  "publication/tags": "- Machine Learning\n- Sepsis Recognition\n- Neonatal Intensive Care Unit\n- Electronic Health Records\n- Predictive Modeling\n- Infant Sepsis\n- Model Performance\n- Cross-Validation\n- Feature Selection\n- Learning Curves",
  "dataset/provenance": "The dataset used in our study was sourced from electronic health records (EHRs) of infants in the neonatal intensive care unit (NICU). The data includes both control samples and sepsis cases, with a total of 375 sepsis case samples and 1,100 control samples. The sepsis cases are further divided into culture-positive and clinically-positive evaluations. The dataset includes demographic information, clinical observations, and vital signs that are routinely collected in most EHRs. This approach ensures that the features used in our models are readily available and generalizable to other institutions. The data was curated and extracted with the help of Mark Ramos and Evanette Burrows, and chart reviews were conducted by Stacey Kleinman to determine sepsis group assignments. The dataset reflects real-world conditions, with case prevalences of 9% and 25.0% for the CPOnly and CP+Clinical datasets, respectively, which are closer to the estimated true incidence rates. This imbalanced dataset presents a significant challenge for machine learning models, but it also ensures that our results are more representative of real-world scenarios. The dataset is part of an ongoing research effort, and we continue to collect data to improve our models.",
  "dataset/splits": "Two data subsets were used for the analysis: CPOnly and CP+Clinical. Both subsets include all controls. The CPOnly subset includes only the culture positive sepsis cases, whereas the CP+Clinical subset contains both the culture positive and the clinically positive cases. The prevalence of cases in the CPOnly dataset was 9%, and in the CP+Clinical dataset, it was 25.0%. The CPOnly dataset likely has fewer cases compared to the CP+Clinical dataset due to the inclusion of clinically positive cases in the latter. The controls dataset consists of 1,100 samples, while the cases dataset consists of 375 samples. The cases dataset includes both culture positive evaluations and clinically positive evaluations. The distribution of data points in each split reflects the prevalence of sepsis cases in the respective datasets. The CP+Clinical dataset is expected to have a higher number of positive cases due to the inclusion of clinically positive evaluations, which may include cases that were treated as sepsis by clinicians but might not have been confirmed by blood culture.",
  "dataset/redundancy": "The datasets used in our study were split into two subsets: CPOnly and CP+Clinical. Both subsets include all controls, but CPOnly includes only culture-positive sepsis cases, while CP+Clinical contains both culture-positive and clinically positive cases. The prevalence of cases in the CPOnly dataset was 9%, and in the CP+Clinical dataset, it was 25.0%.\n\nTo ensure the independence of training and test sets, we employed a nested cross-validation procedure. This method involves an outer loop that splits the data into training and validation sets, and an inner loop that further splits the training data into training and test sets for hyperparameter tuning. This approach helps to prevent data leakage and ensures that the models are evaluated on independent data.\n\nThe distribution of our datasets reflects a significant class imbalance, which is a common challenge in machine learning, especially in medical datasets. The case prevalence in our datasets is closer to the estimated true incidence rates that a real-world model would encounter. This imbalance is more representative of real-world scenarios compared to some previously published machine learning datasets that used more balanced test sets, which can lead to overly optimistic results.\n\nTo address the class imbalance, we used techniques such as balanced class weights in our models and evaluated performance at fixed sensitivity levels. This allowed us to compare models under uniform conditions and assess their ability to handle imbalanced data. Additionally, we used learning curves to evaluate potential model overfitting and bias, ensuring that our models generalize well to new, unseen data.",
  "dataset/availability": "The data used in this study is available within the paper and its supporting information files. These files include the cases data file and the controls data file, which contain the respective samples for sepsis cases and controls. The data is provided in CSV format, with each row representing an individual episode.\n\nThe code used for the analysis is publicly available on GitHub at the following URL: https://github.com/chop-dbhi/sepsis_01. This repository contains all the necessary code to reproduce the results presented in the paper. The code is released under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction, provided that the original author and source are credited.\n\nThe data and code availability ensures that other researchers can replicate the study, validate the findings, and build upon the work. The use of a public repository like GitHub enforces transparency and accessibility, allowing the scientific community to benefit from the research outcomes.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. These include AdaBoost, Gradient Boosting, Gaussian Process, k-nearest neighbors (KNN), Logistic Regression, Naïve Bayes, Random Forest, and Support Vector Machine (SVM) with a radial basis function kernel. These algorithms are part of the Python scikit-learn library, which is a comprehensive toolkit for machine learning in Python.\n\nThe algorithms employed are not new; they have been extensively studied and applied in various domains. The choice of these algorithms was driven by their proven effectiveness in handling classification tasks, particularly in medical diagnostics. The focus of this study is on applying these algorithms to the specific problem of infant sepsis prediction using electronic health record (EHR) data, rather than developing new machine-learning algorithms.\n\nThe decision to use established algorithms is justified by their robustness and the availability of well-documented implementations in scikit-learn. This allows for reproducible research and facilitates the adoption of the models in clinical settings. The study's contributions lie in the application of these algorithms to a novel dataset and the evaluation of their performance in predicting sepsis in infants, rather than in the development of new machine-learning techniques.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure the data was suitable for training and evaluation. Initially, missing values were addressed using mean imputation, where the population mean from the entire dataset was used to replace any missing values. This method was chosen to avoid introducing additional model parameters that could increase the risk of overfitting, especially given the limited size of the training dataset.\n\nFollowing imputation, each continuous-valued feature was normalized to have a zero mean and unit variance. This standardization process is crucial for many machine-learning algorithms, as it ensures that all features contribute equally to the model's performance, regardless of their original scales.\n\nTo control model overfitting, a second feature selection process was implemented as part of the hyper-parameter tuning. This automated feature selection method was based on mutual information, which estimates the dependency between each individual feature and the sepsis class (case or control). Mutual information quantifies the amount of information that can be inferred about one variable based on the observed value of another. For each step in the outer loop of the cross-validation procedure, the top features were selected based on the mutual information estimate from the data in the k-1 data folds used to train the model. A heuristic was employed to determine the number of features, requiring at least 10 samples from each class per feature.\n\nThis preprocessing pipeline ensured that the data was clean, standardized, and relevant for training the machine-learning models, ultimately aiming to differentiate between sepsis-negative and sepsis-positive cases effectively.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varied depending on the specific machine learning algorithm employed. Each model, except for naïve Bayes and Gaussian process, included hyper-parameters that needed to be systematically selected independently of the evaluation data. These hyper-parameters were crucial for controlling potential model overfitting to the training data.\n\nFor instance, logistic regression with L2 regularization required the selection of an inverse regularization parameter. Support vector machines with a radial basis function kernel needed both an inverse regularization parameter and a kernel coefficient. Random forests required the specification of the number of trees, the maximum depth of individual trees, and the split criterion. Gradient boosting models needed the number of estimators and the maximum depth of individual estimators.\n\nThe selection of these hyper-parameters was performed using a grid search over candidate parameters within the inner loop of the nested cross-validation procedure. The inner loop included a (k-1)-fold cross-validation procedure, where each parameter setting was evaluated. The hyper-parameters that yielded the best average cross-validation area under the receiver operating characteristic (AUC) were selected. This process ensured that the model was optimized for the given dataset and reduced the risk of overfitting.\n\nThe specific values tested for each hyper-parameter are detailed in supplementary tables, providing a comprehensive overview of the parameter space explored during the optimization process. This systematic approach allowed us to identify the optimal configuration for each model, enhancing their predictive performance on the held-out test data.",
  "optimization/features": "In our study, the number of input features varied depending on the dataset used. For the CPOnly dataset, which includes controls and culture-positive cases, 11 features were selected. For the CP+Clinical dataset, which includes controls, culture-positive cases, and clinically positive cases, all 35 available features were used.\n\nFeature selection was performed using an automated univariate method based on mutual information. This process was integrated into the nested k-fold cross-validation procedure to ensure that the selection was done using only the training set. For each iteration of the cross-validation, the top features were selected based on their mutual information with the sepsis class, ensuring that the selection process did not incorporate information from the validation set. This approach helped in mitigating overfitting and enhancing the generalizability of the models.",
  "optimization/fitting": "In our study, we employed a nested cross-validation approach to ensure robust model evaluation and parameter tuning. This method helps to mitigate overfitting by providing a more reliable estimate of model performance. The outer loop of the cross-validation procedure reserves one fold for testing, while the inner loop performs feature selection and hyperparameter tuning on the remaining folds. This ensures that the model is evaluated on data it has not seen during training, reducing the risk of overfitting.\n\nTo further control overfitting, we used mean imputation for handling missing values, which does not introduce additional model parameters. We also implemented automated feature selection based on mutual information between each feature and the sepsis class. This selection process helps to retain only the most relevant features, reducing the complexity of the model and the risk of overfitting.\n\nWe evaluated potential model overfitting and bias through learning curve analysis. Learning curves plot a selected performance metric as a function of the number of training samples for both training and validation sets. In the absence of bias and variance, both curves should approach optimum performance as the training set size increases. If the training curve approaches the optimum value but the validation curve does not, it indicates the presence of variance (overfitting). Conversely, if both curves fail to approach the optimum value, it suggests the presence of bias (underfitting).\n\nAdditionally, we compared model performance at fixed sensitivity values, adjusting the decision threshold independently for each model to achieve the desired sensitivity. This approach allows for a fair comparison of model performance across different sensitivity levels.\n\nIn summary, our use of nested cross-validation, mean imputation, automated feature selection, and learning curve analysis helps to rule out overfitting and underfitting, ensuring that our models generalize well to unseen data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and ensure the robustness of our models. One of the primary methods used was L2 regularization, specifically in the logistic regression model. This technique adds a penalty term to the loss function, which helps to constrain the coefficients and prevent the model from fitting the noise in the training data.\n\nAdditionally, we implemented a nested cross-validation approach. This involved an outer evaluation loop and an inner parameter selection and training loop. The inner loop performed automated feature selection and model parameter tuning using a grid search over candidate parameters. This process helped to systematically select hyper-parameters that were independent of the evaluation data, thereby reducing the risk of overfitting.\n\nFurthermore, we utilized learning curve analysis to evaluate potential model overfitting and bias. Learning curves plot a selected performance metric as a function of the number of training samples, providing insights into how the model's performance improves with more data. This analysis helped us identify models that were more resilient to overfitting, such as the logistic regression model, which showed minimal sensitivity to training data sample variance.\n\nIn summary, our approach included L2 regularization, nested cross-validation, and learning curve analysis to mitigate overfitting and ensure that our models generalized well to new data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and reported in detail. Specifically, the hyper-parameters evaluated for each model are provided in a table, which includes the range of values tested for each parameter. This table is part of the supplementary materials accompanying the publication.\n\nThe optimization schedule and model files are not explicitly detailed in the main text, but the methodology section describes the use of a nested cross-validation approach. This approach involves an outer evaluation loop and an inner parameter selection and training loop, ensuring that hyper-parameters are systematically selected independent of the evaluation data.\n\nAll code used in this study, including the implementation of the models and the optimization procedures, is available on a public repository. The repository can be accessed at https://github.com/cho p-dbhi/sepsis_01. This repository contains the scripts and configurations necessary to replicate the experiments and optimize the models as described in the publication.\n\nThe data used for this study is also provided in the supplementary files, labeled as S1 and S2 Files. These files contain the necessary datasets for training and evaluating the models.\n\nRegarding the license, the code and data are made available under terms that allow for reuse and modification, facilitating further research and development in this area. However, specific license details should be checked directly in the repository and supplementary files.",
  "model/interpretability": "The models we employed for predicting sepsis in infants hospitalized in the NICU span a range of interpretability, from relatively transparent to more complex, black-box approaches. Among the models, logistic regression stands out as the most interpretable. It is a linear model, which means that the relationship between the input features and the output (sepsis prediction) is straightforward and easy to understand. The coefficients in a logistic regression model indicate the direction and magnitude of the relationship between each feature and the likelihood of sepsis. For instance, a positive coefficient suggests that an increase in the feature value is associated with a higher probability of sepsis, while a negative coefficient indicates the opposite.\n\nIn contrast, models like gradient boosting, random forest, and support vector machines (SVM) are more complex and can be considered black-box models. These models are non-linear and involve multiple layers of decision-making processes, making it challenging to interpret the exact contribution of each feature to the final prediction. However, techniques such as feature importance scores and partial dependence plots can provide some insights into how these models make predictions.\n\nFor the logistic regression model, we examined features selected for the CPOnly dataset for more than half of the cross-validation iterations. We identified features with a mean magnitude of the logistic regression coefficient greater than a certain threshold, providing a clear example of how specific features influence the model's predictions. This analysis helps in understanding which features are most important in predicting sepsis and how they contribute to the model's decisions.\n\nIn summary, while some of our models are more interpretable than others, logistic regression offers a transparent view of the relationships between input features and sepsis prediction. This transparency is crucial for clinical applications, where understanding the model's decisions can aid in trust and adoption.",
  "model/output": "The model is a classification model designed to predict sepsis in infants hospitalized in the NICU. It generates a numeric score between 0 and 1, which represents the probability of sepsis. This score is then used to classify the input as either \"sepsis positive\" or \"sepsis negative\" based on a selected threshold. The performance of the model is evaluated using metrics such as area under the receiver operating characteristic curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The models considered include AdaBoost, gradient boosting, logistic regression, Naïve Bayes, random forest, and support vector machine (SVM). The logistic regression model, in particular, showed robust performance and was less affected by data sample variance, making it a strong candidate for generalizing to other datasets. The models were evaluated on two datasets: CPOnly, which includes controls and culture-positive cases, and CP+Clinical, which includes controls, culture-positive cases, and clinically positive cases. The performance metrics were computed as the mean over 10 iterations of cross-validation, providing a reliable assessment of the models' predictive capabilities.",
  "model/duration": "Once trained, the prediction models can generate new predictions for a given input quickly using only modest computational resources, such as a single personal computer. This indicates that the execution time for making predictions is efficient and suitable for real-time or near real-time applications. However, the specific time taken for the model to run during the training phase is not detailed. The use of the Python scikit-learn library for automated feature selection, hyper-parameter selection, model training, and model evaluation suggests that the implementation was streamlined and likely optimized for efficiency. The models were evaluated using a nested cross-validation procedure, which involves multiple iterations of training and validation, but the exact duration of this process is not specified.",
  "model/availability": "The source code for the models and algorithms used in this study is publicly available. It can be accessed via a GitHub repository. The repository contains all the necessary code to implement the machine learning models for early sepsis recognition in neonatal intensive care units. The code is released under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction, provided that the original authors and source are credited. This open-access approach ensures that other researchers and institutions can utilize and build upon the work presented in this study.",
  "evaluation/method": "The evaluation method employed in this study involved a nested cross-validation approach to ensure robust and unbiased assessment of the machine learning models. This approach consisted of an outer evaluation loop and an inner parameter selection and training loop. The dataset was divided into k folds, with each iteration of the outer loop reserving one fold for testing while the remaining k-1 folds were used for training and parameter tuning in the inner loop.\n\nIn the inner loop, automated feature selection was performed followed by a grid search over candidate hyper-parameters. The performance of each parameter setting was evaluated using a (k-1)-fold cross-validation procedure. The hyper-parameters that yielded the best average cross-validation area under the receiver operating characteristic (AUC) were selected. The model was then trained on the k-1 folds using the best parameters and evaluated on the held-out fold in the outer loop. This process was repeated k times, resulting in k evaluations of model performance, allowing the model to be tested on every sample.\n\nThe models were evaluated on two data subsets: CPOnly, which included controls and culture-positive sepsis cases, and CP+Clinical, which included controls, culture-positive cases, and clinically positive cases. The prevalence of cases in these datasets was 9% and 25%, respectively. Model performance was compared using the AUC, and the significance of differences in AUC was assessed using Friedman’s rank sum test and Nemenyi’s test for post-hoc analysis.\n\nAdditionally, model performance was evaluated at fixed sensitivity values by adjusting the decision threshold independently for each model. Metrics such as specificity, positive predictive value (PPV), and negative predictive value (NPV) were reported for sensitivity levels of 80%, 90%, and 95%. Learning curve analysis was also conducted to evaluate potential model overfitting and bias.\n\nThe evaluation method ensured that the models were thoroughly tested and validated, providing a comprehensive assessment of their performance in predicting infant sepsis.",
  "evaluation/measure": "In our evaluation, we reported several key performance metrics to assess the effectiveness of our machine learning models for predicting sepsis. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). The AUC provides a comprehensive measure of the model's ability to distinguish between positive and negative cases across all possible threshold values. Sensitivity, also known as recall, measures the proportion of true positive cases correctly identified by the model. Specificity measures the proportion of true negative cases correctly identified. PPV indicates the probability that a positive prediction is a true positive, while NPV indicates the probability that a negative prediction is a true negative.\n\nWe evaluated these metrics at fixed sensitivity levels, adjusting the decision threshold independently for each model to achieve the desired sensitivity. This approach allows for a fair comparison of model performance at specified sensitivity levels, which is crucial in clinical settings where high sensitivity is often prioritized. For each reported performance measure, we obtained 10 observed values through nested cross-validation, reporting the mean value and range for each metric over these observations.\n\nOur set of performance metrics is representative of standard practices in the literature. AUC is widely used as a primary metric for evaluating classification models, particularly in medical diagnostics where the trade-off between sensitivity and specificity is critical. Reporting sensitivity, specificity, PPV, and NPV at fixed sensitivity levels provides a detailed view of model performance, ensuring that the models are evaluated comprehensively. This approach aligns with established methods in the field, making our evaluation robust and comparable to other studies in sepsis prediction.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, we focused on evaluating and comparing the performance of eight different machine learning models specifically tailored for predicting infant sepsis using electronic health record (EHR) data. These models included AdaBoost, Gradient Boosting, Gaussian Process, K-Nearest Neighbors, Logistic Regression, Naïve Bayes, Random Forest, and Support Vector Machine (SVM).\n\nTo ensure a comprehensive evaluation, we compared the models' performance on two distinct datasets: CPOnly, which included controls and culture-positive sepsis cases, and CP+Clinical, which included controls, culture-positive cases, and clinically positive cases. The prevalence of sepsis cases in these datasets was 9% and 25%, respectively, reflecting real-world imbalances.\n\nWe assessed model performance using the Area Under the Receiver Operating Characteristic Curve (AUC) and conducted statistical tests to determine the significance of differences between models. The Friedman rank sum test was used to evaluate the null hypothesis of equal inter-model AUC distributions, which was rejected for both datasets with p-values of less than 0.001. Post-hoc analysis with Nemenyi's test identified statistically significant differences between specific model pairs.\n\nAdditionally, we compared model performance at fixed sensitivity values (80%, 90%, and 95%) by adjusting the decision threshold for each model to achieve the desired sensitivity. We reported the corresponding specificity, positive predictive value (PPV), and negative predictive value (NPV) for each model.\n\nWhile we did not compare our models to simpler baselines in the traditional sense, we did evaluate the performance of various models with different complexities and architectures. For instance, Logistic Regression, which is a relatively simpler model, performed competitively with more complex models like Gradient Boosting and Random Forest. This comparison provided insights into the trade-offs between model complexity and performance.\n\nIn summary, our evaluation focused on comparing the performance of multiple machine learning models on our specific datasets, using robust statistical methods to ensure the reliability of our findings. We did not perform a direct comparison to publicly available methods or simpler baselines on benchmark datasets, but our approach provided a thorough assessment of model performance in the context of infant sepsis prediction.",
  "evaluation/confidence": "The evaluation of the models included a thorough statistical analysis to ensure the robustness and significance of the results. Performance metrics such as the area under the receiver operating characteristic curve (AUC) were computed as the mean over 10 iterations of cross-validation, with the range of values provided to indicate variability. This approach helps in understanding the confidence intervals around the performance metrics.\n\nTo assess the statistical significance of the differences in model performance, the Friedman rank sum test was employed. This test evaluated the null hypothesis that all models have equal AUC distributions over the 10 cross-validation folds. The null hypothesis was rejected with p-values of less than 0.001 for both the CPOnly and CP+Clinical datasets, indicating that there are statistically significant differences among the models.\n\nPost-hoc analysis using Nemenyi's test was conducted to compare pairwise differences between models. For the CPOnly dataset, significant differences were found where AdaBoost had a higher AUC than Gaussian process and k-nearest neighbors, and logistic regression had a higher AUC than k-nearest neighbors. For the CP+Clinical dataset, k-nearest neighbors had a lower AUC than several other models, including gradient boosting, logistic regression, random forest, and support vector machine. These findings provide confidence that the observed performance differences are not due to random chance.\n\nAdditionally, the models were evaluated at fixed sensitivity levels, with performance metrics such as specificity, positive predictive value (PPV), and negative predictive value (NPV) reported. The decision threshold was adjusted independently for each model to achieve the desired sensitivity, ensuring a fair comparison across models. This detailed evaluation process, including statistical testing and cross-validation, supports the claim that the methods are superior to others and baselines.",
  "evaluation/availability": "The raw evaluation files are not directly available. However, all relevant data used in the study are provided within the paper and its supporting information files. These files can be accessed to understand the specifics of the evaluation process. Additionally, the code used for the analysis is publicly available on GitHub under the username chop-dbhi, in the repository named sepsis_01. This code includes implementations of all models used in the study, as well as the procedures for automated feature selection, hyper-parameter selection, model training, and model evaluation. The data and code are made available under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction, provided that the original author and source are credited."
}