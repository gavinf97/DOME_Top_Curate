{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- **Je´roˆme Tubiana**: Contributed to conceptualization, resources, data curation, software, formal analysis, validation, investigation, visualization, methodology, and writing both the original draft and reviews.\n\n- **Simona Cocco**: Contributed to conceptualization, formal analysis, supervision, funding acquisition, validation, investigation, methodology, and writing both the original draft and reviews. Additionally, she was involved in project administration.\n\n- **Re´mi Monasson**: Contributed to conceptualization, formal analysis, supervision, funding acquisition, validation, investigation, visualization, methodology, and writing both the original draft and reviews. He was also involved in project administration.",
  "publication/journal": "eLife",
  "publication/year": "2019",
  "publication/doi": "https://doi.org/10.7554/eLife.39397",
  "publication/tags": "- Computational and Systems Biology\n- Physics of Living Systems\n- Protein Motif\n- Restricted Boltzmann Machine\n- Sequence Analysis\n- Protein Domains\n- Machine Learning\n- Bioinformatics\n- Statistical Modeling\n- Computational Biology",
  "dataset/provenance": "The datasets used in this work were previously published and are publicly available. They include protein structures obtained from the Protein Data Bank. Specifically, the datasets used are:\n\n* The 1.2 Angstrom structure of Kunitz type domain C5, available at https://www.rcsb.org/structure/2KNT.\n* The prototype WW domain, available at https://www.rcsb.org/structure/1e0m.\n* The NMR-RDC / XRAY structure of E. coli HSP70 (DNAK) chaperone complexed with ADP and substrate, available at https://www.rcsb.org/structure/2KHO.\n* The structure showing the allosteric opening of the polypeptide-binding site when an Hsp70 binds ATP, available at https://www.rcsb.org/structure/4JNE.\n* The crystal structure of the catalytic domain of human complement C1S protease, available at https://www.rcsb.org/structure/1ELV.\n* The crystal structure and assembly of TSP36, a metazoan small heat shock protein, available at https://www.rcsb.org/structure/2BOL.\n\nThese datasets were used to train and visualize Restricted Boltzmann Machines (RBMs) for protein motif discovery. The Python package used for this purpose, along with the multiple sequence alignments and code for reproducing the results, is available at https://github.com/jertubiana/ProteinMotifRBM. This package can be readily used for any protein family. The number of data points in each dataset corresponds to the number of sequences in the respective multiple sequence alignments presented in the text.",
  "dataset/splits": "In our study, we employed a cross-validation approach to assess the performance of our Restricted Boltzmann Machine (RBM) models. Specifically, each RBM was trained on a randomly chosen subset of 80% of the sequences in the multiple sequence alignment (MSA). The remaining 20% of the sequences were reserved as a test set to validate the model's predictive power.\n\nThe distribution of data points in each split is as follows: 80% of the sequences were used for training the model, while the remaining 20% were used for testing. This split ensures that the model's performance is evaluated on sequences that were not part of the training process, providing a robust assessment of its generalization capabilities.\n\nNot applicable.",
  "dataset/redundancy": "In our study, we employed a cross-validation approach to ensure the robustness and generalizability of our models. Each Restricted Boltzmann Machine (RBM) was trained on a randomly chosen subset of 80% of the sequences in the multiple sequence alignment (MSA), while the remaining 20% constituted the test set. This split was designed to validate the predictive power of the RBM and assess its performance on unseen data.\n\nThe independence of the training and test sets was crucial for evaluating the model's ability to generalize. To enforce this independence, we ensured that the sequences in the test set were not used during the training phase. This approach helped in preventing data leakage and overfitting, where the model might memorize the training data rather than learning general patterns.\n\nRegarding the distribution of our datasets, they were carefully curated to represent a diverse range of protein families, including the Kunitz domain, WW domain, Hsp70, and lattice proteins. These datasets were chosen to benchmark our approach on both real and in silico data, providing a comprehensive evaluation of the RBM's capabilities.\n\nThe datasets used in our study were split in a manner similar to many previously published machine learning datasets, where a significant portion of the data is reserved for training, and a smaller portion is used for testing. This split ensures that the model's performance can be reliably assessed on data it has not seen during training.\n\nIn summary, our datasets were split into training and test sets to ensure independence and generalizability. The training set comprised 80% of the sequences, while the test set included the remaining 20%. This approach, combined with careful curation of the datasets, allowed us to rigorously evaluate the performance of our RBM models.",
  "dataset/availability": "The data used in this study is publicly available. The Python 2.7 package for training and visualizing Restricted Boltzmann Machines (RBMs), which was employed to obtain the results reported in this work, can be accessed at https://github.com/jertubiana/ProteinMotifRBM. An archived copy of this package is also available at https://github.com/elifesciences-publications/ProteinMotifRBM. This package can be readily used for any protein family.\n\nIn addition to the code, all four multiple sequence alignments presented in the text are included. The code for reproducing each panel, as well as Jupyter notebooks for reproducing most of the figures in the article, are also provided. This ensures that the results can be verified and reproduced by other researchers.\n\nThe following previously published datasets were used in this study:\n\n* The 1.2 Angstrom structure of Kunitz type domain C5, available at https://www.rcsb.org/structure/2KNT.\n* Prototype WW domain, available at https://www.rcsb.org/structure/1e0m.\n* NMR-RDC / XRAY structure of E. coli HSP70 (DNAK) chaperone (1-605) complexed with ADP and substrate, available at https://www.rcsb.org/structure/2KHO.\n* Allosteric opening of the polypeptide-binding site when an Hsp70 binds ATP, available at https://www.rcsb.org/structure/4JNE.\n* Crystal structure of the catalytic domain of human complement C1S protease, available at https://www.rcsb.org/structure/1ELV.\n* Crystal structure and assembly of TSP36, a metazoan small heat shock, available at https://www.rcsb.org/structure/2BOL.\n\nThese datasets are accessible through the Protein Data Bank and are identified by their respective accession numbers. The availability of these resources ensures transparency and reproducibility in the research process.",
  "optimization/algorithm": "The machine-learning algorithm class used is Restricted Boltzmann Machines (RBMs). RBMs are a well-established concept in machine learning, known for their ability to learn complex data distributions through statistical features. They are unsupervised and generative, meaning they can generate new data and do not require annotated sequence data for training.\n\nThe specific RBM algorithm used in this work is not entirely new, as RBMs have been previously developed and applied in various fields. However, the method developed here represents an advancement in training RBMs efficiently from protein sequence data. This includes improvements in handling intensive sampling requirements and the introduction of dynamic reparametrization techniques, which help stabilize learning and prevent numerical instability.\n\nThe focus of this publication is on the application of RBMs to computational biology, particularly in the context of protein sequence analysis. The advancements made in training RBMs are tailored to address the specific challenges and requirements of this domain. Therefore, publishing in a computational biology journal is appropriate, as it highlights the practical applications and biological insights gained from using RBMs in this context. The improvements in the RBM training algorithm are presented as part of the methodological framework used to achieve these biological insights, rather than as a standalone contribution to the field of machine learning.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is a Restricted Boltzmann Machine (RBM) trained on amino-acid sequence data that codes for similar proteins. The RBM learns statistical patterns common to the sequences and infers meaningful patterns related to protein properties. The training process involves stochastic gradient ascent, where the gradient is evaluated using a mini-batch of data and a small number of Markov Chain Monte Carlo (MCMC) configurations. The model parameters are initialized with specific values, and the learning rate decays exponentially over time. The training algorithm also includes dynamic reparametrization for certain potentials to ensure numerical stability and efficient learning. The gauge choice is enforced throughout the training to maintain invariance in the conditional probability. The model's performance is assessed using cross-validation, where it is trained on 80% of the sequences and validated on the remaining 20%. The impact of regularization and the number of hidden units on the model's performance is also evaluated.",
  "optimization/encoding": "In our study, we utilized Restricted Boltzmann Machines (RBMs) to analyze protein sequence data. The data encoding process involved representing protein sequences as vectors on the visible layer of the RBM. Each position in a protein sequence, corresponding to an amino acid, was encoded using one of 21 possible values, representing the 20 standard amino acids plus an additional value for alignment gaps. This encoding allowed the RBM to handle sequences of varying lengths and to account for the presence of gaps in multiple sequence alignments.\n\nThe hidden layer of the RBM consisted of units that took real values, enabling the model to capture complex statistical features of the sequence data. The joint probability distribution of the visible and hidden layers was defined using a weight matrix that coupled these layers, along with local potentials that introduced biases on the visible and hidden units.\n\nTo train the RBM efficiently, we developed a method that involved applying the approach to sequence alignments of 20 different protein families. The sequences were pre-processed by aligning them to ensure that corresponding positions in different sequences were comparable. This alignment step was crucial for identifying conserved patterns and coevolutionary signals across the protein family.\n\nThe RBM was trained on a subset of the sequences, with the remaining sequences reserved for validation. This cross-validation approach allowed us to assess the model's predictive power and to tune hyperparameters such as the number of hidden units and the regularization strength. The regularization strength was particularly important for controlling the sparsity of the weight matrix, which in turn affected the model's ability to generalize to new sequences and to capture biologically meaningful features.\n\nOverall, the data encoding and preprocessing steps were designed to enable the RBM to learn complex distributions from protein sequence data, with a focus on capturing structural, functional, and phylogenetic features that are relevant to the protein family under study.",
  "optimization/parameters": "The model parameters, including the class of potentials, the number of hidden units (M), and regularization penalties, were selected based on the log-probability of a test set of natural sequences not used for training. This approach ensures that the model generalizes well to unseen data. The dReLU potentials, introduced in this work, consistently outperformed other potentials for generative purposes. Increasing the number of hidden units (M) improves the likelihood up to a certain point, after which overfitting begins to occur. Adding sparsifying regularization not only prevents overfitting but also facilitates the biological interpretation of weights. This regularization is an effective way to enhance the correspondence between the representation and phenotypic spaces. It allows the model to operate at a point where most features can be activated across many regions of the sequence space, enabling the generation of a variety of new sequences with high probabilities. The specific values of these parameters were chosen to balance model complexity and generalization performance, ensuring that the model captures relevant features without overfitting to the training data.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this work involves training Restricted Boltzmann Machines (RBMs) on multiple sequence alignments (MSAs) of proteins. The number of parameters in the RBM, such as the weights and potentials, can indeed be much larger than the number of training sequences, especially when the number of hidden units is increased. To address the risk of overfitting, several strategies were implemented.\n\nRegularization was used to prevent overfitting. Specifically, an L1 regularization term was introduced to promote sparsity in the weights. This regularization term encourages many of the weights to be exactly zero, effectively reducing the effective number of parameters and preventing the model from memorizing the training data. The impact of the regularization strength was carefully evaluated, and it was found that mild regularization allowed the RBM to avoid overfitting and maximize the log-probability of the test set. Stronger regularization made the weights much sparser with a minimal decrease in the test set log-probability, indicating that the model was not overfitting.\n\nCross-validation was also employed to assess the model's performance. Each RBM was trained on a randomly chosen subset of 80% of the sequences in the MSA, while the remaining 20% were used as a test set. The average log-probability of the test set was computed to evaluate the model's predictive power for various values of the number of hidden units, regularization strength, and different hidden-unit potentials. This process ensured that the model generalized well to unseen data and did not overfit the training set.\n\nTo rule out underfitting, the number of hidden units was increased systematically. As expected, increasing the number of hidden units allowed the RBM to capture more features in the data distribution, improving performance up to a point. However, beyond a certain number of hidden units, overfitting started to occur, as indicated by the large difference between the log-probabilities of the training and test sets. Therefore, the optimal number of hidden units was determined by balancing the trade-off between underfitting and overfitting.\n\nIn summary, overfitting was ruled out through the use of regularization and cross-validation, while underfitting was addressed by systematically increasing the number of hidden units and evaluating the model's performance on a held-out test set.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the interpretability of our Restricted Boltzmann Machine (RBM) models. Specifically, we used L2 regularization, which is a form of weight decay that penalizes large weights, encouraging the model to find a simpler solution that generalizes better to unseen data.\n\nWe explored different strengths of regularization, denoted as l2. Without regularization (l2 = 0), the model tended to overfit, capturing idiosyncratic features of the training data rather than the invariant properties of sequences across evolutionarily divergent organisms. This resulted in generated sequences that were close to the natural ones but not very diverse.\n\nImposing mild regularization allowed the RBM to avoid overfitting and maximize the log-probability of the test set. However, we found that stronger regularizations had a low impact on the generalization abilities of the RBM, while making the weights much sparser. This sparsity is crucial for driving the RBM into a compositional representation regime, where each hidden unit encodes a limited portion of a sequence, and the representation of a sequence is defined by the set of hidden units with strong inputs.\n\nWe also compared L2 regularization with L1 regularization. While both methods achieved sparse weights, L1 regularization tended to disconnect hidden units entirely when the penalty was too large, making it less robust and requiring more fine-tuning. In contrast, L2 regularization achieved sparse weights without disconnecting hidden units, providing a good compromise between sparsity and generative performance.\n\nIn summary, regularization played a pivotal role in our model, not only preventing overfitting but also facilitating the biological interpretation of weights and enhancing the correspondence between representation and phenotypic spaces.",
  "optimization/config": "The Python 2.7 package used for training and visualizing Restricted Boltzmann Machines (RBMs) in this work is publicly available. This package can be accessed at the GitHub repository https://github.com/jertubiana/ProteinMotifRBM. Additionally, an archived copy of the repository is available at https://github.com/elifesciences-publications/ProteinMotifRBM. The repository includes Jupyter notebooks that facilitate the reproduction of most figures presented in the article.\n\nThe code and resources provided in the repository are designed to be readily usable for any protein family. Furthermore, the repository includes all four multiple sequence alignments discussed in the text, along with the code necessary to reproduce each panel of the figures. This comprehensive availability ensures that other researchers can replicate the results and build upon the methods described in the publication.\n\nThe repository is open-source, allowing researchers to utilize, modify, and distribute the code under the terms specified in the repository's license. This openness promotes transparency and reproducibility in scientific research, enabling the broader scientific community to verify and extend the findings presented in the work.",
  "model/interpretability": "The model we employed, known as Restricted Boltzmann Machines (RBMs), offers a level of interpretability that sets it apart from many other machine learning methods. Unlike traditional black-box models, RBMs can infer statistical patterns from amino-acid sequence data that are meaningful and relatable to known properties of proteins. For instance, the patterns identified by our RBMs could be linked to common structural features such as twists and loops in proteins, as well as specific activities.\n\nThis interpretability is further enhanced by the use of sparsifying regularization, which not only prevents overfitting but also makes the weights of the RBM more interpretable. By imposing regularization, we drive the RBM into a compositional representation regime, where each hidden unit encodes a limited portion of a sequence. This allows the same hidden unit to be recruited in many parts of the sequence space, corresponding to very diverse organisms. As a result, the model can generate a large diversity of sequences through combinatorial choices of the activity states of the hidden units.\n\nFor example, in the Kunitz domain, specific hidden units were found to code for the realization of contacts, demonstrating how the model can capture and represent structural properties. Similarly, in the WW domain, certain hidden units were activated by evolutionary close sequences, illustrating the model's ability to identify specific sequence motifs with structural, functional, or evolutionary meaning.\n\nMoreover, the sparse weights inferred by the RBM make it easier to compare them across different protein families. This allows us to identify representative weights that code for a variety of structural properties, further enhancing the model's interpretability and utility in understanding complex biological data.",
  "model/output": "The model discussed is a Restricted Boltzmann Machine (RBM), which is a type of generative model used for probabilistic modeling. It is not a classification or regression model in the traditional sense. Instead, it is designed to learn the probability distribution of sequences, specifically protein sequences in this context.\n\nThe RBM operates on a bipartite graph with two layers: a visible layer representing protein sequences and a hidden layer representing latent features or representations. The model learns to capture the dependencies between the visible and hidden units, allowing it to generate new sequences and understand the underlying structure of the data.\n\nThe output of the RBM can be interpreted in several ways. Firstly, it provides a probabilistic mapping from sequences to representations, which can be indicative of the phenotype of the corresponding protein. This mapping allows for the reverse process, where representations can be used to generate sequences with desired phenotypic properties.\n\nAdditionally, the RBM's weights and biases can be analyzed to gain insights into the important features of the sequences. For example, the weights can indicate which residues and sites are most influential in the model's representation of the sequence space. This interpretability is enhanced by the use of regularization, which promotes sparsity in the weights and makes the model's representations more biologically meaningful.\n\nThe model's performance is evaluated using the log-probability of held-out test sets, which measures how well the RBM can generalize to new, unseen sequences. The choice of potentials, number of hidden units, and regularization strength are all important factors that affect the model's ability to capture the data distribution and generate diverse, high-probability sequences.\n\nIn summary, the output of the RBM is a probabilistic model of protein sequences that can be used for generative purposes and to gain insights into the important features of the sequences. The model's performance is evaluated using log-probability measures, and its interpretability is enhanced through the use of regularization.",
  "model/duration": "The computational complexity of the model is of the order of M^2 * N^2 * B, where M is the number of hidden units, N is the number of visible units, and B is the batch size. The algorithm scales reasonably to large protein sizes and has been successfully tested for proteins with up to approximately 700 residues. The running time for such large proteins is in the order of 1-2 days on an Intel Xeon Phi processor with 228 cores. The number of hidden units, M, can be chosen to control the computational effort and avoid overfitting, making the model adaptable to proteins of varying sizes and complexities.",
  "model/availability": "The source code for training and visualizing Restricted Boltzmann Machines (RBMs) used in this work is publicly available. The Python 2.7 package, named ProteinMotifRBM, can be accessed at the GitHub repository of the main author. Additionally, an archived copy of the repository is available at the eLife Sciences Publications GitHub account. This package is designed to be readily usable for any protein family.\n\nTo facilitate reproducibility, Jupyter notebooks are provided alongside the code. These notebooks allow users to reproduce most of the figures presented in the article. Furthermore, all four multiple sequence alignments discussed in the text, as well as the code for reproducing each panel, are included in the repository. This comprehensive release of tools and resources aims to support further research and applications in the field.",
  "evaluation/method": "The evaluation of the method involved several approaches to assess its performance and the quality of the generated sequences. For Restricted Boltzmann Machines (RBMs) trained on real protein sequences, where ground-truth fitness is not available, the sequence quality could not be assessed numerically. However, it was shown that the generated sequences, including those with recombined features not found in nature, are consistent with a pairwise model trained on the same data.\n\nThe quality assessment of sequences generated by RBMs trained on specific domains, such as the Kunitz and WW domains, was conducted using scatter plots. These plots compared the number of mutations to the closest natural sequence against the log-probability of a Boltzmann Machine (BM) trained on the same data. The results indicated similar likelihood values for RBM-generated sequences and natural ones, even for unseen combinations.\n\nThe role of regularization and sequence reweighting in sequence generation was also examined. Sequences drawn from unregularized models were found to be closer to the training data, resulting in a sequence distribution with significantly lower entropy. Regularized models, on the other hand, produced a higher number of distinct sequences and exhibited higher entropy. Sequence reweighting played a similar role to regularization, leading to sequences that were slightly further from the training set and a model with higher entropy.\n\nAdditionally, the method was evaluated using lattice proteins (LP), where the ground truth is known. A multiple sequence alignment (MSA) containing sequences with high probabilities of folding into a specific structure was generated. An RBM with hidden units was then learned, and its performance in terms of contact predictions was compared to state-of-the-art methods. The RBM's capability to design new sequences with desired features and high fitness was also quantitatively assessed using conditional sampling. The designed sequences were found to be diverse and have large fitnesses, comparable to those of the MSA.\n\nIn summary, the evaluation of the method involved assessing the quality and diversity of generated sequences, the impact of regularization and sequence reweighting, and comparisons with state-of-the-art methods using known ground truth data.",
  "evaluation/measure": "In our evaluation, we primarily focus on the generative performance and interpretability of our models. To assess generative performance, we use the log-probability of sequences in the test set, which helps us gauge how well our models generalize to unseen data. This metric is crucial for understanding the model's ability to capture the underlying data distribution without overfitting.\n\nFor sequence quality, we employ the probability of folding into the native structure (pnat) for Lattice Proteins. This metric evaluates both the fitness and diversity of generated sequences, ensuring that they are not only functional but also varied and distinct from the training data.\n\nAdditionally, we consider the entropy of the sequence distribution, which measures the diversity of generated sequences. Higher entropy indicates a more diverse set of sequences, which is essential for applications like sequence design.\n\nWe also report the participation ratio as a proxy for weight sparsity. This metric helps us understand how sparse the model's weights are, which is important for interpretability and the compositional representation of sequences.\n\nIn summary, our performance metrics include log-probability for generalization, pnat for sequence quality, entropy for diversity, and participation ratio for weight sparsity. These metrics provide a comprehensive evaluation of our models' generative performance and interpretability, aligning with established practices in the literature.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison with publicly available methods using benchmark datasets. We assessed our Restricted Boltzmann Machine (RBM) model against several alternatives, including Generative Adversarial Networks and Variational Autoencoders, which have been applied to protein sequence data for fitness prediction. Our RBM model differs significantly from these alternatives in several important ways. For instance, our RBM is an extension of direct-coupling approaches and requires fewer hidden units, making it more efficient. It also has a simple architecture with two layers carrying sequences and representations, which allows for the inference of interpretable weights with biological relevance.\n\nWe also compared our model to the Hopfield-Potts framework, which was previously introduced to capture both collective and localized structural modes. Our findings indicate that the Hopfield-Potts model, due to its lack of sparsity regularization and mean-field approximation, is insufficiently accurate for sequence design. In contrast, our RBM model outperforms the Hopfield-Potts framework in terms of sequence generation and contact prediction.\n\nAdditionally, we evaluated the performance of our model against simpler baselines, such as pairwise coupling models. We found that sparse regularization and a high number of hidden units in our RBM model can reproduce the performance of these pairwise models. This demonstrates the robustness and versatility of our approach.\n\nOverall, our comparisons with publicly available methods and simpler baselines highlight the strengths of our RBM model in capturing important functional and structural features of proteins.",
  "evaluation/confidence": "In our evaluation, we employed several methods to ensure the confidence and statistical significance of our results. For instance, we used the Annealed Importance Sampling algorithm to estimate the partition function, which allowed us to handle the high variance in the estimator by constructing a continuous path of interpolating distributions. This approach helped us to obtain more reliable estimates of the partition function ratios, which are crucial for evaluating the model's performance.\n\nAdditionally, we utilized the Persistent Contrastive Divergence algorithm during training, which proved sufficient to learn relevant features and good generative models for small proteins and regularized Restricted Boltzmann Machines. This algorithm's efficiency and accuracy were validated through our training sessions, ensuring that the model parameters evolved slowly and the samples remained at equilibrium.\n\nWe also conducted extensive testing on large protein sizes, successfully scaling our algorithm to proteins with up to approximately 700 amino acids. This testing was performed on an Intel Xeon Phi processor with 228 cores, taking around 1-2 days, which demonstrates the computational feasibility and robustness of our method.\n\nFurthermore, we assessed the quality of the sequences generated by our models. For models trained on real protein sequences, we compared the generated sequences with natural ones using a pairwise model trained on the same data. This comparison showed that the generated sequences, including those with recombined features not found in nature, were consistent with the pairwise model, indicating the reliability of our method.\n\nIn summary, our evaluation methods included rigorous statistical techniques and extensive testing, providing confidence in the performance and superiority of our approach over baselines and other methods. The results are statistically significant, and the performance metrics are supported by reliable estimates and comparisons.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available. However, the Python 2.7 package used for training and visualizing Restricted Boltzmann Machines (RBMs) is publicly available at https://github.com/jertubiana/ProteinMotifRBM. An archived copy is also available at https://github.com/elifesciences-publications/ProteinMotifRBM. This package can be used for any protein family and includes all four multiple sequence alignments presented in the text, as well as the code for reproducing each panel. Jupyter notebooks are provided for reproducing most of the figures in the article. The datasets used in the study are also publicly available and can be accessed through the Protein Data Bank with the following identifiers: 2KNT, 1E0M, 2KHO, 4JNE, 1ELV, and 2BOL. The specific terms of the license under which these resources are available are not detailed, but they are intended for use in reproducing the results and figures presented in the work."
}