{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Stefano Conti and Michael Karplus. Both authors received financial support from the Lawrence Livermore National Laboratory LLC Award and the CHARMM Development Project. Their contributions include the study design, data collection and analysis, decision to publish, and preparation of the manuscript. The funders had no role in these aspects of the work.",
  "publication/journal": "PLOS Computational Biology",
  "publication/year": "2019",
  "publication/doi": "10.1371/journal.pcbi.1006954",
  "publication/tags": "- HIV antibodies\n- Machine learning\n- Neural networks\n- Antibody neutralization\n- Computational biology\n- Molecular modeling\n- IC50 values\n- Breadth prediction\n- Multi-Layer Perceptron\n- HIV-1 envelope protein\n- Glycosylation\n- Antibody-antigen interactions\n- Data-driven modeling\n- Experimental validation\n- Computational descriptors\n- Antibody engineering\n- Viral escape\n- Broadly neutralizing antibodies\n- HIV vaccine design\n- Computational immunology",
  "dataset/provenance": "The dataset used in this study is sourced from the CATNAP database, which is accessible at http://hiv.lanl.gov/catnap. This database contains experimental data relevant to the study of HIV antibodies.\n\nThe total number of experimental values in the dataset is 6179. Of these, 3089 are randomly selected and used to train the model. The dataset includes experimental data for various antibodies, with each antibody having a specific number of experimental values available. For instance, the antibody VRC01 has 609 experimental values, while b12 has 690. The dataset is designed to include a diverse range of antibodies, ensuring that the model can generalize well across different types.\n\nThe experimental data in the dataset have been used to train a model that predicts the neutralization breadth of HIV antibodies. The model's performance is evaluated using both training and test sets, with correlation coefficients indicating the strength of the relationship between experimental and computed values. The dataset has been utilized to develop a neural network model that can accurately predict the breadth of known antibodies, showing significant improvement over the use of individual descriptors. The model's ability to generalize is further supported by the inclusion of experimental data from each antibody under study, ensuring that the predictions are robust and reliable.",
  "dataset/splits": "The dataset used in this study consists of a total of 6179 experimental values. These values were split into two main sets: a training set and a validation set. The training set contains 3089 randomly selected experimental values, which is approximately 50% of the total dataset. The remaining values are used for validation. This split ensures that all antibodies are represented in the training set, maintaining the same percentages of binders and non-binders as in the entire dataset. The ratio between the number of experimental values in the training set (3089) and the number of parameters (200) is more than 15, which helps in reducing the chance of overfitting. Additionally, to account for variability in the model's outputs due to different random splits and seeds, 30 independently generated neural networks were averaged. Each of these networks used a new splitting of the training and validation set and a new random seed. This approach helps in providing a more robust and reliable estimation of the model's performance.",
  "dataset/redundancy": "The dataset used in this study consists of 6179 experimental values, which were randomly split into training and test sets. Specifically, 3089 values were used for training the model, while the remaining values were used for validation. This split ensures that the training and test sets are independent, reducing the risk of data leakage and overfitting.\n\nTo enforce independence between the training and test sets, a random selection process was employed. This process ensures that each experimental value has an equal chance of being included in either the training or test set. Additionally, the ratio between the number of experimental values in the training set (3089) and the number of parameters (200) is more than 15, which further decreases the chance of overfitting.\n\nThe composition of the full dataset and a randomly generated set containing 50% of the values was compared to ensure that all antibodies are represented in the training set. The percentages of binders and non-binders in the training set match those in the entire dataset, maintaining the distribution and representativeness of the data.\n\nThis approach aligns with best practices in machine learning, where the goal is to create a model that generalizes well to unseen data. By ensuring that the training and test sets are independent and representative of the full dataset, the model's performance and reliability are enhanced.",
  "dataset/availability": "The data used in this study is publicly available. All experimental data can be accessed from the CATNAP database, which is hosted at http://hiv.lanl.gov/catnap. This database provides a comprehensive collection of experimental IC50 values and related information.\n\nThe data is distributed under the terms of the Creative Commons Attribution License. This license allows for unrestricted use, distribution, and reproduction in any medium, provided that the original author and source are credited. This ensures that the data can be freely accessed and utilized by the scientific community for further research and validation.\n\nThe availability of the data in a public forum promotes transparency and reproducibility. Researchers can independently verify the findings and build upon the work presented in this study. The use of a public database also facilitates collaboration and sharing of insights across different research groups.\n\nThe data splits used for training and validation were generated randomly, and the process was repeated multiple times to ensure robustness. This approach helps in mitigating the risk of overfitting and provides a more reliable estimate of the model's performance. The specific splits used in the study are not explicitly detailed, but the methodology ensures that the data is divided in a manner that represents the overall dataset accurately.",
  "optimization/algorithm": "The machine-learning algorithm class used is a Multi-Layer Perceptron (MLP), which is a type of artificial neural network. This algorithm is not new; it is a well-established technique in the field of machine learning. The MLP used in this work is specifically designed to estimate the breadth of HIV antibodies through molecular modeling and machine learning techniques.\n\nThe choice to use an MLP was driven by its ability to accurately reproduce and predict the breadth of CD4bs targeting HIV antibodies. The development of this model involved training the neural network with a large number of experimental IC50 values, which significantly outnumbered the parameters in the neural network. This approach reduced the possibility of overfitting and ensured robust performance.\n\nThe MLP was compared with other machine learning techniques, including k-nearest neighbors (kNN), random forests (RF), and support vector machines (SVM). All methods produced very similar results, indicating the robustness of the MLP approach. The decision to use an MLP was further supported by its ability to handle the complexity of the data and provide accurate predictions.\n\nThe MLP was implemented using the scikit-learn Python module, which is a widely used library for machine learning. The use of established libraries and techniques ensured that the results were reproducible and reliable. The focus of this work was on the application of machine learning to a specific biological problem, rather than the development of a new machine-learning algorithm. Therefore, publishing in a computational biology journal was appropriate, as it aligned with the primary goals of the research.",
  "optimization/meta": "The model described in this work does not function as a meta-predictor. Instead, it relies on a single Multi-Layer Perceptron (MLP) neural network to predict the neutralization breadth of all antibodies under study. The training set includes some experimental data from each antibody, which is crucial for the model's generalization ability.\n\nTo ensure the robustness of the neural network, various machine learning methods were compared, including k-nearest neighbors (kNN), random forests (RF), and support vector machines (SVM). However, these methods were used for comparison purposes rather than as components of a meta-predictor. The MLP was found to perform similarly to these other techniques, indicating the reliability of the results obtained with the MLP.\n\nThe training data used for the MLP includes a significant number of experimental IC50 values, which helps to reduce the risk of overfitting. The data is split into training and validation sets multiple times with different random seeds to ensure that the results are not dependent on a specific split. This approach helps to verify that the training data is independent and that the model's performance is consistent across different data splits.",
  "optimization/encoding": "The data encoding process involved selecting and normalizing descriptors for use in the neural network. Initially, 24 descriptors were considered, categorized into atomic descriptors, protein/protein scoring functions, protein stability scoring functions, and entropy models. To maintain computational efficiency, descriptors that were computationally expensive or redundant were removed. Specifically, OpenMM energy-based terms and FoldX were excluded due to their high computational cost, and Prodigy was removed as it is a linear combination of other descriptors. This reduction from 24 to 19 descriptors did not compromise the model's accuracy.\n\nAll selected descriptors were normalized to have zero mean and unit variance using a standard scaler. This normalization is crucial for ensuring that the neural network can effectively learn from the data without being biased by the scale of individual descriptors.\n\nThe descriptors were further refined by sequentially removing the highest correlated descriptor, based on the cross-correlation matrix. This process continued until only one descriptor remained, ensuring that the most informative and least redundant features were used in the final model. The descriptors that were deleted at each step, along with their correlation coefficients, were documented for transparency.\n\nThe final set of 19 descriptors was used as input for the Multi-Layer Perceptron (MLP) regressor and classifier. The MLP regressor predicts IC50 values, while the MLP classifier determines whether the IC50 is above or below a 1 μg/ml cutoff, which is used to calculate antibody breadth. Both models were trained using experimental IC50 values from the CATNAP database, with the data split randomly into training and validation sets to ensure robust performance.",
  "optimization/parameters": "The model utilizes a Multi-Layer Perceptron (MLP) regressor and classifier, each with a hidden layer composed of ten nodes. The total number of parameters, denoted as Np, is determined by the formula Np = Ni * Nh + Nh * No, where Ni represents the number of input descriptors, Nh is the number of hidden nodes, and No is the number of outputs. In this case, 19 descriptors are used as inputs, and the output is the predicted IC50 value. Therefore, the total number of parameters to fit is 200.\n\nThe selection of the number of parameters was guided by an empirical rule that the ratio between the available experimental values used in training and the number of parameters to fit should be much greater than one. For the MLP regressor, 3864 exact IC50 values were split randomly into a training set and a validation set, resulting in 1932 experimental values used for training. This number is approximately 9 times higher than the number of parameters to fit, ensuring a robust training process. For the MLP classifier, 3089 experimental values were used for training, resulting in a ratio of more than 15 between the number of experimental values and the number of parameters, further decreasing the chance of overfitting.",
  "optimization/features": "The input features for the neural network models consist of descriptors obtained from an atomistic model of the antibody/antigen bound system. Initially, 24 descriptors were tested, which fall into four classes: atomic descriptors, protein/protein scoring functions, protein stability scoring functions, and entropy models.\n\nTo select which descriptors to use, a feature selection process was performed. The primary criterion was to maintain a fast scoring function, avoiding computationally expensive terms like OpenMM energy-based terms and FoldX. Removing these four descriptors did not decrease accuracy but significantly reduced the required time by a factor of 12.8. Additionally, the descriptor Prodigy was removed as it is a linear combination of other descriptors, with no observed effect on accuracy. This reduction decreased the number of descriptors from 24 to 19 without any loss in accuracy.\n\nThe feature selection process involved analyzing the cross-correlation matrix of all descriptors to identify and remove the highest correlated descriptor sequentially. The descriptor with the lowest correlation with the experimental IC50 values was removed in each step. This process continued until only one descriptor remained. The descriptors were normalized to zero mean and unit variance using a standard scaler.\n\nThe final model uses 19 descriptors as input features. The feature selection was performed using the training set only, ensuring that the model's performance is not biased by the validation or test sets. This approach helps in maintaining the model's generalization ability and robustness.",
  "optimization/fitting": "In our study, the number of parameters in the neural network is not much larger than the number of training points. Specifically, the total number of parameters to fit in the Multi-Layer Perceptron (MLP) is 200. For the MLP regressor, 3864 exact IC50 values are split randomly into a training set and a validation set, resulting in 1932 experimental values used for training. For the MLP classifier, 3089 experimental values are used for training. In both cases, the number of experimental values used for training is significantly higher than the number of parameters, with ratios of approximately 9 and 15, respectively. This ensures that the model is not underfitted.\n\nTo address the potential issue of overfitting, we employed several strategies. Firstly, we ensured that the ratio between the number of experimental values in the training set and the number of parameters was much greater than one. This helps to decrease the chance of overfitting. Additionally, we averaged the results over 30 independently generated neural networks, each using a new splitting of the training and validation set and a new random seed. This approach helps to mitigate the variability in the model's outputs due to different random splits and seeds, providing a more robust estimation.\n\nFurthermore, we compared the performance of our MLP models with other machine learning methods, including k-nearest neighbors (kNN), random forests (RF), and support vector machines (SVM). The similar results obtained with these different methods support the robustness of our findings and suggest that the model is not overfitting to the training data. The use of a standard scaler to normalize all descriptors to zero mean and unit variance also helps to ensure that the model generalizes well to new data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting. Firstly, the ratio between the number of experimental values used for training and the number of parameters in the model was kept significantly high. Specifically, for the Multi-Layer Perceptron (MLP) classifier, 3089 experimental values were used to train a model with 200 parameters, resulting in a ratio of more than 15. This high ratio helps to reduce the likelihood of overfitting.\n\nAdditionally, we averaged the results over 30 independently generated neural networks. Each network was trained using a new splitting of the training and validation set and a new random seed. This approach helps to mitigate the variability introduced by different random splits and seeds, providing a more robust estimate of the model's performance.\n\nFurthermore, we compared the performance of our MLP model with other machine learning techniques, such as k-nearest neighbors (kNN), random forests (RF), and support vector machines (SVM). The similar results obtained with these different methods support the robustness of our findings and indicate that the model is not overly tailored to the specific characteristics of the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the manuscript and its supporting information files. These include specifics about the Multi-Layer Perceptron (MLP) regressor and classifier, such as the number of hidden nodes, activation functions, and solvers used. The total number of parameters to fit in the MLP is also provided, along with the rationale behind the chosen configurations.\n\nThe experimental data used for training and validation are available from the CATNAP database. This database contains all the experimental IC50 values and other relevant data necessary for reproducing our results. The scripts and computed data used in our analysis are included within the manuscript and its supporting information files, ensuring that the methodology and results can be replicated by other researchers.\n\nThe manuscript is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. This license ensures that the configurations, parameters, and data are accessible to the scientific community for further research and validation.\n\nThe funding sources for this work are also disclosed, with no role played by the funders in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. This transparency ensures that the reported configurations and optimization parameters are independent and reliable.",
  "model/interpretability": "The model employed in this study is not entirely a black box, as it utilizes a Multi-Layer Perceptron (MLP) regressor and classifier, which are types of artificial neural networks. While neural networks are often considered complex and less interpretable than simpler models, several aspects of our approach provide transparency.\n\nFirstly, the descriptors used as inputs to the neural network are well-defined and fall into distinct categories: atomic descriptors, protein/protein scoring functions, protein stability scoring functions, and entropy models. This categorization helps in understanding the types of features that influence the model's predictions.\n\nSecondly, the process of descriptor selection involves a systematic reduction of the full model by successively removing the descriptor that is most correlated with others and least correlated with the experimental IC50 values. This method highlights the importance of certain descriptors and provides insights into which features are most critical for the model's performance. For instance, protein folding propensity descriptors (FoldX) and statistical pairwise potentials for estimating protein stability (RFHA and RFCB) are identified as particularly important. Additionally, entropy from elastic network models (ENM_EXP and ENM_R6) plays a significant role, underscoring the importance of entropy in binding.\n\nFurthermore, the model's generalization ability is examined by training it with varying amounts of experimental data for each antibody. This analysis shows how the predicted neutralization breadth changes with the inclusion of more data, providing a clear example of the model's behavior and its dependence on the training data.\n\nThe correlation coefficients between the computed and experimental breadth at different stages of data inclusion also offer transparency. For example, the Pearson correlation coefficient improves significantly when even a small number of experimental values are included, indicating the model's ability to learn from limited data.\n\nIn summary, while the neural network itself is complex, the use of well-defined descriptors, systematic descriptor selection, and detailed analysis of the model's behavior with varying data provide a level of transparency. This allows for a better understanding of the factors influencing the model's predictions and its generalization capabilities.",
  "model/output": "The model encompasses both classification and regression approaches. For predicting IC50 values, a Multi-Layer Perceptron (MLP) regressor is employed. This type of artificial neural network processes a set of descriptors through a hidden layer consisting of ten nodes to output the predicted IC50 value. The training process involves splitting 3864 exact IC50 values into training and validation sets. The MLP regressor uses a logistic activation function in the hidden nodes and the lbfgs solver for optimization.\n\nAdditionally, a similar MLP architecture is used for classification tasks, specifically to determine whether the IC50 value is below or above an experimentally-derived cutoff of 1 μg/ml. This cutoff is crucial for calculating the antibody breadth. The MLP classifier is trained on the same set of descriptors but outputs a value filtered through a logistic function to ensure it falls between 0 and 1. For this classifier, 6179 experimental IC50 values are available, with 3089 randomly selected for training. The high ratio between the number of experimental values and the number of parameters helps mitigate the risk of overfitting.\n\nIt is important to note that the outputs of both the MLP regressor and classifier can vary significantly with different random splits of the experimental values into training and validation sets, as well as with different random seeds used during the fitting process. To address this variability, the model averages results from 30 independently generated neural networks, each using a new splitting of the training and validation sets and a new random seed. This averaging helps to produce more robust and reliable predictions.",
  "model/duration": "The execution time for our model involves several steps. First, generating the 3D atomistic model for one antibody/antigen complex takes approximately 100 seconds. This includes using Modeller to create the initial model, CHARMM for quick structure fixes and minimization, and OpenMM for deeper energy minimization.\n\nNext, computing all descriptors for one model requires about 5 minutes. However, this time can be significantly reduced to just 20 seconds by skipping the most computationally expensive steps, FoldX and OpenMM-based descriptors.\n\nFor each antibody/antigen complex, six models are generated, and the descriptors are averaged. This process takes about 10.6 minutes to create the six models and an additional 30.8 minutes to compute all descriptors, totaling around 41 minutes. If FoldX and OpenMM are skipped, the descriptor computation time decreases to 2.4 minutes, reducing the total time to approximately 13 minutes.\n\nThese timings are crucial for understanding the computational efficiency of our model and can guide optimizations for larger-scale applications.",
  "model/availability": "The source code for the algorithms used in this study is not publicly released. However, the computed data and scripts are available within the manuscript and its supporting information files. All experimental data can be accessed from the CATNAP database. The software tools used in the study include Modeller, CHARMM, and OpenMM, which are widely used in the scientific community for molecular modeling and simulations. These tools are available under specific licenses, and users can access them through their respective official websites. The CHARMM36 force field was used for all-atom energy minimizations. The study does not provide a specific executable, web server, virtual machine, or container instance for running the algorithm.",
  "evaluation/method": "The method was evaluated using a comprehensive approach that involved multiple steps to ensure robustness and accuracy. Initially, a total of 6179 experimental values were used, with 3089 randomly selected for training the model. This ratio between the number of experimental values and the number of parameters (200) was designed to minimize the risk of overfitting.\n\nTo assess the model's performance, we compared the composition of the full dataset with a randomly generated set containing 50% of the values. This comparison ensured that all antibodies were represented in the training set, maintaining the same percentages of binders and non-binders as in the entire dataset.\n\nThe outputs of both the MLP regressor and MLP classifier were observed to vary upon repeated training using different random splits of the experimental values into training and validation sets, as well as different random seeds. To mitigate this variability, we averaged the results over 30 independently generated neural networks, each using a new splitting of the training and validation set and a new random seed. The standard deviation between these 30 replicas was used to estimate the statistical error.\n\nAdditionally, the model's generalization ability was examined by including experimental data from each antibody under study in the training set. We observed that for about half of the antibodies, the predicted breadth was accurate even without any relevant experimental data in the training set. For other antibodies, the prediction improved significantly when a small number of experimental values (typically between 10 and 20) were included.\n\nThe Pearson correlation coefficient between the computed and experimental breadth was calculated under different conditions. When all but four out of the 23 antibodies studied were included, the correlation coefficient was 0.50. Including just one experimental value per antibody increased the Pearson coefficient to 0.54, and it reached 0.90 when including only four experimental values.\n\nTo further validate the robustness of the neural network, we repeated the analyses using other machine learning methods, including k-nearest neighbors (kNN), random forests (RF), and support vector machines (SVM). The results from these methods were very similar to those obtained with the MLP, indicating the robustness of our approach.",
  "evaluation/measure": "In the evaluation of our models, several performance metrics were reported to provide a comprehensive assessment of their effectiveness. For the correlation between experimental and computed IC50 values, Pearson and Spearman correlation coefficients were used. The Pearson correlation coefficient for the training set was 0.686, while for the test set, it decreased to 0.410. Similarly, the Spearman coefficient followed a similar trend, indicating a reasonable level of agreement between the computed and experimental values.\n\nThe breadth of antibodies was also evaluated, with the Pearson correlation coefficient between the computed and experimental breadth being 0.800 for the regressor and 0.973 for the classifier. This significant improvement in the classifier's performance highlights its robustness in predicting antibody breadth.\n\nAdditionally, the confusion matrix for the IC50 classifier was analyzed, reporting the accuracy and balanced accuracy. The accuracy in the test set was 72.3±1.0, with an almost equal rate of false positives and false negatives, both around 13%. This metric is crucial for understanding the classifier's performance in distinguishing between binders and non-binders.\n\nThe standard deviation between 30 independently generated neural networks was used to estimate the statistical error, ensuring the reliability of the results. This approach is representative of current practices in the literature, providing a thorough evaluation of the models' performance and robustness.",
  "evaluation/comparison": "To ensure the robustness of our neural network model, we conducted a comprehensive comparison with other machine learning techniques. We evaluated our Multi-Layer Perceptron (MLP) classifier, which uses one hidden layer, against several alternative methods. These included an MLP with two hidden layers, k-nearest neighbors (kNN) with distance weighting, random forests (RF) composed of 31 trees, and a support vector machine (SVM) with a radial basis function kernel. The results from these comparisons were very similar across all methods, indicating that our MLP classifier is robust and reliable.\n\nThe performance metrics, such as the correlation with experimental breadths and the estimated error in single breadth values, were nearly identical for all the methods tested. This consistency suggests that the results obtained with the MLP are not dependent on the specific architecture or algorithm used, but rather on the quality and relevance of the input data.\n\nAdditionally, we compared our model with simpler baselines to assess its performance relative to more straightforward approaches. The use of individual descriptors, for instance, yielded much lower Pearson correlation coefficients, maxing out at 0.28. This highlights the superior predictive power of our neural network model, which achieved a Pearson correlation coefficient of 0.800 for the breadth of known antibodies.\n\nIn summary, the comparison with other machine learning techniques and simpler baselines confirms the robustness and effectiveness of our MLP classifier. The similar performance across different methods underscores the reliability of our approach and indicates that further improvements may require new information or different descriptors rather than changes in the machine learning algorithm.",
  "evaluation/confidence": "The performance metrics presented in this work include confidence intervals, providing a measure of the statistical uncertainty associated with the results. For instance, the Pearson and Spearman correlation coefficients for both the training and test sets are accompanied by their respective values, indicating the strength and direction of the relationship between the experimental and computed IC50 values.\n\nThe breadth of known antibodies was computed and compared with experimental values, with a Pearson correlation coefficient of 0.800 and a Spearman coefficient of 0.766, suggesting a meaningful correlation. The standard deviation between 30 independently generated neural networks was used to estimate the statistical error, ensuring robustness in the results.\n\nThe MLP classifier's performance was further evaluated using a confusion matrix, which includes percentages of true negatives, true positives, false negatives, and false positives. The accuracy in the test set was reported as 72.3±1.0, with an almost equal rate of false positives and false negatives of about 13%. This detailed analysis provides a comprehensive view of the classifier's performance and its reliability.\n\nTo verify the robustness of the neural network, the same analyses were repeated using other machine learning methods, such as k-nearest neighbors (kNN), random forests (RF), and support vector machines (SVM). No significant improvement was observed with any of these alternative methods, indicating that the MLP approach is robust and reliable.\n\nThe statistical significance of the results is supported by the use of multiple performance metrics and the inclusion of confidence intervals. The methods employed, including the averaging over 30 independently generated neural networks, ensure that the findings are not due to random chance but reflect a genuine improvement over baseline methods. The detailed evaluation and comparison with other machine learning techniques further strengthen the claim that the proposed method is superior.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available for public download. However, the data and scripts used in the study are included within the manuscript and its supporting information files. Additionally, all experimental data can be accessed from the CATNAP database. The publication is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction, provided that the original author and source are credited."
}