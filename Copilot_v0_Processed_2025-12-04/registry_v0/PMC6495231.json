{
  "publication/title": "Intermittent hypoxia and hypercapnia reproducibly change the gut microbiome and metabolome across rodent model systems.",
  "publication/authors": "The authors who contributed to the article are:\n\nAnupriya Tripathi, Zhenjiang Zech Xu, Jin Xue, Orit Poulsen, Antonio Gonzalez, Gregory Humphrey, Michael J. Meehan, Alexey V. Melnik, Gail Ackermann, Dan Zhou, Atul Malhotra, Gabriel G. Haddad, Pieter C. Dorrestein, and Rob Knight.\n\nAnupriya Tripathi, Zhenjiang Zech Xu, Jin Xue, Orit Poulsen, Antonio Gonzalez, Gregory Humphrey, Michael J. Meehan, Alexey V. Melnik, Gail Ackermann, Dan Zhou, and Atul Malhotra contributed to the experimental design, data collection, and analysis. Anupriya Tripathi, Zhenjiang Zech Xu, Jin Xue, Orit Poulsen, Antonio Gonzalez, Gregory Humphrey, Michael J. Meehan, Alexey V. Melnik, Gail Ackermann, and Dan Zhou also contributed to the writing of the manuscript.\n\nGabriel G. Haddad, Pieter C. Dorrestein, and Rob Knight provided oversight and guidance for the project. Gabriel G. Haddad and Rob Knight also contributed to the writing of the manuscript.",
  "publication/journal": "mSystems",
  "publication/year": "2019",
  "publication/doi": "10.1128/mSystems.00058-19",
  "publication/tags": "- Cardiovascular\n- Machine Learning\n- Metabolism\n- Microbiome\n- Sleep Apnea\n- Intermittent Hypoxia\n- Hypercapnia\n- Gut Microbiome\n- Metabolome\n- Animal Models",
  "dataset/provenance": "The data generated in this study is publicly available in the GNPS/MassIVE repository. For metabolomics data, the accession numbers are MSV000081482 for Ldlr knockout animals, MSV000082813 for ApoE knockout animals, and MSV000081853 for commercial standards. The microbiome data is available under the accession numbers ERP106495 for Ldlr knockout animals and ERP110592 for ApoE knockout animals in the EBI database.\n\nThe study involved a limited number of animals, with 58 Ldlr/H11002//H11002 mice and 512 ApoE/H11002//H11002 mice per group. However, due to the longitudinal sample collection scheme, there were more data points available for learning. Fecal pellets were collected twice every week starting from 10 weeks of age (baseline) and profiled for the microbiome and metabolome using 16S rRNA amplicon sequencing and liquid chromatography-tandem mass spectrometry (LC-MS/MS)-based untargeted mass spectrometry, respectively.\n\nThe data used in this study builds upon previous findings in Ldlr/H11002//H11002 mice, where significant shifts in the bacterial and chemical composition of the gut were reported on IHH exposure. The current study aims to investigate the cross-applicability of these findings to ApoE/H11002//H11002 mice, another widely used atherosclerosis model. The data has also been used by the community, with data analysis documented in Jupyter notebooks available on GitHub.",
  "dataset/splits": "In our study, we employed a cross-validation strategy to ensure robust and unbiased evaluation of our predictive models. Specifically, we used a leave-one-mouse-out cross-validation approach. This means that for each iteration of the cross-validation, all samples from a single mouse were either in the training set or the validation set, but not both. This method was chosen to prevent overoptimistic accuracy scores that could result from the model learning idiosyncrasies of individual mice rather than the treatment effects.\n\nThe number of data splits corresponds to the number of mice in each model. For the Ldlr/H11002//H11002 model, there were 58 mice, resulting in 58 data splits. For the ApoE/H11002//H11002 model, there were 512 mice, leading to 512 data splits. In each split, the training set included samples from all mice except one, and the validation set included samples from the left-out mouse.\n\nThis approach ensured that our models were evaluated on data from mice that were not part of the training set, providing a more realistic assessment of their predictive performance. Additionally, we accounted for the longitudinal nature of the data by ensuring that all samples from the same time point for a given mouse were included in either the training or validation set but not both. This prevented the model from overfitting to temporal patterns specific to individual mice.",
  "dataset/redundancy": "To ensure the robustness of our predictive models, we employed a rigorous cross-validation strategy. For each mouse model, we trained and evaluated random forest classifiers using microbial or chemical features as predictors. During cross-validation, we ensured that all samples from the same mouse appeared only in either the training or validation dataset, but not both. This approach prevented the classifier from learning idiosyncrasies of individual mice rather than the treatment effects, which could lead to overoptimistic accuracy scores.\n\nThe longitudinal nature of our study provided multiple data points for each individual, which was beneficial for training our models. However, to avoid overfitting, we carefully managed the distribution of samples. By ensuring that observations from the same individual were not split between training and validation sets, we maintained the independence of these datasets. This method is crucial for assessing the generalizability of our findings across different mouse models and potentially to human subjects.\n\nOur dataset distribution differs from some previously published machine learning datasets in that we prioritized the independence of training and validation sets based on individual mice. This approach is particularly important in biological studies where individual variability can significantly impact model performance. By enforcing this independence, we aimed to create a more reliable and generalizable model that can accurately predict IHH exposure across different genotypes and potentially in clinical settings.",
  "dataset/availability": "The data generated in this study is publicly available in the GNPS/MassIVE repository. For metabolomics data, the accession numbers are MSV000081482 for Ldlr knockout animals, MSV000082813 for ApoE knockout animals, and MSV000081853 for commercial standards. These datasets can be accessed via FTP at the respective URLs provided. For microbiome data, the accession numbers are ERP106495 for Ldlr knockout animals in the EBI database and ERP110592 for ApoE knockout animals.\n\nThe data analysis has been thoroughly documented in Jupyter notebooks, which are available on GitHub. The specific repository for these notebooks is https://github.com/knightlab-analyses/crossmodel_prediction. This ensures that the methods and analyses are transparent and reproducible.\n\nAdditionally, supplemental material for this article is available online. This includes various figures and tables that provide further details and support the findings presented in the main text. The supplemental material can be accessed at the provided DOI link.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest (RF) classifier. This is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is not a new algorithm; it has been widely used and studied in the field of machine learning for many years.\n\nThe reason it was not published in a machine-learning journal is that our focus is on applying this well-established algorithm to a specific biological problem, rather than developing a new machine-learning technique. Our work involves using RF classification to investigate the cross-applicability of microbiome and metabolome responses to interventions in different mouse models. This application is novel, but the algorithm itself is not. We chose RF because it has consistently shown strong performance in high-dimensional datasets, which are characteristic of microbiome and metabolome data. The algorithm's ability to handle many features and provide robust predictions makes it well-suited for our analytical needs.",
  "optimization/meta": "The model employed in this study is a random forest (RF) classifier, which is an ensemble machine learning algorithm. It operates by fitting multiple decision trees on random subsamples of the original data and then aggregates the results of each decision tree to enhance prediction accuracy. This approach is particularly effective for high-dimensional datasets, such as those involving microbial reads or metabolites.\n\nThe RF classifier was trained and evaluated using cross-validation for each mouse model, utilizing microbial or chemical features as predictors. During cross-validation, samples from the same mouse appeared only in either the training or validation dataset, but not both. This method ensures that the classifier does not learn idiosyncrasies of individual mice, thereby avoiding overoptimistic cross-validation accuracy scores.\n\nThe classifiers trained for each mouse model were then applied to samples from the other mouse model to assess cross-genotype prediction. For longitudinal prediction, an RF classifier was trained and evaluated on samples collected at each time point to compute the area under the curve (AUC). This process involved using the abundance of each feature as a score to plot the receiver operating characteristic (ROC) curve and compute the AUC, highlighting features that could distinguish IHH-exposed animals from controls.\n\nThe data used for training and validation were carefully managed to ensure independence. Samples from the same individual were not split between training and validation datasets, preventing the model from overfitting to specific individuals. This rigorous approach ensures that the predictions are robust and generalizable across different mouse models and conditions.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps to ensure the quality and consistency of the input features. The raw sequence data were processed using the Deblur workflow with default parameters in Qiita, generating a sub-operational taxonomic unit (sOTU) abundance per sample in BIOM format. Taxonomies for sOTUs were assigned using a sklearn-based taxonomy classifier trained on the Greengenes 13_8 99% OTUs. The sOTU table was rarefied to a depth of 2,000 sequences per sample to control for sequencing effort.\n\nFor metabolomics data, LC-MS/MS data were acquired from fecal samples using identical protocols. The raw data sets were converted to m/z extensible markup language (mzXML) in centroid mode using MSConvert. Feature extraction was performed in MZmine2 with specific parameters for signal intensity threshold, minimum peak width, and mass and retention time tolerances. The detected peaks were aligned across all samples to produce the final feature table used in the analyses.\n\nMolecular networking was performed in GNPS to putatively identify molecular features using MS/MS-based spectral library matches. This process ensured that the data were accurately encoded and preprocessed, allowing for reliable input into the machine-learning algorithm. The random forest classifier was then trained and evaluated with cross-validation, using microbial or chemical features as predictors. During cross-validation, all samples from the same mouse appeared only in either the training or validation data to avoid overoptimistic accuracy scores. This preprocessing and encoding strategy ensured that the data were robust and suitable for the machine-learning tasks performed.",
  "optimization/parameters": "Not enough information is available.",
  "optimization/features": "The study utilized a comprehensive set of features derived from microbiome and metabolome data layers. For the microbiome data, after quality control, 635 features were retained for Ldlr/H11002//H11002 animals and 582 for ApoE/H11002//H11002 animals, with 248 sequences shared between the two models. For the metabolomic data, 267 features were identified in Ldlr/H11002//H11002 animals and 374 in ApoE/H11002//H11002 animals, with 137 metabolites shared between the models.\n\nFeature selection was implicitly performed during the data preprocessing steps, where only prevalent and significant features were retained. This process ensured that the features used in the analysis were robust and relevant. The selection criteria included a sum relative abundance threshold and prevalence in a certain percentage of samples, which helped in avoiding sequencing noise and low-prevalence ions.\n\nThe feature selection was conducted using the entire dataset, ensuring that the selected features were representative of the underlying biological variations. This approach helped in maintaining the integrity of the data and preventing overfitting. The selected features were then used to train random forest classifiers, which demonstrated high prediction accuracies both within and across animal models.",
  "optimization/fitting": "In our study, we employed random forest (RF) classification, an ensemble machine learning algorithm, to investigate the cross-applicability of our findings across different mouse models. The RF algorithm fits multiple decision trees on random subsamples of the original data and then aggregates the results to improve prediction accuracy. This approach is particularly well-suited for high-dimensional datasets, such as ours, which include many microbial reads or metabolites.\n\nGiven the high dimensionality of our data, the number of parameters (features) is indeed much larger than the number of training points (samples). To address the potential issue of overfitting, we implemented a rigorous cross-validation strategy. Specifically, during cross-validation, all samples from the same mouse appeared only in either the training or validation dataset, but not both. This ensured that the classifier did not learn idiosyncrasies of individual mice but rather the general patterns associated with the treatment. Additionally, we used a longitudinal sample collection scheme, which provided more data points for learning despite the limited number of animals per group. This approach helped to mitigate overfitting by ensuring that the model generalized well to new, unseen data.\n\nTo rule out underfitting, we evaluated the performance of our classifiers using the area under the curve (AUC) of the receiver operating characteristic (ROC). The classifiers achieved nearly perfect prediction within each animal model (99% AUC) and maintained high cross-model prediction accuracies. This indicates that our models were sufficiently complex to capture the underlying patterns in the data without being too simplistic.\n\nFurthermore, we ensured that our models were not underfitted by using a comprehensive set of features derived from both microbiome and metabolome data. The features were carefully selected and quality-controlled to retain only those that were prevalent and present in a significant proportion of samples. This approach helped to ensure that our models had access to a rich set of informative features, reducing the risk of underfitting.\n\nIn summary, our use of RF classification, combined with a robust cross-validation strategy and a comprehensive feature selection process, allowed us to effectively address the challenges of overfitting and underfitting in our high-dimensional dataset.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive models. One key method involved the use of cross-validation, specifically ensuring that all samples from the same mouse appeared only in either the training or validation dataset, but not both. This approach helped to avoid overoptimistic accuracy scores that could result from the classifier learning idiosyncrasies of individual mice rather than the treatment effects.\n\nAdditionally, we utilized random forest (RF) classification, an ensemble machine learning algorithm known for its ability to handle high-dimensional data and reduce overfitting. RF fits multiple decision trees on random subsamples of the original data and aggregates their results, which improves prediction accuracy and generalizability.\n\nFurthermore, we accounted for the longitudinal nature of our data by ensuring that observations from the same individual were not split between training and validation sets. This prevented the model from overfitting to the temporal patterns of individual mice, thereby enhancing the reliability of our cross-validation results.\n\nThese techniques collectively helped to mitigate overfitting and ensured that our models could generalize well to new, unseen data.",
  "optimization/config": "Not applicable.",
  "model/interpretability": "The models employed in this study are not entirely black-box systems. The random forest (RF) classifiers used for predicting IHH exposure are inherently interpretable to a certain extent. RF is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. This structure allows for some level of interpretability.\n\nEach decision tree in the RF can be visualized and analyzed to understand which features (microbial or chemical) are most important in making predictions. The feature importance scores, which indicate the contribution of each feature to the prediction, can be extracted and examined. This provides insights into which microbial or metabolic features are most influential in distinguishing between IHH-exposed and control animals.\n\nAdditionally, the use of ROC curves and AUC scores helps in understanding the model's performance and the discriminatory power of individual features. Features that significantly contribute to the AUC can be highlighted, providing a clearer picture of the key drivers behind the model's predictions.\n\nWhile the RF model itself is not as transparent as simpler models like linear regression, the ability to inspect individual decision trees and feature importance scores offers a level of interpretability. This allows researchers to gain insights into the biological mechanisms underlying the predictions, making the model more transparent and useful for further scientific inquiry.",
  "model/output": "The model employed in our study is a classification model. Specifically, we utilized random forest (RF) classifiers to investigate the cross-applicability of our findings across different mouse models. The RF classifiers were trained and evaluated using cross-validation for each mouse model, with microbial or chemical features serving as predictors. The performance of these classifiers was assessed using the area under the receiver operating characteristic curve (AUC), which measures the ability of the model to distinguish between IHH-exposed and control animals. The classifiers demonstrated nearly perfect prediction within each animal model, achieving an AUC of 99%. Additionally, the models maintained high cross-model prediction accuracies when trained on one mouse model and tested on the other, further validating their effectiveness in classifying IHH exposure based on gut microbial and chemical compositions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the data analysis has been made publicly available on GitHub. Specifically, the data analysis has been documented in Jupyter notebooks, which can be accessed via the provided GitHub repository. This allows other researchers to reproduce the analyses and build upon the findings.\n\nAdditionally, the MZmine2 batch processing file used for feature extraction in metabolomic data processing is available on GitHub. This file can be used to replicate the chromatographic deconvolution and peak alignment processes described in the study.\n\nFor molecular networking, the parameters used for the analysis are available on the University of California—San Diego GNPS site. This includes the details necessary to reproduce the molecular networking results, ensuring transparency and reproducibility.\n\nThe data generated in this study are also publicly available. Metabolomics data can be accessed through the GNPS/MassIVE repository under specific accession numbers, and microbiome data can be found in the EBI database. This ensures that the raw data supporting the conclusions are accessible for further investigation and validation.\n\nThe software and data availability facilitate the reproducibility of the study and encourage further research in the field.",
  "evaluation/method": "The evaluation method employed in this study involved a robust cross-validation approach to ensure the reliability and generalizability of the predictive models. Specifically, random forest (RF) classifiers were trained and evaluated using cross-validation for each mouse model. This process utilized microbial or chemical features as predictors. To prevent overoptimistic accuracy scores, all samples from the same mouse were included either in the training or validation dataset, but not both. This strategy avoided the classifier learning idiosyncrasies of individual mice rather than the treatment effects.\n\nThe classifiers trained for each mouse model were then applied to samples from the other mouse model to assess cross-genotype prediction capabilities. For longitudinal prediction, RF classifiers were trained and evaluated on samples collected at each time point to compute the area under the curve (AUC). The performance of individual 16S sequences and metabolites in distinguishing between IHH-exposed and control animals was assessed by using their abundance as scores to plot the receiver operating characteristic (ROC) curve and compute the AUC. Features that could single-handedly distinguish IHH exposure were highlighted on the ROC plots. These analyses were conducted using the scikit-learn Python package.",
  "evaluation/measure": "In our study, we primarily focused on the area under the receiver operating characteristic curve (AUC) as our key performance metric. This metric is widely used and well-established in the literature for evaluating the performance of classification models, particularly in high-dimensional datasets like ours.\n\nThe AUC provides a single scalar value that represents the ability of the model to distinguish between the positive and negative classes. An AUC of 1 indicates perfect classification, while an AUC of 0.5 suggests that the model performs no better than random chance. In our case, we achieved high AUC values, indicating strong predictive performance.\n\nWe reported AUC values for both within-model and cross-model predictions. Within each animal model, our random forest classifiers yielded nearly perfect prediction of IHH exposure, with AUC values of 99%. For cross-model predictions, we still achieved very high accuracies, with AUC values of 95% and 89% for microbiome data, and 97% and 84% for metabolomics data, depending on the direction of the cross-model prediction.\n\nThese AUC values demonstrate the robustness and generalizability of our models, as they maintain high performance even when applied to different animal models. This is particularly noteworthy given the inherent differences between the gut ecosystems of the two models we studied.\n\nIn addition to AUC, we also highlighted specific features that could single-handedly distinguish IHH exposure on ROC plots. This provides further insight into the most influential factors driving the model's predictions.\n\nOverall, our choice of performance metrics is representative of the literature and provides a comprehensive evaluation of our models' predictive capabilities.",
  "evaluation/comparison": "Not applicable. The publication focuses on the analysis of microbiome and metabolome data from two different mouse models using random forest classification. It does not involve a comparison to publicly available methods or simpler baselines on benchmark datasets. Instead, it investigates the cross-applicability of findings between the two mouse models and the impact of various covariates on the microbiome and metabolome composition. The study uses random forest classifiers to predict IHH exposure within and across animal models, demonstrating high prediction accuracies. The methods used for data processing, effect size analysis, and machine learning are described in detail, but there is no comparison to other methods or baselines.",
  "evaluation/confidence": "The evaluation of our models involved rigorous statistical methods to ensure the reliability and significance of our results. We employed cross-validation techniques to assess the performance of our random forest classifiers. During this process, we ensured that all samples from the same mouse appeared only in either the training or validation data, preventing overoptimistic accuracy scores due to the model learning idiosyncrasies of individual mice rather than the treatment effects.\n\nFor the microbiome and metabolome data layers, we calculated effect sizes using Cohen’s d or the absolute difference between the mean of each group divided by the pooled standard deviation. This approach allowed us to quantify the magnitude of differences between groups, providing a clearer picture of the impact of various covariates such as mouse number, age, and cage number.\n\nWe also utilized the mixed directional false-discovery rate (mdFDR) methodology to test the significance of each pairwise comparison among the groups. This statistical method helped us control for false discoveries, ensuring that our findings were robust and reliable.\n\nThe performance metrics, such as the area under the receiver operating characteristic curve (AUC), were computed to evaluate the classifiers' accuracy. The high AUC values (e.g., 99% within animal models and 95% to 89% for cross-model predictions) indicate strong predictive performance. These metrics were accompanied by statistical significance tests, confirming that the observed differences were not due to random chance.\n\nIn summary, our evaluation process included comprehensive statistical analyses and cross-validation techniques to ensure the confidence and significance of our results. The performance metrics, such as AUC, were supported by rigorous statistical testing, providing a solid foundation for claiming the superiority and reliability of our methods.",
  "evaluation/availability": "The raw evaluation files generated in this study are publicly available. For metabolomics data, the files can be accessed through the GNPS/MassIVE repository under the accession numbers MSV000081482 for Ldlr knockout animals, MSV000082813 for ApoE knockout animals, and MSV000081853 for commercial standards. These files are available for download via FTP at the respective URLs provided. For microbiome data, the raw sequence data are available in the EBI database under the accession numbers ERP106495 for Ldlr knockout animals and ERP110592 for ApoE knockout animals.\n\nAdditionally, the data analysis has been documented in Jupyter notebooks, which are available on GitHub. These notebooks provide detailed steps and code used for the analysis, ensuring reproducibility. The specific link to the GitHub repository is provided for access.\n\nThe supplemental material for this article, including figures and tables, is also available online. This material includes additional data and visualizations that support the findings presented in the main text. The supplemental material can be accessed via the provided DOI link."
}