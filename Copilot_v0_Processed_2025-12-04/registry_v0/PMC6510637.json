{
  "publication/title": "Prediction of Radiation Pneumonitis",
  "publication/authors": "The authors who contributed to this article are:\n\n- Sunan Cui, who is associated with the Applied Physics Program at the University of Michigan, Ann Arbor, MI, USA. This author is likely the primary contributor to the article, given the typical conventions of academic publishing.\n\n- Yi Luo, Huan-Hsin Tseng, Randall K. Ten Haken, and Issam El Naqa, who are all affiliated with the Department of Radiation Oncology at the University of Michigan, Ann Arbor, MI, USA. Their specific contributions to the paper are not detailed, but they likely played significant roles in the research, data analysis, and writing process.",
  "publication/journal": "Medical Physics",
  "publication/year": "2019",
  "publication/doi": "10.1002/mp.13556",
  "publication/tags": "- Radiation pneumonitis\n- Machine learning\n- Predictive modeling\n- Dosimetric information\n- Clinical factors\n- Cytokines\n- MicroRNA\n- Single nucleotide polymorphisms\n- Feature selection\n- Deep learning\n- VAE\n- MLP\n- SVM\n- RF\n- Kemeny aggregation\n- Nested cross-validation\n- Biomarkers\n- Lung cancer\n- Inflammatory disease\n- Treatment planning",
  "dataset/provenance": "The dataset used in this study is a large-scale heterogeneous collection focused on nonsmall cell lung cancer (NSCLC) patients who underwent radiotherapy. It comprises a pool of 230 variables, including clinical factors such as dose, Karnofsky Performance Status (KPS), and stage, as well as biomarkers like single nucleotide polymorphisms (SNPs), cytokines, and micro-RNAs. The dataset includes information from 106 NSCLC patients, among whom 22 experienced grade 2 or higher radiation pneumonitis (RP).\n\nThis dataset is unique and has not been previously used by the community. The variables considered in the RP prediction include dosimetric information, clinical factors, cytokine levels measured at different times during treatment, and various biomarkers. The dosimetric information includes metrics like mean lung dose, maximum lung dose, and volumes receiving specific radiation doses. Clinical factors encompass a range of variables such as smoking status, chronic obstructive pulmonary disease (COPD), chemotherapy history, gender, and tumor characteristics. Cytokine levels were measured at pretreatment, two weeks, and four weeks during treatment, covering a wide array of inflammatory markers. Additionally, the dataset includes information on micro-RNAs and SNPs, which were identified from previous studies on lung cancer or inflammatory diseases.\n\nThe dataset was analyzed using various machine learning methods, including feature selection and extraction techniques, as well as deep learning approaches like variational autoencoders (VAEs) combined with multilayer perceptrons (MLPs). The goal was to predict RP, a common side effect of radiotherapy, by leveraging the diverse set of variables available. The methods employed aimed to address the challenge of a large number of variables relative to the limited sample size, utilizing dimensionality reduction techniques to enhance predictive performance.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam algorithm, which is an advanced gradient-based optimization algorithm commonly used in deep learning. This algorithm is not new and has been widely adopted in the field of machine learning for its efficiency and effectiveness in training neural networks.\n\nThe Adam algorithm was not published in a machine-learning journal because it is a well-established optimization technique that has been extensively studied and validated in the literature. It was introduced by Diederik P. Kingma and Jimmy Ba in their paper \"Adam: A Method for Stochastic Optimization,\" which was published at the International Conference on Learning Representations (ICLR) in 2015. Since then, it has become a standard choice for optimizing neural networks due to its ability to adapt the learning rate for each parameter, which helps in achieving faster convergence and better performance.\n\nIn our work, the Adam algorithm was used to optimize the cross-entropy loss function in the multilayer perceptron (MLP) model. The cross-entropy loss function is commonly used for classification tasks, and the Adam algorithm was chosen for its robustness and efficiency in handling such tasks. The use of the Adam algorithm in our study is consistent with its widespread application in the machine learning community.",
  "optimization/meta": "In the \"Meta-predictor\" subsection, we explored the combination of different machine learning strategies to enhance the prediction of radiation pneumonitis. The meta-predictor approach integrates outputs from various machine learning methods to create a more robust predictive model.\n\nThe meta-predictor leverages data from other machine-learning algorithms as input. Specifically, it combines handcrafted features selected through conventional machine learning techniques with latent variables derived from a Variational Autoencoder (VAE) and Multilayer Perceptron (MLP) joint architecture. This integration allows the meta-predictor to benefit from both the explicit feature selection process and the implicit feature extraction capabilities of deep learning models.\n\nThe machine-learning methods that constitute the whole meta-predictor include Random Forest (RF), Support Vector Machine (SVM), and Multilayer Perceptron (MLP) classifiers. These methods are used in different cases to select and rank features, as well as to build predictive models. The VAE-MLP joint architecture is also a crucial component, providing latent variables that capture complex patterns in the data.\n\nRegarding the independence of training data, it is important to note that the meta-predictor is designed to mitigate statistical bias by using nested cross-validation. This approach ensures that the feature and parameter selection are tuned in inner-loop cross-validation, while the model with optimal parameters is identified and evaluated in outer-loop cross-validation. This process helps to maintain the independence of training and test data, ensuring that the meta-predictor's performance is evaluated on unseen data.",
  "optimization/encoding": "In our study, the data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms. Initially, we collected a large-scale heterogeneous dataset containing 230 variables, including clinical factors such as dose, KPS, and stage, as well as biomarkers like single nucleotide polymorphisms (SNPs), cytokines, and micro-RNAs. This dataset was derived from a population of 106 nonsmall cell lung cancer (NSCLC) patients who received radiotherapy, with 22 patients experiencing grade 2 or higher radiation pneumonitis (RP).\n\nTo handle the imbalanced ratio of a large number of variables to a limited sample size, we applied a primitive dimensionality reduction technique based on single-variable AUCs. This step reduced the number of variables to around 70, mitigating redundancy effects and ensuring that the classifiers did not produce random predictions. This preprocessing was essential for the subsequent dimensionality reduction techniques, including feature selection and feature extraction.\n\nFor the feature selection approach (case A), we implemented Random Forest (RF), Support Vector Machine (SVM), and Multilayer Perceptron (MLP) to rank features within several cross-validated (CV) resampled sets. The top features from these rankings were then aggregated using the top 5% and Kemeny graph methods to identify the final ranking for prediction. A synthetic minority oversampling technique was also applied to correct for class imbalance during this process.\n\nIn the feature extraction approach (case B), we used Variational Autoencoders (VAEs) to encode the original inputs. Unlike the conventional setting where VAEs and classifiers were trained separately, we developed a joint VAE-MLP architecture (case C) that combined a VAE for dimensionality reduction and an MLP for classification. This joint architecture was trained simultaneously, improving representation learning and prediction performance. The joint architecture reduced the need for a burdensome feature selection process and was computationally efficient, although it required more data to perform optimally.\n\nAdditionally, we considered combining handcrafted features from the feature selection approach with latent variables from the VAE-MLP joint architecture (case D). This combination achieved the best predictive performance, with an AUC of 0.831, indicating that the latent variables from the VAE-MLP architecture complemented the handcrafted features effectively.\n\nIn summary, the data encoding and preprocessing involved dimensionality reduction based on single-variable AUCs, followed by feature selection and extraction techniques. The joint VAE-MLP architecture played a significant role in improving prediction performance by efficiently encoding the original inputs and reducing the need for extensive feature selection.",
  "optimization/parameters": "In our study, the number of parameters used in the model varied depending on the specific architecture and feature selection method employed. For the conventional machine learning feature selection and prediction (Cases A), we initially reduced the number of variables to around 70 through preprocessing based on single-variable AUCs. This step was crucial due to our limited sample size, which could not support direct dimensionality reduction techniques on the original dataset without preselection.\n\nFor the VAE-related settings (Cases B & C), all preselected features contributed to the prediction task. In Case B, which involved separate VAE and classifiers (MLP, SVM, RF), the dimension of the latent space varied from 1 to 8. In Case C, which utilized a VAE-MLP joint architecture, the latent space dimension was also a critical parameter. The optimal number of features was determined through nested cross-validation, where the feature and parameter selection were tuned in inner-loop cross-validation, and the model with optimal parameters was identified from outer-loop training sets and evaluated on outer-loop test sets.\n\nThe parameter k in the VAE-MLP joint architecture denoted the trade-off between VAE reconstruction and output prediction. This parameter was set to 100 to weigh the magnitude of the two losses appropriately. Additionally, the dropout rate in the MLP was searched among values of 0.1, 0.2, 0.3, and 0.4 on the training set to find the optimal rate for regularization.\n\nIn the VAE-MLP joint architecture, L2 regularization was added to the first-layer weights of the loss function to address overfitting issues when a large number of features was available. The strength of this regularization was varied, and its impact on the weight matrices was visualized through heat maps. The optimal regularization strength was determined by evaluating the model's performance on the validation set.\n\nOverall, the selection of parameters was a meticulous process involving cross-validation, regularization techniques, and careful tuning to ensure the model's robustness and generalizability.",
  "optimization/features": "In our study, we utilized a diverse set of input features, including dosimetric information, cytokines, miRNA, and SNP variables. The exact number of features used as input varied depending on the case and the method of feature selection employed.\n\nFeature selection was indeed performed in our analysis. For conventional machine learning approaches, we used multiple numbers of inner cross-validations (CVs) to rank the features. The cutoff number of CVs was determined by evaluating the convergence of the rankings. We then applied the top 5% method to the ranking lists from the series of CVs and combined them into a single ranking using Kemeny aggregation. This process was repeated 100 times to ensure robustness.\n\nFor the VAE-related settings, all preselected features contributed to the prediction task, rather than a subset of selected features. This approach allowed us to leverage the latent representation derived from the VAE, which takes all features into account.\n\nThe feature selection process was conducted using the training set only, ensuring that the evaluation of feature importance was not biased by the test data. This approach helps to maintain the integrity of the validation process and provides a more reliable assessment of the model's performance.\n\nIn summary, feature selection was a crucial step in our analysis, and it was performed using the training set to ensure unbiased evaluation. The number of features used as input varied depending on the case and the method of feature selection employed.",
  "optimization/fitting": "In our study, we encountered a scenario where the number of parameters was potentially much larger than the number of training points, particularly when dealing with complex models like the VAE-MLP joint architecture. To address the risk of overfitting, we employed several strategies.\n\nFirstly, we applied dimensionality reduction techniques to mitigate redundancy effects. This involved preselecting variables based on single-variable AUCs, reducing the number of variables to around 70. This step was crucial because our limited sample size could not support direct dimension reduction techniques on the original dataset without preselection.\n\nAdditionally, we incorporated L2 regularization into the loss function of the VAE-MLP joint architecture. This regularization term was added to the first-layer weights to promote sparse weight matrices, which helped in addressing overfitting issues when a large number of features were available. The strength of regularization was varied, and heat maps of the weight parameters showed that increasing the regularization strength led to sparser weight matrices, thereby reducing the number of effective features in the models.\n\nTo further ensure that our models were not underfitting, we performed nested cross-validation. This involved tuning feature and parameter selection in inner-loop cross-validation and then evaluating the model with optimal parameters on outer-loop test sets. This approach helped in consolidating the results and ensuring that the models were neither overfitting nor underfitting.\n\nMoreover, we compared multiple methodologies, including conventional machine learning feature selection and prediction, VAE analysis with separate and joint classifications, and combinations of handcrafted features and latent variables. This comprehensive evaluation allowed us to identify the most effective strategies for predicting outcomes while balancing the complexity of the models and the risk of overfitting or underfitting.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting, particularly when dealing with a large number of features. Specifically, we added an L2 regularization term to the first-layer weights of the loss function. This regularization was aimed at promoting sparse weight matrices, which helps in addressing overfitting issues. The strength of the regularization was varied, and we observed that as the regularization strength increased, the weight matrices became sparser, leading to fewer features being effective in the models. This approach was crucial in mitigating the redundancy effects and improving the predictive performance, especially when the sample size was limited.",
  "optimization/config": "In our study, we have made efforts to ensure that our methodologies and findings are reproducible and accessible to the broader scientific community. The hyper-parameter configurations and optimization schedules used in our experiments are detailed within the publication. Specifically, we have discussed the use of nested cross-validation (CV) for feature and parameter selection, where the inner loop CVs were used for tuning hyper-parameters, and the outer loop CVs were used for evaluating the model performance.\n\nThe model files and optimization parameters are not directly provided in the publication, but we are in the process of preparing a Medical Physics Dataset article. This upcoming article will include the release of the data and associated code, which will allow others to explore and replicate our approaches. The dataset and code will be made available under a license that permits use and modification for research purposes, ensuring that the community can build upon our work.\n\nFor the VAE-MLP joint architecture, we have specified the parameter k, which was set to 100 for weighing the magnitude of the two losses in the joint architecture. Additionally, we have discussed the use of L2 regularization to address overfitting issues, particularly when dealing with a large number of features. The strength of the regularization and its impact on the model's performance are also detailed in the publication.\n\nWhile the exact model files and optimization parameters are not provided in the current publication, the detailed descriptions of our methods and the upcoming release of the dataset and code will facilitate reproducibility and further research in this area.",
  "model/interpretability": "The models employed in this study encompass both transparent and black-box components, depending on the specific approach used.\n\nIn the conventional machine learning feature selection and prediction (Case A), the models utilized, such as Random Forests (RF), Support Vector Machines (SVM), and Multilayer Perceptrons (MLP), offer varying degrees of interpretability. RF, for instance, provides feature importance scores, which indicate the contribution of each feature to the prediction. This makes RF relatively transparent, as one can understand which features are most influential. SVM, on the other hand, is less interpretable due to its reliance on complex hyperplanes in high-dimensional space. MLP, being a type of neural network, is generally considered a black-box model, as its decisions are based on intricate, non-linear transformations that are difficult to interpret directly.\n\nIn the Variational Autoencoder (VAE) analysis with separate and joint classifications (Cases B and C), the VAE itself is a black-box model. It learns a latent space representation of the data, which is then used for prediction. The latent variables are not directly interpretable, as they are continuous and often abstract representations of the input data. However, when combined with classifiers like MLP, SVM, or RF, the interpretability of the overall model depends on the classifier used. For example, if RF is used, the feature importance scores can still be derived, providing some level of interpretability.\n\nIn the combination of handcrafted features and latent variable representation (Case D), the model integrates both interpretable handcrafted features and the latent variables from the VAE. This hybrid approach allows for some interpretability, as the handcrafted features can be analyzed individually, while the latent variables contribute to the overall predictive power of the model. The classifiers used in this case (MLP, SVM, RF) also influence the interpretability, as discussed earlier.\n\nOverall, while some components of the models are transparent and provide clear examples of feature importance, other components, particularly those involving VAEs and MLPs, remain black-box models. The interpretability of the models varies depending on the specific combination of techniques used.",
  "model/output": "The model is primarily focused on classification tasks. Specifically, it aims to predict radiation pneumonitis, which is a binary outcome. Various classifiers such as Multilayer Perceptron (MLP), Random Forest (RF), and Support Vector Machine (SVM) were employed to evaluate the predictive performance. The models were trained and validated using nested cross-validation, and their performance was assessed using the Area Under the Curve (AUC) metric. Additionally, the models were tested with different numbers of top features to optimize prediction accuracy. The final rankings of features were aggregated using methods like Kemeny optimal aggregation to ensure consistency across different trials. The results indicate that the models achieved varying levels of predictive performance, with some configurations outperforming others based on the AUC values.",
  "model/duration": "All experiments were conducted on a NVIDIA K40 GPU, utilizing the Advanced Research Computing â€” Technology Services (ARC-TS), FLUX, at the University of Michigan. The specific execution time for the models was not explicitly detailed, but the use of a high-performance GPU suggests that the computations were efficient. The models implemented included conventional machine learning algorithms such as Random Forest (RF), Support Vector Machine (SVM), and Multilayer Perceptron (MLP), as well as deep learning techniques using Variational Autoencoders (VAEs) and joint VAE-MLP architectures. The nested cross-validation process, which involved multiple iterations of feature selection, model training, and evaluation, would have been computationally intensive. However, the use of a powerful GPU likely facilitated the timely completion of these tasks.",
  "model/availability": "We are in the process of preparing a Medical Physics Dataset article that will include the release of our data and the associated code. This initiative aims to allow others in the community to explore our approaches. The code will be made available to facilitate reproducibility and further research. The specific details regarding the license and the platform for distribution will be provided in the upcoming publication.",
  "evaluation/method": "In our evaluation, we employed nested cross-validation (CV) to ensure robust performance assessment. This involved an inner loop for feature and parameter selection, and an outer loop for model evaluation. Specifically, we used multiple rankings based on different subsets of the data to mitigate the sensitivity to data portions due to the noisy nature of our dataset. These rankings were aggregated to yield a single ranking, which was then used to select the top features for evaluation.\n\nWe implemented all methodologies in the same validation pipeline to ensure comparability and to mitigate statistical bias. This approach is referred to as a type 2b analysis in the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis.\n\nFor feature selection, we applied wrapper methods based on MLP, RF, and SVM to search for the optimal set of features. The \"goodness\" of features within each learning scheme was evaluated, and multiple rankings were generated and aggregated to yield a single ranking. This process involved running multiple times of inner loop fivefold CV and analyzing the resulting ranking lists for convergence.\n\nThe top p% method was used to aggregate rankings based on different CVs. This method uses the frequency of a feature in the top p% as its final score. For example, with p = 5, the frequency of a feature appearing in the top 5% among the ranking lists was counted, and features were ranked based on these frequencies.\n\nTo decide the cutoff number of CV times, we defined a deviance value to evaluate the convergence of ranking lists. This value quantifies the deviations of orderings given a certain number of CV times and is expected to decrease with increasing CV times. We fixed the number of CV times large enough to make the deviance reasonably small.\n\nAfter deciding the cutoff number, the top p% method was applied again to aggregate rankings from the CV ranking lists. Kemeny aggregation was then used to reach a final ranking. Kemeny aggregation minimizes the sum of Kendall distances, which is defined by the number of pairwise disagreements between any two ranking lists.\n\nWe varied the number of top features included in the predictive models, including MLP, RF, and SVM, according to the final rankings and evaluated their resulting AUCs. To consolidate the results, we repeated the experiment by doing the outer loop of the nested CVs ten times. The average test AUCs and their error bars were reported.\n\nIn summary, our evaluation method involved nested CV, feature selection using wrapper methods, ranking aggregation, and performance evaluation using multiple classifiers. This comprehensive approach ensured robust and unbiased performance assessment of our methodologies.",
  "evaluation/measure": "In our study, we primarily focused on the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve as our key performance metric. The AUC provides a comprehensive measure of the model's ability to distinguish between positive and negative classes, making it a robust indicator of predictive performance. We reported the average test AUCs along with their error bars to account for variability across different experiments.\n\nTo ensure the robustness of our results, we employed nested cross-validation (CV). This involved performing multiple iterations of inner loop CVs for feature and parameter selection, followed by outer loop CVs to evaluate the model's performance on unseen data. This approach helps to mitigate overfitting and provides a more reliable estimate of the model's generalizability.\n\nWe compared four different methodologies (A, B, C, and D) within the same validation pipeline to ensure consistency and to mitigate statistical bias. This type of analysis, referred to as a type 2b analysis, is crucial for transparent reporting of multivariate prediction models for individual prognosis or diagnosis.\n\nAdditionally, we conducted a DeLong test to compare the performance of different cases at specific points, ensuring that our conclusions were statistically significant. This test helps to determine whether the differences in AUCs between models are statistically meaningful.\n\nThe use of AUC as a primary metric is well-established in the literature for evaluating predictive models, particularly in medical and biological studies. It allows for a clear and concise comparison of model performance across different datasets and methodologies. Our approach of using nested CV and comparing multiple methodologies ensures that our results are both robust and representative of current best practices in the field.",
  "evaluation/comparison": "In our evaluation, we implemented all four methodologies (A, B, C, D) in the same validation pipeline to ensure a fair comparison and mitigate statistical bias. This approach is referred to as a type 2b analysis in the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis.\n\nTo consolidate the results, we repeated the experiment by doing the outer loop of the nested cross-validations ten times. This process helped us to evaluate the performance of each methodology more robustly.\n\nFor the conventional machine learning feature selection and prediction (Case A), we performed multiple numbers of inner cross-validations to rank the features. The cutoff number was decided by the convergence evaluation of the rankings. The top 5% method was applied to the ranking lists from the cross-validations and was further combined into a single ranking by Kemeny aggregation.\n\nIn the VAE-related setting (Cases B & C), all preselected features contributed to the prediction task. We compared the prediction results from Case B, which involved separate VAE and classifiers, with Case C, which involved a VAE-MLP joint architecture. The dimension of the latent space varied from 1 to 8 in these cases.\n\nWe also considered combining handcrafted features and latent variables from VAEs for prediction (Case D). This combination improved predictive performance, especially with a small number of samples.\n\nThe results showed that the joint architecture (Case C) outperformed the separate training (Case B) across all latent sizes. Additionally, combining handcrafted features with latent variables (Case D) improved prediction by pure handcrafted features at an arbitrary number of features.\n\nIn summary, our evaluation involved a comprehensive comparison of different methodologies within the same validation pipeline, ensuring a fair and unbiased assessment of their performance.",
  "evaluation/confidence": "The evaluation of our methods included a thorough assessment of performance metrics with confidence intervals. Specifically, we repeated experiments using nested cross-validation, where the outer loop was performed ten times to consolidate results. This approach allowed us to calculate average test AUCs along with their error bars, providing a clear indication of the variability and reliability of our performance metrics.\n\nTo ensure the statistical significance of our findings, we employed the DeLong test. This test was conducted between different cases, such as comparing case B and case C, and case A and case D, at specific points where the best AUC was observed. The results of these tests confirmed that the differences in performance were statistically significant, supporting the claim that certain methods outperformed others.\n\nAdditionally, we used Kemeny aggregation to combine multiple ranking lists into a single, generalized ranking. This method minimizes the sum of Kendall distances, ensuring that the final ranking is optimal and robust. The use of Kemeny aggregation further enhanced the confidence in our feature selection process, as it accounts for the consistency and reliability of feature importance across different trials.\n\nOverall, the inclusion of confidence intervals, repeated experiments, and statistical tests like the DeLong test provided a comprehensive evaluation of our methods, ensuring that the performance metrics are reliable and that the claimed superiority of certain methods is statistically significant.",
  "evaluation/availability": "Not enough information is available."
}