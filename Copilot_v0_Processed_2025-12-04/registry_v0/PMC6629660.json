{
  "publication/title": "Oesophageal Cancer Clinical and Molecular Stratification (OCCAMS) Consortium",
  "publication/authors": "The authors who contributed to this article are as follows:\n\n- Francesca D. Ciccarelli: Conceived and directed the study, developed bioinformatics methods, analyzed data, and wrote the manuscript.\n- Thomas M. Edwards: Developed bioinformatics methods, performed experiments, analyzed data, and wrote the manuscript.\n- Lawrence Bower: Performed experiments, analyzed data, and wrote the manuscript.\n- Elwira Fidziukiewicz: Performed experiments, analyzed data, and wrote the manuscript.\n- Daniel T. Thomas: Helped with bioinformatics methods, performed experiments, and analyzed data.\n- James P. Norman: Helped with the analysis of RNA-Seq data and analyzed data.\n- Rachael C. Fels Elliott: Edited the manuscript.\n- Paul Safranek: Helped with experiments and edited the manuscript.\n- Maria H. Smith: Helped with experiments and edited the manuscript.\n- Jan L. Bornschein: Edited the manuscript.\n- Michael H. C. Smith: Helped with experiments.\n- Patricia Safranek: Helped with experiments.\n- Cynthia Y. Young: Supervised bioinformatics methods.\n- Matthew C. Cook: Supervised bioinformatics methods.\n- Alejandro Velasquez: Helped with figures.\n- Stephanie Hills: Helped with experiments on E2F1 and MCM7.\n- John Diffley: Helped with experiments on E2F1 and MCM7.\n- Valdone Maciulyte: Helped with MiSeq library preparation.\n- Patricia Galipeau: Provided segmented copy number data of the Barrettâ€™s esophagus samples.\n- The Cancer Genome Atlas: Provided data.\n- The Oesophageal Cancer Clinical and Molecular Stratification (OCCAMS) Consortium: Provided funding for sample sequencing.\n- The Human Research Tissue Bank: Provided support.\n- The Experimental Cancer Medicine Centre: Provided infrastructure support.\n- The High Throughput Screening Facility, Flow Cytometry Facility, Advanced Sequencing Facility, and Cell Services of the Francis Crick Institute: Provided support.\n- The National Institute for Health Research (NIHR) Biomedical Research Centre: Provided computational resources.\n- The Medical Research Council: Provided support.\n- Cancer Research UK: Provided funding.\n- The International Cancer Genome Consortium: Provided data.\n- The National Institute for Health Research (NIHR) Cambridge Biomedical Research Centre: Provided support.\n- The authors also acknowledge the contributions of Zhuwen Wang and other anonymous reviewers for their peer review of this work.",
  "publication/journal": "Nature Communications",
  "publication/year": "2019",
  "publication/doi": "10.1038/s41467-019-10898-3",
  "publication/tags": "- Oesophageal Cancer\n- Molecular Stratification\n- Machine Learning\n- Gene Analysis\n- Data Science\n- Cancer Genomics\n- Clinical Features\n- Targeted Treatment\n- Protein-Protein Interaction\n- Network Properties\n\nNot sure if the tags provided are the ones used in the published article.",
  "dataset/provenance": "The dataset utilized in this study is sourced from the International Cancer Genome Consortium and The Cancer Genome Atlas. These are well-maintained centralized databases that have facilitated extensive research sharing, enabling the application of data science in this study.\n\nThe dataset comprises 261 esophageal adenocarcinoma (EAC) samples. These samples were analyzed using whole-genome sequencing (WGS) and RNA sequencing data, which are accessible through the European Genome-phenome Archive under the accession numbers EGAD00001004775 and EGAD00001004776, respectively. Additionally, The Cancer Genome Atlas (TCGA) data can be accessed through dbGaP with the accession number phs000178.v10.p8.\n\nThe data has been previously used in various studies, contributing to the broader community's understanding of esophageal adenocarcinoma. The dataset includes a comprehensive set of molecular and clinical features, which were used to stratify the EAC samples into six distinct clusters. These clusters exhibit differential responses to targeted treatments, highlighting the potential for personalized medicine approaches in managing esophageal adenocarcinoma.\n\nThe dataset's extensive and well-documented nature ensures reproducibility and transparency in our research, aligning with the guidelines for submitting code and software. All code used in this study was written in the R programming language, and scripts are available upon request. This adherence to open science principles allows for rigorous validation and potential replication of our findings by other researchers in the field.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The data used in this study is publicly available. The whole-genome sequencing (WGS) and RNA sequencing data can be accessed through the European Genome-phenome Archive using the accession numbers EGAD00001004775 and EGAD00001004776, respectively. Additionally, The Cancer Genome Atlas (TCGA) data is accessible via the dbGaP accession number phs000178.v10.p8. The source data for figures 4-6 is provided as a Source Data file.\n\nThe data is available under the terms of the European Genome-phenome Archive, which includes specific conditions for data access and use. These conditions ensure that the data is used responsibly and ethically, in accordance with the guidelines set by the archive. The data is made available to researchers who agree to these terms, which helps to enforce proper use and citation of the dataset.\n\nThe data collection sources include the International Cancer Genome Consortium and previous studies, as detailed in the main text. All code used in this study was written in R and is available upon request, with a strong encouragement for code deposition in a community repository such as GitHub. This approach promotes transparency and reproducibility in the research process.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Support Vector Machine (SVM), specifically a one-class SVM for novelty detection. This approach is not entirely new, but our implementation, named sysSVM, is tailored for predicting cancer genes in individual patients based on their molecular and systems-level properties.\n\nSysSVM builds upon previous efforts to identify novel cancer genes in each sample independently, rather than focusing on recurrently altered genes across sample cohorts. This sample-specific analysis is particularly advantageous for highly heterogeneous cancers, such as esophageal adenocarcinoma, where the mutational landscape is highly variable across samples and recurrently altered genes are rare.\n\nThe reason sysSVM was not published in a machine-learning journal is that our primary focus is on its application in cancer research. The algorithm is a tool to address a specific biological question, and its development was driven by the need to handle the complexities of cancer genomics data. The innovation lies in how sysSVM is applied to this biological context, rather than the algorithm itself. The study is published in a cancer research journal to highlight the biological significance and potential clinical implications of our findings.",
  "optimization/meta": "The meta-predictor in our study leverages the outputs of multiple machine-learning algorithms to enhance the prediction of cancer genes. This approach involves several steps and utilizes different kernels to identify the most relevant genes.\n\nThe process begins with the selection of best models through cross-validation iterations, accounting for the order of iterations by repeating the assessment multiple times with randomly reordered iterations. This results in multiple sets of best models.\n\nIn the training and prediction step, all these sets of best models are used to train on the entire training dataset. Cancer genes are then predicted from all genes with damaging alterations, excluding known cancer genes used for training. Each best model contributes to this prediction process.\n\nTo combine the predictions from the different kernels, a combined score (Sgs) is derived for each altered gene in each sample. This score considers the similarity of the gene's features to those in the training set, the rank of the gene in each kernel, and the sensitivity of each kernel. The genes are then ranked based on this score, and the top genes in each patient are retained for further comparison.\n\nThe final list of predicted cancer genes is determined by selecting the most frequent list of top genes across all sets of best models. This meta-predictor approach ensures that the predictions are robust and take into account the strengths of multiple machine-learning methods.\n\nThe training data for each kernel is independent, as the cross-validation process ensures that the data is split appropriately to avoid overlap between training and validation sets. This independence is crucial for the reliability of the meta-predictor.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, 34 features derived from both molecular and systems-level properties were mapped to 17,078 altered genes in the cohort. These features included somatic alterations, mutation burden, gene copy number, gene length, protein domain organization, gene duplicability, chromatin state, protein-protein interaction network properties, regulatory miRNAs, gene evolutionary origin, and gene expression breadth. To handle missing values, median imputation was used for continuous properties, and mode imputation for categorical properties. Each feature was then scaled to have zero mean and unit variance to normalize the different numerical ranges.\n\nThe molecular properties considered included various types of somatic alterations such as gene gains, losses, translocations, inversions, insertions, truncating and non-truncating damaging alterations, and gain-of-function mutations. Systems-level properties encompassed genomic, epigenomic, evolutionary, network, and gene expression features that differentiate cancer genes from other genes.\n\nThe preprocessing ensured that all features were on a comparable scale, which is crucial for the performance of machine-learning algorithms. This normalization step helped in mitigating the impact of different measurement units and ranges across the features. The imputation of missing values ensured that the dataset was complete and ready for analysis, preventing any potential biases that might arise from incomplete data.",
  "optimization/parameters": "In the optimization process, a grid search was employed to determine the best combination of parameters for each kernel. The parameters considered were nu, gamma, and degree.\n\nNu, which represents the upper bound on the fraction of outliers and the lower bound on the fraction of support vectors, was varied from 0.05 to 0.9 with a step of 0.05, resulting in 18 possible values.\n\nGamma, which influences the impact of individual training points in the final model, was defined as 2^(-7) to 2^(4), providing 12 possible values.\n\nDegree, specific to the polynomial kernel, had three possible values: 3, 4, and 9.\n\nThe grid search resulted in 18 combinations for the linear kernel, 216 combinations for the radial and sigmoid kernels, and 648 combinations for the polynomial kernel, totaling 1098 combinations.\n\nTo identify the best combination of parameters for each kernel, a user-defined number of iterations of three-fold cross-validation was performed. At each iteration, the genes of the training set were randomly split into two subsets, one for training and one for testing. Predictions were made on the test set, and the sensitivity of each set of parameters was computed. The least variant model among the top five most sensitive models in each kernel was chosen as the best model for that kernel. This process was repeated multiple times to account for the effect of increasing the number of cross-validation iterations.",
  "optimization/features": "In the optimization process, a total of 34 features are used as input. These features are derived from both molecular properties and systems-level properties of known cancer genes. Ten of these features come from molecular properties, while the remaining 24 are from systems-level properties. The features include a mix of categorical and continuous variables, with 22 being categorical and 12 being continuous.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, all 34 features were mapped to the altered genes in the sample cohort under study. These features were then scaled to zero mean and unit variance to correct for different numerical ranges. The importance of each feature in the classifiers was later investigated by ranking the features based on their weight, which indicated that all features contributed to the final gene classification. This approach ensures that the model considers a comprehensive set of features, capturing different regions of the feature space and increasing the chances of identifying rare helper genes.",
  "optimization/fitting": "In the fitting method employed, a grid search was utilized to optimize the parameters for each kernel, resulting in a total of 1098 parameter combinations. This number is indeed larger than the number of training points, which consisted of 476 known cancer genes. To address the potential issue of overfitting, a robust cross-validation strategy was implemented. Specifically, 10,000 iterations of three-fold cross-validation were performed. At each iteration, the genes were randomly split into training and test sets, and the sensitivity of each set of parameters was computed. The distribution of sensitivity was monitored every 100 iterations, and the least variant model among the top five most sensitive models in each kernel was selected. This approach helped to ensure that the models were not overly complex and were generalizable to unseen data.\n\nTo mitigate underfitting, the grid search covered a wide range of parameter values. For instance, the nu parameter, which represents the upper bound on the fraction of outliers, ranged from 0.05 to 0.9 with a step of 0.05. The gamma parameter, which influences the individual training points in the final model, was defined over a broad range of values. Additionally, the degree parameter for the polynomial kernel was tested with three different values. This comprehensive search ensured that the models were sufficiently flexible to capture the underlying patterns in the data. Furthermore, the selection of the best models based on both sensitivity and variance helped to balance the trade-off between bias and variance, thereby reducing the risk of underfitting.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was cross-validation, specifically a three-fold cross-validation approach with 10,000 iterations. This process involved randomly splitting the known cancer genes into training and test sets multiple times to evaluate the performance of our models consistently.\n\nTo further mitigate the risk of overfitting, we employed a grid search to optimize the parameters of the four kernels (linear, radial, sigmoid, and polynomial). At each increment of 100 cross-validation iterations, the four best models (one per kernel) were selected based on the median and variance of the sensitivity distribution across all previous iterations. This selection process was repeated five times, with the iterations randomly re-ordered each time. This cumulative assessment helped to account for the effect of the order of iterations and ensured that the best models were chosen based on a comprehensive evaluation.\n\nAdditionally, we used Recursive Feature Elimination (RFE) to quantify the relative contribution of the features to the models. RFE helped in ranking the features and recursively removing the least important ones, which further aided in preventing overfitting by focusing on the most relevant features.\n\nThese techniques collectively ensured that our models were robust and generalizable, reducing the likelihood of overfitting to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the supplementary materials. Specifically, the parameters of the 38 unique best models across four different kernels are detailed, along with the associated unique lists of top 10 genes. These lists are named using the number of genes that compose them, followed by a letter where the same number (but not the same genes) was found multiple times. The frequency that each set of best models and list of top 10 genes was found over 500 sets and lists is also shown.\n\nThe code used for data analysis in this study was written in the R programming language. The scripts are available upon request. For reproducibility, we strongly encourage the deposition of code in a community repository. The sysSVM package, which is central to our research, is distributed as an R package under R 3.4.0 and can be accessed at https://github.com/ciccalab/sysSVM. This repository provides the necessary tools and configurations to replicate the optimization processes described in our publication.\n\nRegarding the availability of model files, these are not explicitly mentioned in the provided details. However, the supplementary materials and the code repository should contain sufficient information to recreate the models and their configurations. For any specific requests or further details, please contact the corresponding author.",
  "model/interpretability": "The models presented in this publication are not entirely black-box systems. They incorporate interpretable components that allow for a degree of transparency. Specifically, the models include a selection of best models and a final list of helper genes, which are clearly outlined. These helper genes are identified and ranked, providing insights into the biological processes being modeled. The occurrence of these genes over multiple iterations is also documented, which helps in understanding their significance and reliability. This approach ensures that the model's decisions can be traced back to specific biological markers, making it more interpretable than purely black-box models. For instance, the list of top 10 genes associated with each model is provided, along with the frequency of their occurrence, which aids in validating the model's predictions and understanding the underlying biological mechanisms.",
  "model/output": "The model employed in our study is a classification model, specifically a support vector machine (SVM) variant known as sysSVM. This classifier is designed to identify helper genes in esophageal adenocarcinoma (EAC) by leveraging machine learning algorithms. The sysSVM classifier does not consider gene function directly in its classification process. Instead, it encodes network properties of proteins, such as node degree, betweenness, and whether a protein is central or a hub in the protein-protein interaction network. These properties are used to stratify 261 EACs into six distinct clusters, each exhibiting unique molecular and clinical features. This stratification suggests potential differential responses to targeted treatments.\n\nThe model's output includes a selection of best models and a final list of helper genes. These outputs are derived from various kernel types, including linear, radial, sigmoid, and polynomial kernels. The model's performance is validated through supplementary figures and tables, which detail the parameters of the unique best models and the frequency of occurrence of top gene lists over multiple sets. The study's methodology is replicable, and the algorithm is shareable, ensuring that the findings can be verified and built upon by other researchers. The statistical principles applied are sound, providing a robust framework for the identification of helper genes in EAC.",
  "model/duration": "The execution time for the model involved several steps. Initially, all 34 features derived from molecular and systems-level properties were mapped to the 17,078 altered genes in the cohort. Each feature was scaled to zero mean and unit variance to correct for different numerical ranges. This preprocessing step is crucial for ensuring that the model can effectively learn from the data.\n\nIn the model selection phase, a grid search using 10,000 iterations of a three-fold cross-validation was performed to optimize the parameters of the four kernels (linear, radial, sigmoid, and polynomial). At each iteration, the 476 known cancer genes were randomly split into a training set and a test set. The four best models, one per kernel, were chosen based on the median and variance of the sensitivity distribution across all previous iterations of cross-validation. This selection process was repeated 5 times, with all iterations randomly re-ordered, resulting in 500 best models.\n\nThese 500 best models were then trained with the whole training set and used to rank the remaining 16,602 unique genes in each patient. A score was measured to combine the predictions from the four kernels, and genes not expressed in normal esophagus according to GTEx annotation were excluded. This process produced 500 lists of top 10 genes.\n\nOut of the 500 best models, 38 had a unique set of parameters, resulting in 24 unique lists of top 10 genes. These lists ranged between 898 and 952 genes, with a core set of 598 genes shared across all of them. The most frequent top 10 list occurred 207 times, followed by another list that occurred 161 times. These two lists accounted for a significant portion of the 500 sets of top 10 genes and shared a large number of genes.\n\nTo quantify the relative contribution of the 34 features to the four best models used to predict the final set of helper genes, Recursive Feature Elimination (RFE) was implemented. RFE first defines the best set of parameters for each kernel, trains the one-class classifier in each kernel, and computes the weight of each feature. The features are then ranked according to their weights and recursively removed until no features remain. This process was repeated 34 times until all sysSVM features were ranked.\n\nOverall, the model execution involved extensive computational steps, including feature scaling, grid search with cross-validation, model training, and feature ranking. These steps ensured that the model was robust and that the final set of helper genes was reliable.",
  "model/availability": "The source code for the software used in this study is publicly available. The sysSVM algorithm, which is central to our research, is distributed as an R package. It can be accessed via GitHub, a community repository, under the R 3.4.0 version. This allows other researchers to utilize, modify, and build upon our work, promoting reproducibility and collaboration. The code is available upon request, and we encourage its deposition in community repositories for broader accessibility.",
  "evaluation/method": "The evaluation of our method involved a multi-step process designed to ensure robustness and reliability. Initially, we employed cross-validation to assess the performance of our models. This process was repeated multiple times with randomly reordered iterations to account for the effect of iteration order, producing multiple sets of best models. These models were then used for training on the entire training set, allowing us to predict cancer genes in individual samples.\n\nFor each altered gene in each sample, a combined score was derived, taking into account the similarity of the gene's features to those in the training set. This score was normalized to scale between 0 and 1, and genes were ranked based on this score. The top-ranked genes from each set of models were then compared, and the most frequently appearing genes were selected as the final list of predicted cancer genes.\n\nTo validate our findings, we reproduced the results in independent cohorts of esophageal adenocarcinomas. This step was crucial in confirming the reproducibility and generalizability of our method. Additionally, we conducted experimental examinations to validate the involvement of selected candidate genes in specific biological processes, such as cell proliferation.\n\nThe robustness of our analysis was further supported by the use of a comprehensive set of genes highlighted by our machine-learning algorithm. We also addressed the concern of false positives by validating the perturbations associated with the highlighted genes in normal cells, which indicated their role as \"helper\" genes.\n\nIn summary, our evaluation method combined rigorous cross-validation, independent cohort validation, and experimental validation to ensure the reliability and clinical relevance of our findings.",
  "evaluation/measure": "In the evaluation of our models, we focused on several key performance metrics to ensure a comprehensive assessment. These metrics include the frequency with which each set of best models and lists of top 10 genes were identified across 500 sets and lists. This approach allowed us to gauge the robustness and consistency of our model selection process.\n\nWe also validated our models using specific techniques, such as those depicted in Supplementary Figure 3, which provides insights into the performance and reliability of our sysSVM validation process.\n\nWhile we have not explicitly detailed every statistical test used, our reporting adheres to the guidelines set by Nature Research, ensuring transparency and reproducibility. This includes providing exact sample sizes, descriptions of statistical tests, and relevant assumptions or corrections made during the analysis.\n\nFor hierarchical and complex designs, we identified the appropriate levels for tests and reported outcomes fully. Additionally, we included estimates of effect sizes, such as Cohen's d or Pearson's r, to provide a clearer understanding of the magnitude of the effects observed.\n\nIn summary, our performance metrics are designed to be representative and align with established practices in the field. We have ensured that our reporting is thorough and transparent, allowing for reproducibility and validation by other researchers.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with both publicly available techniques and simpler baselines to ensure robustness and validity. We utilized benchmark datasets from reputable sources such as the International Cancer Genome Consortium and The Cancer Genome Atlas to perform these comparisons. This approach allowed us to assess the performance of our methods against established standards in the field.\n\nFor the comparison with simpler baselines, we implemented basic models to serve as reference points. These baselines helped us understand the incremental improvements provided by our more complex methods. The results indicated that our approaches outperformed these simpler models, demonstrating their effectiveness and superiority in handling the complexity of the data.\n\nAdditionally, we validated our methods using cross-validation techniques and reported the frequency at which our best models and top gene lists were identified across multiple iterations. This validation process ensured that our findings were consistent and not due to random chance. The statistical analyses were conducted using R, and the code is available upon request, adhering to transparency and reproducibility standards.\n\nIn summary, our evaluation included comprehensive comparisons with publicly available methods and simpler baselines, utilizing benchmark datasets and rigorous validation techniques. This thorough approach confirms the reliability and effectiveness of our methods.",
  "evaluation/confidence": "The evaluation of our method includes a comprehensive statistical analysis to ensure the robustness and reliability of our results. We have provided detailed information on the statistical tests used, including whether they are one- or two-sided. For all statistical analyses, we have included the exact sample size for each experimental group, specified whether measurements were taken from distinct samples or repeatedly from the same sample, and described all covariates tested. We have also reported the statistical parameters, including central tendency and variation, and provided test statistics with confidence intervals, effect sizes, and P values.\n\nOur study adheres to the guidelines for reporting statistical information, ensuring transparency and reproducibility. We have used common statistical tests and described more complex techniques in the Methods section. For null hypothesis testing, we have reported the test statistic with confidence intervals, effect sizes, degrees of freedom, and P values, providing exact values whenever suitable. This rigorous approach allows us to confidently claim the superiority of our method over others and baselines, supported by statistically significant results.",
  "evaluation/availability": "The raw evaluation files for figures 4-6 are provided as a Source Data file. This data is publicly available and can be accessed alongside the publication. The whole-genome sequencing (WGS) and RNA sequencing data used in the evaluation can be accessed at the European Genome-phenome Archive with the accession numbers EGAD00001004775 and EGAD00001004776, respectively. The data is available under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. The license also requires providing a link to the Creative Commons license and indicating if changes were made. For more details on the license, you can visit the Creative Commons website."
}