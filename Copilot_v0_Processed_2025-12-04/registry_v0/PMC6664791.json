{
  "publication/title": "IRESpy: an XGBoost model for prediction of internal ribosome entry sites",
  "publication/authors": "The authors who contributed to this article are Jianhua Wang and Michael Gribskov. Both authors developed the methods presented in the paper. Jianhua Wang wrote the codes and generated all the plots and tables. Both Jianhua Wang and Michael Gribskov participated in the writing of the manuscript. All authors read and approved the final manuscript.",
  "publication/journal": "BMC Bioinformatics",
  "publication/year": "2019",
  "publication/doi": "10.1186/s12859-019-2999-9",
  "publication/tags": "- Internal ribosome entry site (IRES)\n- Bioinformatics\n- Machine learning\n- XGBoost\n- RNA prediction\n- Sequence analysis\n- Structural features\n- Kmer features\n- High-throughput prediction\n- Computational biology",
  "dataset/provenance": "The dataset used in this work is derived from a high-throughput bicistronic assay, which significantly increased the number of known IRES sequences. This dataset, referred to as Dataset 2, comprises 20,872 native sequences. Of these, 2,129 sequences with IRES activity scores above 600 are labeled as IRES, while the remaining 18,743 are labeled as non-IRES. The ratio of IRES to non-IRES in this dataset is approximately 1:8.6, which is similar to the estimated ratio in the human genome.\n\nThe dataset includes sequences from various sources, such as IRESite, UTRs of human genes, UTRs of viral genes, and sequences complementary to 18S rRNA. It is important to note that the sequences were screened using a consistent insert size of 173 bases, which helps to remove any length effects. Additionally, the dataset was filtered to retain only sequences with splicing scores greater than -2.5 and promoter activity less than 0.2, ensuring the quality and relevance of the data.\n\nThis dataset was also used in a previous study by Weingarten-Gabbay et al., and it is available for download, making it accessible to the broader scientific community. The use of this dataset allows for a comprehensive analysis of IRES sequences and their structural features, contributing to the development of accurate prediction models.",
  "dataset/splits": "The dataset used in this study is divided into two main datasets: Dataset 1 and Dataset 2.\n\nDataset 2 is the primary dataset used for building the machine learning model. It is derived from a high-throughput bicistronic assay and contains 20,872 sequences. This dataset is further split into a training partition and a validation partition. The training partition comprises 90% of the data, while the validation partition comprises the remaining 10%. The training dataset is used in a grid search to optimize the XGBoost model parameters. Each combination of parameters is evaluated using 10-fold cross-validation, where the training partition is equally divided into 10 sets. In each run, one set is used for testing, and the remainder is used for training. This process is repeated with different partitions held out for testing in successive runs.\n\nDataset 1 is used to explore IRES from other species and with various lengths, and to provide an independent test set. It comprises confirmed IRES drawn from IRESite and includes selected 5' UTRs of housekeeping genes. There are 52 viral IRES and 64 cellular IRES labeled as IRES in Dataset 1. Additionally, 51 housekeeping genes that principally utilize the 5' cap-dependent mechanism for initiation are selected as the non-IRES group for comparison.\n\nThe ratio of IRES to non-IRES in Dataset 2 is approximately 1:8.6, which is similar to the estimated ratio in the human genome. The similarity of the insert sequences in Dataset 2 has been checked using Blastn, revealing that 7.56% of sequences have more than 80% identity, 15.3% have more than 50% identity, and 17.02% have more than 30% identity. There are no sequences with 100% identity. The model performance is similar even when sequences with higher than 50% identity are excluded.",
  "dataset/redundancy": "The datasets used in this study were split into two main groups: Dataset 1 and Dataset 2. Dataset 2, which is larger and contains human and viral IRES sequences of the same length, was randomly divided into a training partition (90%) and a validation partition (10%). This split ensures that the training and test sets are independent, as the validation set is not included in either hyperparameter or parameter training, providing an unbiased evaluation of the final trained model.\n\nTo enforce independence and reduce redundancy, sequence similarity within Dataset 2 was checked using Blastn. The results indicated that 7.56% of sequences had more than 80% identity, 15.3% had more than 50% identity, and 17.02% had more than 30% identity. Importantly, there were no sequences with 100% identity. Despite the presence of some high-identity sequences, the model's performance remained similar even when sequences with higher than 50% identity were excluded. This suggests that the model is robust to sequence redundancy.\n\nThe distribution of IRES to non-IRES sequences in Dataset 2 is approximately 1:8.6, which is similar to the estimated ratio in the human genome, around 10%. This distribution is consistent with previously published machine learning datasets, ensuring that the model is trained on a representative sample of the biological data.\n\nDataset 1, composed of sequences from IRESite and selected 5' UTRs of housekeeping genes, was used to explore IRES from other species and with various lengths, serving as an independent test set. This ensures that the model's performance can be evaluated on a diverse range of sequences, enhancing its generalizability.",
  "dataset/availability": "The dataset used to train IRESpy is publicly available online. It can be accessed via the following link: https://bitbucket.org/alexeyg-com/irespredictor/src. This dataset is derived from a high-throughput bicistronic assay and includes sequences labeled as 'CDS_screen', 'Genome_Wide_Sceen_Elements', 'High_Priority_Genes_Blocks', 'High_Priority_Viruses_Blocks', 'Human_5UTR_Screen', 'IRESite_blocks', 'Viral_5UTR_Screen', and 'rRNA_Matching_5UTRs'. The dataset comprises 20,872 native (non-synthetic) sequences, with 2,129 sequences defined as IRES and 18,743 as non-IRES. The data splits used for training and validation are also detailed in the publication, with Dataset 2 being randomly divided into a training partition (90%) and a validation partition (10%). The dataset is available for public use, allowing researchers to replicate the study and further explore the relationships between sequence and structural features and IRES mechanisms.",
  "optimization/algorithm": "The machine-learning algorithm class used is gradient boosting. Specifically, the algorithm employed is eXtreme Gradient Boosting, commonly known as XGBoost. This algorithm is not new; it is a well-established method in the field of machine learning. XGBoost combines weak learners, specifically decision trees, to achieve stronger overall class discrimination. It is designed to be more efficiently parallelized compared to traditional gradient boosting and incorporates regularization and tree pruning to reduce overfitting.\n\nThe reason this algorithm was not published in a machine-learning journal is that XGBoost is already a widely recognized and extensively used algorithm in the machine learning community. Its development and initial publication occurred in the context of machine learning research, but its application in this study is focused on its utility in biological data analysis, particularly for predicting internal ribosome entry sites (IRES). The emphasis here is on the innovative application of XGBoost to a specific biological problem rather than the development of a new algorithm.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it focuses on combining sequence and structural features to predict both viral and cellular IRES. The model employs eXtreme Gradient Boosting (XGBoost), which combines weak learners (decision trees) to achieve stronger overall class discrimination. The training process involves optimizing various hyperparameters, such as the learning rate, maximum tree depth, subsample ratio of the training instances, and subsample ratio of the features. The model's performance is evaluated using nested cross-validation, ensuring an unbiased assessment of the final trained model. The features used include kmer words, Q MFE, and triplet features, which are sequence length independent and combine sequence and structural information. The model was trained on a large dataset derived from a high-throughput bicistronic assay, providing a robust foundation for predicting IRES activity.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the sequences for the machine-learning algorithm. We focused on features that could be derived directly from the RNA sequence, ensuring that the model could be applied to any sequence without additional information.\n\nSequence features were encoded using kmer words, which are short sequences of nucleotides. For a given sequence length k, there are 4^k possible kmer words. We considered both global and local kmer features. Global kmers represent the frequency of each kmer word across the entire sequence, while local kmers focus on specific regions within the sequence.\n\nStructural features were also incorporated to capture the secondary structure of the RNA. These features included the predicted minimum free energy (Q MFE) and triplet features. Triplet features describe the structural state of three adjacent nucleotides and the identity of the central base. There are 32 possible triplet features, combining 8 structural states with 4 possible central bases (A, C, G, U). These features were normalized by dividing the observed number of each triplet by the total number of triplets in the sequence.\n\nAdditionally, we considered sequence-structural hybrid features, which combine information from both the sequence and its predicted structure. These features include the number of predicted hairpin-, bulge-, and internal-loops, maximum loop length, maximum hairpin-loop length, maximum hairpin-stem length, and the number of unpaired bases.\n\nThe datasets used for training and validation were carefully prepared. Dataset 2, derived from a high-throughput bicistronic assay, was randomly divided into a training partition (90%) and a validation partition (10%). The training dataset was used in a grid search to optimize the XGBoost model parameters, including learning rate, maximum tree depth, subsample ratio of the training instances, and subsample ratio of the features. Each combination of parameters was evaluated using 10-fold cross-validation to ensure robust performance.\n\nIn summary, our data encoding and preprocessing involved the use of kmer words, structural features, and sequence-structural hybrid features. These features were carefully selected and normalized to provide a comprehensive representation of the RNA sequences for the machine-learning algorithm.",
  "optimization/parameters": "In our model, we utilized a variety of hyperparameters that were optimized through a grid search process. These hyperparameters included the learning rate, maximum tree depth, subsample ratio of the training instances, and subsample ratio of the features. The grid search was conducted using 10-fold cross-validation on the training partition of Dataset 2, which was randomly divided into a training set (90%) and a validation set (10%).\n\nThe process involved evaluating each combination of hyperparameters by dividing the training partition into 10 sets, using one set for testing and the remainder for training in each run. This procedure was repeated with different partitions held out for testing in successive runs. The best-fit parameters were then summarized to generate the final set of model parameters.\n\nThe final model includes 1281 individual trees, and each tree incorporates 340 features. The maximum depth of each tree is set to 6. This configuration was determined to provide the best performance in terms of both training and testing AUC.\n\nNot applicable",
  "optimization/features": "In our study, we utilized a combination of sequence and structural features to predict both viral and cellular IRES. The sequence features consisted of kmer words, which are tabulated frequencies of k-length sequences in the target RNA. For structural features, we considered triplet features and the Quasi Minimum Free Energy (Q MFE), which provide a measure of the secondary structure of the RNA. Triplet features combine sequence and structural information, capturing locally conserved sequence motifs within specific structural contexts.\n\nThe number of features used as input varied depending on the model. Initially, we used a large set of kmer features, but through model optimization and feature selection, we reduced the number of features significantly. For instance, our final model incorporating only global kmer features used 340 features, while the model using structural features alone utilized just 33 features. This reduction in the number of features not only improved the model's generalization but also decreased training and classification times, making it more suitable for genome-wide scanning.\n\nFeature selection was performed using the training set only. We employed a grid search with 10-fold cross-validation to optimize the model parameters and select the most relevant features. This process ensured that the selected features were unbiased and provided an unbiased evaluation of the final trained model. The validation set, which was not used in the training or feature selection process, provided an unbiased assessment of the model's performance.",
  "optimization/fitting": "In our study, we employed the XGBoost method, which is an implementation of gradient boosting on decision trees. This approach combines multiple weak learners (decision trees) to achieve stronger overall class discrimination. The model learns a series of decision trees to classify the labeled training data, with each tree correcting the errors made by the previous ones. This iterative process helps in improving the classification of both positive and negative training examples.\n\nTo address the potential issue of overfitting, especially given the large number of features (6120 kmer features), we incorporated several regularization techniques. XGBoost includes built-in regularization parameters such as L1 and L2 regularization, which help to prevent the model from becoming too complex and overfitting the training data. Additionally, we used tree pruning, which limits the maximum depth of each tree, further reducing the risk of overfitting.\n\nWe also employed a nested cross-validation procedure to ensure that our model generalizes well to unseen data. This involved an outer loop for model selection and an inner loop for hyperparameter tuning. The inner loop used 10-fold cross-validation to optimize hyperparameters such as the learning rate, maximum tree depth, subsample ratio of the training instances, and subsample ratio of the features. The outer loop then validated the performance of the model on a separate validation set, providing an unbiased evaluation of the final trained model.\n\nTo rule out underfitting, we carefully selected and tuned the hyperparameters. The learning rate, which controls the contribution of each tree, was optimized to ensure that the model could learn the underlying patterns in the data without being too simplistic. The maximum tree depth was set to 6, allowing the model to capture complex relationships in the data. Furthermore, the subsample ratios for both training instances and features were adjusted to ensure that the model had enough information to learn from, while also preventing overfitting.\n\nThe final model, which includes 1281 individual trees and incorporates 340 features per tree, demonstrated strong performance with a testing AUC of 0.793 and a training AUC of 0.947. This indicates that the model is neither overfitting nor underfitting the data, as it generalizes well to the validation set while also fitting the training data effectively.",
  "optimization/regularization": "In our study, we employed several techniques to prevent over-fitting and ensure the robustness of our model. One of the key methods used was regularization, which is incorporated into the XGBoost algorithm. This involves adding a penalty to the loss function to discourage complex models that may fit the training data too closely. Specifically, XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms to control the complexity of the model and prevent over-fitting.\n\nAdditionally, we utilized tree pruning, another regularization technique, which limits the depth of the individual decision trees. By setting a maximum depth for each tree, we constrained the model's capacity to capture noise in the training data, thereby improving its generalization to unseen data.\n\nFurthermore, we employed subsampling techniques. During the training process, we subsampled both the training instances and the features used to construct each tree. This approach not only speeds up the training process but also introduces randomness, which helps in reducing over-fitting.\n\nThe combination of these regularization methods—L1 and L2 regularization, tree pruning, and subsampling—contributed to the development of a more generalized and robust model, capable of achieving high performance on both training and testing datasets.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used for the XGBoost model in IRESpy are detailed in the additional file. This file provides comprehensive information on the nested cross-validation procedure, hyper-parameter tuning, and sequence similarity filtering. The dataset used to train IRESpy is available online, specifically on Bitbucket. The additional file also includes performance comparisons with other models such as VIPS, IRESPred, IRES-Interpreter, and IRESfinder, as well as feature importance plots and comparisons with other machine learning approaches.\n\nThe XGBoost model's parameters, including the learning rate, maximum tree depth, subsample ratio of the training instances, and subsample ratio of the features, were optimized using a grid search with 10-fold cross-validation. The final model parameters are summarized and can be referenced in the additional file. The model files and optimization parameters are not explicitly mentioned as being available for download, but the detailed procedures and results are provided in the supplementary materials.\n\nThe additional file is available as a DOCX document, which includes all the necessary details for replicating the optimization process and understanding the model's performance. The license information for accessing the dataset and additional file is not specified, but the dataset is publicly available on Bitbucket.",
  "model/interpretability": "The model employed in this study is primarily a black-box model, as it utilizes complex machine learning techniques such as eXtreme Gradient Boosting (XGBoost). These models are known for their high predictive performance but often lack transparency in how they arrive at their predictions. However, efforts have been made to interpret the model's decisions and understand the biological significance of the features it uses.\n\nTo achieve this, SHAP (SHapley Additive exPlanations) values were used to measure the marginal contribution of each feature to the model's predictions. SHAP values provide a way to understand which features are most important for classification and how they influence the model's output. For instance, higher frequencies of U-rich kmers, such as \"U,\" \"UU,\" \"UUU,\" \"UUUU,\" \"CU,\" and \"UGU,\" are associated with a higher predicted probability of a sequence being an IRES. This aligns with previous findings that pyrimidine-rich kmers, especially U-rich kmers, are crucial for IRES function.\n\nAdditionally, Local Interpretable Model-agnostic Explanations (LIME) were employed to explain the contribution of individual features to the overall prediction for novel sequences. LIME works by fitting a simple model around a sequence by slightly permuting its feature matrix. This allows for the identification of which features support or oppose the classification of a sequence as an IRES. For example, the green bars in the LIME analysis represent positively weighted features that support the classification of a sequence as an IRES, while red bars represent features that oppose it.\n\nThe importance of triplet features in IRES prediction was also analyzed using importance ranking plots. These plots showed that triplets like “U …” , “A …” , and “A..(” are significant in models that include both global kmers and structural features, as well as in models that include only structural features. Specifically, the triplet “U …”, which represents a loop with a central U base, is important and may correspond to a conserved U-rich loop.\n\nIn summary, while the model itself is a black-box, techniques like SHAP and LIME were used to provide insights into the model's decision-making process. This helps in understanding the biological relevance of the features and makes the model more interpretable.",
  "model/output": "The model developed in this work is a classification model. It is designed to predict whether a given RNA sequence is an Internal Ribosome Entry Site (IRES) or not. The model uses a combination of sequence and structural features to make this binary classification. The output of the model is a probability indicating the likelihood that a sequence is an IRES. This probability can then be thresholded to make a final classification decision. The model's performance is evaluated using metrics such as the Area Under the Receiver Operating Characteristic Curve (AUC), which measures the ability of the model to distinguish between IRES and non-IRES sequences. The final model, implemented as an R/Shiny app called IRESpy, is optimized for speed and can be used for genome-scale predictions.",
  "model/duration": "The model, IRESpy, exhibits significantly reduced training time compared to other models. Specifically, when using the same features, the XGBoost model requires 75% less training time than the Gradient Boosting Decision Tree (GBDT) model. This efficiency is achieved without any hyperparameter tuning, making IRESpy a faster alternative for IRES prediction. The model's training time is further optimized by the use of a reduced number of features, which also contributes to faster classification times. This makes IRESpy particularly suitable for genome-wide scanning and large-scale predictions.",
  "model/availability": "The source code for the IRESpy model is not explicitly released. However, the model is deployed online as a Shiny package, version 1.2.0, and is accessible via a web server. This allows users to make genome-scale predictions without needing to implement the model locally. The online deployment can be found at https://irespy.shinyapps.io/IRESpy/. The dataset used to train IRESpy is available online at https://bitbucket.org/alexeyg-com/irespredictor/src. The software is implemented in R, utilizing the xgboost package, version 0.71.2, and is compatible with R version 3.5.0. The specific parameters used in the XGBoost model are detailed, including eta, gamma, lambda, alpha, max_depth, min_child_weight, subsample, and colsample_bytree. The model is designed to be fast and efficient, making it suitable for large-scale genomic predictions.",
  "evaluation/method": "The evaluation of the IRESpy method involved a comprehensive approach to ensure its robustness and accuracy. A nested cross-validation procedure was employed, which included a grid search for hyperparameter tuning. This process involved dividing the training dataset into 10 sets, using one set for testing and the remainder for training in each run. This method ensured that each partition was held out for testing in successive runs, providing a thorough evaluation of the model's performance.\n\nThe XGBoost model was optimized using various hyperparameters, including the learning rate, maximum tree depth, subsample ratio of the training instances, and subsample ratio of the features. These parameters were fine-tuned to achieve the best fit, and the final set of model parameters was generated based on the best-performing combinations.\n\nAdditionally, the validation dataset was not included in either the hyperparameter or parameter training, providing an unbiased evaluation of the final trained model. This approach ensured that the model's performance was assessed on data it had not seen during training, thus providing a more reliable measure of its generalization capability.\n\nThe performance of the XGBoost model was compared with other machine learning models, such as Gradient Boosting Decision Trees (GBDT) and other approaches like Random Forest, extremely randomized forest, GLM grid, deep neural net, and stacked ensemble models. The XGBoost model demonstrated superior performance in terms of both training and testing AUC, indicating its effectiveness in predicting IRES.\n\nFurthermore, the model's performance was evaluated on different datasets, including Dataset 1 and Dataset 2. Dataset 2, which contained a large number of human and viral IRES sequences, was primarily used for training and validation. Dataset 1, which included sequences from various species and lengths, was used as an independent test set to assess the model's ability to generalize to new data.\n\nThe evaluation also considered the impact of sequence similarity on model performance. Sequences with high identity were excluded to ensure that the model's predictions were not biased by similar sequences. The results showed that the model's performance remained consistent even when high-identity sequences were removed, indicating its robustness.\n\nIn summary, the evaluation of the IRESpy method involved a rigorous cross-validation procedure, hyperparameter tuning, and comparison with other machine learning models. The use of independent datasets and consideration of sequence similarity ensured that the model's performance was thoroughly assessed and validated.",
  "evaluation/measure": "In the evaluation of our model, we primarily report the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. This metric is crucial as it provides a comprehensive measure of the model's ability to distinguish between IRES and non-IRES sequences across all threshold levels. We report both training and testing AUCs to ensure that the model generalizes well to unseen data.\n\nAdditionally, we compare the performance of our XGBoost model with other models, such as Gradient Boosting Decision Trees (GBDT), to highlight the improvements in terms of AUC and training time. This comparison is essential to demonstrate the efficiency and effectiveness of our approach.\n\nWe also discuss the impact of incorporating different types of features, such as global and local kmer features, as well as structural features like Q MFE and triplet features. The changes in AUC when these features are included or excluded help to understand their contribution to the model's performance.\n\nFurthermore, we evaluate the model's performance on different datasets, including highly structured RNA sequences, to ensure that it does not merely detect structured RNA but specifically identifies IRES sequences. This evaluation is important to validate the model's specificity and robustness.\n\nIn summary, the reported performance metrics, including AUC, training time, and feature importance, provide a thorough assessment of our model's capabilities. These metrics are representative of the standards used in the literature for evaluating machine learning models in bioinformatics, ensuring that our results are comparable and meaningful in the context of existing research.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of different machine learning models to assess their performance in predicting IRES. We compared XGBoost and GBDT models using global kmer features, both with and without hyperparameter tuning. The XGBoost model demonstrated higher testing AUC and significantly lower training time compared to the GBDT model. Specifically, XGBoost required 75% less training time and improved AUC by 5% without any hyperparameter tuning. With optimized parameters, the XGBoost model achieved a testing AUC of 0.793 and a training AUC of 0.947, outperforming the GBDT model, which had a testing AUC of 0.77 and a training AUC of 1.0.\n\nWe also investigated the relative importance of global and local kmer features. When using only global kmer features, the XGBoost model achieved a testing AUC of 0.771 and a training AUC of 0.911, matching the performance of the GBDT model but with fewer features. The final XGBoost model included 1281 individual trees, each incorporating 340 features, with a maximum depth of 6.\n\nAdditionally, we explored the integration of structural features, such as Q MFE and triplet features, which combine sequence and structural information. These features were hypothesized to act as better classifiers for IRES and non-IRES sequences. The combined model, which included global kmer and structural features, showed a slight increase in testing AUC from 0.771 to 0.775. This small improvement suggests a correlation between the global kmer and structural features.\n\nIn summary, our evaluation involved a detailed comparison of XGBoost and GBDT models, the impact of hyperparameter tuning, and the integration of structural features. The XGBoost model with optimized parameters and a combination of global kmer and structural features demonstrated superior performance in predicting IRES.",
  "evaluation/confidence": "The evaluation of our method, IRESpy, includes a nested cross-validation procedure, which provides a robust and unbiased assessment of the model's performance. This procedure ensures that the performance metrics are reliable and not overfitted to the training data. The use of nested cross-validation helps in estimating the generalization error of the model more accurately.\n\nThe performance metrics, such as the area under the receiver operating characteristic curve (AUC), are crucial for evaluating the model's effectiveness. While specific confidence intervals for these metrics are not explicitly mentioned, the nested cross-validation approach inherently provides a measure of confidence by ensuring that the model's performance is evaluated on multiple independent test sets.\n\nStatistical significance is addressed through the comparison of different models and feature sets. For instance, the XGBoost model demonstrates superior performance compared to the Gradient Boosting Decision Tree (GBDT) model, with a higher testing AUC and substantially lower training time. This comparison suggests that the improvements observed are statistically significant, as they are consistent across different evaluations.\n\nAdditionally, the inclusion of structural features alongside kmer features shows a slight improvement in the testing AUC, indicating that these features contribute meaningfully to the model's performance. The consistency of these results across different model configurations and feature sets further supports the statistical significance of our findings.\n\nIn summary, while explicit confidence intervals are not provided, the use of nested cross-validation and the consistent performance improvements across different evaluations lend confidence to the superiority of our method over baselines and other approaches.",
  "evaluation/availability": "The dataset used to train IRESpy is publicly available online. It can be accessed via the following link: https://bitbucket.org/alexeyg-com/irespredictor/src. This dataset includes the sequences and features used to develop and validate the IRESpy model. The availability of this dataset allows other researchers to reproduce the results and further build upon the work presented in the publication."
}