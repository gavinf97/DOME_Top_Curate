{
  "publication/title": "Modeling and machine learning for ECG feature engineering",
  "publication/authors": "The authors who contributed to the article are Carlos A. Ledezma, Xin Zhou, Blanca Rodríguez, P. J. Tan, and Vanessa Díaz-Zuccarini.\n\nCarlos A. Ledezma was involved in conceptualization, data curation, formal analysis, methodology, software, validation, visualization, and writing the original draft and reviewing & editing the manuscript.\n\nXin Zhou contributed to the methodology and writing the original draft and reviewing & editing the manuscript.\n\nBlanca Rodríguez was involved in conceptualization, supervision, and writing the original draft and reviewing & editing the manuscript.\n\nP. J. Tan contributed to supervision and writing the original draft and reviewing & editing the manuscript.\n\nVanessa Díaz-Zuccarini was involved in conceptualization, supervision, and writing the original draft and reviewing & editing the manuscript.",
  "publication/journal": "PLOS ONE",
  "publication/year": "2019",
  "publication/doi": "10.1371/journal.pone.0220294",
  "publication/tags": "- Modeling\n- Machine Learning\n- ECG\n- Ischemia\n- Cardiac Cells\n- Neural Networks\n- Biomarkers\n- Coronary Heart Disease\n- Virtual Databases\n- Simulation\n- Feature Engineering\n- Deep Learning\n- Classification\n- Cardiovascular Research\n- Biomedical Data\n- Computational Biology\n- Medical Diagnostics\n- Data Analysis\n- Heart Disease Detection\n- Artificial Neural Networks",
  "dataset/provenance": "The dataset used in this work consists of pECG feature data derived from three distinct datasets. These datasets were generated from the TP06 cable models under different conditions: Control ePoM, Mild ePoM, and Severe ePoM. The signals utilized were the pECG calculated from the last beat of each model once a steady-state response was achieved.\n\nEach of the three datasets contains 2044 signals, resulting in a total database of 6132 pECG beats used for training. This balance between the classes is advantageous for training, as it prevents the network from being biased towards any particular class and enhances the model's generalization.\n\nThe data used in this study can be simulated by following the methods explained in the paper, ensuring reproducibility and accessibility for further research. The synthetic databases and machine learning methods presented here serve as a valuable precedent for future clinical studies, providing a foundation for exploring the effects of ischemia on cardiac cells and validating proof-of-concept models for the early detection of coronary heart disease.",
  "dataset/splits": "The dataset used in this study was divided into three main groups based on the severity of the condition: Control, Mild, and Severe. Each of these groups contained 2044 signals, resulting in a total of 6132 pECG beats used for training. The data was split into a training set and an evaluation set, with the training set containing 75% of the data and the evaluation set containing 25%. The training set was further divided using a 10-fold cross-validation approach, ensuring that the model's performance was robust and generalizable. This method helps in minimizing overfitting and bias, and it aids in reducing the impact of outliers in the data. The evaluation set was used to test the model's performance on data that was not part of the training process, providing an estimate of the model's generalization error.",
  "dataset/redundancy": "The datasets used in this work were split into training and evaluation sets to ensure independence between the data used for training the models and the data used for evaluating their performance. Specifically, 75% of the data was allocated to the training set, while the remaining 25% was reserved for the evaluation set. This split was done randomly to maintain the balance between the classes, which is crucial for avoiding bias and improving the generalization of the models.\n\nTo enforce the independence of the training and test sets, a 10-fold cross-validation procedure was employed during the training phase. In each fold, the weights of the artificial neural networks (ANNs) were initialized to random small values, and all training examples were propagated through the network. The cost was calculated using a binary cross-entropy function regularized with weight decay. The derivatives of the cost with respect to the network's weights were computed using backpropagation, and the weights were updated following the gradient descent method. The weights that performed best during the cross-validation were then tested on the evaluation set, and the performance on this set was reported in the results.\n\nThe distribution of the datasets used in this study is unique compared to previously published machine learning datasets. The datasets were formed by three groups of pECG feature data: the TP06 cable Control ePoM, the TP06 cable Mild ePoM, and the TP06 cable Severe ePoM. Each of these datasets contains 2044 signals, resulting in a total database of 6132 pECG beats. This balanced distribution is favorable for training because it avoids biasing the network towards a given class and improves the generalization of the model. The use of synthetic data allowed for a noise-free environment, which simplifies the measurement of pECG features. However, it is important to note that real-life acquisitions may introduce noise and variability that could impact the performance of the machine learning models. Future work will address these limitations by using real-life ECG databases in conjunction with the simulated data presented here.",
  "dataset/availability": "The data used in this study is not publicly released in a forum. All relevant data is contained within the paper and its supporting information files. The data used in this study can be simulated by following the methods explained in the paper. This ensures that the data can be reproduced by others, maintaining the integrity and reproducibility of the research. The data is not subject to a specific license, but it is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",
  "optimization/algorithm": "The machine-learning algorithms used in this work belong to the class of artificial neural networks (ANNs), specifically deep learning algorithms. These algorithms are not new; they are well-established in the field of machine learning and have been widely used for various classification tasks, including ECG signal analysis.\n\nThe choice of using deep learning algorithms was driven by their proven capability to model complex non-linear relationships in data, which is crucial for accurately detecting and classifying ischemic events in ECG signals. Deep learning algorithms, particularly ANNs, are known for their ability to learn from large datasets and generalize well to unseen data, making them suitable for this study.\n\nThe focus of this publication is on the application of these algorithms to the specific problem of detecting and classifying ischemic events in ECG signals, rather than on the development of new machine-learning algorithms. Therefore, the algorithms used are well-documented and have been extensively validated in previous research. The novelty of this work lies in the application of these algorithms to a unique dataset of simulated ECG signals, which includes inter-subject variability and captures the evolution of ischemic events in cardiac cells.\n\nThe results demonstrate the effectiveness of deep learning algorithms in this context, highlighting their potential as a tool for assisting clinicians in the diagnosis of coronary heart disease. Future work may explore other machine-learning approaches, such as Bayesian methods, support vector machines, or decision trees, to compare their performance with deep learning algorithms and to enhance the interpretability of the models.",
  "optimization/meta": "The models discussed in this work do not use data from other machine-learning algorithms as input. Instead, they directly use the magnitudes of pECG biomarkers as inputs. The models include logistic regression classifiers and artificial neural networks (ANNs).\n\nThree logistic regression classifiers were trained to provide a baseline performance. These classifiers were designed to distinguish between different classes: LR1 for Control vs. any other signal, LR2 for Mild vs. any other signal, and LR3 for Severe vs. any other signal.\n\nThe ANN models include three different architectures. ANN1 is designed to discriminate between Control, Mild, and Severe populations using one-hot encoded vectors. ANN2 is a cascade of two networks: ANN21 discriminates between non-ischemic (Control) and ischemic (Mild or Severe) signals, while ANN22 distinguishes between Mild and Severe ischemic signals using the ratio of change between the biomarkers.\n\nThe training process for these models involves dividing the data into a training set and an evaluation set, with the training set further divided into ten subsets for 10-fold cross-validation. This approach ensures that the models are tested on data that was not used during training, providing an estimate of their generalization capability. The use of cross-validation helps to avoid issues such as overfitting and bias, and it minimizes the effect of outliers in the data.\n\nThe performance of the models is evaluated using metrics such as sensitivity, positive predictive value, and F1-score. The best-performing model, as determined by the highest F1-score during cross-validation, is then applied to the evaluation set. This method ensures that the training data is independent and that the models are robust and generalizable.",
  "optimization/encoding": "The data encoding process involved creating a design matrix X, where each row represented an example and each column represented a feature. Additionally, a target matrix Y was defined, with each row corresponding to the target for each training example and each column to an element in the class-encoding vector. One-hot encoding was used to produce the classification targets.\n\nThe input to the network was a matrix that included a bias term, resulting in an input matrix of the form Xb, with size N ×(M + 1), where N is the number of training examples, M is the number of input features, and xij corresponds to feature j of example i. This matrix structure allowed for efficient computation using matrix operations.\n\nThe training data consisted of pECG feature data from three datasets: Control, Mild, and Severe ePoM signals from TP06 cable models. Each dataset contained 2044 signals, totaling 6132 pECG beats. The signals used were the pECG calculated from the last beat of each model once a steady-state response was achieved. The balance between the classes was maintained, which is favorable for training as it avoids biasing the network towards a given class and improves the generalization of the model.\n\nThe learning task for each of the networks was to classify the signals into their respective datasets based on the features measured from a given pECG signal. The training process involved dividing the examples into a training set and an evaluation set, with 75% and 25% of the data, respectively. The training set was used to perform 10-fold cross-validation, where the weights of the ANN were initialized to random small values in each iteration. The cost was calculated using a binary cross-entropy function regularized with weight decay, and the weights were updated using gradient descent.\n\nThe performance of each trained model was measured using the Positive Predictive Value (PPV) and Sensitivity (Se), and the models were compared using the F1-score. The hyperparameter search method involved varying the number of hidden layers and hidden neurons per layer until the best-performing network was found. This method is similar to those used to achieve state-of-the-art performances in ECG classification.",
  "optimization/parameters": "In the optimization process, the number of parameters (p) used in the model varied depending on the specific neural network architecture being trained. For the artificial neural networks (ANNs), the parameters included the weights (θi) associated with the input features. The input features consisted of the magnitudes of the pECG biomarkers.\n\nThe selection of the number of hidden layers (L) and hidden neurons per layer (S) was determined through a grid search approach. This involved incrementally varying L and S starting from 1, until an optimal performance was achieved. The optimal topology was identified when no significant improvement was observed by increasing the complexity of the network or when a decrease in performance was noted.\n\nFor the logistic regression classifiers, the parameters included the weights (θi) associated with the input features, which were the magnitudes of the pECG features. These weights were trained following a learning procedure and cross-validation to ensure robust performance.\n\nThe evaluation metrics, such as sensitivity, positive predictive value, and F1-score, were used to assess the performance of the models. The best-performing models were selected based on these metrics, ensuring that the models generalized well to unseen data.",
  "optimization/features": "The input features used in the neural networks were derived from the pECG signals. Specifically, the magnitudes of the pECG biomarkers were used as inputs for the networks. These biomarkers included the QT interval, ST deviation, QRS amplitude, T wave amplitude, QRS duration, and T wave duration. The number of input features (f) is six.\n\nFeature selection was not explicitly mentioned as a separate process. However, the relative importance of each feature was determined using the approach explained by Garson and Goh after finding the optimal topology for each network. This analysis was conducted to understand which features were most relevant for the classification tasks.\n\nThe determination of feature importance was likely performed using the training data, as the optimal networks were identified through a process involving training and cross-validation. This process ensured that the models were evaluated on data not used during training, helping to estimate the generalization error and avoid issues like overfitting.",
  "optimization/fitting": "The fitting method employed in this study involved training artificial neural networks (ANNs) using a gradient descent algorithm with a learning rate (ε) that was carefully selected to ensure optimal convergence. The training process was regulated to prevent overfitting through several mechanisms.\n\nFirstly, the cost function included a weight decay term, which penalizes large weights and helps to prevent the model from becoming too complex and overfitting the training data. This regularization technique ensures that the network generalizes well to unseen data.\n\nSecondly, a 10-fold cross-validation approach was used. The dataset was divided into a training set (75% of the data) and an evaluation set (25% of the data). The training set was further split into ten subsets, and the model was trained and validated iteratively, ensuring that each subset was used for validation exactly once. This method helps to estimate the generalization error of the model and avoids overfitting by providing a robust evaluation of the model's performance on different subsets of the data.\n\nAdditionally, the training was stopped early if the cost function converged to a low value (J < 10^-4), if the difference in cost between epochs was small (ΔJ < 10^-10), or if the maximum number of epochs (50,000) was reached. These stopping criteria help to prevent overfitting by ensuring that the model does not continue training unnecessarily once it has achieved a satisfactory level of performance.\n\nTo address underfitting, a grid search approach was used to find the optimal topology of the neural networks. The number of hidden layers and hidden neurons per layer were varied incrementally until the best performance was achieved. This systematic approach ensures that the model is complex enough to capture the underlying patterns in the data without being too simple to learn effectively.\n\nThe learning rate was also carefully selected by analyzing the training error. High learning rates resulted in oscillating behavior and failure to converge to the local minimum, while low learning rates converged too slowly. The optimal learning rate was chosen to balance these trade-offs and ensure efficient training.\n\nIn summary, the fitting method employed in this study included regularization techniques, cross-validation, early stopping criteria, and a systematic search for the optimal network topology. These measures collectively help to prevent both overfitting and underfitting, ensuring that the models generalize well to new data.",
  "optimization/regularization": "In our work, we employed a regularization technique known as weight decay to prevent overfitting. This method helps to keep the network weights from becoming excessively large, which in turn avoids the model from overfitting the training data. The cost function used in our study was the binary cross-entropy, which was regularized with weight decay. This cost function is calculated at the end of each training epoch and is defined as follows:\n\nJ(Θ) = −1/M * ∑(N∑i=1 O∑j=1 yij ln(oji(Θ)) + (1−yij) ln(oji(Θ))) + 1/2M * ∑i θi\n\nHere, Θ represents a vector containing all the weights used in the network. By incorporating weight decay into the cost function, we ensure that the model generalizes better to unseen data, thereby mitigating the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not directly available in the main text. However, the implementation details and the training algorithm are provided in a supporting file, specifically the S1 File. This file contains complete details about the implementation of the neural networks and the training algorithm used.\n\nThe optimization parameters, such as the learning rates for different networks, are discussed in the text. For instance, the learning rate for ANN1 was determined to be 0.005 after analyzing the training error. Similarly, the learning rates for ANN21 and ANN22 were found to be 0.01 and 0.1, respectively.\n\nThe training process involved a grid search approach to find the optimal number of hidden layers and hidden neurons per layer for each neural network. The performance metrics, including sensitivity, positive predictive value, and F1-score, are reported for various network topologies. The best-performing topologies are highlighted in the tables provided.\n\nRegarding the license, the specific licensing details for the supporting file are not mentioned in the provided text. Therefore, it is not clear under what terms the supporting file is available.",
  "model/interpretability": "The models employed in this study are primarily deep learning algorithms, which are known for their capability to capture complex, non-linear relationships in data. However, this strength comes with a trade-off in terms of interpretability. Deep learning models are often considered black-box models, meaning that the internal workings and decision-making processes are not easily understandable or interpretable by humans. This lack of transparency can be a limitation, especially in medical applications where understanding the rationale behind a model's predictions is crucial.\n\nTo mitigate this issue, we have taken steps to enhance the interpretability of our models. For instance, after determining the optimal topology for each neural network, we assessed the relative importance of the input features in each classification task. This approach, inspired by the methods proposed by Garson and Goh, helps in understanding which features contribute most significantly to the model's predictions. By identifying these key features, we can gain insights into the underlying patterns that the model has learned, making the decision-making process more transparent.\n\nAdditionally, we have provided detailed information about the implementation of the neural networks and the training algorithm in the Supporting Information. This includes the specific architectures used, the training procedures, and the evaluation metrics. By making this information available, we aim to increase the reproducibility of our results and allow other researchers to build upon our work.\n\nWhile these efforts improve the interpretability to some extent, it is important to acknowledge that deep learning models will always have inherent limitations in this regard. Future work could explore the use of more interpretable machine learning approaches, such as Bayesian methods, support vector machines, or decision trees, to complement the deep learning models and provide additional insights into the classification of ischemic events.",
  "model/output": "The model discussed in this work is primarily focused on classification tasks. Specifically, it involves classifying different types of pseudo-ECG (pECG) signals to detect and assess the severity of ischemic events. The models were trained to discriminate between various classes of signals, including Control, Mild, and Severe ischemia. Three logistic regression classifiers were used as a baseline, each designed to classify between a specific class and any other signal. Additionally, neural network models were employed, with different architectures tailored to handle multi-class classification and cascaded classification tasks. The output of these models is a classification result that indicates the presence and severity of ischemia based on the input pECG features. The performance of these models was evaluated using metrics such as sensitivity, positive predictive value, and F1-score, which are standard for classification tasks.",
  "model/duration": "The execution time for the models varied depending on the computational resources and the specific tasks performed. The reaction-diffusion system used for cable simulations was solved using the numerical method of lines, which was deployed on the UCL Computer Science cluster. This setup allowed for efficient computation of the models needed to build populations of models.\n\nThe artificial neural network routines were implemented in Python, utilizing libraries such as NumPy and Theano. The training and evaluation of these networks were conducted on a DELL Precision Tower 5810 equipped with an Intel Xeon E5-1620 CPU and 32GB of RAM. The operating system used was Ubuntu 16.04 LTS.\n\nThe models were trained using a procedure that involved dividing the data into training and evaluation sets, followed by 10-fold cross-validation. This process ensured that the models were robust and generalizable, minimizing issues like overfitting and bias. The optimal topology for each network was determined through a grid search approach, varying the number of hidden layers and neurons per layer until the best performance was achieved.\n\nThe specific execution times for individual tasks, such as solving the differential equations or training the neural networks, are not detailed here, but the computational infrastructure and methods used were designed to handle the complexity and scale of the tasks efficiently.",
  "model/availability": "The source code for the implementations of the TP06 and ORd models is available. The codes were adapted from the original papers and can be found at the provided URLs. Additionally, the artificial neural network routines were implemented in Python using NumPy and Theano. The implementations and training algorithms are detailed in the Supporting Information files. The data used in this study can be simulated by following the methods explained in the paper. The source code and data are released under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction, provided the original author and source are credited.",
  "evaluation/method": "The evaluation method employed for this work involved a rigorous process to ensure the robustness and generalization capability of the models. The dataset was initially divided into a training set and an evaluation set, with the training set comprising 75% of the data and the evaluation set containing the remaining 25%. This division allowed for the estimation of the generalization error of the trained models, as the evaluation set was not used during the training phase.\n\nTo further enhance the reliability of the models, a 10-fold cross-validation technique was utilized. In this process, the training set was divided into ten subsets of equal size. The model was then trained using nine of these subsets and tested on the remaining one. This procedure was repeated ten times, each time using a different subset for testing. The model that achieved the highest F1-score during the cross-validation was selected as the best model.\n\nThe performance metrics reported in this work, including sensitivity, positive predictive value, and F1-score, were obtained by applying the best-performing model from the cross-validation process to the evaluation set. This approach helped in avoiding common issues in machine learning such as overfitting and bias, and it minimized the effect of outliers in the data.\n\nAdditionally, three logistic regression classifiers were trained to provide a baseline performance. These classifiers were used to classify between different conditions (Control, Mild, and Severe) based on the magnitudes of the pECG features. The results from these baseline models highlighted the complexity of the classification task and the need for more sophisticated models.\n\nThe deep learning models were evaluated using a grid search approach, where the number of hidden layers and hidden neurons per layer were varied incrementally. The performance of each model topology was measured using sensitivity, positive predictive value, and F1-score. The optimal topology was identified when no significant improvement was observed by increasing the complexity of the network or when a decrease in performance was noted.\n\nIn summary, the evaluation method involved a combination of training and evaluation set division, 10-fold cross-validation, and a grid search approach for hyperparameter tuning. This comprehensive evaluation strategy ensured that the models were robust, generalizable, and capable of handling the complexity of the classification task.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our models. The primary metrics reported are sensitivity (Se), positive predictive value (PPV), and the F1-score. Sensitivity measures the ability of the model to correctly identify positive cases, while the positive predictive value indicates the proportion of positive identifications that are actually correct. The F1-score is the harmonic mean of sensitivity and PPV, providing a single metric that balances both concerns.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in medical and biological contexts where the cost of false positives and false negatives can be significant. The F1-score is particularly useful because it provides a single value that reflects both the precision and recall of the model, making it easier to compare different models and topologies.\n\nIn addition to these metrics, we also considered the generalization error of our models. By dividing our dataset into training and evaluation sets, we ensured that our models were tested on data they had not seen during training. This approach helps in estimating how well the models will perform on new, unseen data, which is crucial for their practical application.\n\nWe also employed 10-fold cross-validation to further validate our models. This technique involves dividing the training data into ten subsets, training the model on nine of them, and testing it on the remaining one. This process is repeated ten times, with each subset used as the test set once. The model that performed best during cross-validation, as measured by the F1-score, was then applied to the evaluation set. This method helps in avoiding overfitting and bias, and it minimizes the effect of outliers in the data.\n\nOverall, the set of metrics we used is representative of standard practices in the field and provides a comprehensive evaluation of our models' performance.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of different methods to assess their performance in classifying pECG signals. We did not use publicly available benchmark datasets for comparison, as our study focused on specific datasets derived from simulations of cardiac models. Instead, we compared our deep learning approaches to simpler baselines to establish a foundation for performance evaluation.\n\nTo provide a baseline for classification performance, we trained three logistic regression classifiers. These classifiers were designed to distinguish between different conditions: Control versus any other signal, Mild versus any other signal, and Severe versus any other signal. The logistic regression models used the magnitudes of the pECG features as inputs. This approach helped us illustrate that the classification task was not trivial and that complex non-linear relationships exist in the data.\n\nThe performance of these logistic regression classifiers was evaluated using sensitivity, positive predictive value, and F1-score. The results showed varying levels of performance, with the classifier distinguishing between Severe and any other signal achieving the highest F1-score. This baseline performance was crucial in demonstrating the complexity of the classification task and the need for more sophisticated models.\n\nIn addition to the logistic regression baselines, we implemented two different deep learning structures. The first approach involved a single neural network designed to discriminate between Control, Mild, and Severe populations of virtual signals. The second approach consisted of a cascade of two networks: the first network distinguished between non-ischemic and ischemic signals, while the second network further classified the ischemic signals into Mild and Severe categories.\n\nBoth deep learning approaches were trained using a procedure that involved dividing the data into training and evaluation sets, followed by 10-fold cross-validation. The performance of each model was measured using sensitivity, positive predictive value, and F1-score. The best-performing models were selected based on their evaluation set performance, and their topologies were optimized through a grid search approach.\n\nThe comparison between the logistic regression baselines and the deep learning models highlighted the superior performance of the neural networks in capturing the complex relationships in the pECG data. The deep learning models achieved higher F1-scores, demonstrating their effectiveness in classifying the different conditions. This comparison underscored the importance of using advanced machine learning techniques for accurate classification in this domain.",
  "evaluation/confidence": "The evaluation of the models involved a rigorous process to ensure the reliability and generalizability of the results. The models were trained using a 10-fold cross-validation approach, which helps in minimizing issues like overfitting and bias, and in reducing the impact of outliers. This method allows for a more robust estimation of the model's performance by testing it on different subsets of the data.\n\nThe performance metrics reported, including sensitivity, positive predictive value, and F1-score, were obtained from the model that performed best during the cross-validation process. These metrics were then applied to an evaluation set that was not used during training, providing an estimate of the generalization error.\n\nTo determine the best model, a grid search approach was employed, varying the number of hidden layers and neurons per layer. The optimal topology was identified when no significant improvement was observed with increased complexity or when performance started to decline. This systematic approach ensures that the selected model is not only the best-performing but also the most efficient in terms of complexity.\n\nThe results from the logistic regression classifiers provided a baseline performance, demonstrating that the classification task is not trivial and that complex non-linear relationships exist in the data. The deep learning models outperformed these baselines, indicating their superior capability in handling the classification task.\n\nWhile specific confidence intervals for the performance metrics are not explicitly mentioned, the use of cross-validation and the evaluation on a separate set of data suggest a high level of confidence in the reported results. The statistical significance of the results is implied by the consistent performance across different folds of the cross-validation and the clear superiority of the deep learning models over the logistic regression baselines.",
  "evaluation/availability": "All relevant data used in this study can be simulated by following the methods explained in the paper. This means that the raw evaluation files are not directly available for download, but the information needed to reproduce them is provided. The data used in this study is within the paper and its supporting information files. The paper is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
}