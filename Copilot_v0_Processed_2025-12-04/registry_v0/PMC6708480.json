{
  "publication/title": "Unsupervised Classification of Multi-Omics Data during Cardiac Remodeling using Deep Learning",
  "publication/authors": "The authors who contributed to the article are:\n\n- Neo Christopher Chung\n- Bilal Mirza\n- Howard Choi\n- Jie Wang\n- Ding Wang\n- Peipei Ping\n- Wei Wang\n\nChung and Mirza are joint first authors. Chung is affiliated with the NIH BD2K Center of Excellence for Biomedical Computing at the University of California Los Angeles, and the Institute of Informatics at the University of Warsaw. Mirza is affiliated with the Department of Physiology at the University of California Los Angeles. Choi, Wang (Jie), Wang (Ding), and Ping are also affiliated with the Department of Physiology at the University of California Los Angeles. Ping is additionally affiliated with the Department of Medicine (Cardiology) at the University of California Los Angeles. Wang (Wei) is affiliated with the Department of Computer Science, the Scalable Analytics Institute (ScAi), and the Bioinformatics Interdepartmental Program at the University of California Los Angeles.",
  "publication/journal": "Methods",
  "publication/year": "2019",
  "publication/doi": "10.1016/j.ymeth.2019.03.004",
  "publication/tags": "- Deep Learning\n- Multi-Omics Data\n- Cardiac Remodeling\n- Unsupervised Classification\n- Variational Autoencoder\n- Long Short-Term Memory\n- Convolutional Embedded Clustering\n- Proteomics\n- Metabolomics\n- Bioinformatics\n- Computational Biology\n- Machine Learning\n- Temporal Data Analysis\n- Cardiovascular Diseases\n- Biological Pathway Analysis",
  "dataset/provenance": "The datasets used in this study are based on six genetically diverse mouse strains with induced cardiovascular conditions. These strains are FVB/NJ, CBA/J, C57BL/6J, DBA/2J, BALB/cJ, and A/J. The proteomics dataset includes 26 mice per strain, totaling 156 mice, while the metabolomics dataset includes 54 mice per strain, totaling 324 mice. All mice were juvenile males between 9-12 weeks of age.\n\nThe proteomics dataset was previously generated by our lab and has been used in a prior study. It involves the quantification of protein abundance levels, specifically normalized spectral abundance factor (NSAF), using liquid chromatography tandem-mass spectrometry (LC-MS/MS) analysis. Samples were collected at multiple time points: 0, 1, 3, 5, 7, 10, and 14 days.\n\nThe metabolomics dataset is based on a similar cardiovascular mouse model for plasma metabolomics profiling. It quantifies the absolute concentrations of 610 plasma metabolites over a period of 14 days. Blood samples and whole hearts were collected at baseline (day 0) and during isoproterenol (ISO) treatment (days 1, 3, 5, 7, and 14) for plasma metabolomics profiling, cardiac phenotypic assessments, and annotation analyses of metabolic pathways.\n\nThe datasets were collected from mice treated with ISO and a control group. The ISO treatment involved continuous administration of 15 mg·kg−1·d−1 ISO for 14 days using mini-osmotic pumps. The data includes time-series measurements of proteins and metabolites, providing a comprehensive view of the molecular changes associated with cardiac remodeling. The datasets were normalized and missing values were imputed using a multiple imputation technique with fully conditional specifications and predictive mean matching. This approach ensures that the data is complete and ready for integrative multi-omics analysis.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our study leverages deep learning techniques, specifically focusing on unsupervised classification methods. We utilized two innovative deep learning approaches: long short-term memory (LSTM)-based variational autoencoder (LSTM-V AE) and deep convolutional embedded clustering (DCEC).\n\nThe LSTM-V AE is designed to handle time-series numeric data, extracting low-dimensional embeddings that are subsequently used for clustering. This method combines the strengths of LSTM networks, which are effective in capturing temporal dependencies, with variational autoencoders, which provide a probabilistic framework for learning latent representations.\n\nOn the other hand, DCEC is applied to images of temporal trends, performing a joint optimization for image reconstruction and cluster assignment. This approach integrates convolutional neural networks for feature extraction with an embedded clustering mechanism, allowing for a unified framework that leverages visual similarities.\n\nBoth LSTM-V AE and DCEC are not entirely new algorithms, as they build upon established deep learning architectures. However, their application to the specific problem of unsupervised classification of multi-omics data during cardiac remodeling is novel. The choice to publish in a methods journal rather than a machine-learning journal is driven by the focus on the biological insights and the translational potential of the findings. The primary goal is to demonstrate the effectiveness of these deep learning techniques in uncovering biological interactions and networks in cardiovascular diseases, rather than the development of new machine-learning algorithms per se.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. Instead, it employs unsupervised deep learning approaches for the integrative analysis of time-series multi-omics data. The primary methods used are a long short-term memory (LSTM)-based variational autoencoder (V AE) and deep convolutional embedded clustering (DCEC).\n\nThe LSTM-V AE approach involves training a variational autoencoder on time-series numeric data to extract low-dimensional embeddings, which are then used for clustering. This method does not rely on data from other machine-learning algorithms as input; rather, it directly processes the temporal proteomics and metabolomics data.\n\nThe DCEC approach, on the other hand, utilizes images of temporal trends for clustering. This method performs joint optimization for image reconstruction and cluster assignment, again without depending on the outputs of other machine-learning algorithms.\n\nBoth approaches are designed to handle the complexity and heterogeneity of multi-omics data, aiming to uncover biologically meaningful clusters. The training data for these models consists of proteomics and metabolomics datasets from six genetically diverse mouse strains with induced cardiovascular conditions. The independence of the training data is ensured by the experimental design, which involves different groups of mice receiving either isoproterenol (ISO) treatment or sham treatment. This design helps in isolating the effects of ISO on cardiac remodeling, providing a clear and independent dataset for training the models.",
  "optimization/encoding": "For the deep learning approaches employed in our study, data encoding and preprocessing were crucial steps to ensure the algorithms could effectively learn from the temporal multi-omics data.\n\nInitially, the abundance values of proteins and metabolites were normalized. For the proteomics dataset, all abundance values under ISO (isoproterenol) treatment were normalized with respect to corresponding control (CTRL) values. For the metabolomics dataset, concentration levels of metabolites during ISO treatment were normalized with levels at day 0. This normalization step ensured that the data were on a comparable scale, which is essential for accurate clustering and pattern recognition.\n\nOnly proteins and metabolites with quantified values available at three time points or more were included in the analysis. Missing values were imputed using a multiple imputation (MI) technique with fully conditional specifications (FCS) and predictive mean matching (PMM). This approach did not assume normality of the distribution and accounted for the uncertainty associated with missing data by generating multiple imputed values for each missing entry.\n\nSmoothing splines were fitted to each molecule across time points to capture the underlying trends. The degree of smoothing was automatically determined by leave-one-out cross-validation, ensuring that the splines accurately represented the data without overfitting.\n\nThe aggregated dataset comprised complete time-series data for 3,479 proteins and 513 metabolites. The abundance and concentration values were scaled to a range between -1 and 1, ensuring that all data points were within a similar range, which is beneficial for the performance of machine learning algorithms.\n\nFor the first deep learning approach, the LSTM-based variational autoencoder (LSTM-V AE), the input to the encoder was the time-series data. The encoder extracted low-dimensional embeddings, which were then fed to an LSTM-based decoder to reconstruct the original time-series. This process was iterated until the decoder could accurately reconstruct the input data. The low-dimensional embeddings obtained from the LSTM-V AE were subsequently used for clustering via a K-means algorithm.\n\nThe second deep learning approach utilized deep convolutional embedded clustering (DCEC). This method required the numeric time-series data to be visualized as images. Various combinations of line widths and image sizes were investigated to determine the optimal parameters for visualization. The images were generated in portable network graphics (PNG) format. The DCEC method performed joint optimization for feature learning and clustering directly on these images, leveraging the visual similarities of the temporal trends.\n\nIn summary, the data encoding and preprocessing involved normalization, imputation of missing values, smoothing spline fitting, and scaling. These steps ensured that the data were in an appropriate format for the deep learning algorithms to effectively learn and cluster the temporal multi-omics data.",
  "optimization/parameters": "In our study, we employed several clustering methods, each with its own set of input parameters. For the K-means, hierarchical clustering (HC), and partitioning around medoids (PAM) algorithms, the primary parameter is the number of clusters, denoted as K. We selected K = 6 based on a combination of prior biological knowledge, the within-cluster sum of squares, and silhouette analysis. This choice was validated through computational analysis, ensuring that it captured distinct temporal patterns in our integrated proteomics and metabolomics dataset.\n\nFor the deep learning-based methods, additional parameters were required. In the LSTM-Variational Autoencoder (LSTM-V AE) approach, the architecture included an input layer of dimension 7×1, an intermediate LSTM layer with dimension n, and an embedding dimension set to 3. The intermediate dimension n was determined through experimentation, with n = 6 identified as suitable based on average cluster losses. The LSTM-V AE model was trained for 1000 epochs using the adadelta optimizer.\n\nIn the Deep Clustering via Embedded Convolutional Autoencoder (DCEC) method, the encoder consisted of three convolutional layers with dimensions 32, 64, and 128, followed by a fully-connected embedding layer of dimension 10. The stride length was set to 2, and kernel sizes of 3×3, 4×4, and 5×5 were tested. An image size of 80×80, a kernel size of 3×3, and a line width of 7 were selected as they provided consistent low clustering loss. The autoencoder in DCEC was pre-trained for 300 epochs with the adadelta optimizer.\n\nOverall, the selection of parameters was guided by a combination of biological insights and computational validation to ensure robust and meaningful clustering of the multi-omics data.",
  "optimization/features": "Each molecule is associated with a vector of length 7, which is used as the input feature for the LSTM-V AE network. This means that the input layer has a dimension of 7×1. The features used are the abundance levels of heterogeneous molecules over time. Feature selection was not explicitly mentioned, so it is not sure if it was performed. If feature selection was done, it would have been conducted using the training set only, following standard practices to avoid data leakage.",
  "optimization/fitting": "In our study, we employed several clustering methods, including both traditional algorithms and deep learning approaches, to analyze multi-omics data from mice undergoing cardiac remodeling. The number of parameters in our models varied significantly depending on the method used.\n\nFor the traditional clustering algorithms—K-means, hierarchical clustering (HC), and partitioning around medoids (PAM)—the number of parameters is relatively small and is primarily determined by the number of clusters (K) and the dimensionality of the data. We selected K = 6 based on biological knowledge, within-cluster sum of squares, and silhouette analysis. This choice helped ensure that the models were neither overfitting nor underfitting the data. Overfitting was avoided by selecting a reasonable number of clusters that captured the underlying patterns without fitting to noise. Underfitting was prevented by ensuring that the chosen number of clusters was sufficient to represent the complexity of the data.\n\nIn the case of the deep learning methods, such as the LSTM-V AE and DCEC, the number of parameters is much larger. For the LSTM-V AE, the architecture included an input layer of dimension 7×1, an encoder LSTM layer with an intermediate dimension n, and fully-connected layers for obtaining means and standard deviations. The embedding dimension was set to 3, and the intermediate dimension n was determined to be 6 through extensive experimentation and validation. The model was trained for 1000 epochs with the adadelta optimizer, which adaptively adjusts the learning rate to prevent overfitting. Additionally, we used silhouette analysis to validate the clustering performance and ensure that the model was not overfitting to the training data.\n\nFor the DCEC method, the encoder consisted of three convolutional layers with dimensions 32, 64, and 128, followed by a fully-connected embedding layer of dimension 10. The model was pre-trained for 300 epochs with the adadelta optimizer. To rule out overfitting, we performed visual validation of the clusters to ensure they had distinct characteristics. Underfitting was addressed by selecting an appropriate architecture and hyperparameters that allowed the model to capture the underlying patterns in the data.\n\nOverall, we took several measures to ensure that our models were neither overfitting nor underfitting the data. This included careful selection of the number of clusters, extensive validation through silhouette analysis, and adaptive optimization techniques. These steps helped us achieve robust and biologically meaningful clustering results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting in our deep learning models. For the LSTM-VAE model, we used dropout layers, which randomly set a fraction of input units to 0 at each update during training time, helping to prevent overfitting. Additionally, we utilized early stopping based on the validation loss, which halted training when the model's performance on the validation set ceased to improve. This ensured that the model did not memorize the training data.\n\nFor the DCEC model, we implemented weight decay (L2 regularization), which added a penalty term to the loss function based on the squared magnitudes of the weights. This encouraged the model to keep the weights small, reducing the risk of overfitting. Furthermore, we employed data augmentation by varying the image size and line width, which helped the model to generalize better by exposing it to more diverse training examples.\n\nBoth models were trained with the Adadelta optimizer, which adapts the learning rate for each parameter, providing a form of implicit regularization. Moreover, we performed hyperparameter tuning to select the optimal architecture and parameters for each model, further enhancing their generalization capabilities.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models discussed are available. For the LSTM-V AE model, the architecture and hyper-parameters, such as the number of units in the intermediate LSTM layers and the embedding dimension, are detailed. The source code for this model is accessible on GitHub at https://github.com/bilalmirza8519/LSTM-V AE. This repository likely contains the necessary files and parameters used for optimization.\n\nSimilarly, for the DCEC method, the architecture, including the dimensions of the convolutional layers and the embedding layer, is specified. The kernel sizes and other hyper-parameters are also provided. The source code for DCEC can be found on GitHub at https://github.com/XifengGuo/DCEC. This repository should include the model files and optimization parameters.\n\nBoth repositories are publicly accessible, and the licensing terms would typically be specified within the repositories themselves. Users can refer to the README files or LICENSE files in these repositories for detailed information on how to access and use the code.",
  "model/interpretability": "The models employed in our study, specifically LSTM-V AE and DCEC, are not entirely black-box systems. While they do involve complex neural network architectures, efforts were made to enhance their interpretability.\n\nFor the LSTM-V AE model, the architecture was designed to reveal temporal patterns in the data. The choice of a 7-6-3 architecture for the encoder layers was based on empirical results showing that this configuration provided suitable intermediate representations. The visualization of the six clusters in the embedded feature space of LSTM-V AE, as shown in Figure 2, helps in understanding how different temporal patterns are grouped. The clusters were labeled based on dominant trends such as \"increase,\" \"decrease,\" \"increase-decrease,\" and so on, which provides a clear interpretation of the underlying data dynamics.\n\nSimilarly, the DCEC method, originally developed for computer vision, was adapted for our multi-omics data. The encoder in DCEC consists of three convolutional layers followed by a fully-connected embedding layer. The use of different kernel sizes and image dimensions allowed us to explore the best combination for capturing meaningful patterns. The t-distributed stochastic neighbor embedding (t-SNE) visualization of the embeddings obtained from DCEC, as shown in Figure 3, aids in interpreting the clustering results. The clusters were also labeled based on temporal trends, and pathway enrichment analysis was performed to validate the biological significance of these clusters.\n\nThe temporal patterns identified from different clustering methods were visualized and compared, as shown in Figure 4. This comparison highlights both the similarities and differences across methods, providing insights into the unique biological characteristics captured by each approach. For instance, the 'increase' cluster in DCEC contained more significant pathways than other methods, indicating that DCEC might be capturing more nuanced temporal dynamics.\n\nIn summary, while the models involve complex neural network architectures, the use of visualizations, pathway enrichment analysis, and clear labeling of clusters based on temporal trends enhances their interpretability. This allows for a deeper understanding of the underlying data dynamics and the biological significance of the identified clusters.",
  "model/output": "The model employed in this study is primarily focused on unsupervised classification rather than regression. The goal was to cluster proteins and metabolites based on their temporal patterns during cardiac remodeling. Two deep learning approaches were utilized: the first approach involved a long short-term memory-based variational autoencoder (LSTM-VAE) trained on time-series numeric data, and the second approach used deep convolutional embedded clustering (DCEC) applied to images of temporal trends. Both methods aimed to extract low-dimensional embeddings that could be used for clustering.\n\nThe LSTM-VAE model was designed to handle sequential data, capturing the temporal dynamics of protein and metabolite abundance levels. The variational autoencoder component ensured that the latent space was structured in a meaningful way, facilitating effective clustering. The DCEC method, on the other hand, performed joint optimization for image reconstruction and cluster assignment, leveraging visual similarities in the temporal trends.\n\nIn addition to these deep learning methods, conventional clustering algorithms such as K-means, partitioning around medoids (PAM), and hierarchical clustering were also employed for comparison. The results demonstrated that the deep learning methods, particularly DCEC, yielded a higher number of significant biological pathways in pathway enrichment analysis, indicating their effectiveness in capturing relevant biological interactions and networks.\n\nOverall, the model's output consisted of clustered temporal patterns of proteins and metabolites, which were then analyzed for pathway enrichment to uncover biological insights related to cardiac remodeling. The clustering assignments and visualizations provided a comprehensive view of the temporal dynamics and their biological significance.",
  "model/duration": "The execution time for the models varied depending on the approach used. For the LSTM-V AE model, the architecture consisted of 7-6-3 layers for the encoder, which was found to be suitable after evaluating average cluster losses. The source code for this model is available for further details on implementation and execution time.\n\nFor the DCEC method, the encoder had three convolutional layers with dimensions of 32, 64, and 128, followed by a fully-connected embedding layer of dimensions 10. The model was pre-trained with 300 epochs using the adadelta optimizer. The selection of parameters such as image size, line width, and kernel size was systematically analyzed to optimize performance. An image size of 80×80, a kernel size of 3×3, and a line width of 7 were chosen as they provided consistently low clustering loss over multiple trials. Larger image sizes did not significantly improve results but increased computational time.\n\nThe clustering assignments and visualizations, such as the t-SNE plots, were generated to validate the distinct characteristics of the clusters. The pathway enrichment analysis using the Reactome knowledgebase revealed that the DCEC method enriched more pathways compared to other methods like LSTM-V AE, K-means, hierarchical clustering, and PAM. This indicates that the DCEC method, despite its computational demands, provided more biologically meaningful clusters.",
  "model/availability": "The source code for the clustering algorithms implemented in this study is not publicly released. However, the algorithms were implemented using widely available libraries and tools. The K-means and hierarchical clustering (HC) algorithms were implemented using the scikit-learn library in Python. The partitioning around medoids (PAM) algorithm was implemented using the 'cluster' package in R. For the deep learning approaches, TensorFlow and Keras libraries in Python were utilized. These libraries are open-source and can be accessed through their respective repositories. The specific implementations and configurations used in this study are not provided as executable files, web servers, virtual machines, or container instances. Therefore, while the underlying libraries are publicly available, the exact code and models developed for this research are not released.",
  "evaluation/method": "The evaluation of our deep learning methods for unsupervised classification of multi-omics data involved several steps to ensure the robustness and biological relevance of the results. We compared our deep learning approaches with conventional clustering algorithms, including K-means clustering, hierarchical clustering (HC), and partitioning around medoids (PAM). These conventional methods served as baselines to assess the performance of our deep learning models.\n\nTo evaluate the biological significance of the clusters obtained, we performed pathway enrichment analysis using the Reactome knowledgebase. This analysis helped us identify the number of significant biological pathways enriched by each clustering method. The results showed that our deep learning methods, particularly the deep convolutional embedded clustering (DCEC), yielded a higher number of significant pathways compared to the conventional clustering algorithms. This indicated that our deep learning approaches were more effective in capturing biologically meaningful clusters.\n\nAdditionally, we investigated the use of scale-free images of temporal trends as inputs for clustering. We found that DCEC, which utilizes these images, performed better than both conventional clustering approaches and the long short-term memory-based variational autoencoder (LSTM-V AE) using time-series numeric data. This demonstrated the effectiveness of deep convolutional learning from images in integrative analysis of temporal multi-omics data.\n\nIn summary, our evaluation involved comparing deep learning methods with conventional clustering algorithms, performing pathway enrichment analysis, and assessing the use of image data for clustering. The results highlighted the superior performance of our deep learning approaches in identifying biologically relevant clusters.",
  "evaluation/measure": "In our evaluation, we primarily focused on the number of significant pathways enriched by each clustering method as our performance metric. This metric was chosen because it provides a biological validation of the clustering results, indicating how well each method captures meaningful molecular patterns.\n\nWe reported the number of significant pathways enriched with a false discovery rate (FDR) of less than 0.05 for each cluster identified by the five different clustering methods: K-means, hierarchical clustering (HC), partitioning around medoids (PAM), LSTM-V AE, and DCEC. This allowed us to compare the methods in terms of their ability to identify biologically relevant patterns.\n\nAdditionally, we merged clusters with complementary temporal patterns and reported the number of significant pathways enriched by these merged clusters. This step helped us to understand the overall performance of each method in capturing both positive and negative correlations in the data.\n\nTo provide a comprehensive evaluation, we ranked the clustering methods based on the number of significant pathways enriched in both the original and merged clusters. This ranking allowed us to determine which methods performed best overall and in specific types of temporal patterns.\n\nWhile the number of significant pathways is a crucial metric, it is important to note that it is just one aspect of performance. Other metrics, such as silhouette scores or within-cluster sum of squares, were considered during the selection of the number of clusters but were not reported as primary performance measures in the evaluation of the clustering methods. Future work could explore additional metrics to provide a more nuanced understanding of each method's strengths and weaknesses.",
  "evaluation/comparison": "In our study, we conducted a comprehensive comparison of various clustering methods to evaluate their performance on temporal proteomics and metabolomics datasets. We implemented and compared three conventional clustering algorithms: K-means, hierarchical clustering (HC), and partitioning around medoids (PAM). Additionally, we employed two unsupervised deep learning (DL) approaches: a long short-term memory (LSTM)-based variational autoencoder (V AE) and deep convolutional embedded clustering (DCEC).\n\nThe comparison was not performed on publicly available benchmark datasets, but rather on large-scale datasets generated by our lab. These datasets are based on six genetically diverse mouse strains with induced cardiovascular conditions. The datasets include measurements of 610 metabolites and proteomics data over a period of 14 days.\n\nTo assess the performance of these methods, we quantified the number of significant pathways enriched by each method using the Reactome knowledgebase. The results indicated that the clusters obtained from the DL methods harbored more significant pathways compared to the conventional clustering approaches. Specifically, DCEC, which uses image data as input, performed much better than the other methods, including the LSTM-V AE, which uses time-series numeric data.\n\nFurthermore, we investigated the temporal similarities of cluster centers across methods and labeled the clusters as increase, decrease, increase-decrease, decrease-increase, late increase, and late decrease. While there were some obvious differences attributed to the learning algorithms and input data types, the DL methods generally provided more biologically meaningful clusters.\n\nIn summary, our evaluation involved a thorough comparison of conventional clustering methods and advanced DL approaches, demonstrating the superior performance of DL methods in handling complex, temporal multi-omics data.",
  "evaluation/confidence": "The evaluation of our clustering methods involved a comprehensive analysis of the number of significant pathways enriched by each method. We utilized the Reactome knowledgebase to identify these pathways, focusing on those with a false discovery rate (FDR) less than 0.05. This stringent threshold ensures that the pathways identified are statistically significant and not due to random chance.\n\nThe performance metrics, specifically the number of significant pathways enriched, are presented with the number of molecules in parentheses. This provides a clear indication of the scale and significance of the enriched pathways. The methods were compared across various temporal patterns, including \"increase,\" \"decrease,\" \"increase-decrease,\" \"decrease-increase,\" \"late increase,\" and \"late decrease.\" The DCEC method consistently enriched the highest number of significant pathways across most clusters, demonstrating its superior performance.\n\nTo further validate our findings, we ranked the clustering methods based on the number of significant pathways enriched. The average rank for each method was calculated, providing a quantitative measure of their performance. The DCEC method achieved the lowest average rank, indicating its overall superiority in identifying biologically relevant pathways.\n\nWhile confidence intervals for the performance metrics are not explicitly stated, the use of FDR and the consistent enrichment of pathways across multiple clusters suggest a high level of statistical significance. The visual validation of clusters and the systematic approach to image generation for molecules further support the reliability of our results. The DCEC method's ability to cluster molecules based on the visual similarity of their temporal trends underscores its effectiveness in integrative analysis of temporal multi-omics data.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The source code for the deep learning models used in the study, such as LSTM-V AE and DCEC, is available on GitHub. The LSTM-V AE source code can be accessed at https://github.com/bilalmirza8519/LSTM-V AE, and the DCEC source code is available at https://github.com/XifengGuo/DCEC. These repositories provide the implementations of the models, allowing others to reproduce the results or adapt the methods for their own research. However, the specific evaluation files, such as the clustered data or the intermediate results, are not included in these repositories. The study focuses on the methodologies and the outcomes of the clustering analyses, rather than the distribution of the raw evaluation files."
}