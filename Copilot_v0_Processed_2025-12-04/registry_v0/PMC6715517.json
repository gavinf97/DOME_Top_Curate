{
  "publication/title": "Quantification of uncertainty in peptide-MHC binding prediction improves high-affinity peptide selection for therapeutic design",
  "publication/authors": "The authors who contributed to this article are Haoyang Zeng and David K. Gifford. Haoyang Zeng designed the study, with input from David K. Gifford. Both Haoyang Zeng and David K. Gifford developed the method and analyzed the results. David K. Gifford supervised the study. Haoyang Zeng and David K. Gifford wrote the paper.",
  "publication/journal": "Cell Systems",
  "publication/year": "2020",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- MHC-peptide binding\n- Machine learning models\n- Predictive performance\n- Uncertainty metrics\n- Epistemic uncertainty\n- Aleatoric uncertainty\n- Deep neural networks\n- Residual convolutional networks\n- Binding affinity prediction\n- Peptide vaccine design",
  "dataset/provenance": "The primary source of the dataset used in our study is the Immune Epitope Database and Analysis Resource (IEDB). This database is a comprehensive collection of MHC-peptide binding affinity data, encompassing over 80 human and mouse alleles for class II MHC. However, it's important to note that the data distribution is skewed, with only six MHC alleles having more than 5000 peptide examples, and the most abundant allele containing just 10,000 examples. This dataset has been utilized in previous studies, such as those by Nielsen et al. and Jensen et al., to develop and validate computational methods for predicting MHC-peptide binding affinities.\n\nFor class I MHC, we utilized a benchmark dataset constructed by Bhattacharya et al., which ensures that no peptide in the test set has identical length and greater than 80% sequence identity to any peptide in the training set. This dataset covers 51 class I MHC alleles. Additionally, we employed the IEDB-based dataset collected by Nielsen et al., which includes five cross-validation folds to ensure no peptide shares a 9-mer sequence with any peptide in a different fold. This setup allows for a thorough performance analysis on all pairs of MHC and peptide available in the dataset.\n\nFor class II MHC, we used the IEDB-based dataset collected by Jensen et al., which also includes five cross-validation folds created in a similar manner to Nielsen et al.'s dataset. In both the class I and class II MHC datasets, only alleles with more than 100 examples were included to ensure the quality of training. The IC50 values in all three datasets have been normalized to a range between 0 and 1 using the formula 1 − log(IC50)/log(50000).",
  "dataset/splits": "Not applicable",
  "dataset/redundancy": "For the datasets used in our study, we employed a cross-validation approach to ensure that the training and test sets were independent. Specifically, for class I MHC-peptide binding, we utilized a dataset constructed by Bhattacharya et al., which ensured that no peptide in the test set had identical length and greater than 80% sequence identity to any peptide in the training set. This strict criterion was applied to 51 class I MHC alleles covered in this dataset.\n\nAdditionally, for both class I and class II MHC-peptide binding, we used datasets collected by Nielsen et al. and Jensen et al., respectively. These datasets were split into five cross-validation folds, ensuring that no peptide shared a 9-mer sequence with any peptide in a different fold. This setup allowed us to analyze the performance on all pairs of MHC and peptide available in the dataset in a cross-validation manner. By training a model on four folds and testing on the remaining fold, we could evaluate the model's performance on held-out examples, ensuring that the training and test sets were independent.\n\nThe distribution of the datasets we used compares favorably to previously published machine learning datasets in the field. By ensuring that the training and test sets were independent and that there was no overlap in peptide sequences, we aimed to provide a robust evaluation of our model's performance. This approach helps to mitigate the risk of overfitting and ensures that the model's predictions are generalizable to new, unseen data.",
  "dataset/availability": "The data used in our study is publicly available. Specifically, the MHC-peptide binding affinity data was obtained from the Immune Epitope Database (IEDB). For class I MHC, the data is accessible through the NetMHCpan-3.0 service, and for class II MHC, it is available via the NetMHCIIpan-3.2 service. These datasets include the necessary splits used for training and testing our models.\n\nAdditionally, we have made the curated class I MHC benchmark dataset available on Mendeley Data. This dataset was provided to us by Bhattacharya et al. and can be accessed at the following DOI: [10.17632/jwhmrdx268.1](https://doi.org/10.17632/jwhmrdx268.1). This ensures that other researchers can reproduce our results and build upon our work.\n\nThe data is released under standard academic sharing practices, which typically allow for non-commercial use with proper citation. This approach ensures transparency and reproducibility in our research.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is deep neural networks, specifically residual convolutional neural networks. This class of algorithms is well-established in the field of machine learning and has been widely used for various tasks, including image recognition and natural language processing.\n\nThe algorithm employed in our study is not entirely new, as it builds upon existing architectures and techniques. However, the specific application and modifications made to the algorithm for predicting MHC-peptide binding affinity are novel. The use of an ensemble of deep neural networks to model both epistemic and aleatoric uncertainty is a key innovation in our approach.\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of our work is on its application in immunology, specifically in predicting MHC-peptide binding affinity. The development of the algorithm is a means to achieve this biological goal, and the evaluation of its performance is conducted within the context of immunological data and challenges. Therefore, the most relevant audience for this work is the immunology community, which is why it was published in a systems biology journal.\n\nThe optimization algorithm used for training the neural networks is the adaptive stochastic gradient descent method known as Adam. This method is widely used in the training of deep learning models due to its efficiency and effectiveness in handling sparse gradients on noisy problems. The choice of Adam was based on its empirical performance in cross-validation, ensuring that the model parameters were optimized effectively.",
  "optimization/meta": "The model described in this publication is not a traditional meta-predictor that uses predictions from other machine-learning algorithms as input. Instead, it employs an ensemble of deep neural networks to predict MHC-peptide binding affinity and associated uncertainties.\n\nThe ensemble consists of multiple deep residual convolutional neural networks. Each network in the ensemble is trained on different training-validation splits and with different random initializations. This approach allows the model to capture epistemic uncertainty by evaluating the variance in predictions across the ensemble.\n\nThe networks are trained independently, ensuring that the training data for each network is distinct. This independence is crucial for accurately estimating epistemic uncertainty, as it reflects the variability in model predictions due to different training data and initializations.\n\nThe ensemble-wide average of the affinity means is used to produce the final affinity prediction, while the variance of these means quantifies the epistemic uncertainty. Additionally, the average of the affinity variances across the ensemble characterizes the aleatoric uncertainty, which arises from inherent observational noise.\n\nIn summary, while the model utilizes an ensemble of neural networks, it does not rely on predictions from other machine-learning methods as input. The training data for each network in the ensemble is independent, ensuring robust uncertainty estimates.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the MHC-peptide pairs for input into the machine-learning algorithm. Each MHC allele was represented by a pseudo-sequence consisting of 34 amino acid residues that are in contact with the peptide. These contact residues were defined as polymorphic residues within 4.0 Å of the peptide in the structure of major MHC alleles.\n\nPeptide sequences were padded to a uniform length, 30 for class I MHC and 40 for class II MHC, using a placeholder amino acid. This padding ensured consistency in input dimensions for the neural network.\n\nEach amino acid in both the MHC pseudo-sequence and the peptide sequence was encoded using a feature vector of 40 values. This vector was a concatenation of two representations: a one-hot encoding vector of 20 values to denote the 20 standard amino acids, and a row from the BLOSUM50 matrix corresponding to the amino acid. The BLOSUM50 matrix was used to capture evolutionary similarities between amino acids. For the placeholder amino acids, a dummy one-hot encoding vector full of zeros and a dummy BLOSUM50 row with the lowest substitution score were used.\n\nThis encoding resulted in each MHC allele being represented as a 40×34 matrix, which was reshaped into a one-dimensional feature vector of 1360 values. Each peptide was represented as a matrix of size 40×30 for class I MHC and 40×40 for class II MHC. The MHC feature vector was embedded into the first dimension of the peptide feature matrix to form the final input matrix, which was 1400 × 30 for class I MHC and 1400 × 40 for class II MHC.\n\nAdditionally, the difference between the peptide length and the expected length was encoded using a sigmoid function. This encoded length information was concatenated with the feature matrix for the final input to the neural network. This comprehensive encoding approach ensured that the model could effectively learn the relationships between MHC alleles and peptide sequences, leading to accurate predictions of peptide-MHC binding affinity.",
  "optimization/parameters": "The model, PUFFIN, is an ensemble of deep neural networks implemented in PyTorch. Each network in the ensemble is a deep residual convolutional neural network followed by two fully-connected layers. The residual network consists of an initial convolutional layer and five convolutional residual blocks. Each residual block has two convolutional layers with 256 convolutional kernels and a stride of 1. ReLU activation is used as non-linearity across the network.\n\nThe number of parameters (p) in the model is not explicitly stated, but it can be inferred from the architecture. The residual network has an initial convolutional layer and five residual blocks, each with two convolutional layers. Assuming a typical convolutional layer setup, the number of parameters would be significant, especially considering the depth and width of the network.\n\nThe selection of hyper-parameters, including the number of layers and the number of convolutional kernels, was based on the loss on the validation set. For a given pair of training and test sets, 1/8 of the training set was randomly held out as a validation set. The final training was performed for 50 epochs with early stopping if no improvement on validation loss was observed for 10 epochs. The model weights from the epoch with the lowest validation loss were selected. This process was repeated for 10 random splits of the training and validation sets, and for each split, two models were trained with different random weight initializations, resulting in an ensemble of 20 models.",
  "optimization/features": "The input features for our model, PUFFIN, are derived from the MHC-peptide pairs. Each MHC allele is represented by a pseudo-sequence consisting of 34 amino acid residues in contact with the peptide. These residues are defined as polymorphic residues within 4.0 Å of the peptide in the structure of major MHC alleles. Each amino acid in these sequences is encoded using a feature vector of 40 values. This vector is a concatenation of two representations: a one-hot encoding vector of 20 values to denote the 20 amino acids of interest, and a row from the BLOSUM50 matrix that corresponds to the amino acid, providing evolutionary similarities between amino acids.\n\nPeptides are padded to a uniform length of 30 for class I MHC and 40 for class II MHC using a placeholder amino acid. Each peptide is then represented as a matrix of size 40×30 for class I MHC and 40×40 for class II MHC. The MHC feature vector is embedded into the first dimension of the peptide feature matrix to form a final input matrix of size 1400 × 30 for class I MHC and 1400 × 40 for class II MHC.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the features were carefully chosen based on established biological knowledge and previous literature. The use of pseudo-sequences and BLOSUM50 matrix rows ensures that the input features are biologically relevant and informative for predicting MHC-peptide binding affinity. The training set was used to validate the effectiveness of these features through cross-validation, ensuring that the model's performance is robust and generalizable.",
  "optimization/fitting": "The fitting method employed in our study involves an ensemble of deep neural networks implemented in PyTorch. Each network in the ensemble is a deep residual convolutional neural network followed by two fully-connected layers. The residual network learns a high-level representation of the MHC-peptide pair, which is then concatenated with the sigmoid-transformed peptide length for the final prediction of the affinity distribution.\n\nTo address the potential issue of overfitting, given the complexity of the model and the relatively limited size of the training datasets, several strategies were implemented. First, we used a validation set, which was randomly held out from the training set, to monitor the model's performance during training. This allowed us to tune hyper-parameters and select the model with the lowest validation loss, ensuring that the model generalizes well to unseen data. Additionally, early stopping was employed to halt training if no improvement in validation loss was observed for 10 consecutive epochs, preventing the model from overfitting to the training data.\n\nFurthermore, the use of an ensemble of models helps to mitigate overfitting. By training multiple models with different random weight initializations and different splits of the training data, the ensemble averages out the predictions, reducing the variance and improving the robustness of the predictions. This ensemble approach also provides a way to quantify epistemic uncertainty, which reflects the model's confidence in its predictions.\n\nTo rule out underfitting, we ensured that the model architecture was sufficiently complex to capture the underlying patterns in the data. The deep residual convolutional network with multiple layers and convolutional kernels allows the model to learn intricate representations of the MHC-peptide pairs. Additionally, the adaptive stochastic gradient descent method Adam was used for training, which adapts the learning rate for each parameter, enabling more efficient optimization.\n\nThe final ensemble consists of 20 models, each trained on different splits of the data and with different initializations. This ensemble approach not only helps in reducing overfitting but also ensures that the model is not underfitting by providing a diverse set of predictions that can be averaged to improve accuracy. The use of cross-validation further ensures that the model's performance is evaluated on held-out examples, providing a fair assessment of its generalization capability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model, PUFFIN. One of the key methods used was ensemble learning. By training multiple deep neural networks with different random initializations and validation splits, we created an ensemble that averaged predictions across these models. This approach helps to reduce the variance and improve the generalization performance of the model.\n\nAdditionally, we implemented early stopping during the training process. This technique monitors the model's performance on a validation set and halts training when the performance stops improving. Specifically, we trained our models for up to 50 epochs but stopped early if there was no improvement in the validation loss for 10 consecutive epochs. This prevented the model from overfitting to the training data by avoiding excessive training.\n\nWe also utilized dropout layers within our neural network architecture. Dropout randomly sets a fraction of the input units to zero at each update during training time, which helps prevent overfitting. This technique forces the network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n\nFurthermore, we employed data augmentation techniques. By creating variations of the training data, we increased the effective size of our dataset and helped the model generalize better to unseen data. This was particularly important given the limited size of current training datasets for MHC-peptide binding affinity prediction.\n\nBatch normalization was another technique used to stabilize and accelerate training. By normalizing the inputs of each layer, we reduced internal covariate shift, which allowed for faster convergence and better generalization.\n\nLastly, we carefully selected hyperparameters based on the performance on a validation set. This included the number of layers, the number of convolutional kernels, and the parameters of the optimizer. By tuning these hyperparameters, we ensured that the model was well-suited to the task at hand and less likely to overfit to the training data.",
  "optimization/config": "The code for PUFFIN, including the hyper-parameter configurations, optimization schedule, and model files, is available on GitHub at http://github.com/gifford-lab/PUFFIN. The repository provides access to the necessary components for reproducing the experiments and understanding the optimization parameters used in the study. The specific details of the hyper-parameters, such as the number of layers, convolutional kernels, and optimizer parameters, were chosen based on validation loss and are documented within the codebase. The training process involved using the adaptive stochastic gradient descent method Adam, with early stopping based on validation loss. The final models were selected from the epoch with the lowest validation loss, ensuring robust performance. The code is open-source, allowing researchers to access and utilize the configurations and parameters for their own studies.",
  "model/interpretability": "The model PUFFIN is not a blackbox model. It provides interpretable outputs through its uncertainty estimates, which offer insights into the model's predictions. PUFFIN quantifies two types of uncertainty: epistemic and aleatoric. Epistemic uncertainty reflects the model's lack of knowledge about certain inputs, particularly those that are distant from the training examples. This uncertainty increases for sequences that the model has not encountered during training, indicating that the model is aware of its limitations in such cases. Aleatoric uncertainty, on the other hand, represents the inherent noise in the observations and remains largely unchanged regardless of the input. This distinction allows users to understand whether a prediction's uncertainty is due to the model's lack of knowledge or the natural variability in the data.\n\nPUFFIN's architecture also contributes to its interpretability. It uses an ensemble of deep residual convolutional neural networks, which predict the parameters of a probability distribution (beta for class I MHC, normal for class II MHC) that models the affinity of an MHC-peptide pair. The dispersion of this distribution characterizes the aleatoric uncertainty specific to the input. The ensemble-wide variance of the affinity mean characterizes the epistemic uncertainty. This approach provides a clear and quantifiable measure of uncertainty for each prediction, making the model's behavior more transparent.\n\nAdditionally, PUFFIN's binding likelihood, defined as the probability that a peptide binds to a given MHC allele at a specified affinity threshold, offers further interpretability. This metric allows users to prioritize peptides based on their binding likelihood, improving precision in high-affinity peptide design. The model's ability to provide such probabilistic outputs enhances its interpretability and practical utility.",
  "model/output": "The model, PUFFIN, is primarily a regression model. It predicts the binding affinity of an MHC-peptide pair, which is a continuous value. This affinity is modeled as a random sample from a probability distribution, with the distribution's parameters predicted from the input MHC and peptide sequence. For class I MHC, a beta distribution is used, while for class II MHC, a normal distribution is employed. The model outputs the parameters of these distributions, allowing for the calculation of both the mean affinity (the expected value) and the uncertainty associated with the prediction.\n\nIn addition to the continuous affinity prediction, PUFFIN also provides uncertainty metrics. These include aleatoric uncertainty, which characterizes the inherent noise in observations, and epistemic uncertainty, which reflects the model's uncertainty about the true underlying model generating the data. These uncertainties are quantified using the dispersion of the probability distribution (for aleatoric uncertainty) and the predictive variance across an ensemble of neural network models (for epistemic uncertainty).\n\nWhile the primary output is a regression task (predicting continuous affinity values), the model also facilitates classification-like tasks by calculating the binding likelihood. This is the probability that the observed affinity is beyond a specified threshold, using the predicted distribution parameters. This allows for the selection of peptides based on their binding likelihood, which can be crucial for applications such as peptide vaccine design.",
  "model/duration": "The execution time for the model, PUFFIN, involved several stages. Training was performed for 50 epochs with early stopping if no improvement on validation loss was observed for 10 epochs. The model weights from the epoch with the lowest validation loss were selected. This process was repeated for 10 random splits of the training and validation set, with two models trained for each split using different random weight initializations. This resulted in a final ensemble of 20 models. The exact runtime can vary depending on the computational resources used, but this structured approach ensured robust training and validation. Additionally, predictions for each fold in the cross-validation datasets were made using models trained on the other four folds, ensuring fair evaluation across all examples.",
  "model/availability": "The source code for PUFFIN is publicly available on GitHub. This allows users to access, review, and utilize the algorithm as needed. The repository can be found at http://github.com/gifford-lab/PUFFIN. The availability of the source code facilitates transparency and reproducibility, enabling other researchers to verify the results and potentially build upon the work. The specific details regarding the licensing terms can be found on the GitHub repository, ensuring that users are aware of how they can legally use and distribute the code.",
  "evaluation/method": "The evaluation of PUFFIN involved several rigorous methods to ensure its performance and reliability. For class I MHC-peptide binding, a benchmark dataset was used where no peptide in the test set shared identical length and greater than 80% sequence identity with any peptide in the training set. This dataset covered 51 class I MHC alleles. Additionally, the IEDB-based dataset collected by Nielsen et al. was utilized, which included five cross-validation folds to ensure no peptide shared a 9-mer sequence with any peptide in a different fold. This setup allowed for the analysis of performance on all pairs of MHC and peptide available in the dataset.\n\nFor class II MHC-peptide binding, the IEDB-based dataset collected by Jensen et al. was used, which also included five cross-validation folds created in a similar manner. Only MHC alleles with more than 100 examples were included to ensure the quality of training. In all datasets, the IC50 values were normalized to be between 0 and 1.\n\nThe performance of PUFFIN was evaluated using various metrics, including mean-squared-error (MSE), R2, Spearman correlation, and Point-Biserial correlation. For auROC, F1 score, and Point-Biserial correlation, positive examples were defined as those with a binding affinity stronger than 500 nM. PUFFIN's mean estimate outperformed NetMHCIIpan in all considered metrics when evaluated on all MHC-peptide pairs. Combining predictions from PUFFIN and NetMHCIIpan yielded further performance improvement, suggesting that complementary features might be captured by the two approaches.\n\nWhen evaluated on each MHC allele separately, PUFFIN demonstrated a lower mean-squared-error than NetMHCIIpan for 44 of the 55 MHC alleles considered. Furthermore, PUFFIN outperformed competing methods, including NetMHCpan, MHCflurry, and MHCNugget, in auROC and Kendall’s tau and showed competitive performance in F1 score when trained and tested on the same benchmark.\n\nThe evaluation also included an analysis of uncertainty estimates from PUFFIN, which provided a way to gauge predictive error on unseen examples. Uncertainty was characterized as either epistemic (lack of model selection and training data) or aleatoric (observation noise). PUFFIN's uncertainty estimates were found to highly correlate with prediction error, demonstrating that lower uncertainty predictions are more accurate. This faithful stratification of predictive performance on held-out observations showed that PUFFIN's uncertainty estimations reflect its predictive confidence and provide useful guidance for utilizing its computational predictions.",
  "evaluation/measure": "In our evaluation of PUFFIN's performance, we employed several key metrics to comprehensively assess its predictive accuracy and reliability. These metrics include the area under the receiver operating characteristic curve (auROC), F1 score, mean squared error (MSE), R2, Spearman correlation, and Point-Biserial correlation. These metrics are widely used in the literature for evaluating the performance of models predicting MHC-peptide binding affinities.\n\nThe auROC measures the model's ability to distinguish between positive and negative examples, providing a single scalar value that represents the quality of the model's predictions across all classification thresholds. The F1 score, which is the harmonic mean of precision and recall, offers a balanced measure of a model's accuracy, especially useful when dealing with imbalanced datasets.\n\nMean squared error (MSE) and R2 are crucial for evaluating the model's predictive accuracy in terms of continuous affinity values. MSE quantifies the average squared difference between predicted and actual affinities, while R2 indicates the proportion of variance in the observed data that is predictable from the model.\n\nSpearman correlation assesses the monotonic relationship between predicted and observed affinities, providing insight into the model's ability to rank peptides correctly based on their binding affinities. Point-Biserial correlation is used to measure the relationship between the observed binding status (binary) and the continuous predictions, offering a way to evaluate the model's performance in a binary classification context.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that PUFFIN's performance is assessed from multiple angles, including classification accuracy, predictive power, and correlation with observed data. This approach aligns with established practices in the field, ensuring that our results are comparable and meaningful within the broader scientific community.",
  "evaluation/comparison": "In the evaluation of PUFFIN, a comparison was conducted with publicly available methods using benchmark datasets. Specifically, for class I MHC-peptide binding affinity prediction, PUFFIN was evaluated on a benchmark dataset provided by Bhattacharya et al. This dataset has been used to assess the performance of several recent computational methods. PUFFIN outperformed competing methods, including NetMHCpan, MHCflurry, and MHCNuggets, in metrics such as auROC and Kendall’s tau, and showed competitive performance in the F1 score.\n\nFor class II MHC-peptide binding affinity prediction, PUFFIN was compared with NetMHCIIpan3.2. To ensure a fair comparison, NetMHCIIpan3.2 was retrained using the same dataset and platform as PUFFIN. The performance was evaluated using cross-validation, and PUFFIN's mean estimate outperformed NetMHCIIpan in various metrics, including auROC, F1 score, mean squared error, and Spearman correlation. Additionally, combining the predictions from PUFFIN and NetMHCIIpan yielded further performance improvements, suggesting that the two approaches capture complementary features.\n\nRegarding simpler baselines, the evaluation focused on state-of-the-art methods rather than basic benchmarks. The comparison was designed to assess PUFFIN's performance against established and advanced models in the field of MHC-peptide binding affinity prediction.",
  "evaluation/confidence": "The evaluation of PUFFIN's performance includes several metrics such as auROC, F1 score, mean-squared-error, and Point-Biserial correlation. These metrics are used to compare PUFFIN against other methods like NetMHCIIpan, MHCflurry, and MHCNugget. The statistical significance of the results is defined as p<0.05. For instance, when evaluated on a benchmark dataset, PUFFIN outperforms competing methods in auROC and Kendall’s tau, and shows competitive performance in F1 score. This indicates that the results are statistically significant and that PUFFIN's performance is superior to other methods.\n\nThe evaluation also includes correlation analysis with observed affinity, where the Point-Biserial correlations between the observed binding status and the computational predictions are calculated. Wilcoxon one-sided signed rank tests are performed to compare these correlations, further ensuring the statistical significance of the results.\n\nIn addition to these metrics, the uncertainty estimates provided by PUFFIN are also evaluated. The epistemic and aleatoric uncertainties are quantified and shown to correlate highly with the prediction error. This demonstrates that PUFFIN's uncertainty estimations reflect its predictive confidence, providing useful guidance on how to utilize its computational predictions.\n\nThe evaluation process involves cross-validation, where the dataset is split into training and test sets multiple times. This ensures that the performance metrics are robust and not dependent on a specific split of the data. The results are combined and evaluated against the observed affinities, providing a comprehensive assessment of PUFFIN's performance.\n\nOverall, the evaluation of PUFFIN's performance is rigorous and statistically significant, demonstrating its superiority over other methods and baselines. The inclusion of uncertainty estimates further enhances the reliability of PUFFIN's predictions.",
  "evaluation/availability": "Not applicable."
}