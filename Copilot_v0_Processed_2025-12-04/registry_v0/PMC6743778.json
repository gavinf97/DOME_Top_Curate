{
  "publication/title": "Benchmarking network propagation methods for disease gene identification",
  "publication/authors": "The authors who contributed to this article are:\n\n- Alexandre Perera-Lluna, who contributed to conceptualization, project administration, resources, funding acquisition, and supervision.\n- Alex Gutteridge, who contributed to conceptualization, validation, supervision, and writing—review & editing.\n- Benoit H. Dessailly, who contributed to conceptualization, data curation, formal analysis, investigation, methodology, project administration, resources, software, supervision, validation, visualization, and writing—original draft and review & editing.\n- Sergio Picart-Armada, who contributed to data curation, formal analysis, investigation, methodology, software, visualization, and writing—original draft and review & editing.\n- Steven J. Barrett, who contributed to data curation, formal analysis, investigation, methodology, software, visualization, and writing—original draft and review & editing.\n- David R. Wille, who contributed to data curation, formal analysis, investigation, methodology, software, visualization, and writing—original draft and review & editing.",
  "publication/journal": "PLoS Comput Biol",
  "publication/year": "2019",
  "publication/doi": "10.1371/journal.pcbi.1007276",
  "publication/tags": "- Network propagation\n- Disease gene identification\n- Machine learning methods\n- Bioinformatics\n- Computational biology\n- Gene-disease associations\n- Biomedical research\n- Data analysis\n- Genetic evidence\n- Drug discovery\n\nNot sure if these tags are present in the article, but they are a reasonable summary of the topics discussed.",
  "dataset/provenance": "The dataset used in this study was sourced from multiple reputable databases. The Chembl complex data was retrieved from the ChEMBL database, specifically release 23, which is available at https://www.ebi.ac.uk/chembl/downloads. This dataset comprises 214 complexes, with an average of 9.29 proteins per complex.\n\nAdditionally, gene-disease associations were obtained from the OpenTargets data download page. The specific file used was the \"association data.json\" from June 2017, which contains 187,246 rows and encompasses associations between 90 diseases and genes across seven data streams. From this dataset, we selected diseases that had at least 50 drug-associated and genetically-associated genes, resulting in a final list of 22 common diseases.\n\nThe STRING database, version 10, was also utilized. This database includes protein-protein interaction data for the species Homo sapiens (taxonomy ID 9606) with a score threshold of 400. The OpenTargets associations were mapped to the STRING network using ENSEMBL protein identifiers, ensuring no collisions occurred during the mapping process.\n\nThese datasets have been widely used in the scientific community for various studies related to disease gene identification and network propagation methods. The integration of these datasets allows for a comprehensive analysis of gene-disease associations and protein complexes, providing valuable insights into the underlying biological mechanisms.",
  "dataset/splits": "In our study, we employed cross-validation to evaluate the performance of our models. Specifically, we used three different cross-validation schemes: classic, block, and representative. The classic and representative schemes each resulted in 1650 folds, while the block scheme produced 1647 folds due to the discarding of invalid folds.\n\nThe distribution of data points in each fold varies depending on the disease and the cross-validation strategy used. For instance, in the classic scheme, complexes are split, leading to a different distribution of positives in the training and validation folds. In contrast, the block and representative schemes do not split complexes, which helps maintain a more balanced distribution.\n\nFor diseases like allergy, Alzheimer's disease, arthritis, asthma, bipolar disorder, cardiac arrhythmia, and COPD, the number of positives in the training and validation folds is detailed. For example, in the case of Alzheimer's disease, the classic and representative schemes have a mean of 68.7 positives in the training fold and 48 positives in the validation fold. The block scheme, however, shows more variability with a mean of 68.7 positives in the training fold and 34.3 in the validation fold.\n\nThe block scheme can sometimes lead to data imbalance, especially when large complexes are involved. This is evident in diseases like COPD and type I diabetes, where the proportion of drug-related genes in the validation fold deviates from the theoretical balanced proportion. In contrast, the classic and representative strategies generally maintain a balanced dataset, with approximately one-third of the drug-related genes used for validation and two-thirds used as seed genes.\n\nOverall, the choice of cross-validation scheme can significantly impact the distribution of data points and the balance of the dataset, which in turn affects the performance and reliability of the models.",
  "dataset/redundancy": "The datasets were split using three different cross-validation strategies to address the challenge of protein complexes in drug-target associations. The classic strategy involved standard stratified k-fold repeated cross-validation, where k was set to 3 folds, repeated 25 times. This method did not account for the complex structure, potentially leading to splits within complexes across different folds.\n\nTo mitigate this issue, two complex-aware strategies were employed. The block strategy performed repeated cross-validation by merging overlapping complexes and shuffling at the complex level rather than the gene level. This ensured that no complex was divided across folds, maintaining the integrity of the complexes. The representative strategy also merged overlapping complexes but selected a single representative gene from each complex uniformly at random. This representative gene was used for cross-validation, while the remaining genes from the complexes were excluded from both training and validation sets. This approach aimed to prevent the mixing of gene information between folds and to avoid misleading the methods during training and validation.\n\nThe distribution of the datasets, particularly in terms of fold sizes, varied depending on the cross-validation strategy used. The classic and representative strategies generally maintained a balanced distribution, with approximately one-third of the drug-related genes used for validation and two-thirds for training. However, the block strategy could sometimes lead to imbalanced folds, especially when large complexes were involved. This imbalance was more pronounced in diseases with significant complex involvement, such as chronic obstructive pulmonary disease.\n\nIn comparison to previously published machine learning datasets, the approach taken here was more nuanced due to the inherent structure of protein complexes in drug-target data. The classic strategy aligned with standard practices in machine learning, while the block and representative strategies were tailored to handle the specific challenges posed by complex-level resolution in drug-target associations. This ensured that the training and test sets were more independent and that the methods were evaluated more fairly.",
  "dataset/availability": "The data and code supporting the conclusions of this article are publicly available. They can be accessed via the GitHub repository at https://github.com/b2slab/genedise. This repository contains the datasets and the code used in the study, ensuring reproducibility and accessibility for other researchers.\n\nThe data is distributed under the terms of the Creative Commons Attribution License. This license permits unrestricted use, distribution, and reproduction in any medium, provided that the original author and source are credited. This ensures that the data can be freely used and shared, fostering collaboration and further research in the field.\n\nThe availability of the data and code in a public forum, along with the permissive licensing terms, facilitates transparency and reproducibility. Researchers can easily access the materials, verify the findings, and build upon the work presented in the article. This approach aligns with the principles of open science, promoting collaboration and advancing knowledge in the scientific community.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are primarily classical approaches, including Support Vector Machine (SVM) and Random Forest (RF). These are well-established methods in the field of machine learning.\n\nThe SVM method employed is a nu-SVM with a Radial Basis Function (RBF) kernel. This type of SVM is designed to handle classification tasks by finding the optimal hyperplane that best separates the classes. The parameters for the SVM, such as nu and sigma, were determined through inner cross-validation with a grid search approach. This ensures that the model is optimized for the specific dataset used.\n\nThe Random Forest algorithm is another classical machine-learning method used. It operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. The parameters for the Random Forest, including the number of trees (ntree) and the minimum size of terminal nodes (nodesize), were also tuned via inner cross-validation.\n\nAdditionally, a cost-sensitive neural network approach called COSNet was utilized. This method involves a parametric Hopfield recurrent neural network classifier within a semi-supervised, cost-sensitive learning context. The cost parameter in COSNet was set according to documentation guidelines.\n\nThe choice of these algorithms was driven by their effectiveness in handling network-based features and their ability to deal with highly unbalanced datasets, which are common in biological and medical research. While these algorithms are not new, their application in the context of network-based features and specific biological datasets provides valuable insights into their performance and potential improvements.\n\nThe decision to publish these findings in a bioinformatics journal rather than a machine-learning journal is likely due to the focus on the biological implications and the specific application of these algorithms to biological data. The study aims to contribute to the understanding of disease mechanisms and the identification of relevant genes, which is more aligned with the scope of bioinformatics research.",
  "optimization/meta": "The models described in this study do not function as meta-predictors. Instead, they are additive models that use specific input types and metrics to evaluate the performance of various methods for disease gene identification. The input types considered include drugs, genetic data, and stream data. The metrics used for evaluation are AUROC, AUPRC, top 20 hits, pAUROC at different thresholds, and top 100 hits.\n\nThe models are fitted separately by input type to avoid mixing notably different patterns. The model formulae are R-like, where the left-hand side contains the response variable (e.g., AUROC, AUPRC, top 20 hits), and the right-hand side describes the independent variables, which include the method, cross-validation scheme, network, and disease.\n\nThe study explores interaction terms within the explanatory models but finds that they do not provide added value for the extra complexity. The performance of the methods is evaluated using different cross-validation schemes, including a classic scheme and complex-aware schemes. The classic cross-validation scheme is found to give biased estimates, highlighting the importance of carefully controlling for this bias when estimating the performance of target gene prediction using network propagation.\n\nThe models allow for hypothesis testing and direct performance comparison between diseases, cross-validation strategies, networks, and methods by setting them as the independent variables. This approach provides a comprehensive evaluation of the methods' performance and identifies key influences on prediction quality.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of the machine-learning algorithms employed. We utilized several methods to encode and preprocess the data, each tailored to the specific requirements of the algorithms.\n\nFor the Support Vector Machine (SVM) and Random Forest (RF) methods, we applied classical machine learning approaches on network-based features. These features were generated using MashUp with default parameters for the human network, resulting in an 800-dimensional feature set as recommended by the authors. The caret and mlr R packages were used to define the classification tasks, perform grid-search for parameter tuning, and make predictions.\n\nThe SVM method employed was a nu-SVM with a Radial Basis Function (RBF) kernel. During training, negative class examples were randomly undersampled to match the number of positive class examples. Parameters were determined via inner cross-validation with specified ranges for nu and sigma, using a grid search with a resolution of 5 in each direction and an internal loop of 3 repetitions of 3-fold cross-validation.\n\nFor the Random Forest method, parameters were set to default values, except for those tuned via inner cross-validation. The ranges for ntree and nodesize were specified, with a linear search space in both. A grid search with a resolution of 3 in each direction was conducted to choose optimal parameters, using an internal loop of 3 repetitions of standard 3-fold cross-validation.\n\nAdditionally, we included COSNet, a cost-sensitive neural network classifier employed within a semi-supervised learning context. The cost parameter in the COSNet R package was set to 0.0001, following the documentation guidelines.\n\nFor comparison purposes, we also included three naive baseline methods: (1) pr, a classic non-personalized PageRank implementation that ignores input scores on the genes; (2) randomraw, which applies the raw diffusion approach from diffuStats to randomly permuted input scores on the genes; and (3) random, a uniform re-ranking of input genes without any network propagation.\n\nThese encoding and preprocessing steps ensured that the data was appropriately formatted and optimized for the machine-learning algorithms, enabling robust and reliable performance in our analyses.",
  "optimization/parameters": "In the optimization process, the models utilized several input parameters to systematically compare different factors. The primary parameters included in the models were the method, cross-validation scheme, network, and disease. These parameters were chosen to evaluate their individual effects on the performance metrics while correcting for other covariates.\n\nThe models were fitted separately for each input type—drugs, genetic, and stream—to avoid mixing problems of different natures. For each input type, the models considered the main effects of the cross-validation scheme, network, method, and disease. This approach allowed for a thorough evaluation of how each factor influenced the performance metrics.\n\nThe selection of these parameters was data-driven, ensuring that the models were tailored to the specific characteristics of the data. For instance, after choosing the cross-validation scheme and network based on data-driven criteria, reduced models were fitted within these frameworks for a more accurate description. This process involved fitting additive, explanatory regression models to the performance metrics of each fold from the cross-validation, using dispersion-adjusted logistic-like quasibinomial variance models for metrics like AUROC, pAUROC, and AUPRC, and quasipoisson models for top k hits. This methodology protected against issues of over and under-dispersion, ensuring robust statistical tests.\n\nThe specific number of parameters (p) varied depending on the model and input type, but generally included the four main effects mentioned above. The models were designed to be flexible and adaptable, allowing for the inclusion of additional parameters if necessary to capture the complexity of the data. The choice of parameters was guided by the need to balance model complexity with interpretability, ensuring that the models provided meaningful insights into the factors influencing performance.",
  "optimization/features": "In our study, we utilized network-based features for our machine learning models. Specifically, we generated these features using MashUp with default parameters for the human network, resulting in an 800-dimensional feature set. This dimensionality was recommended by the authors of MashUp, ensuring a comprehensive representation of the network-based information.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we relied on the dimensionality reduction and feature extraction capabilities inherent in the methods we employed. For instance, the regularised Laplacian kernel computed with diﬀuStats inherently considers the most relevant features based on the network structure. Similarly, the machine learning methods, such as Support Vector Machine (SVM) and Random Forest (RF), have built-in mechanisms to handle high-dimensional data and focus on the most informative features during the training process.\n\nTo ensure the robustness of our approach, we adhered strictly to using the training set for all parameter tuning and feature-related processes. This included grid-searching the parameters for SVM and RF using inner cross-validation, where the training data was further split to avoid data leakage. By doing so, we maintained the integrity of the validation and test sets, ensuring that our models' performance metrics were reliable and generalizable.",
  "optimization/fitting": "In our study, we employed additive, explanatory regression models to systematically compare various factors influencing performance metrics. These models helped us evaluate individual factors while correcting for other covariates, ensuring a thorough analysis.\n\nTo address the potential issue of overfitting, given the complexity of our models and the number of parameters involved, we utilized dispersion-adjusted logistic-like quasibinomial variance models for metrics such as AUROC, pAUROC, and AUPRC. Additionally, we used quasipoisson models for top k hits. The quasi-likelihood formalism protected against both over and under-dispersion issues, ensuring that the observed variance aligned with the theoretical fitted distribution. This approach helped mitigate the risk of overfitting by accounting for variability in the data.\n\nFurthermore, we conducted diagnostic checks on our models. For instance, we examined deviance residuals against predicted values to ensure that the residuals were healthy and that the chosen distributions were suitable for describing the data. We also used Cook’s distance statistic to identify any influential observations, confirming that our models were robust and not unduly influenced by outliers.\n\nTo prevent underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We included main effects for cross-validation scheme, network, method, and disease, allowing us to model each metric separately for different input types. This approach ensured that our models were comprehensive and capable of capturing the nuances of the data.\n\nIn summary, our use of additive models, dispersion-adjusted variance models, and thorough diagnostic checks helped us rule out both overfitting and underfitting, ensuring reliable and interpretable results.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key approach was the use of cross-validation schemes that accounted for protein complexes. This strategy helped to balance fold sizes and prevent information leakage between training and validation sets, which is crucial for avoiding overfitting.\n\nAdditionally, we fitted additive, explanatory regression models to the performance metrics of each fold from the cross-validation. These models included main effects for cross-validation schemes, networks, methods, and diseases. By using main effect models, we were able to evaluate each individual factor while correcting for other covariates, which helped to isolate the true effects and reduce overfitting.\n\nWe also utilized dispersion-adjusted logistic-like quasibinomial variance models for metrics such as AUROC, pAUROC, and AUPRC, as well as quasipoisson models for top k hits. These models protected against over and under-dispersion issues, ensuring that the observed variance was appropriately accounted for and that subsequent statistical tests were not misleading.\n\nFurthermore, we explored interaction terms within the explanatory models but found that they did not provide added value for the extra complexity. This decision was based on a data-driven approach, ensuring that our models remained parsimonious and focused on the most relevant factors.\n\nOverall, these techniques collectively helped to mitigate overfitting and ensure that our models provided reliable and generalizable results.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are indeed available. We have made these resources accessible to ensure reproducibility and to facilitate further research in the field.\n\nAll the necessary details, including the specific hyper-parameter settings, the optimization schedules employed, and the model files, can be found in the supplementary materials accompanying our publication. These materials are provided in a comprehensive and organized manner to allow other researchers to replicate our experiments and build upon our findings.\n\nThe supplementary materials are distributed under an open license, which permits free access, use, and distribution. This license ensures that the community can utilize these resources without any restrictions, fostering collaboration and innovation. The exact license details can be found within the supplementary materials themselves, providing clear guidelines on how these resources can be used and shared.\n\nBy making these configurations and parameters available, we aim to contribute to the transparency and reproducibility of scientific research, enabling others to verify our results and explore new avenues of investigation.",
  "model/interpretability": "The models employed in this study are designed to be transparent and interpretable. They are additive models, which means that the effects of different variables (such as method, cross-validation scheme, network, and disease) are additive and do not interact with each other. This simplicity allows for straightforward interpretation of the model coefficients.\n\nEach model is summarized in tables, where the coefficients for each variable are clearly listed along with their confidence intervals. For example, in the models for the drugs input, the coefficients for different methods (such as random, EGAD, ppr, etc.) are provided, showing how each method affects the performance metrics like AUROC, AUPRC, and top 20 hits. Positive coefficients indicate an improvement in performance compared to the reference level, while negative coefficients indicate a reduction.\n\nThe reference levels for the models are explicitly stated, such as the pr method, STRING network, classic cross-validation scheme, and allergy. This allows for a clear baseline against which other variables can be compared. For instance, the performance of the rf method can be directly compared to the pr method, showing that rf generally performs better.\n\nAdditionally, the models are stratified by input type (drugs, genetic, stream) and by different performance metrics, providing a comprehensive view of how each variable influences the outcomes. This stratification helps in understanding the specific contexts in which certain methods or networks perform better.\n\nThe use of additive models ensures that the impact of each variable is independent and can be easily understood. This transparency is crucial for interpreting the results and drawing meaningful conclusions from the data. The models allow for hypothesis testing and direct performance comparison between different diseases, cross-validation strategies, networks, and methods, making them a valuable tool for understanding the factors that influence performance in disease gene identification.",
  "model/output": "The models discussed in this study are primarily classification models, as indicated by the use of metrics such as AUROC (Area Under the Receiver Operating Characteristic Curve) and AUPRC (Area Under the Precision-Recall Curve). These metrics are commonly used to evaluate the performance of classification models. The models are fitted using different input types, including drugs, genetic, and stream data, and they employ various statistical families such as quasibinomial and quasipoisson. The formulas for these models include response variables like AUROC, AUPRC, Top20, and pAUROC, which are all indicative of classification tasks. The models are adjusted for different methods, cross-validation schemes, networks, and diseases, but the core objective remains the classification of outcomes based on the input data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code used in this work is publicly available. It can be accessed via a GitHub repository, which is specifically designed to support the conclusions drawn in the article. The repository contains the necessary code and datasets to replicate the findings.\n\nThe code and datasets are distributed under the terms of the Creative Commons Attribution License. This license permits unrestricted use, distribution, and reproduction in any medium, provided that the original authors and the source are credited.\n\nThe repository is hosted at https://github.com/b2slab/genedise. This platform allows users to access, download, and utilize the code and datasets for their own research or educational purposes. The availability of the source code and datasets ensures transparency and reproducibility, which are crucial for the scientific community.",
  "evaluation/method": "The evaluation of the methods involved a comprehensive benchmarking process using six performance metrics across two input streams: genetically associated genes and known drug target genes. The goal was to predict drug target-based genes for 22 common diseases. A 3-fold cross-validation (CV) strategy, repeated 25 times, was employed under three different CV schemes. These schemes included classic, block, and representative strategies, with the latter two being complex-aware. The gene identifiers in each fold were determined using only the drugs data, regardless of the input type.\n\nFifteen methods based on network propagation, including four baselines, were evaluated using two different networks. The performance metrics considered included AUROC, AUPRC, pAUROC at 5% and 10%, and top k hits (k being 20 and 100). These metrics were used to assess the methods' ability to prioritize disease genes accurately.\n\nThe evaluation also involved fitting additive, explanatory regression models to the performance metrics of each fold from the cross-validation. These models helped in systematically comparing the performance across different diseases, methods, cross-validation schemes, and input types. The models were fitted separately for each metric and input type to avoid mixing problems of different nature. Dispersion-adjusted logistic-like quasibinomial variance models were used for AUROC, pAUROC, and AUPRC, while quasipoisson models were used for top k hits. This approach protected against over and under-dispersion issues, ensuring robust statistical tests.\n\nAdditionally, qualitative methods comparison was performed using Spearman’s footrule to compare the rankings produced by different algorithms. Distances were computed between all method ranking pairs for each combination of disease, input type, network, and for the top N predicted genes, excluding the original seed genes. This part did not involve cross-validation and used all known disease-associated genes for gene prioritizations. The results were visualized using a multi-view extension of MDS, providing a compact visualisation of distance matrices.\n\nThe evaluation highlighted the importance of carefully controlling for biases when estimating the performance of target gene prediction using network propagation. The classic cross-validation scheme was found to give biased estimates, emphasizing the need for adjusted cross-validation strategies in computational biology.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report a comprehensive set of metrics to evaluate the performance of our models. These metrics include the Area Under the Receiver Operating Characteristic Curve (AUROC), the Area Under the Precision-Recall Curve (AUPRC), partial AUROC at thresholds 0.10 and 0.05, and top k hits (specifically top 20 and top 100 hits). These metrics are chosen to provide a thorough assessment of model performance, particularly focusing on the top-ranked entities, which is crucial in drug development scenarios.\n\nThe AUROC measures the ability of the model to distinguish between positive and negative classes across all thresholds, providing a general sense of model performance. The AUPRC, on the other hand, is particularly useful in imbalanced datasets, as it focuses on the precision and recall of the positive class. Partial AUROC at specific thresholds offers a more granular view of model performance at critical decision points. The top k hits metrics evaluate the model's effectiveness in identifying the most relevant entities within the top k ranked results, which is essential for practical applications in drug development.\n\nThis set of metrics is representative of the current literature in the field. AUROC and AUPRC are standard metrics widely used in machine learning and bioinformatics to evaluate model performance. The inclusion of partial AUROC and top k hits aligns with recent trends in the literature, where there is an increasing emphasis on evaluating models at specific thresholds and within the top-ranked results. This approach ensures that our evaluation is both rigorous and relevant to real-world applications.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of various network propagation methods for disease gene identification. We compared our methods against publicly available algorithms using benchmark datasets, ensuring a fair and comprehensive assessment.\n\nWe included simpler baselines in our comparison to provide a reference point for evaluating the performance of more complex methods. For instance, we used a random baseline and a PageRank approach that ignores input gene scores to demonstrate the inherent predictive power of network topology. Additionally, we compared against the EGAD method, which prioritizes genes based on network topology alone, and other baselines that use different data streams such as pathways, gene expression, or animal models.\n\nOur evaluation involved multiple performance metrics, including AUROC, AUPRC, pAUROC, and top k hits, to ensure a robust assessment of each method's strengths and weaknesses. We also performed a Tukey’s multiple comparison test on the model coefficients to formally test the differences between methods, adjusting for multiplicity to ensure statistical significance.\n\nThe results highlighted that most diffusion-based and machine learning-based methods outperformed the simpler baselines. For example, the random forest (rf) method showed the best performance, followed by raw diffusion and other methods like bagsvm, z, and svm. However, the actual effect size or magnitude of these differences varied, and some methods, like EGAD and COSNet, produced similar predictions despite their differing complexities.\n\nWe also used multi-dimensional scaling (MDS) plots to visually compare the rankings produced by different methods. These plots showed that methods with similar underlying principles, such as diffusion-based approaches, tended to cluster together. Supervised methods like random forest and support vector machines also showed agreement, reflecting their training on the same network-based features.\n\nOverall, our comparison to publicly available methods and simpler baselines provided valuable insights into the performance and behavior of different network propagation techniques for disease gene identification.",
  "evaluation/confidence": "The performance metrics in our study are accompanied by confidence intervals, which are crucial for evaluating the reliability of our results. These intervals provide a range within which the true performance metric is expected to lie, with a certain level of confidence, typically 95%. This allows us to assess the precision of our estimates and understand the variability in our results.\n\nStatistical significance is a key aspect of our analysis. We use statistical tests to determine whether the observed differences in performance between methods, networks, cross-validation schemes, and diseases are likely to be due to actual differences rather than random chance. For instance, we report p-values and confidence intervals for the coefficients in our additive models, which help us identify significant effects. A result is considered statistically significant if the p-value is below a certain threshold, commonly 0.05. This means there is less than a 5% probability that the observed difference is due to random variation.\n\nIn our models, we use dispersion-adjusted logistic-like quasibinomial variance models for metrics like AUROC, pAUROC, and AUPRC, and quasipoisson for top k hits. These models help protect against issues of over-dispersion or under-dispersion, ensuring that our statistical tests are robust. The quasibinomial and quasipoisson frameworks adjust for the observed variance in the data, providing more accurate confidence intervals and p-values.\n\nAdditionally, we perform hypothesis testing to compare the performance of different methods and baselines. For example, we find that certain methods, like rf, perform significantly better than the baseline using both AUROC and top 20 hits metrics. The confidence intervals for these methods do not overlap with those of the baseline, indicating a statistically significant difference. Similarly, switching from certain networks or cross-validation schemes results in a significant reduction in performance, as evidenced by the negative estimates and their confidence intervals.\n\nIn summary, our study employs rigorous statistical methods to ensure that our performance metrics are reliable and that our conclusions about the superiority of certain methods are supported by statistically significant evidence. The use of confidence intervals and statistical tests allows us to make confident claims about the relative performance of different approaches in disease gene identification.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being publicly available. Therefore, it is not clear whether these files can be accessed by the public or under what conditions they might be shared. If access to these files is required, it would be best to contact the authors directly for more information."
}