{
  "publication/title": "Ensembling multiple raw coevolutionary features with deep residual neural networks for contact-map prediction in CASP13",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Proteins",
  "publication/year": "2019",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Protein structure prediction\n- Contact-map prediction\n- Coevolutionary analysis\n- Deep learning\n- Residual neural networks\n- Multiple sequence alignment\n- Covariance matrix\n- Precision matrix\n- Potts model\n- Machine learning in bioinformatics",
  "dataset/provenance": "The dataset used in our study is derived from the CASP13 experiment, which included a total of 90 full-length protein targets. Out of these, 82 targets have had their final structures released. These targets were split into 122 domains by the assessors. The dataset is composed of multiple sequence alignments (MSAs) generated using DeepMSA, which searches against multiple sequence databases. The key coevolutionary features extracted from these MSAs include the covariance matrix (COV), the precision matrix (PRE), and the coupling parameters of the Potts model by pseudolikelihood maximization (PLM). These features are crucial for contact-map prediction based on coevolutionary analysis.\n\nThe dataset leverages well-established sequence databases such as UniClust, UniRef90, and Metaclust. The process involves iterative searches and alignments to ensure a comprehensive and accurate MSA. For instance, HHblits is used to search against UniClust for three iterations, followed by Jackhmmer searches against UniRef90 if sufficient sequences are not obtained. Additionally, HMMbuild from the HMMER package is used to search against the Metaclust metagenome sequence database if necessary. This multi-step approach ensures that the MSAs are robust and include a sufficient number of effective sequences.\n\nThe dataset is not entirely novel but builds upon existing methods and databases that have been used in the community. The use of DeepMSA and the iterative search strategy are common practices in the field of protein structure prediction. The focus is on enhancing the accuracy of contact-map prediction by integrating multiple coevolutionary features and employing advanced neural network architectures. The dataset and methods have been validated through the CASP13 experiment, demonstrating their effectiveness in predicting protein contacts.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation strategy for both TripletRes and ResTriplet models. This means the dataset was divided into 10 subsets. Each subset was used as a validation set once, while the remaining 9 subsets were used as the training set. This process was repeated 10 times, ensuring that each subset served as the validation set exactly once. Consequently, each model was trained 10 times, and the final output was the average of these 10 models. This approach helped in reducing the risk of overfitting and provided a more robust evaluation of the models' performance.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in our work utilizes deep residual neural networks for contact-map prediction. Specifically, we use convolutional neural networks (CNNs) with residual connections, which are a well-established class of machine-learning algorithms in the field of deep learning.\n\nThe use of residual networks is not novel in the context of machine learning, as they have been extensively studied and applied in various domains, including computer vision and natural language processing. However, their application to the specific problem of contact-map prediction in protein structure prediction is part of our innovative approach.\n\nThe reason these algorithms were not published in a machine-learning journal is that our primary focus is on the application of these techniques to biological problems, particularly in the context of protein structure prediction. The development and optimization of these models are tailored to the unique challenges and data characteristics of contact-map prediction, which is a specialized area within computational biology.\n\nOur implementation of these neural networks is done using the PyTorch framework, which is a widely-used open-source machine-learning library. The models are trained using the Adam optimizer, a popular stochastic gradient descent method known for its efficiency and effectiveness in training deep neural networks. The initial learning rate is set to 1e-3, and the training process runs for 50 epochs. This setup ensures that the models converge to an optimal solution while managing computational resources efficiently.",
  "optimization/meta": "In our study, we developed two meta-predictors, TripletRes and ResTriplet, which integrate information from multiple components to enhance contact prediction accuracy, particularly for hard free-modeling (FM) targets. These meta-predictors use raw coevolutionary features, including the covariance matrix, the precision matrix, and the parameter matrix of a pseudolikelihood maximized Potts model. These features are derived from direct coupling analysis (DCA) methods, which aim to reduce transitional noise in contact predictions.\n\nTripletRes and ResTriplet employ different strategies to ensemble these features. TripletRes uses end-to-end training, where multiple models are trained on different subsets of the data, and their outputs are averaged to produce the final prediction. This approach helps to mitigate overfitting and improves the robustness of the model, especially for FM targets with limited sequence homologs.\n\nResTriplet, on the other hand, uses a stacking strategy. In the first stage, it generates predicted contact-maps using 10-fold cross-validation for each coevolutionary feature type. These predicted contact-maps serve as features for the second stage, where a final model is trained without cross-validation due to time constraints. This stacking approach allows ResTriplet to leverage the strengths of different coevolutionary features and improve overall prediction accuracy.\n\nBoth meta-predictors were trained using the Adam optimizer with a default initial learning rate of 1e-3 for 50 epochs. The training data for TripletRes and ResTriplet is independent, as they share the same training set but use different data splitting strategies for model training. This independence ensures that the performance of the meta-predictors is not biased by overlapping training data.\n\nIn summary, TripletRes and ResTriplet are meta-predictors that combine information from multiple coevolutionary features using end-to-end training and stacking strategies, respectively. They demonstrate improved performance, particularly for FM targets, by leveraging the complementary information from different components and ensuring independent training data.",
  "optimization/encoding": "In our study, we utilized three coevolutionary features derived from multiple sequence alignments (MSAs) to encode the data for our machine-learning algorithms. The first feature is the covariance matrix, which captures the marginal correlations between residue pairs at different positions in the protein sequence. This matrix is computed based on the observed relative frequencies of residue pairs and individual residues.\n\nThe second feature is the precision matrix, obtained by minimizing an objective function that includes the trace of the product of the covariance and precision matrices, the log determinant of the precision matrix, and an L2 regularization term. This matrix provides direct couplings between pairs of sites conditional on other positions, offering better performance in contact-map prediction compared to the covariance matrix.\n\nThe third feature involves the coupling parameters of the Potts model, approximated through pseudolikelihood maximization. These parameters are efficiently calculated using CCMpred. All three features are represented as 21∙L by 21∙L matrices, where L is the sequence length and 21 accounts for the 20 standard amino acid types plus a gap type. Each position pair is represented by a 21 by 21 sub-matrix of coupling parameters, which are then reshaped into three input features of size L by L by 441 for each sequence.\n\nFor the neural network architectures, we employed deep residual neural networks (ResNet) and did not apply any pre-normalization to the input features. Instead, an instance normalization layer was added after each convolutional layer, except the last one. This approach helps in stabilizing the training process and improving the performance of the models. The training set was divided into subsets for cross-validation, and the output was averaged across multiple models to enhance the robustness of the predictions. Dynamic batch sizes were used during training to accommodate sequences of different lengths, ensuring efficient use of GPU resources. The choice of hyperparameters, such as the number of layers, was a balance between memory usage and performance, aiming to fit the models into GPU memory for efficient training.",
  "optimization/parameters": "The models discussed in this publication utilize deep residual neural networks (ResNet) for contact-map prediction. The specific architectures employed are TripletRes and ResTriplet.\n\nIn TripletRes, each of the three coevolutionary features (COV, PRE, and PLM) is processed through a set of 24 residual blocks. Each residual block consists of two convolutional layers, with instance normalization and dropout layers added to enhance training speed and prevent overfitting. The dropout rate is set to 0.2. The kernel size of the convolutional layers is 3 × 3 with padding to maintain spatial information. Additionally, a 1 × 1 convolutional layer is used to transform each coevolutionary input feature into 64 channels. The final contact-map prediction is obtained through a sigmoid activation function over the output of a convolutional layer with one output channel.\n\nResTriplet follows a two-stage ensemble approach. In Stage I, three individual base models are trained separately using the three coevolutionary features. Each base model consists of 22 residual blocks. In Stage II, a shallow neural network combines the predictions from Stage I, along with predicted secondary structures, using a collection of 5 dilated convolutional layers with a dilation value of 2 and 16 channels. The final output is obtained through a dilated convolutional layer with a dilation value of 2 and one output channel, followed by a sigmoid activation function.\n\nThe choice of hyperparameters, including the number of layers, is a compromise between memory usage and performance. Deeper models can theoretically yield better performance, but the number of layers is limited by GPU memory constraints. The models are trained using the Adam optimizer with a default initial learning rate of 1e-3 for 50 epochs. A dynamic batch size strategy is employed based on the sequence length to optimize GPU resource usage.",
  "optimization/features": "Three coevolutionary features are used as input for the contact-map prediction pipeline. These features are derived from multiple sequence alignments (MSAs) and include the covariance matrix, the precision matrix, and the coupling parameters of the Potts model. The covariance matrix captures the marginal correlations between residue pairs, while the precision matrix provides direct couplings between pairs of sites conditional on other positions. The coupling parameters of the Potts model are calculated using pseudolikelihood maximization.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the focus was on extracting meaningful coevolutionary features from the MSAs. The choice of these three features was based on their demonstrated effectiveness in previous studies and their ability to capture different aspects of residue-residue interactions.\n\nThe data splitting strategy ensures that the training and validation sets are independent, which is crucial for avoiding overfitting and ensuring the generalizability of the models. For TripletRes, the training set was divided into 10 subsets, with each subset used as a validation set while the remaining subsets were used for training. This 10-fold cross-validation approach was also applied in Stage I of ResTriplet for each coevolutionary feature type. However, in Stage II of ResTriplet, cross-validation was not performed due to time constraints before the CASP experiment.",
  "optimization/fitting": "In our study, we employed two deep learning models, TripletRes and ResTriplet, for contact-map prediction. Both models utilize raw coevolutionary features derived from multiple sequence alignments (MSAs), specifically the covariance matrix, the precision matrix, and the coupling parameters of a pseudolikelihood maximized Potts model.\n\nThe number of parameters in our neural networks is indeed much larger than the number of training points, which is a common scenario in deep learning. To mitigate the risk of over-fitting, we implemented several strategies.\n\nFor TripletRes, we used 10-fold cross-validation during training. The training set was divided into 10 subsets, with each subset serving as the validation set in turn, while the remaining subsets were used for training. This process was repeated 10 times, and the final output was the average of all 10 models. This approach helps to ensure that the model generalizes well to unseen data.\n\nIn the case of ResTriplet, we also used 10-fold cross-validation in Stage I to reduce over-fitting. However, due to time constraints before the CASP experiment, cross-validation was not performed in Stage II. Instead, we relied on the diversity and depth of the MSAs generated by different alignment protocols to improve the robustness of our predictions.\n\nTo address the potential issue of under-fitting, we carefully designed our neural network architectures based on deep residual networks (ResNet). These architectures allow for the training of very deep networks, which can capture complex patterns in the data. Additionally, we used a dynamic batch size strategy during training to efficiently utilize GPU resources, which helps in better convergence of the model.\n\nThe training of our models was conducted using the Adam optimizer with a default initial learning rate of 1e-3 for 50 epochs. This optimizer is known for its efficiency and adaptability, which helps in finding the optimal parameters for the model.\n\nIn summary, we employed cross-validation, deep residual network architectures, and efficient training strategies to balance the trade-off between over-fitting and under-fitting in our contact-map prediction models.",
  "optimization/regularization": "In our study, several techniques were employed to prevent over-fitting during the training of our models. One of the key methods used was instance normalization, which was applied after each convolutional layer except the last one in both TripletRes and ResTriplet. This helped to stabilize the training process and reduce the risk of over-fitting by normalizing the features within each instance.\n\nAdditionally, dropout layers were incorporated into the neural network architecture. The dropout rate was set to 0.2, meaning that 20% of the input signals were randomly masked at each training batch. This technique helps to prevent the model from becoming too reliant on specific neurons, thereby improving generalization.\n\nFor TripletRes, a 10-fold cross-validation strategy was implemented. The training set was divided into 10 subsets, with each subset serving as the validation set while the remaining subsets were used for training. This approach ensured that the model was evaluated on different portions of the data, reducing the likelihood of over-fitting to any single subset.\n\nIn the first stage of ResTriplet, a similar 10-fold cross-validation was used to generate predicted contact-maps for each base model. This method helped to mitigate over-fitting by ensuring that the model's performance was assessed across multiple validation sets.\n\nFurthermore, a dynamic batch size strategy was adopted during training to accommodate the varying lengths of sequences. This strategy helped to efficiently utilize GPU resources and ensured that the model could handle sequences of different lengths without over-fitting to any particular batch size.\n\nOverall, these regularization techniques played a crucial role in enhancing the robustness and generalization capabilities of our models, thereby improving the accuracy of contact-map predictions.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, we employed an instance normalization layer after each convolutional layer, except the last one, in both TripletRes and ResTriplet models. The training process involved 50 epochs using the Adam optimizer with a default initial learning rate of 1e-3. For TripletRes, training required 4 GPUs running concurrently, while ResTriplet could be handled with a single GPU. A dynamic batch size strategy was implemented to manage GPU resources effectively, with batch sizes varying based on sequence length.\n\nThe model architectures, including the number of residual blocks and convolutional layers, are also described. TripletRes and ResTriplet share the same training set, divided into 10 subsets for cross-validation in TripletRes. ResTriplet uses a two-stage approach, where Stage I involves 10-fold cross-validation to reduce overfitting, and Stage II combines predictions from the base models without cross-validation due to time constraints.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly mentioned in the provided context. Therefore, it is not clear whether the specific model files and optimization parameters are publicly available or under what license they might be distributed. For precise information on the availability and licensing of model files and optimization parameters, one would need to refer to supplementary materials or contact the authors directly.",
  "model/interpretability": "The models discussed, TripletRes and ResTriplet, are primarily based on deep residual neural networks, which are known for their complexity and are generally considered black-box models. This means that the internal workings of these models are not easily interpretable, and it is challenging to directly understand how specific inputs lead to particular outputs.\n\nHowever, there are some aspects of the models that provide insights into their functioning. For instance, the use of coevolutionary features derived from multiple sequence alignments (MSAs) offers a degree of interpretability. These features, such as the covariance matrix, precision matrix, and coupling parameters of the Potts model, capture evolutionary relationships between residue pairs. By examining these matrices, one can gain some understanding of the residue interactions that the models are learning.\n\nAdditionally, the architecture of the models includes residual blocks, which help in mitigating the vanishing gradient problem and allow for deeper networks. The use of instance normalization and dropout layers also provides some transparency by helping to regularize the model and prevent overfitting.\n\nIn the case of ResTriplet, the two-stage ensemble approach adds another layer of interpretability. In Stage I, separate base models are trained on different coevolutionary features, and their predictions are combined in Stage II. This stacking strategy allows for the examination of how different types of coevolutionary information contribute to the final contact-map prediction.\n\nMoreover, the models incorporate predicted secondary structures as additional features, which can be visualized and interpreted. This inclusion helps in understanding how secondary structure information influences the contact-map predictions.\n\nOverall, while the models are complex and largely black-box, the use of interpretable features and a structured ensemble approach provides some level of transparency. This allows researchers to gain insights into the factors that contribute to the models' predictions.",
  "model/output": "The model is a regression model designed for contact map prediction in proteins. It predicts the likelihood of contacts between residue pairs, rather than classifying them into discrete categories. The final output is obtained through a sigmoid activation function, which provides a probability score for each potential contact. This approach is crucial for understanding protein folding and structure, as it helps identify which amino acid residues are likely to be in close proximity within the three-dimensional structure of the protein.\n\nThe model employs two main architectures: TripletRes and ResTriplet. TripletRes directly ensembles three coevolutionary features using neural networks, transforming each input feature through a series of residual blocks and concatenating the outputs. The final prediction is made using a convolutional layer followed by a sigmoid activation.\n\nResTriplet, on the other hand, uses a two-stage ensemble strategy. In the first stage, three base models are trained separately on different coevolutionary features. These models produce predicted contact maps, which are then used as input features in the second stage. Additionally, predicted secondary structures are included as extra features. The second stage employs a shallow neural network with dilated convolutional layers to combine the predictions from the base models, ultimately producing the final contact map prediction through a sigmoid function.\n\nBoth models utilize residual blocks with instance normalization and dropout layers to enhance training efficiency and prevent overfitting. The training process involves cross-validation and dynamic batch sizes to optimize performance given the constraints of GPU resources. The final output is a contact map that indicates the probability of contacts between residue pairs, aiding in the prediction of protein structures.",
  "model/duration": "The execution time for the models varied due to differences in their architectures and resource requirements. The training of TripletRes required the use of 4 GPUs running concurrently, which allowed for parallel processing and faster training times. In contrast, the training procedures of ResTriplet could be handled with just one GPU. This difference in GPU usage reflects the varying computational demands of the two models.\n\nA dynamic batch size strategy was employed during training to optimize the use of GPU resources. Specifically, a batch size of 1 was used for sequences with a length greater than 300, a batch size of 2 for sequences between 200 and 300 in length, and a batch size of 4 for sequences with a length less than 200. This strategy helped to manage the limited GPU resources effectively, ensuring that the models could be trained efficiently without exceeding the memory capacity of the GPUs.\n\nThe choice of hyperparameters, particularly the number of layers, was a compromise between memory usage and performance. While deeper convolutional neural network (CNN) models could theoretically yield better performance, the practical constraints of GPU memory limited the number of layers that could be used. This balance was crucial for ensuring that the models could be trained efficiently within the available computational resources.\n\nIn summary, the execution time for the models was influenced by the number of GPUs used, the dynamic batch size strategy, and the trade-offs made in choosing hyperparameters to optimize performance within the constraints of available GPU memory.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods TripletRes and ResTriplet was conducted using a comprehensive approach that included cross-validation and performance assessment on specific targets. For TripletRes, a 10-fold cross-validation strategy was employed, where the training set was divided into 10 subsets. Each subset was used as a validation set while the remaining subsets were used for training. This process was repeated 10 times, and the final output was the average of all 10 models. This method ensured that each data point was used for both training and validation, providing a robust evaluation of the model's performance.\n\nIn Stage I of ResTriplet, a similar 10-fold cross-validation approach was used to reduce the risk of overfitting. For each coevolutionary feature type, 10 models were built using the same data splitting strategy as TripletRes. The predicted contact-maps of the validation set from each model were then used as features for Stage II. However, in Stage II, cross-validation was not performed due to time constraints before the CASP experiment.\n\nThe overall performance of the methods was evaluated on the CASP13 dataset, which included 90 full-length protein targets. The assessment focused on the precision of contact predictions at different ranges: short range, medium range, and long-range. The precision was measured for the top L, L/2, and L/5 contacts, providing a detailed evaluation of the methods' accuracy. The results showed that both TripletRes and ResTriplet achieved high precision in long-range contact predictions, demonstrating their effectiveness in predicting distant interactions within proteins.\n\nAdditionally, the methods were evaluated on free-modeling (FM) targets, which lack homologs in structure and sequence databases. This evaluation highlighted the robustness of TripletRes and ResTriplet, particularly in handling challenging targets with limited sequence information. The performance on FM targets was assessed by comparing the precision of contact predictions with the number of effective sequences (Nf) in the multiple sequence alignments (MSAs). The results indicated that TripletRes was less dependent on the quality of MSAs, while ResTriplet showed a higher sensitivity to the content of the MSAs. Despite this, both methods achieved reasonable contact accuracy even with a very limited number of sequence homologs, underscoring their ability to learn underlying contact patterns from sparse data.",
  "evaluation/measure": "In our evaluation, we focused on several key performance metrics to assess the effectiveness of our methods, TripletRes and ResTriplet, in predicting protein contacts. The primary metrics we reported include precision for short, medium, and long-range contacts. These ranges are defined based on the sequence separation between residue pairs: short-range contacts have a sequence separation of 6 to 11 residues, medium-range contacts have a separation of 12 to 23 residues, and long-range contacts have a separation of 24 or more residues. We evaluated precision at different thresholds, specifically top L, L/2, and L/5 contacts, where L is the length of the query sequence.\n\nFor long-range contacts, which are particularly challenging to predict, we observed that TripletRes and ResTriplet achieved comparable performance. TripletRes showed a slight edge in accuracy for top L and L/5 long-range contacts, but the differences were statistically insignificant. This indicates that both methods are robust and reliable for predicting long-range contacts.\n\nWe also examined the impact of multiple sequence alignments (MSAs) on prediction accuracy. The quality of MSAs, measured by the number of effective sequences (Nf), is crucial for the performance of our methods. We found that deeper MSAs do not always lead to better contact prediction due to potential alignment noise. However, both TripletRes and ResTriplet demonstrated the ability to learn underlying contact patterns even from a limited number of sequence homologs, which is vital for modeling free-modeling (FM) targets that lack homologous sequences.\n\nAdditionally, we compared the performance of our ensemble methods against individual component predictors. The ensemble approaches significantly outperformed the individual components, particularly for FM targets. This highlights the importance of integrating information from multiple sources to improve prediction accuracy.\n\nIn summary, our performance metrics are representative of the current literature in protein contact prediction. We focused on precision for different contact ranges and thresholds, which are standard metrics in the field. Our results demonstrate the effectiveness of TripletRes and ResTriplet in predicting protein contacts, especially for challenging FM targets.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed methods, TripletRes and ResTriplet, against simpler baselines and publicly available methods on benchmark datasets. Specifically, we compared the results of our hybrid methods against three predictors using individual component input features on CASP13 FM targets. These components included PLM, PRE, and COV features.\n\nThe comparison revealed that the ensemble methods significantly outperformed the individual component predictors for FM targets. For instance, the mean precision of long-range top L/5 contacts for TripletRes was 64.6%, which was 10.6%, 8.6%, and 12.0% higher than the precisions achieved by predictors based on PLM, PRE, and COV, respectively. Similarly, the stacked ensemble of ResTriplet achieved a mean precision of 64.0%, which was 9.6%, 7.6%, and 10.9% higher than the mean precisions of each component.\n\nWhen considering all targets, the ensemble methods only slightly outperformed the component predictors. For example, the top L/5 long-range average precision of TripletRes and ResTriplet for all targets were 75.4% and 76.2%, respectively, which were marginally higher than the 75.2%, 74.6%, and 71.0% achieved by predictors based on PLM, PRE, and COV features.\n\nThis comparison highlights the effectiveness of our ensemble strategies in integrating information from different components, leading to improved contact prediction accuracy, especially for challenging FM targets.",
  "evaluation/confidence": "The evaluation of our methods, TripletRes and ResTriplet, includes statistical significance tests to ensure the robustness of our claims. For instance, the improvement in contact prediction precision when using DeepMSA over HHBlits was found to be statistically significant, with p-values of 2.9e-06 and 1.1e-04 for TripletRes and ResTriplet, respectively. This indicates that the observed improvements are unlikely to be due to chance.\n\nIn terms of comparing TripletRes and ResTriplet, the differences in performance for long-range contact predictions were found to be statistically insignificant. The p-values for the differences in top L and L/5 long-range contact accuracies were 0.82 and 0.65, respectively. This suggests that while TripletRes may have a slight edge in accuracy, the difference is not statistically significant.\n\nAdditionally, the Pearson correlation coefficients between precision and the logarithm of the number of effective sequences (Nf) in the multiple sequence alignments (MSAs) were calculated. These coefficients were 0.584 for TripletRes and 0.551 for ResTriplet, indicating modest correlations. This analysis helps to understand the relationship between the quality of MSAs and the performance of our methods.\n\nOverall, the statistical analyses provide confidence in the performance metrics and the claims made about the superiority of our methods in certain contexts. However, it is important to note that statistical significance does not always translate to practical significance, and the context of the results should be considered when interpreting their importance.",
  "evaluation/availability": "Not enough information is available."
}