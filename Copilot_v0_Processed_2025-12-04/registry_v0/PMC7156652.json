{
  "publication/title": "Deep learning-based classification of TFE3 renal cell carcinoma and clear cell renal cell carcinoma using whole-slide images",
  "publication/authors": "The authors who contributed to the article are:\n\n- J.C. and J.Z. conceived and designed the study.\n- J.C. and Z.H. performed the computational analysis with assistance from W.S., R.M., M.C., Q.F., and D.N.\n- The paper was written by J.C., J.Z., and K.H. with contributions by all co-authors.\n- The authors declare no competing interests.",
  "publication/journal": "Nature Communications",
  "publication/year": "2020",
  "publication/doi": "10.1038/s41467-020-15671-5",
  "publication/tags": "- Renal Cell Carcinoma\n- TFE3-RCC\n- ccRCC\n- Machine Learning\n- Image Analysis\n- Histopathology\n- Feature Extraction\n- Color Normalization\n- Bag-of-Visual-Words\n- Classification Models\n- Feature Selection\n- Whole-Slide Images\n- Histogram Features\n- Distribution Statistics\n- Cross-Validation\n- External Validation\n- AUC\n- Logistic Regression\n- SVM\n- Random Forest",
  "dataset/provenance": "The dataset used in this study consists of H&E-stained whole-slide images. Two datasets were collected, totaling 148 images. The first dataset, referred to as dataset 1, includes 50 TFE3-RCC patients and 50 ccRCC patients, all sourced from Indiana University. The second dataset, dataset 2, serves as an external validation set and comprises 14 TFE3-RCC patients from the University of Michigan, 10 TFE3-RCC patients from TCGA, and 24 ccRCC patients from TCGA. All tumor samples were obtained through surgical excision and scanned at ×40 magnification. The ratio of TFE3-RCC patients to ccRCC patients is 1:1, and the gender and tumor grade information between the two subtypes were matched. No TFEB rearranged translocation RCC was included in the analysis. Personal health information was de-identified, making this an institutional review board approval-exempt study.",
  "dataset/splits": "In our study, we utilized two datasets for training and validating our classification models. The first dataset, referred to as dataset 1, consisted of 100 patients, evenly split between TFE3-RCC and ccRCC, with 50 patients in each category. This dataset was used for internal validation through a five-fold cross-validation process. In each round of cross-validation, dataset 1 was randomly partitioned into two sets: 80% for training and 20% for internal validation.\n\nThe second dataset, known as dataset 2, also contained 48 patients, with an equal number of TFE3-RCC and ccRCC cases, 24 in each category. This dataset served as an external validation set to assess the generalization performance of our models. The models were trained using dataset 1 and then evaluated on dataset 2 to ensure their robustness and applicability to new, unseen data.",
  "dataset/redundancy": "In our study, we utilized two distinct datasets to ensure the robustness and generalizability of our findings. Dataset 1, obtained from Indiana University, consisted of 50 TFE3-RCC patients and 50 ccRCC patients, matched by gender and tumor grade. This dataset was randomly partitioned into training and internal validation sets using five-fold cross-validation. In each round, 80% of the data was allocated to the training set, while the remaining 20% was used for internal validation. This process was repeated five times to ensure that each data point was used for both training and validation, thereby maximizing the use of available data.\n\nTo further validate our method, we employed an external validation set, referred to as dataset 2. This dataset, sourced from the University of Michigan and The Cancer Genome Atlas (TCGA), included 24 TFE3-RCC patients and 24 ccRCC patients, also matched by gender and tumor grade. The external validation set was used to evaluate the performance of the classification models trained on dataset 1. This approach ensured that the training and test sets were independent, as dataset 2 was not used in any capacity during the training phase of our models.\n\nThe distribution of our datasets is comparable to previously published machine learning datasets in the field of computational pathology. By matching gender and tumor grade, we aimed to control for potential confounding variables that could affect the performance of our models. This careful matching process is a common practice in medical research to ensure that the comparisons between different groups are as fair and unbiased as possible.\n\nTo enforce the independence of the training and test sets, we strictly adhered to the principle of not using any data from dataset 2 during the training phase. This ensured that our models were evaluated on completely unseen data, providing a more accurate assessment of their generalizability and performance in real-world scenarios. The use of an external validation set is a crucial step in validating the robustness of machine learning models, as it helps to identify any potential overfitting to the training data.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and widely recognized in the field. Specifically, the algorithms employed include logistic regression, support vector machines (SVM) with both linear and Gaussian kernels, and random forest. These algorithms are part of the supervised machine learning framework, which is commonly used for classification tasks.\n\nThe choice of these algorithms was driven by their robustness and effectiveness in handling high-dimensional data, which is characteristic of image feature extraction from histopathological slides. The algorithms were implemented using established packages in R, including glmnet for logistic regression, randomForest for random forest, and e1071 for SVM. These packages are well-documented and have been extensively used in various research and practical applications.\n\nThe decision to use these specific algorithms was also influenced by the need to avoid overfitting, given the high dimensionality of the image features and the relatively small sample size. Feature selection was performed using the minimum redundancy maximum relevance (mRMR) algorithm to reduce dimensionality and select informative, non-redundant features. This approach has been shown to be effective in various tasks and was applied to all image features with regard to the class label of the samples.\n\nThe performance of these algorithms was evaluated using five-fold cross-validation on the training dataset and further validated using an external validation set. The results demonstrated that all classifiers achieved high area under the curve (AUC) values, indicating their effectiveness in distinguishing between TFE3-RCC and ccRCC. The top features selected by mRMR included metrics related to nucleus size, shape, staining intensity, and density, which are crucial for morphological analysis in histopathological images.\n\nIn summary, the machine-learning algorithms used in this study are not new but are established methods chosen for their suitability in handling the specific challenges posed by the data. The focus of this study is on the application of these algorithms to a novel problem in histopathological image analysis, rather than the development of new machine-learning techniques. Therefore, publishing in a machine-learning journal was not the primary objective, as the innovation lies in the application and validation of these methods in the context of renal cell carcinoma classification.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process involved several steps to prepare the images for machine-learning algorithms. Initially, whole-slide images were subdivided into tiles of 2000 × 2000 pixels to facilitate processing. To address color variations between different institutions, a structure-preserving color normalization algorithm was applied to transform the color appearance of images in dataset 2 to match that of dataset 1.\n\nFor feature extraction, a bag-of-visual-words model was utilized. This model, originally used in natural language processing, represents images as histograms of visual words. In this context, nucleus-level features were clustered using the K-means algorithm to learn representative cluster centroids, which served as the visual words. The number of clusters was determined through cross-validation. Each nucleus-level feature from a whole-slide image was then assigned to the nearest bin using Euclidean distance, resulting in a histogram for each type of nucleus-level feature.\n\nTo aggregate nucleus-level features into patient-level features, histograms and distribution statistics were employed. For each type of nucleus-level feature, a 10-bin histogram was created, and five distribution statistics—mean, standard deviation, skewness, kurtosis, and entropy—were calculated. The entropy was computed based on the normalized histograms. This process ensured that the features were comparable across patients.\n\nBefore building classification models, feature selection was performed using the minimum redundancy maximum relevance (mRMR) algorithm to reduce dimensionality and avoid overfitting. This algorithm selected an informative and non-redundant set of features, which were then used to train machine-learning models. The models included logistic regression, support vector machines (SVM) with linear or Gaussian kernels, and random forest. The performance of these models was evaluated using five-fold cross-validation and an external validation set.",
  "optimization/parameters": "The model utilized 150 image-level features for classification. These features were derived from nucleus-level features extracted from whole-slide images. The nucleus-level features included measurements related to nucleus size, shape, staining intensity, and density. Each of these nucleus-level features was then transformed into image-level features using a 10-bin histogram and five distribution statistics (mean, standard deviation, skewness, kurtosis, and entropy). The number of bins in the histogram was determined using a cross-validation approach to ensure optimal clustering of nucleus-level features. This method allowed for a comprehensive and comparable feature set across different patients.",
  "optimization/features": "In our study, we initially calculated 150 image-level features from whole-slide images. These features were derived from nucleus-level characteristics such as size, shape, staining intensity, and density, which were further dissected into image-level features using histograms and distribution statistics.\n\nTo mitigate the risk of overfitting due to the high dimensionality of these features and the relatively small sample size, we performed feature selection. We employed the minimum redundancy maximum relevance (mRMR) algorithm to select a subset of informative and non-redundant features. This algorithm was applied to all image features with regard to the class label of the samples, ensuring that the selected features were relevant to distinguishing between TFE3-RCC and ccRCC.\n\nThe feature selection process was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the robustness of our models. Through this process, we identified 30 features that were most relevant for classification. These selected features were then used to train and validate our classification models, including logistic regression, support vector machines (SVM) with linear and Gaussian kernels, and random forest.",
  "optimization/fitting": "The study involved a high-dimensional feature set extracted from histopathological images, which indeed had a much larger number of parameters than the number of training points. To address the potential issue of overfitting, several strategies were employed.\n\nFirst, feature selection was performed using the minimum redundancy maximum relevance (mRMR) algorithm. This algorithm helped in selecting a subset of features that were both informative and non-redundant, thereby reducing the dimensionality of the feature space and mitigating the risk of overfitting.\n\nSecond, multiple machine-learning models were trained and evaluated using five-fold cross-validation. This technique ensured that each model was trained on different subsets of the data, providing a robust estimate of its performance and generalizability.\n\nAdditionally, the models were further validated using an external dataset, which was not used during the training process. This external validation step helped in assessing the models' performance on unseen data, providing further evidence of their generalizability and reducing the likelihood of overfitting.\n\nTo address underfitting, the study utilized a combination of different machine-learning algorithms, including logistic regression, support vector machines (SVM) with linear and Gaussian kernels, and random forest. The performance of these models was compared, and it was found that they did not differ significantly, indicating that the models were appropriately complex for the task at hand.\n\nFurthermore, the study employed a structure-preserving color normalization algorithm to handle color variations between institutions, ensuring that the features extracted were consistent and reliable. This step helped in maintaining the integrity of the data and preventing underfitting due to inconsistencies in the input images.\n\nIn summary, the study employed feature selection, cross-validation, external validation, and a combination of machine-learning algorithms to address both overfitting and underfitting, ensuring the robustness and generalizability of the models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting, given the high dimensionality of the image features and the relatively small sample size. One of the key steps was feature selection. We used the minimum redundancy maximum relevance (mRMR) algorithm to reduce feature dimensionality. This algorithm helps in selecting an informative and non-redundant set of features, which is crucial for avoiding overfitting. The mRMR algorithm was applied to all image features with regard to the class label of the sample, ensuring that the selected features were relevant to distinguishing between TFE3-RCC and ccRCC.\n\nAdditionally, we utilized cross-validation techniques. Specifically, we performed five-fold cross-validation in dataset 1. This method involves partitioning the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This approach helps in assessing the model's performance more robustly and reduces the risk of overfitting.\n\nFurthermore, we validated our method using an external validation set (dataset 2). The models were trained using dataset 1 and then evaluated on dataset 2. This external validation step is essential for ensuring that the model generalizes well to new, unseen data, thereby reducing the likelihood of overfitting.\n\nIn summary, we implemented feature selection using the mRMR algorithm, five-fold cross-validation, and external validation to prevent overfitting and ensure the robustness and generalizability of our classification models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available for public access. The quantitative image features extracted from H&E-stained whole-slide images can be found on GitHub at the repository https://github.com/chengjun583/tRCC-ccRCC-classification. This repository also contains the source code of the work, which includes details about the hyper-parameter configurations and the optimization schedule employed.\n\nThe data availability statement explicitly mentions that the remaining data is available in the article, supplementary information files, or can be obtained from the authors upon reasonable request. This ensures that all necessary information for reproducing the results and understanding the optimization process is accessible to the research community.\n\nThe source code and data are provided under a license that allows for their use, modification, and distribution, facilitating further research and development in the field. This transparency is crucial for validating our findings and encouraging collaborative efforts to advance the computational models for distinguishing TFE3-RCC from ccRCC.",
  "model/interpretability": "The model employed in this study is not a blackbox. It is based on well-defined cellular image features, which are interpretable and have clear meanings in cellular and tissue morphology. These features are extracted from nuclei in histopathological images and include measurements related to nucleus size, shape, staining intensity, and spatial distribution. For instance, features like nucleus area, major and minor axis lengths, and the ratio of these axes provide quantitative information about the size and shape of nuclei. Additionally, features such as mean pixel values in different color channels (R, G, B) offer insights into the staining intensity of nuclei. The spatial distribution of nuclei is captured through features like mean, maximal, and minimal distances to neighboring nuclei.\n\nThe use of these interpretable features allows for a better understanding of the model's decisions, making it more suitable for clinical diagnosis. Unlike deep learning models, which often learn complex and abstract features that are difficult to interpret, our classification pipeline relies on features that have direct biological significance. This transparency is crucial in a clinical setting, where understanding the basis of a model's predictions is essential for trust and adoption by healthcare professionals.",
  "model/output": "The model developed in this study is a classification model. It is designed to distinguish between two types of renal cell carcinoma: TFE3-RCC and ccRCC. The model utilizes quantitative histopathological features extracted from H&E-stained whole-slide images. Four different machine learning algorithms were employed: logistic regression, support vector machine (SVM) with linear kernel, SVM with Gaussian kernel, and random forest. These algorithms were used to classify patients into one of the two groups based on the extracted features.\n\nThe performance of these classifiers was evaluated using five-fold cross-validation on dataset 1, where 80% of the patients were used as the training set and the remaining 20% as the internal validation set. Additionally, the models were validated on an external dataset (dataset 2) to assess their generalization performance. The area under the receiver operating characteristic curve (AUC) was used as the primary metric to evaluate the models' performance. The SVM with Gaussian kernel achieved the highest average AUC of 0.886 in the cross-validation on dataset 1. When evaluated on the external validation set, all classifiers achieved AUCs similar to those obtained in the internal cross-validation, with the SVM with Gaussian kernel again performing the best.\n\nThe top quantitative features selected by the minimum redundancy maximum relevance (mRMR) algorithm included various nucleus-level features such as ratio_bin3, rMean_mean, minor_std, area_bin5, rMean_skewness, distMin_bin5, rMean_std, and ratio_std. These features were found to be important for distinguishing between TFE3-RCC and ccRCC.\n\nIn summary, the model is a classification model that effectively differentiates between TFE3-RCC and ccRCC using histopathological features extracted from whole-slide images. The use of multiple machine learning algorithms and rigorous validation on both internal and external datasets ensures the robustness and reliability of the model's performance.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code of this work is publicly available and can be downloaded from GitHub at the following URL: https://github.com/chengjun583/tRCC-ccRCC-classification. This repository contains the necessary code to replicate the experiments and methods described in the publication. The code is released under a permissive license, allowing for both academic and commercial use, subject to the terms specified in the repository. This ensures that other researchers and developers can build upon the work, facilitating further advancements in the field.",
  "evaluation/method": "The method was evaluated using a combination of cross-validation and external validation techniques. Initially, dataset 1, consisting of 100 patients (50 TFE3-RCC and 50 ccRCC), was randomly partitioned into training and internal validation sets in five rounds. Each round used 80% of the data for training and 20% for internal validation. This process was repeated five times to ensure robustness.\n\nFour different classifiers were employed: logistic regression, support vector machine (SVM) with linear kernel, SVM with Gaussian kernel, and random forest. The performance of these classifiers was assessed using the area under the receiver operating characteristic curve (AUC). The best-performing classifier, SVM with Gaussian kernel, achieved an average AUC of 0.886 across the five-fold cross-validation. Notably, the performance differences among the four classifiers were not statistically significant, as indicated by an ANOVA test with a P-value of 0.77.\n\nTo further validate the method, an external dataset (dataset 2) was used. This dataset included 48 patients from different institutions, ensuring variability in slide preparation and scanning instruments. The classifiers trained on dataset 1 were evaluated on dataset 2. All classifiers maintained similar AUC values to those observed in the internal cross-validation, demonstrating the method's generalizability. Additionally, three out of the four classifiers achieved slightly higher AUCs on the external validation set, likely due to the use of the entire dataset 1 for training.\n\nThe importance of color normalization was also highlighted. When the color normalization step was omitted, there was a significant drop in performance on the external validation set, underscoring the necessity of this preprocessing step when dealing with images from diverse sources.\n\nFurthermore, a convolutional neural network (ResNet-18) was tested on dataset 1 using two training strategies: training from scratch and transfer learning. The transfer learning approach, which involved updating only the last two layers of a pre-trained ResNet-18, outperformed training from scratch, achieving a mean AUC of 0.696 compared to 0.518. However, both ResNet-18 performances were inferior to the traditional machine learning classifiers, which had AUCs between 0.8 and 0.9. This suggests that the features learned by the deep neural network were less effective for this specific classification task compared to the well-defined, interpretable cellular image features used in the traditional classifiers.",
  "evaluation/measure": "In our study, we evaluated the performance of our classification models using several key metrics to ensure a comprehensive assessment. The primary metric reported is the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. This metric provides a single scalar value that summarizes the trade-off between the true positive rate and the false positive rate across all possible classification thresholds. We reported the average AUC from five-fold cross-validation for each classifier, which helps to understand the models' performance in distinguishing between TFE3-RCC and ccRCC.\n\nAdditionally, we provided the sensitivity and specificity for our best-performing classifier, the Support Vector Machine (SVM) with a Gaussian kernel. Sensitivity, also known as the true positive rate, measures the proportion of actual positives that are correctly identified by the model. Specificity, or the true negative rate, measures the proportion of actual negatives that are correctly identified. These metrics are crucial for understanding the model's ability to correctly identify both positive and negative cases.\n\nWe also reported the Youden index, which is a single statistic that captures the performance of a diagnostic test. It is defined as the sensitivity plus specificity minus one. This metric is particularly useful for comparing the performance of different diagnostic tests or models.\n\nTo further validate our results, we compared our performance metrics with those from other studies using immunohistochemistry (IHC) for TFE3 protein detection. This comparison highlights the effectiveness of our image-based classification approach, which relies solely on routine H&E staining, against more specialized molecular staining methods.\n\nIn summary, our performance measures include AUC, sensitivity, specificity, and the Youden index. These metrics are widely used in the literature and provide a robust evaluation of our classification models' performance. The inclusion of these metrics ensures that our results are comparable to other studies in the field and demonstrates the effectiveness of our approach in distinguishing between TFE3-RCC and ccRCC.",
  "evaluation/comparison": "In our study, we compared the performance of our classification models with a convolutional neural network, ResNet-18. The whole-slide images were resized to 224-by-224 pixels to fit the input requirements of ResNet-18. We implemented two training strategies: training the network from scratch and transfer learning. For transfer learning, we used a pretrained ResNet-18 network and updated only the weights of the last two layers, keeping the weights of earlier layers frozen. The mean AUC generated from five-fold cross-validation was 0.518 for training from scratch and 0.696 for transfer learning, indicating that transfer learning performed better, likely due to the reduced number of parameters that needed to be learned.\n\nThe performance of ResNet-18 was inferior compared to our classification models, which achieved AUCs between 0.8 and 0.9. This discrepancy can be attributed to the fact that the features learned by deep neural networks are often difficult to interpret. In contrast, our classification pipeline is based on well-defined cellular image features that have clear meanings in cellular and tissue morphology, making them more interpretable and preferable for clinical diagnosis.\n\nWe also compared the performance of our method with immunohistochemistry (IHC) for TFE3 protein, which is considered a surrogate for the TFE3 translocation. Previous studies using IHC reported varying sensitivity and specificity. Our SVM classifier with a Gaussian kernel achieved a sensitivity of 91.7%, specificity of 79.2%, and a Youden index of 0.708. This performance is notable because our method relies solely on routine H&E staining, unlike IHC, which requires staining for a specific molecule.\n\nAdditionally, we tested the generalization performance of our method on an external validation set that included slides from different institutions, which had varied color appearances. We observed a significant drop in performance when the color normalization step was omitted, highlighting the importance of this step in dealing with whole-slide images from different sources. This comparison underscores the robustness and reliability of our method in real-world clinical settings.",
  "evaluation/confidence": "The evaluation of our method includes confidence intervals for the performance metrics. Specifically, the Area Under the Curve (AUC) values for the classifiers were computed with confidence intervals using the R package pROC. For instance, in the external validation set, the 95% confidence intervals for the AUC of the logistic regression, random forest, SVM with linear kernel, and SVM with Gaussian kernel were reported as (0.763–0.984), (0.736–0.960), (0.725–0.959), and (0.797–0.991), respectively.\n\nStatistical significance was assessed using appropriate tests. For example, the performance of the four classifiers (logistic regression, SVM with linear kernel, SVM with Gaussian kernel, and random forest) did not differ significantly, as indicated by an ANOVA test with a P-value of 0.77. This suggests that while there are differences in performance, they are not statistically significant enough to claim that one method is superior to the others.\n\nAdditionally, the significance of differences in image features between TFE3-RCC and ccRCC was evaluated using a two-sided Mann–Whitney U test, with multiple comparison correction performed using the false discovery rate procedure at a 5% level. An adjusted P value of less than 0.05 was considered statistically significant, ensuring that the identified features are robust and not due to random chance.\n\nOverall, the evaluation provides a comprehensive assessment of the method's performance, including confidence intervals and statistical significance, to support the claims made about its effectiveness.",
  "evaluation/availability": "The quantitative image features extracted from H&E-stained whole-slide images used in our study are publicly available. They can be accessed from GitHub at the following URL: https://github.com/chengjun583/tRCC-ccRCC-classification. This repository contains the data necessary for reproducing our results and for further research.\n\nThe remaining data, including the whole-slide images and any additional information, is available in the article, supplementary information files, or can be obtained from the authors upon reasonable request. This ensures that other researchers have access to the necessary resources to validate and build upon our findings.\n\nFor those interested in the code used in our study, it is also available on GitHub at the same URL. This includes the source code for the feature extraction pipeline, the machine learning models, and the evaluation methods. The code is provided to facilitate reproducibility and to encourage further development in this area."
}