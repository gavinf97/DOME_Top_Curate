{
  "publication/title": "Development of a prediction model for hypotension after induction of anesthesia using machine learning",
  "publication/authors": "The authors who contributed to the article are:\n\n- Ah Reum Kang: Conceptualization, Data curation, Formal analysis, Investigation, Project administration, Supervision, Validation, Writing – original draft, Writing – review & editing\n- Jiyoung Woo: Conceptualization, Formal analysis, Investigation, Methodology, Project administration, Supervision, Writing – review & editing\n- Sang Hyun Kim: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Supervision, Validation, Writing – original draft, Writing – review & editing\n- Jihyun Lee: Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft\n- Misoon Lee: Data curation, Formal analysis, Investigation, Writing – review & editing\n- Woohyun Jung: Data curation, Software, Visualization\n- Sun Young Park: Investigation",
  "publication/journal": "PLoS ONE",
  "publication/year": "2020",
  "publication/doi": "https://doi.org/10.1371/journal.pone.0231172",
  "publication/tags": "- Machine Learning\n- Hypotension\n- Anesthesia\n- Prediction Model\n- Random Forest\n- Naive Bayes\n- Logistic Regression\n- Artificial Neural Network\n- Post-Induction Hypotension\n- Physiological Data\n- Medical Data Analysis\n- Feature Selection\n- Cross-Validation\n- Performance Evaluation\n- Bio-Signal Data",
  "dataset/provenance": "The dataset used in this study was derived from two primary sources: Electronic Health Records (EHR) and Vital Recorder data. The EHR provided demographic information such as age, sex, height, weight, body mass index, ASA classification, and various comorbidities including cardiovascular, respiratory, gastrointestinal, renal, endocrine, and neurologic diseases. The Vital Recorder data included detailed information on anesthetic drugs, vasoactive drug administration, noninvasive blood pressure measurements, heart rate, mechanical ventilation data, and hypotension-related metrics.\n\nA total of 89 features were extracted from these sources. These features were observed from the induction of anesthesia to tracheal intubation, ensuring a comprehensive capture of relevant physiological and clinical data. The dataset is diverse, reflecting the variability in patient conditions and responses to anesthesia.\n\nThe data used in this study is not the same as the one used in previous papers or by the community. The dataset was specifically curated for this research to predict late post-induction hypotension (PIH) using machine learning techniques. The features selected were chosen to avoid issues associated with dimensionality, and the dataset was processed using the caret R package for feature selection. This approach ensures that the model is trained on relevant and informative features, enhancing its predictive performance.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "The dataset used in this study was split into training and test sets to ensure independent evaluation of the models. Specifically, 75% of the total data, comprising 166 cases, was allocated for training, while the remaining 25% was reserved for testing. This split was designed to prevent overfitting and to improve the generalizability of the models.\n\nTo enforce the independence of the training and test sets, repeated k-fold cross-validation was employed. This method involves dividing the data into k folds, where each fold is used as a test set while the model is trained on the remaining k-1 folds. This process is repeated multiple times, with different random splits, to ensure that each fold is used as a test set exactly once. In our case, four-fold cross-validation was repeated 1000 times to generate stable and unbiased performance metrics.\n\nThe distribution of the dataset is notable for its diversity, particularly in biomedical data, which can vary significantly among patients. This heterogeneity necessitated the use of repeated k-fold cross-validation to mitigate the risk of biased performance due to specific fold splits. The approach ensures that the models are robust and can generalize well to new, unseen data.\n\nCompared to previously published machine learning datasets, our dataset is characterized by its high dimensionality and the need for careful feature selection to avoid overfitting. Initially, 89 features were extracted, but through various feature selection strategies, redundant and less important features were removed. This resulted in more compact and effective feature sets, such as Feature set C, which contained 20 features selected using the recursive feature elimination (RFE) method. These steps are crucial for handling the complexity and variability inherent in biomedical data.",
  "dataset/availability": "The data used in this study is publicly available. The analysis was conducted using the R statistical package, with specific packages employed for each machine learning method, including e1071, randomForest, nnet, mlbench, and caret. Additionally, ROC curves were generated using the pROC package. All these packages are publicly accessible, ensuring that there are no special access privileges required. The relevant data is provided in the supporting information files, making it accessible for replication studies. The study adheres to the Creative Commons Attribution License, which allows unrestricted use, distribution, and reproduction of the data, provided that the original authors and source are credited.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are well-established and belong to the supervised learning class. The algorithms employed include Naïve Bayes, logistic regression, random forest, and artificial neural networks (ANN). These algorithms are widely recognized and have been extensively used in various domains, including medical research.\n\nThe algorithms are not new; they have been developed and refined over many years. Naïve Bayes is a probabilistic classifier that applies the Bayesian theorem, assuming independence between properties. Logistic regression is a probabilistic model that uses the relationship between dependent and independent variables for predictions. Random forest creates multiple decision trees from randomly sampled training data and derives the final result by majority vote. Artificial neural networks mimic the brain's information processing system, involving complex neuron connections and computations.\n\nThe reason these algorithms were not published in a machine-learning journal is that the focus of this study is on their application in a specific medical context—predicting hypotension after induction of anesthesia. The primary contribution of this work is the application and evaluation of these algorithms in this particular domain, rather than the development of new machine-learning techniques. The study aims to demonstrate the effectiveness of these established algorithms in improving clinical outcomes, which is more aligned with medical and biomedical research journals.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it directly utilizes various machine-learning algorithms to predict hypotension after the induction of anesthesia. The algorithms employed include Naïve Bayes, logistic regression, random forest, and artificial neural networks (ANN). Each of these models was trained and evaluated independently using the same dataset, which included early anesthesia induction data collected from various medical devices.\n\nThe random forest model demonstrated the highest predictive performance among the four methods, achieving an area under the curve (AUC) of 0.84. This model's success can be attributed to its ability to handle large datasets and many input variables, as well as its robustness to overfitting. The other models, while useful, had varying levels of performance depending on the feature sets used.\n\nThe dataset used for training these models was comprehensive, encompassing not only electronic health records (EHR) but also data from general anesthesia monitors, drug infusion pumps, mechanical ventilators, and anesthesia depth monitors. This rich dataset ensured that the models had access to a wide range of relevant information, enhancing their predictive capabilities.\n\nThe training data for each model was carefully managed to ensure independence. Repeated k-fold cross-validation was performed to guarantee unbiased performance, with the process repeated 1000 times to account for the heterogeneity of the biomedical data. This approach helped to mitigate any potential biases that could arise from the diverse nature of the patient data.\n\nIn summary, the models developed in this study are standalone machine-learning algorithms that do not rely on the outputs of other machine-learning models as input. The training data for each model was independently managed to ensure robust and unbiased performance.",
  "optimization/encoding": "In our study, we initially extracted 89 features from electronic health records and vital recorder data to predict late post-induction hypotension (PIH) using machine learning. To mitigate issues related to high dimensionality, we employed feature selection strategies using the caret R package. This process involved three main steps:\n\nFirst, we removed redundant features by eliminating attributes with an absolute correlation coefficient of 0.5 or greater, resulting in 42 features defined as Feature set A.\n\nSecond, we ranked features by their importance. For decision tree-based models, we utilized built-in mechanisms to report variable importance. For other algorithms, we estimated importance using receiver operating characteristic (ROC) curve analysis for each feature, selecting 20 features defined as Feature set B.\n\nLastly, we applied the recursive feature elimination (RFE) method, a wrapper technique that iteratively builds models and ranks features based on their contribution to model performance. This process yielded 23 features defined as Feature set C.\n\nThe data included various categories such as demographic information, anesthetic drug details, mechanical ventilation data, and hypotension-related features. Measurements were taken every 3 to 5 minutes. To ensure robust performance evaluation, we performed repeated k-fold cross-validation, specifically four-fold cross-validation repeated 1000 times, to account for the heterogeneity in biomedical data.\n\nBefore feeding the data into the machine-learning models, we pre-processed it to handle missing values and normalize the features. This step was crucial for models like logistic regression and artificial neural networks (ANNs) that require scaled data. For random forest, minimal pre-processing was needed due to its robustness to different data scales.\n\nWe developed four machine-learning models: Naïve Bayes, logistic regression, random forest, and artificial neural networks (ANNs). Each model had specific requirements for data encoding. For instance, Naïve Bayes assumes independence between features, while logistic regression assumes a linear relationship between dependent and independent variables. Random forest, on the other hand, can handle large datasets and many input variables without extensive pre-processing. ANNs were used to approximate complex estimation functions, requiring normalized input data.\n\nIn summary, our data encoding and pre-processing involved feature selection to reduce dimensionality, handling missing values, and normalizing features as needed for different machine-learning models. This approach ensured that our models were trained on relevant and well-prepared data, enhancing their predictive performance for late PIH.",
  "optimization/parameters": "In our study, we initially considered 89 features extracted from electronic health records and vital recorder data to predict late post-induction hypotension (PIH). However, to mitigate issues related to high dimensionality and overfitting, we employed feature selection strategies.\n\nFirst, we removed redundant features by eliminating attributes with an absolute correlation coefficient of 0.5 or greater, resulting in 42 features, defined as Feature set A.\n\nSecond, we ranked features by their importance using methods such as decision trees and receiver operating characteristic (ROC) curve analysis, selecting 20 features for Feature set B.\n\nLastly, we used the recursive feature elimination (RFE) method, a popular automatic feature selection technique, to identify the most relevant features. This process involved iteratively building models and removing the least important features until the optimal set was found. Through this method, we selected 23 features for Feature set C.\n\nThe final number of parameters (p) used in our models varied depending on the feature set employed. We evaluated the performance of our machine learning models using Feature sets A, B, and C, as well as the full set of 89 features. The random forest model, in particular, showed the highest predictive performance when using Feature set C, which consisted of 23 features. This approach ensured that we selected only the most useful features, enhancing model performance and generalizability.",
  "optimization/features": "In our study, we initially extracted 89 features from electronic health records and vital recorder data to predict late post-induction hypotension (PIH) using machine learning. These features encompassed a wide range of variables, including demographic data, anesthetic drug information, mechanical ventilation data, and various physiological measurements.\n\nTo mitigate issues related to high dimensionality and prevent overfitting, we performed feature selection using the caret R package. This process was conducted exclusively on the training set to ensure that the selected features were not influenced by the test data, maintaining the integrity of our model evaluation.\n\nThree distinct feature selection strategies were employed:\n\n1. **Redundant Feature Removal**: We eliminated features that were highly correlated with each other, specifically those with an absolute correlation coefficient of 0.5 or greater. This resulted in Feature set A, which consisted of 42 features.\n\n2. **Feature Importance Ranking**: We ranked features based on their importance, as estimated from the data after model creation. This method involved using decision trees and receiver operating characteristic (ROC) curve analysis. Feature set B, comprising 20 features, was derived from this approach.\n\n3. **Recursive Feature Elimination (RFE)**: This method, provided by the caret R package, is a greedy optimization algorithm that iteratively builds models and removes the least significant features. Feature set C, containing 23 features, was selected using this technique.\n\nThe final models were developed using these refined feature sets, and their performance was evaluated to ensure robustness and generalizability.",
  "optimization/fitting": "The study initially considered 89 features, which could have led to a high risk of dimensionality complications, as the number of parameters was much larger than the number of training points. To mitigate this risk and prevent overfitting, three feature selection strategies were employed.\n\nFirst, redundant features were removed by eliminating attributes with an absolute correlation coefficient of 0.5 or greater, resulting in Feature set A with 42 features. This step helped to reduce the complexity of the model and improve its performance.\n\nSecond, features were ranked by their importance using methods such as decision trees or receiver operating characteristic (ROC) curve analysis, leading to Feature set B with 20 features. This approach ensured that only the most relevant features were retained, further reducing the risk of overfitting.\n\nLastly, the recursive feature elimination (RFE) method was used to select specific features, resulting in Feature set C with 23 features. RFE is a greedy optimization algorithm that iteratively builds models and removes the least important features, ensuring that the final model is both accurate and generalizable.\n\nTo further ensure that the models were not overfitting, repeated k-fold cross-validation was performed. Specifically, four-fold cross-validation was repeated 1000 times to generate stable performance metrics. This technique helps to validate the model's performance on new data and ensures that the results are not biased by the specific split of the data.\n\nUnderfitting was addressed by using a variety of machine learning models, including Naïve Bayes, logistic regression, random forest, and artificial neural networks (ANN). Each model has its strengths and can capture different aspects of the data, reducing the likelihood of underfitting. Additionally, the use of feature selection strategies ensured that the most relevant features were included in the models, further enhancing their ability to generalize to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the performance of our machine learning models. One of the primary methods we used was feature selection. We started with 89 extracted features but recognized that using all of them could lead to dimensionality complications and overfitting. To mitigate this, we applied three feature selection strategies.\n\nFirst, we removed redundant features by eliminating attributes with an absolute correlation coefficient of 0.5 or greater. This step helped in reducing the number of features to 42, which we defined as Feature set A.\n\nSecond, we ranked features by their importance. This was done by estimating the importance of each feature after the model was created. For some algorithms, like decision trees, this importance is built-in, while for others, we used receiver operating characteristic (ROC) curve analysis. This process resulted in Feature set B, consisting of 20 features.\n\nLastly, we used the recursive feature elimination (RFE) method, a popular automatic feature selection technique provided by the caret R package. RFE is a greedy optimization algorithm that iteratively builds models and removes the least significant features until the optimal set is found. This method helped us select 23 features, defined as Feature set C.\n\nAdditionally, we performed repeated k-fold cross-validation to ensure unbiased performance. We used four-fold cross-validation repeated 1000 times to generate stable performance metrics, which is particularly important given the diverse nature of our biomedical data.\n\nBy implementing these feature selection strategies and cross-validation techniques, we were able to enhance the predictive performance of our models while minimizing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the publication. However, the study utilized several machine learning methods, including Naïve Bayes, logistic regression, random forest, and artificial neural networks (ANN), to predict hypotension after anesthesia induction. The specific packages used for these methods, such as e1071, randomForest, nnet, mlbench, and caret, are publicly available. The data and relevant information necessary for replication are provided in the supporting information files, which include a dataset in CSV format. The study also employed the pROC package for generating ROC curves, which is also publicly available. The analysis was performed using the R statistical package, version 3.4.4, and all relevant data and code are accessible under the Creative Commons Attribution License, permitting unrestricted use, distribution, and reproduction, provided the original author and source are credited.",
  "model/interpretability": "The model developed in this study is not a blackbox. It employs a random forest algorithm, which is known for its interpretability. Random forest models provide insights into the importance of various features in making predictions. This is achieved through two main importance indicators: mean decrease in accuracy and mean decrease in Gini.\n\nThe mean decrease in accuracy measures the impact of a variable on the model's predictive accuracy. If removing a variable significantly reduces the model's accuracy, it indicates that the variable is crucial for making accurate predictions. For instance, variables like NIBP SBP.min and NIBP MBP.min were found to be highly important, as their removal led to a noticeable drop in the model's performance.\n\nThe mean decrease in Gini, on the other hand, assesses the reduction in impurity or disorder when a variable is used to split the data. A higher mean decrease in Gini for a variable suggests that it effectively groups similar data points together, reducing the impurity within the groups. Variables such as age and NIBP SBP.mean were identified as important using this metric.\n\nThese importance indicators allow for a transparent understanding of which features are most influential in predicting late post-induction hypotension (PIH). This transparency is crucial for medical applications, where understanding the underlying factors contributing to a prediction can inform clinical decisions and interventions.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict the occurrence of hypotension after the induction of anesthesia. The model's performance is evaluated using metrics typical for classification tasks, such as precision, recall, accuracy, and the area under the ROC curve (AUC). These metrics are used to assess how well the model can distinguish between instances where hypotension occurs and where it does not.\n\nThe model employs various machine learning algorithms, including Naïve Bayes, logistic regression, random forest, and artificial neural networks (ANN). Each of these algorithms is trained to classify the input data into one of two categories: hypotension or no hypotension. The random forest model, in particular, showed the highest predictive performance among the four algorithms tested.\n\nThe output of the model provides probabilities or classifications indicating the likelihood of hypotension occurring. These outputs are crucial for clinicians, as they help in making informed decisions during the induction of anesthesia. The model's predictions are based on a variety of features, including blood pressure-related factors, which were found to significantly improve prediction accuracy. The use of repeated k-fold cross-validation ensures that the model's performance is stable and unbiased, making it reliable for clinical use.\n\nThe precision and recall for the hypotension class are noted in a table, providing a clear indication of the model's ability to correctly identify true positive and true negative cases. This information is essential for understanding the model's strengths and limitations in a clinical setting. Overall, the model's classification output is designed to be practical and actionable, aiding in the prevention and management of hypotension during anesthesia.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models developed in this study is not publicly released. However, the statistical software and packages used for the analysis are publicly available. The analysis was performed using the R statistical package, version 3.4.4. Specific packages utilized include e1071, randomForest, nnet, mlbench, caret, and pROC. These packages are all freely accessible and can be obtained from the R Foundation for Statistical Computing. The data used in this study is available in the supporting information files, allowing for replication studies.",
  "evaluation/method": "The evaluation method employed for our prediction models involved repeated k-fold cross-validation to ensure unbiased performance. This statistical technique measures the model's performance on new data by splitting the dataset into k folds. Each fold is then tested as new data for the model built from the remaining k-1 folds, and this process is repeated until all folds have been tested once. This method introduces randomness in sample selection, which can affect performance, especially with heterogeneous data like ours. To mitigate this, we repeated the four-fold cross-validation 1000 times. This approach helped generate stable and reliable performance metrics for our models.\n\nThe performance of the learning models was summarized using several key metrics: the area under the ROC curve (AUC), accuracy, precision, and recall. These metrics were calculated based on whether the model's predictions matched the actual outcomes. True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) were the indices used to derive these metrics. Precision was calculated as TP divided by the sum of TP and FP. Recall was determined by TP divided by the sum of TP and FN. Accuracy was the sum of TP and TN divided by the total number of cases (TP + TN + FP + FN). These metrics provided a comprehensive evaluation of our models' predictive capabilities.",
  "evaluation/measure": "In our study, we evaluated the performance of our machine learning models using several key metrics to ensure a comprehensive assessment. The primary metrics reported include the area under the receiver operating characteristic curve (AUC), accuracy, precision, and recall. These metrics are widely recognized in the literature and provide a robust evaluation of model performance.\n\nThe AUC measures the model's ability to distinguish between positive and negative classes, offering a single scalar value that summarizes the performance across all classification thresholds. Accuracy represents the proportion of true results (both true positives and true negatives) among the total number of cases examined. Precision, also known as the positive predictive value, indicates the proportion of true positive results among all positive results predicted by the model. Recall, or sensitivity, measures the proportion of actual positives that are correctly identified by the model.\n\nThese metrics collectively provide a thorough evaluation of our models' predictive capabilities. The AUC is particularly useful for imbalanced datasets, which is common in medical research. Accuracy gives an overall sense of the model's performance, while precision and recall offer insights into the model's effectiveness in specific contexts, such as identifying true positives and minimizing false negatives.\n\nBy reporting these metrics, we aim to provide a clear and representative assessment of our models' performance, aligning with established practices in the field. This approach ensures that our findings are comparable to other studies and that the strengths and limitations of our models are transparently communicated.",
  "evaluation/comparison": "In our study, we developed and compared multiple machine learning models to predict late post-induction hypotension (PIH). The models included Naïve Bayes, logistic regression, random forest, and artificial neural networks (ANN). Each of these models has distinct characteristics and assumptions.\n\nNaïve Bayes is a probabilistic classifier that assumes independence between features, which can be a limitation when features are interdependent. Logistic regression, while effective for binary outcomes, assumes a linear relationship between dependent and independent variables, which may not always capture the complexity of the data.\n\nRandom forest, on the other hand, is an ensemble method that creates multiple decision trees and aggregates their results through majority voting. This approach generally performs well with minimal parameter tuning and can handle large datasets with many input variables. It showed the highest predictive performance among the models we tested, particularly when using feature sets that included blood pressure-related factors.\n\nANN models, which mimic the brain’s information processing system, can approximate any estimation function with high prediction accuracy. However, they tend to overfit training data, which can be a drawback.\n\nWe also compared the performance of these models using different feature sets. Initially, using all 89 features resulted in poor prediction performance for logistic regression and Naïve Bayes. Feature set A, which included many existing-disease features and excluded blood pressure-related features, showed varied predictive ability depending on the model. Feature sets B and C, which focused on blood pressure-related features, improved predictive performance across most models, with random forest showing the best results.\n\nOur evaluation metrics included the area under the ROC curve (AUC), accuracy, precision, and recall. These metrics provided a comprehensive assessment of model performance, ensuring that we could identify the most effective approach for predicting late PIH.\n\nIn summary, while each model has its strengths and weaknesses, random forest emerged as the most robust and accurate method for predicting late PIH in our study. The inclusion of blood pressure-related features significantly enhanced the predictive performance, highlighting the importance of feature selection in model development.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe performance of the machine learning models was evaluated using several metrics, including the area under the ROC curve (AUC), accuracy, precision, and recall. These metrics provide a comprehensive view of the models' predictive capabilities.\n\nTo ensure the robustness of the results, repeated k-fold cross-validation was employed. Specifically, four-fold cross-validation was repeated 1000 times. This approach helps to mitigate the randomness in sample selection, which is particularly important given the heterogeneity of biomedical data. By repeating the validation process multiple times, the stability and reliability of the performance metrics were enhanced.\n\nStatistical tests were conducted to determine the significance of the results. For continuous data, t-tests or Wilcoxon tests were performed based on the normality of the data, as assessed by the Shapiro–Wilk test. Categorical variables were evaluated using chi-squared or Fisher’s exact tests. These statistical methods ensure that the observed differences in performance are not due to chance.\n\nThe random forest model, which showed the highest predictive performance, was further analyzed using feature importance indicators. The mean decrease in accuracy and the mean decrease in Gini were used to identify the most influential features. These indicators provide a quantitative measure of the importance of each feature in the model's predictions.\n\nThe results indicate that the random forest model outperformed other models, particularly when using Feature set C. The performance metrics, such as recall and AUC, were significantly higher for the random forest model compared to others. This suggests that the method is not only effective but also statistically superior to the baselines and other models tested.\n\nIn summary, the evaluation confidence is high due to the use of robust statistical methods, repeated cross-validation, and comprehensive performance metrics. The results are statistically significant, supporting the claim that the random forest model with Feature set C is superior for predicting late PIH.",
  "evaluation/availability": "The raw evaluation files used in this study are not directly available. However, all relevant data necessary for replication studies are provided in the supporting information files. These files are accessible to the public and can be used for further research or validation purposes.\n\nThe analysis was conducted using the R statistical package, with specific packages such as e1071, randomForest, nnet, mlbench, caret, and pROC. These packages are publicly available and do not require special access privileges. The data and methods used are designed to be reproducible, ensuring that other researchers can verify and build upon the findings presented in this study."
}