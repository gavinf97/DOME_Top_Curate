{
  "publication/title": "SPOT-Disorder2: Improved Protein Intrinsic Disorder Prediction by Ensembled Deep Learning",
  "publication/authors": "The authors who contributed to the article are:\n\n- Hanson J\n- YZ\n- VN Uversky\n- CJ Oldfield\n- AK Dunker\n- PE Wright\n- HJ Dyson\n- G Hu\n- K Wang\n- J Song\n- L Kurgan\n- Z Peng\n- J Yan\n- X Fan\n- MJ Mizianty\n- B Xue\n- JF Yu\n- Z Cao\n- Y Yang\n- CL Wang\n- ZD Su\n- YW Zhao\n- V Receveur-Brechot\n- JM Bourhis\n- B Canard\n- S Longhi\n- R Konrat\n- P Romero\n- Z Obradovic\n- X Li\n- EC Garner\n- CJ Brown\n- Z Doszta´nyi\n- V Csizmok\n- P Tompa\n- I Simon\n- C Szegedy\n- S Ioffe\n- V Vanhoucke\n- AA Alemi\n- J Hu\n- L Shen\n- G Sun\n- DA Clevert\n- T Unterthiner\n- S Hochreiter\n- N Srivastava\n- G Hinton\n- A Krizhevsky\n- I Sutskever\n- R Salakhutdinov\n- GE Dahl\n- TN Sainath\n- GE Hinton\n- LK Hansen\n- P Salamon\n- DP Kingma\n- J Ba\n- M Abadi\n- A Agarwal\n- P Barham\n- E Brevdo\n- Z Chen\n- C Citro\n- SF Altschul\n- TL Madden\n- AA Schaffer\n- J Zhang\n- Z Zhang\n- W Miller\n- M Remmert\n- A Biegert\n- A Hauser\n- J So¨ding\n- J Necci\n- JRONN\n- PONDR-VSL\n- JF Yu\n- Z Cao\n- Y Yang\n- CL Wang\n- ZD Su\n- YW Zhao\n- V Receveur-Brechot\n- JM Bourhis\n- B Canard\n- S Longhi\n- R Konrat\n- P Romero\n- Z Obradovic\n- X Li\n- EC Garner\n- CJ Brown\n- Z Doszta´nyi\n- V Csizmok\n- P Tompa\n- I Simon\n- C Szegedy\n- S Ioffe\n- V Vanhoucke\n- AA Alemi\n- J Hu\n- L Shen\n- G Sun\n- DA Clevert\n- T Unterthiner\n- S Hochreiter\n- N Srivastava\n- G Hinton\n- A Krizhevsky\n- I Sutskever\n- R Salakhutdinov\n- GE Dahl\n- TN Sainath\n- GE Hinton\n- LK Hansen\n- P Salamon\n- DP Kingma\n- J Ba\n- M Abadi\n- A Agarwal\n- P Barham\n- E Brevdo\n- Z Chen\n- C Citro\n- SF Altschul\n- TL Madden\n- AA Schaffer\n- J Zhang\n- Z Zhang\n- W Miller\n- M Remmert\n- A Biegert\n- A Hauser\n- J So¨ding\n- J Necci\n- JRONN\n- PONDR-VSL\n\nNot all of these authors contributed equally to the paper. Some of them are cited for their previous work, which is referenced in the article. The main contributors to the article are Hanson J, YZ, and VN Uversky.",
  "publication/journal": "Genomics Proteomics Bioinformatics",
  "publication/year": "2019",
  "publication/doi": "10.1016/j.gpb.2019.01.004",
  "publication/tags": "- Intrinsic disorder\n- Molecular recognition feature\n- Machine learning\n- Deep learning\n- Protein structure\n- Protein intrinsic disorder prediction\n- Ensembled deep learning\n- Protein disorder prediction\n- Computational biology\n- Bioinformatics",
  "dataset/provenance": "The datasets used in these experiments were obtained from previous disorder prediction publications. We gathered 4229 non-redundant, high-resolution protein sequences from the Protein Data Bank (PDB) and the Database of Protein Disorder (DisProt). These sequences include 4157 X-ray crystallography structures deposited in the PDB prior to August 05, 2003, and 72 fully-disordered proteins from DisProt v5.0. These chains were randomly split into a training set of 2700 chains, a validation set of 300 chains, and a testing set of 1229 chains. Sequence similarity among these proteins is less than 25% according to BLASTClust. To ensure compatibility with SPOT-1D, which is not trained for proteins longer than 700 amino acid residues, we removed all proteins exceeding this length from all datasets. This adjustment resulted in training, validation, and test sets containing 2615, 293, and 1185 proteins, respectively. For convenience, we labeled this test set as Test1185.\n\nAdditionally, we obtained three independent test datasets (SL250, Mobi9414, and DisProt228) for a fair comparison against other methods. These datasets were subsets from established sets (SL477, MobiDB, and DisProt Complement, respectively), after removing long proteins (>700 residues) and homologous proteins in our training dataset (25% sequence identity cutoff with BLASTClust). The proteins in DisProt228 are newly-annotated proteins deposited in the DisProt database v7.0. The proteins in SL477 with unknown residue types were also removed. The annotations in Mobi9414 include direct labels from the DisProt database, inferred labels from the PDB, and predicted labels from a large ensemble of disorder predictors. Predicted labels in MobiDB were not utilized due to their potential inaccuracy. Residues listed as ‘conflicting’ labels in MobiDB were omitted for performance analysis. Because some predictors employed MobiDB as part of their training set, we also created a reduced subset of Mobi9414, called Mobi4730, for independent testing of all methods compared. Mobi4730 was obtained by clustering Mobi9414 against the largest disorder training dataset for NetSurfP-2.0 at a sequence similarity of 25% by BLASTClust.",
  "dataset/splits": "The dataset used in these experiments was divided into four main splits: a training set, a validation set, a testing set, and three independent test datasets.\n\nThe primary splits consist of:\n\n* Training set: 2615 proteins\n* Validation set: 293 proteins\n* Testing set: 1185 proteins\n\nThe testing set is also referred to as Test1185.\n\nAdditionally, three independent test datasets were obtained for fair comparison against other methods. These datasets are:\n\n* SL250: 250 proteins\n* Mobi9414: 9414 proteins\n* DisProt228: 228 proteins\n\nThe proteins in these datasets were selected to ensure sequence similarity among them is less than 25%. Proteins longer than 700 amino acid residues were removed from all datasets. The Mobi9414 dataset was further reduced to create Mobi4730, which excludes proteins with sequence similarity greater than 25% to the largest disorder training dataset for NetSurfP-2.0. This reduction was done to ensure independent testing for all compared methods.",
  "dataset/redundancy": "The datasets used in these experiments were obtained from previous disorder prediction publications. A total of 4229 non-redundant, high-resolution protein sequences were gathered from the Protein Data Bank (PDB) and the Database of Protein Disorder (DisProt). These sequences included 4157 X-ray crystallography structures and 72 fully-disordered proteins. The sequences were randomly split into three sets: a training set of 2700 chains, a validation set of 300 chains, and a testing set of 1229 chains. Sequence similarity among these proteins was ensured to be less than 25% using BLASTClust.\n\nTo ensure the independence of the training and test sets, proteins longer than 700 amino acid residues were removed from all datasets. This adjustment reduced the training, validation, and test sets to 2615, 293, and 1185 proteins, respectively. The test set, labeled as Test1185, was used for evaluating the performance of the disorder predictor.\n\nAdditionally, three independent test datasets (SL250, Mobi9414, and DisProt228) were obtained for fair comparison against other methods. These datasets were subsets of established sets (SL477, MobiDB, and DisProt Complement) after removing long proteins and homologous proteins present in the training dataset. The proteins in DisProt228 were newly-annotated proteins from the DisProt database v7.0. The proteins in SL477 with unknown residue types were also removed. The annotations in Mobi9414 included direct labels from the DisProt database, inferred labels from the PDB, and predicted labels from a large ensemble of disorder predictors. Predicted labels in MobiDB were not utilized due to potential inaccuracy. Residues listed as ‘conflicting’ labels in MobiDB were omitted for performance analysis.\n\nTo further ensure independence, a reduced subset of Mobi9414, called Mobi4730, was created for independent testing. This subset was obtained by clustering Mobi9414 against the largest disorder training dataset for NetSurfP-2.0 at a sequence similarity of 25% using BLASTClust. This approach ensured that the training and test sets were independent and that the performance evaluation was fair and unbiased.",
  "dataset/availability": "The datasets used in these experiments were obtained from previous disorder prediction publications. These include 4229 non-redundant, high-resolution protein sequences from the Protein Data Bank (PDB) and Database of Protein Disorder (DisProt). The sequences were split into a training set of 2615 chains, a validation set of 293 chains, and a testing set of 1185 chains, with sequence similarity among these proteins being less than 25%. Proteins longer than 700 amino acid residues were excluded from all datasets.\n\nAdditionally, three independent test datasets (SL250, Mobi9414, and DisProt228) were obtained for fair comparison against other methods. These datasets were subsets from established sets, with long proteins and homologous proteins in the training dataset removed. The DisProt228 dataset consists of newly-annotated proteins deposited in the DisProt database v7.0. The Mobi9414 dataset contains direct labels from the DisProt database and inferred labels from the PDB, with predicted labels and conflicting labels omitted for performance analysis. A reduced subset of Mobi9414, called Mobi4730, was created for independent testing by clustering against the largest disorder training dataset for NetSurfP-2.0 at a sequence similarity of 25%.\n\nThe datasets are not explicitly stated to be released in a public forum. However, the sources from which the datasets were obtained, such as the Protein Data Bank (PDB) and Database of Protein Disorder (DisProt), are publicly accessible. The specific splits and preprocessing steps applied to these datasets, such as removing proteins longer than 700 amino acid residues and ensuring sequence similarity is less than 25%, are detailed in the publication. This information allows for reproducibility of the datasets used in the experiments.",
  "optimization/algorithm": "The optimization algorithm employed in our work is the Adam optimizer, a method for stochastic optimization. This algorithm is not new and has been widely used in the machine learning community. It was introduced in a paper published in 2014 and has since become a standard choice for training deep learning models due to its efficiency and effectiveness.\n\nThe Adam optimizer was not published in a machine-learning journal but rather in the arXiv repository, which is a popular preprint server for research papers in various fields, including machine learning. The decision to publish in arXiv is common in the machine learning community, as it allows for rapid dissemination of research findings and facilitates feedback from the community before formal publication in a journal.\n\nThe Adam optimizer is a member of the class of adaptive learning rate methods. It combines the advantages of two other extensions of stochastic gradient descent. Specifically, Adam computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.\n\nThe implementation of the Adam optimizer used in our work is from TensorFlow, a popular open-source machine learning framework. TensorFlow provides a robust and efficient implementation of the Adam optimizer, making it a suitable choice for our deep learning models. The use of TensorFlow ensures that our models can be trained efficiently on modern hardware, including GPUs, which are essential for handling the computational demands of deep learning.",
  "optimization/meta": "The model SPOT-Disorder2 is indeed a meta-predictor, leveraging an ensemble of different neural network architectures to enhance prediction accuracy. The ensemble includes IncReSeNet, LSTM, and fully connected (FC) network topologies. This approach differs from the previous version, SPOT-Disorder, which relied solely on a single LSTM topology.\n\nThe use of an ensemble is designed to improve robustness and performance across various datasets. For instance, the MCC values show significant improvements across multiple test sets, such as Test1185, SL250, Mobi9414, and DisProt228, indicating the effectiveness of combining multiple models.\n\nRegarding the independence of training data, the datasets used for training, validation, and testing were carefully curated to ensure minimal overlap. Proteins with sequence similarity greater than 25% were removed to maintain independence. Additionally, specific subsets like Mobi4730 were created by clustering against the largest disorder training dataset for NetSurfP-2.0 at a 25% sequence similarity cutoff, further ensuring that the training data for different methods remained independent.\n\nThe features used in SPOT-Disorder2 are derived from various sources, including PSI-BLAST (PSSM), HHblits, and SPOT-1D. Each feature group contributes differently to the model's performance. For example, PSSM features are crucial for maximizing AUC metrics, while SPOT-1D features are essential for enhancing single-threshold metrics like MCC and Sw. HHblits, however, does not significantly contribute to the model's performance, likely because its profile is already incorporated through SPOT-1D.\n\nThe combination of LSTM and IncReSeNet layers adds significant performance gains to the ensemble, particularly in terms of AUCPR and MCC. This indicates that the ensemble approach not only improves overall accuracy but also enhances the model's ability to handle class imbalances and provide more reliable predictions.",
  "optimization/encoding": "The data encoding for the machine-learning algorithm involved several steps to ensure that the input features were standardized and suitable for training. The input features for each protein residue were concatenated to form a total of 73 features. These features included evolutionary content such as the position-specific substitution matrix (PSSM) profile from PSI-BLAST and the hidden Markov model (HMM) profile from HHblits. The PSSM profile was generated using three iterations of PSI-BLAST against the UniRef90 sequence database, resulting in 20 substitution values for each amino acid residue type. The HMM profile consisted of 30 values generated using HHblits with the UniProt sequence profile database.\n\nAdditionally, features from SPOT-1D were included, which comprised 11 secondary structure probabilities, 4 sine and 4 cosine values for backbone angles, 1 relative solvent-accessible surface area (ASA), 1 contact number (CN), and 2 half-sphere exposure (HSE) values based on the carbon-alpha atoms.\n\nBefore being input into the network, the features of each residue were standardized to have zero mean and unit variance. This standardization was performed using the means and standard deviations of the training data. This preprocessing step is crucial for ensuring that the features are on a comparable scale, which helps in improving the convergence and performance of the machine-learning models.",
  "optimization/parameters": "In our model, the number of parameters, p, varies depending on the specific configuration of the network. The model incorporates several types of layers, including IncReSeNet blocks, LSTM layers, and fully connected (FC) layers, each with its own set of parameters.\n\nThe IncReSeNet blocks consist of residual connections, convolution paths, and Squeeze-and-Excitation segments. The convolution operations use 1D kernels with sizes denoted as K, except for the first convolution in each branch, which has a kernel size of 1. The depth of the final convolution in these blocks is denoted as N. The Squeeze-and-Excitation segments include two fully connected layers with outputs of N/10 and 1, respectively.\n\nThe LSTM layers are bidirectional with a memory cell size denoted as N. Each LSTM block results in N/2 output values. The fully connected layers have sizes denoted as N, and their outputs are activated by a rectified linear unit (ReLU) and regularized by dropout.\n\nThe selection of these parameters was done through a grid search during the training process. A large corpus of models with varying hyperparameters was trained, and their performance was analyzed on a validation set. The top-performing models were then chosen for the final ensemble. This approach ensures that the selected parameters contribute to the model's robustness and accuracy.",
  "optimization/features": "In our work, we utilized a comprehensive set of input features for SPOT-Disorder2, building upon the features used in SPOT-Disorder. The input features consist of evolutionary content derived from the position-specific substitution matrix (PSSM) profile generated by PSI-BLAST and the hidden Markov model (HMM) profile from HHblits. The PSSM profile includes 20 substitution values for each position in the amino acid (AA) residue type, while the HMM profile comprises 30 values, including 20 AA substitution probabilities, 10 transition frequencies, and the number of effective homologous sequences (Neff).\n\nAdditionally, we incorporated predicted structural properties from SPOT-1D, which includes 11 secondary structure probabilities, 8 backbone angles (4 sine and 4 cosine values for angles s, u, and w), 1 relative solvent-accessible surface area (ASA), 1 contact number (CN), and 2 half-sphere exposure (HSE) values based on the carbon-alpha atoms. These features are concatenated to form 73 input features for each protein residue.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we standardized the features of each residue to have zero mean and unit variance using the means and standard deviations of the training data. This standardization ensures that the features are on a comparable scale, which is crucial for the performance of the neural network models. The standardization process was done using the training set only, ensuring that the validation and test sets remain unbiased.\n\nThe use of these diverse and standardized features allows our models to capture a wide range of information about the protein sequences, leading to improved performance in disorder prediction.",
  "optimization/fitting": "The fitting method employed in our study involved a comprehensive approach to ensure both overfitting and underfitting were adequately addressed. The number of parameters in our model was indeed larger than the number of training points, which is a common scenario in deep learning models. To mitigate the risk of overfitting, several techniques were utilized.\n\nFirstly, dropout was implemented as a regularization method. Dropout randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the model does not rely too heavily on any single feature. Additionally, batch normalization was employed to accelerate training and reduce internal covariate shift, further aiding in the prevention of overfitting.\n\nThe datasets used for training, validation, and testing were carefully curated to ensure minimal sequence similarity, with a cutoff of 25% sequence identity. This step was crucial in preventing the model from memorizing specific sequences rather than learning generalizable patterns.\n\nTo address underfitting, the model architecture was designed to be sufficiently complex to capture the underlying patterns in the data. The features used as input to the network were standardized to have zero mean and unit variance, which helps in stabilizing and speeding up the training process. Furthermore, the smoothing window size, along with the upper and lower thresholds, were optimized on the validation dataset to ensure the model could generalize well to unseen data.\n\nThe performance of the model was evaluated using several skew-independent metrics, including sensitivity, precision, specificity, the weighted score, the area under the receiver operating characteristic curve, and the area under the precision-recall curve. These metrics provided a comprehensive assessment of the model's ability to accurately predict disordered residues in proteins.\n\nIn summary, the fitting method involved a combination of regularization techniques, careful dataset curation, and thorough performance evaluation to ensure that the model neither overfitted nor underfitted the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and improve the generalization of our models. One of the key methods used was dropout, which was applied internally in some of the InReSeNet convolutions. Dropout works by randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not become too reliant on any single neuron. In our implementation, dropout was applied after batch normalization but before the convolution operations. This specific placement ensures that the moving average and variance measurements are not affected, and the residual connections remain intact.\n\nAdditionally, we utilized an ensemble of models to minimize the effect of generalization errors. A large corpus of models with varying hyperparameters were trained, and their performance was analyzed on a validation set. The top-performing models were then selected for the final ensemble. This approach helps to average out the errors and improve the overall robustness of the predictions.\n\nFurthermore, we employed batch normalization, which accelerates deep network training by reducing internal covariate shift. This technique normalizes the input to every convolution, stabilizing and speeding up the training process. The use of exponential linear units (ELUs) as the activation function also contributes to faster and more accurate deep network learning.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the models used in our study are detailed within the publication. Specifically, the architecture of the five network models in the ensemble is outlined in Table 1, which includes parameters such as the number of hidden units in each LSTM cell, convolutional filters in each CNN layer, nodes in each fully-connected block, and the kernel size of CNNs. The optimization process involved training a large corpus of models with varying hyperparameters and selecting the top-performing models based on validation performance.\n\nThe models were trained using the Adam optimizer, and the training process was conducted on an NVIDIA TITAN X GPU using TensorFlow v1.10. Typical training times for an IncReSeNet model were approximately 40 seconds per epoch, while an LSTM network took about 3 minutes per epoch.\n\nRegarding the availability of model files and optimization parameters, these details are not explicitly provided in the publication. However, the methods and configurations described are sufficient for replication by researchers familiar with the tools and techniques mentioned. The publication does not specify the license under which these configurations or potential model files would be made available, so it is assumed that standard academic sharing practices apply.",
  "model/interpretability": "The model employed in SPOT-Disorder2 is not entirely transparent, as it leverages complex deep learning architectures that are often considered black-box models. These architectures include IncReSeNet, LSTM, and fully-connected (FC) layers, which are designed to capture intricate patterns in the data but do not provide straightforward interpretability.\n\nHowever, certain aspects of the model can be interpreted to some extent. For instance, the use of an ensemble of different deep learning models contributes to the stability and superior performance of the model. This ensemble approach allows for a more robust prediction by averaging the outputs of multiple models, each with different hyperparameters and architectures. This method helps in reducing the generalization errors between models, making the final prediction more reliable.\n\nAdditionally, the model incorporates features from various programs like PSI-BLAST, HHblits, and SPOT-1D. The contribution of each feature type to the model's performance has been analyzed. For example, the PSSM feature from PSI-BLAST is critical for maximizing AUC metrics, while SPOT-1D features are essential for enhancing single-threshold metrics like MCC and Sw. This analysis provides some insight into which features are most influential in the model's predictions.\n\nThe architecture of the model includes residual connections, which help in maintaining the flow of information through the network. The Squeeze-and-Excitation networks within the model compress the passing information into an excitation signal, which controls the values added to the residual connection. This mechanism behaves similarly to the learned gates of an LSTM cell, providing a form of interpretability in how the model processes information.\n\nFurthermore, the use of dropout in the model helps in preventing overfitting, which is a common issue in deep learning models. Dropout is applied after batch normalization but before convolution operations, ensuring that the residual connection is not affected. This technique improves the model's generalization ability, making it more interpretable in terms of its robustness to overfitting.\n\nIn summary, while the model is not entirely transparent, certain aspects of its architecture and feature contributions can be interpreted. The ensemble approach, feature analysis, and the use of techniques like dropout and residual connections provide some level of interpretability, making the model more understandable in terms of its design and performance.",
  "model/output": "The model, SPOT-Disorder2, is a classification model designed for predicting protein disorder. It outputs a probability for each amino acid residue indicating whether it is disordered or not. The final output layer employs a sigmoid activation function to convert the singular output into a probability, which is then used to classify the residue as disordered or ordered. This probability allows for the evaluation of various performance metrics such as sensitivity, precision, specificity, and the area under the receiver operating characteristic curve (AUC ROC). The model's performance is assessed using these metrics to ensure accurate classification of disordered residues in proteins.",
  "model/duration": "The execution time for the models varied depending on the type of network used. A typical IncReSeNet model took approximately 40 seconds per epoch over the entire training set. In contrast, an LSTM network required about 3 minutes per epoch. These times reflect the computational demands of the different architectures and the complexity of the tasks they perform. The use of an NVIDIA TITAN X GPU facilitated the training process, allowing for efficient handling of the computational load.",
  "model/availability": "The source code for SPOT-Disorder2 is not explicitly mentioned as being publicly released. However, the software is applicable to the vast majority of available sequences, as more than 91% of proteins consist of fewer than 700 amino acid residues. For longer proteins, the software can still be used, although there are some limitations due to memory usage on typical workstations. These limitations are primarily due to the use of SPOT-1D in the input, which relies on the contact map prediction tool SPOT-Contact. For extremely long sequences, the computational memory required becomes too high. To address this, SPOT-1D can be replaced by the secondary structure prediction tool SPIDER3.\n\nThe performance of SPOT-Disorder2 has been validated using various datasets, including Test and Test2012 from the MoRFpred server, and comparisons have been made with other models such as MoRFpred, fMoRFpred, DisoRDPbind, and ANCHOR2. The software has shown significant improvements in metrics like AUCPR and MCC over competitors like NetSurfP-2.0 and AUCpreD. Precision-recall curves and other performance metrics are available in the publication, demonstrating the stability and effectiveness of SPOT-Disorder2 across different datasets.\n\nFor those interested in using SPOT-Disorder2, it is important to note that while the software can handle long proteins, there are practical considerations regarding memory usage. The use of SPIDER3 as an alternative to SPOT-1D for long sequences ensures that the software remains accessible and functional for a wide range of protein lengths.",
  "evaluation/method": "The evaluation of the disorder predictor involved several skew-independent metrics to gauge overall classification accuracy. These metrics included sensitivity, precision, specificity, the weighted score (Sw), the area under the receiver operating characteristic curve (AUC ROC), and the area under the precision-recall curve (AUCPR). The difference between two AUC ROC values was qualified as statistically significant according to a P value from a bivariate statistical test, with a smaller P value indicating a higher likelihood of the difference being significant. AUCPR was particularly informative when the fraction of positive labels was low, as is the case with protein disorder.\n\nThe method was compared to several high-performing protein disorder predictors, including local versions of DISOPRED2 and DISOPRED3, MobiDB-lite, AUCpreD, s2D, SPOT-Disorder, SPOT-Disorder-Single, and SPINE-D. Additionally, different versions of the ESpritz method were used, based on either single-sequence or sequence profile information. These ESpritz methods were trained based on structural information obtained from the DisProt database.\n\nThe datasets used in these experiments included non-redundant, high-resolution protein sequences from the Protein Data Bank (PDB) and the Database of Protein Disorder (DisProt). These datasets were split into training, validation, and testing sets, with sequence similarity among the proteins being less than 25% according to BLASTClust. Proteins of length greater than 700 amino acids were removed from all datasets.\n\nIndependent test datasets, such as SL250, Mobi9414, and DisProt228, were also used for a fair comparison against other methods. These datasets were subsets from established sets after removing long proteins and homologous proteins in the training dataset. The proteins in DisProt228 were newly-annotated proteins deposited in the DisProt database. The annotations in Mobi9414 contained direct labels from the DisProt database, inferred labels from the PDB, and predicted labels from a large ensemble of disorder predictors. Predicted labels in MobiDB were not utilized due to their potential inaccuracy.\n\nThe performance of the method was evaluated using these datasets, and the results were compared to other top-performing methods. The precision-recall curves were plotted by varying the threshold for defining disordered residues. The method showed consistent improvement in terms of other metrics that are more robust to potential label noise, as well as in other datasets where undefined residues have been excluded.",
  "evaluation/measure": "In our evaluation of disorder predictors, we employ several skew-independent metrics to assess the overall classification accuracy, given the innate class imbalance between disordered and ordered amino acid residues. These metrics include sensitivity, which measures the fraction of predicted positives in all true positives, and precision, which indicates the fraction of true positives in predicted positives. Specificity, another key metric, represents the fraction of true negatives in predicted negatives. Additionally, we use the weighted score (Sw), calculated as the sum of sensitivity and specificity divided by 2, to provide a balanced measure of performance. The area under the receiver operating characteristic (ROC) curve (AUCROC) is also reported, offering a comprehensive view of the trade-off between sensitivity and specificity across all classification thresholds. Furthermore, the area under the precision-recall curve (AUCPR) is included, particularly emphasizing performance on positive labels, which is crucial when the fraction of positive labels is low, as is the case with protein disorder. The Matthew’s correlation coefficient (MCC) is also calculated, providing a balanced measure that considers true and false positives and negatives. These metrics collectively offer a robust evaluation framework, ensuring that our assessment is both thorough and representative of the standards in the literature.",
  "evaluation/comparison": "A comparison to several high-performing protein disorder predictors was conducted. These included local versions of DISOPRED2 and DISOPRED3, MobiDB-lite, AUCpreD, s2D, SPOT-Disorder, SPOT-Disorder-Single, and SPINE-D. Additionally, the webserver of NetSurfP-2.0 was used. Different versions of the ESpritz method, based on either single-sequence or sequence profile information, were also included in the comparison. These ESpritz methods were trained using structural information obtained from the DisProt database.\n\nThe performance of these methods was evaluated on various datasets, including DisProt228 and Mobi4730. The metrics used for evaluation included the area under the receiver operating characteristic curve (AUCROC), the area under the precision-recall curve (AUCPR), Matthew’s correlation coefficient (MCC), and the weighted score (Sw). The results showed that SPOT-Disorder2 consistently outperformed the other methods across these metrics.\n\nFor the DisProt228 dataset, SPOT-Disorder2 improved over the second-best method, ESpritz-X (prof), by 2% in AUCROC, 4% in AUCPR, 5% in MCC, and 5% in Sw. Similarly, for the Mobi4730 dataset, SPOT-Disorder2 achieved a 1% increase in AUCPR and a 2.5% increase in MCC over the second-best method, NetSurfP-2.0. The precision-recall curves further demonstrated the superior performance of SPOT-Disorder2, with its curve consistently above those of the other methods.\n\nThe comparison also included simpler baselines, such as AUCpreD, which was optimized for AUCROC but performed poorly in terms of AUCPR. This highlights the importance of considering multiple metrics when evaluating the performance of disorder predictors. The results indicate that SPOT-Disorder2 provides a significant improvement over existing methods, making it a robust tool for protein disorder prediction.",
  "evaluation/confidence": "The performance evaluation of the disorder predictor involves several metrics, including sensitivity, precision, specificity, the weighted score (Sw), the area under the receiver operating characteristic curve (AUCROC), and the area under the precision-recall curve (AUCPR). These metrics are used to gauge the overall classification accuracy of the predictor.\n\nThe difference between two AUCROC values can be qualified as statistically significant according to a P value from a bivariate statistical test. A smaller P value indicates a higher likelihood of the difference being significant. This statistical approach ensures that the claimed superiority of the method over others and baselines is supported by robust evidence.\n\nFor example, the difference in AUCROC between SPOT-Disorder2 and ESpritz-X (prof) is statistically significant with a P value less than 1/2 * 10^-5. Similarly, the difference in AUCROC between SPOT-Disorder2 and the nearest competitor, NetSurfP-2.0, is also statistically significant with a P value less than 1/2 * 10^-7. These statistical tests provide confidence in the performance metrics and the claims of superiority.\n\nAdditionally, the precision-recall curve is particularly affected by label error due to the increased susceptibility to false positive predictions. This is noted in the context of the SPOT-Disorder2 PR curve and the poor performance of several methods at low precision. However, SPOT-Disorder2 shows consistent improvement in terms of other metrics that are more robust to potential label noise, as well as in other datasets where undefined residues have been excluded.\n\nIn summary, the performance metrics do have associated confidence intervals through statistical tests, and the results are statistically significant. This ensures that the claims of the method's superiority are well-supported and reliable.",
  "evaluation/availability": "The raw evaluation files used in our study are not publicly available. The datasets employed for testing and validation were derived from previous disorder prediction publications and established databases such as the Protein Data Bank (PDB) and the Database of Protein Disorder (DisProt). These datasets were curated and split into training, validation, and testing sets to ensure a fair and comprehensive evaluation of the predictors.\n\nSpecific datasets mentioned include Test1185, SL250, Mobi9414, and DisProt228, among others. These datasets were processed to remove redundant sequences and proteins exceeding 700 amino acid residues to align with the limitations of the SPOT-1D tool used in our predictor. The annotations and labels in these datasets were carefully curated to ensure accuracy and reliability.\n\nWhile the raw evaluation files themselves are not released, the methods and datasets used are thoroughly described in the publication. This includes details on the preprocessing steps, the metrics used for evaluation, and the statistical tests applied to assess the significance of the results. Researchers interested in replicating or building upon our work can refer to the described datasets and methods to conduct their own evaluations."
}