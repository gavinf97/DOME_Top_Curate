{
  "publication/title": "BioConceptVec: Creating and evaluating literature-based biomedical concept embeddings on a large scale",
  "publication/authors": "The authors who contributed to the article are:\n\n- Qingyu Chen, who contributed to conceptualization, data curation, formal analysis, investigation, methodology, project administration, software, supervision, validation, and writing the original draft and review & editing.\n- Kyubum Lee, who contributed to data curation, formal analysis, investigation, validation, and writing the review & editing.\n- Shankai Yan, who contributed to data curation, formal analysis, investigation, validation, and writing the review & editing.\n- Sun Kim, who contributed to data curation, formal analysis, investigation, methodology, software, and writing the review & editing.\n- Chih-Hsuan Wei, who contributed to formal analysis and writing the review & editing.\n- Zhiyong Lu, who contributed to conceptualization, investigation, methodology, project administration, supervision, and writing the review & editing.",
  "publication/journal": "PLOS Computational Biology",
  "publication/year": "2020",
  "publication/doi": "10.1371/journal.pcbi.1007617",
  "publication/tags": "- Biomedical Concept Embeddings\n- BioConceptVec\n- Machine Learning Models\n- Named-Entity Recognition\n- PubMed Abstracts\n- Intrinsic Evaluation\n- Extrinsic Evaluation\n- Protein-Protein Interaction\n- Drug-Drug Interaction\n- Biomedical Literature Mining",
  "dataset/provenance": "The datasets used in our study are sourced from various reputable databases and community-recognized resources. For the drug-drug interaction (DDI) classification task, we utilized the SemEval 2013: Task 9 DDI extraction corpus. This dataset includes over 1,000 documents from the DrugBank database and PubMed abstracts, with approximately 5,000 DDIs manually annotated by senior pharmacists. This dataset serves as a gold standard for relation extraction in the community.\n\nFor the protein-protein interaction (PPI) prediction task, we used datasets from the STRING database, which consists of around 8 million PPIs. Additionally, we employed datasets from the Comparative Toxicogenomics Database (CTD) and the Molecular Signatures Database (MSIGDB) for intrinsic evaluation tasks. These datasets include various gene sets and interactions, with a total of 19,082 groups and 17,810,712 pairs across different categories such as positional gene sets, curated gene sets, motif gene sets, computational gene sets, and Gene Ontology (GO) gene sets.\n\nThe datasets used in our study have been widely recognized and utilized in the community for evaluating the performance of concept embeddings and related tasks. The statistics of these datasets, including the number of groups, distinct concepts, and pairs, are summarized in relevant tables within our publication.",
  "dataset/splits": "In our study, we utilized several datasets for different tasks, each with specific splits for training, validation, and testing.\n\nFor the drug-drug interaction (DDI) classification task, we used the SemEval 2013: Task 9 DDI extraction corpus. This dataset consists of over 1,000 documents from the DrugBank database and PubMed abstracts, with approximately 5,000 DDIs manually annotated by senior pharmacists. The dataset is split into training and testing sets. The training set contains 1,319 instances of the 'Mechanism' class, 1,621 instances of the 'Effect' class, 826 instances of the 'Advice' class, 188 instances of the 'Int' class, and 23,772 negative instances. The testing set contains 302 instances of the 'Mechanism' class, 360 instances of the 'Effect' class, 221 instances of the 'Advice' class, 96 instances of the 'Int' class, and 4,737 negative instances.\n\nFor the protein-protein interaction (PPI) prediction task, we used two datasets: the combined-score dataset from the STRING database and the experimental-700 dataset. The combined-score dataset includes 13,802 concepts, with 5,245,358 training instances, 582,818 validation instances, and 2,497,790 testing instances. The experimental-700 dataset includes 13,290 concepts, with 24,684 training instances, 2,743 validation instances, and 11,755 testing instances.\n\nAdditionally, for intrinsic evaluation tasks, we used six datasets: CTD, MSigDB C1, MSigDB C2, MSigDB C3, MSigDB C4, and MSigDB C5. These datasets contain groups of related and unrelated sets of genes based on drug-gene interactions or gene sets. The CTD dataset has 6,383 groups, 14,654 distinct concepts, and 2,146,482 pairs. The MSigDB datasets have varying numbers of groups, distinct concepts, and pairs, with MSigDB C2 being the largest, containing 4,762 groups, 13,783 distinct concepts, and 6,171,976 pairs.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The embeddings and evaluation datasets used in our study are publicly available. They can be accessed and downloaded via a GitHub repository. This repository also includes a Jupyter notebook containing code examples to help users get started with the data. The availability of these resources ensures that other researchers can replicate our findings and build upon our work. The repository is open to the public, allowing for broad access and use. The specific URL for accessing these resources is provided in the publication.",
  "optimization/algorithm": "The optimization algorithm used in our work is based on well-established machine learning models for creating word and concept embeddings. Specifically, we employed four different models: continuous bag of words (cbow), skip-gram, GloVe, and fastText. These models are part of the broader class of neural network-based embedding techniques, which have been extensively used in natural language processing tasks.\n\nThese models are not new; they have been widely studied and applied in various domains, including biomedical text mining. The choice of these models was driven by their proven effectiveness in capturing semantic relationships in text data. The cbow and skip-gram models are part of the Word2Vec framework, which is known for its ability to generate high-quality word embeddings. GloVe (Global Vectors for Word Representation) is another popular method that combines global statistical information with local context information. FastText, developed by Facebook's AI Research lab, extends Word2Vec by incorporating subword information, making it particularly effective for handling rare words and morphological variations.\n\nGiven that these models are well-established, there was no need to publish them in a machine-learning journal. Instead, our focus was on applying these models to the specific domain of biomedical concept embeddings, which required adapting and evaluating them on a large-scale corpus of PubMed abstracts. This adaptation involved using high-performance named-entity recognition (NER) tools to identify and normalize biomedical concepts, ensuring that the embeddings accurately reflect the semantic relationships in the biomedical literature.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding process for our machine-learning algorithm involved several key steps. Initially, we utilized PubTator, a high-performance named-entity recognition (NER) tool, to annotate and normalize biological concepts within approximately 30 million PubMed abstracts. This step was crucial for identifying and standardizing concept mentions, such as converting \"estrogen receptor\" to \"NCBI Gene: 2099\" or \"MLN4924\" to \"MESH:C539933.\"\n\nFollowing the annotation, we preprocessed the corpus by applying a pipeline similar to that used in previous studies. This preprocessing ensured that the data was in a suitable format for training concept embeddings. The embeddings were then trained using four different machine learning models: cbow, skip-gram, GloVe, and fastText. Each model was configured with specific hyperparameters, such as vector dimension, window size, and negative sampling, to optimize performance.\n\nFor the cbow model, we used default hyperparameter values identical to those selected by baseline embeddings, ensuring a fair comparison. Additionally, we experimented with other commonly used hyperparameter values to assess the effectiveness of BioConceptVec under different settings. This approach allowed us to evaluate the robustness and versatility of our embeddings across various configurations.\n\nThe preprocessing and encoding steps were designed to handle the unique challenges of biomedical text, ensuring that the resulting embeddings accurately capture the semantic relatedness of biological entities. This comprehensive approach enabled us to develop BioConceptVec, a large-scale biomedical concept embedding that covers over 400,000 concepts and demonstrates superior performance in both intrinsic and extrinsic evaluations.",
  "optimization/parameters": "In our study, we utilized several hyperparameters to train BioConceptVec using different embedding methods. The shared hyperparameters include vector dimension, window size, negative samples, down-sampling threshold, minimal word occurrence, learning rate, and training epochs. Additionally, fastText-specific hyperparameters include minimal and maximum character n-gram lengths.\n\nThe default values for these hyperparameters were selected based on existing concept embeddings, specifically those used by Yu et al. These values served as our baseline settings. To explore the effectiveness of BioConceptVec under different conditions, we also tested other commonly used hyperparameter values. For vector dimension, we used 100 and 300 in addition to the default 200. For window size, we tested 5 and 10 alongside the default 20. Negative samples were set to 2 and 3, compared to the default 5. Down-sampling thresholds were adjusted to 0.0001 and 0.00001, with the default being 0.001. The minimal word occurrence, learning rate, and training epochs were kept constant at 5, 0.025, and 10, respectively, to ensure a consistent vocabulary and optimization procedure across different settings.\n\nFor fastText, the minimal and maximum character n-gram lengths were set to 2 and 3, respectively. These values were chosen to capture the character-level information effectively while maintaining computational efficiency.\n\nIn summary, we used a combination of default and alternative hyperparameter values to thoroughly evaluate the performance of BioConceptVec. This approach allowed us to assess the model's robustness and adaptability across various settings.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not enough information is available.",
  "optimization/regularization": "In our study, we did not explicitly mention the use of regularization methods as a means of preventing overfitting. However, we did employ several techniques that inherently help in mitigating overfitting.\n\nOne such technique is the use of down-sampling thresholds during the training of our concept embeddings. By setting a down-sampling threshold, we effectively reduce the impact of frequently occurring words, which can help in preventing the model from becoming too specialized to the training data.\n\nAdditionally, we utilized negative sampling, a technique where only a small number of negative samples are drawn to train the model. This approach not only makes the training process more efficient but also helps in generalizing better to unseen data by not overfitting to the negative samples.\n\nFurthermore, we experimented with different hyperparameters, such as vector dimensions, window sizes, and the number of negative samples. This hyperparameter tuning process allows us to find the optimal settings that balance model complexity and performance, thereby reducing the risk of overfitting.\n\nLastly, we evaluated our models on multiple datasets and tasks, ensuring that our embeddings generalize well across different scenarios. This extensive evaluation helps in identifying and mitigating any potential overfitting issues.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our study are thoroughly documented and available for public use. We have made all the embeddings and evaluation datasets publicly accessible. These resources can be downloaded via the GitHub repository at https://github.com/ncbi-nlp/BioConceptVec. Additionally, we provide a Jupyter notebook that includes code examples to help users get started with our models. The repository also includes detailed information on the hyper-parameters and optimization settings used during the training of BioConceptVec, ensuring reproducibility and ease of use for researchers and practitioners in the field. The materials are shared under a license that allows for open access and usage, promoting collaborative advancements in biomedical text mining and bioinformatics.",
  "model/interpretability": "The model discussed in this publication is not entirely a black box. While it leverages deep learning techniques, which are often criticized for their lack of interpretability, efforts have been made to enhance transparency.\n\nOne key aspect of interpretability in this model is the use of concept embeddings, specifically BioConceptVec. These embeddings provide vector representations of biomedical concepts, which are derived from a large corpus of PubMed abstracts. By incorporating these embeddings, the model can capture semantic relatedness between biological entities, making the decision-making process more interpretable.\n\nFor instance, the model's performance on drug-drug interaction (DDI) extraction improved significantly when BioConceptVec was added. This improvement can be attributed to the additional information provided by BioConceptVec, which helps the model distinguish between similar relation types, such as \"effect\" and \"mechanism.\" For example, in the sentence \"Zidovudine competitively inhibits the intracellular phosphorylation of stavudine,\" the model initially struggled to classify the relation correctly. However, with BioConceptVec, it could accurately classify the relation as \"effect\" rather than \"mechanism,\" demonstrating the model's enhanced interpretability.\n\nFurthermore, the model's architecture, which includes an embedding layer using ELMo and an averaged layer, provides a clear pathway for how input sentences are transformed into output class probabilities. This transparency allows researchers to understand how different components of the model contribute to its overall performance.\n\nIn summary, while the model utilizes complex deep learning techniques, the incorporation of concept embeddings and a transparent architecture enhances its interpretability, making it a valuable tool for biomedical text mining and related applications.",
  "model/output": "The model discussed in this publication is a classification model. Specifically, it is designed for drug-drug interaction (DDI) classification, which is a multi-class classification problem. The model takes a sentence containing a pair of drugs as input and outputs the type of drug-drug interaction if one exists. The possible interaction types include advice, effect, mechanism, and int (where the interaction occurs but its type cannot be classified). Additionally, the model can indicate if the pair of drugs does not represent a true drug-drug interaction. The performance of the model is evaluated using metrics such as precision, recall, and F1-score, which are standard for classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for BioConceptVec is publicly available. The embeddings and evaluation datasets can be downloaded via a GitHub repository. Additionally, a Jupyter notebook containing code examples is provided to help users get started. The repository also includes four versions of BioConceptVec trained using different methods (cbow, skip-gram, GloVe, and fastText), allowing users to experiment and choose the model that best fits their tasks. The models and datasets are released under the Creative Commons CC0 public domain dedication, which means they are free of all copyright and can be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose.",
  "evaluation/method": "The evaluation of our method, BioConceptVec, involved both intrinsic and extrinsic evaluations to assess its effectiveness in capturing biomedical concepts.\n\nFor intrinsic evaluations, we focused on identifying related genes based on drug-gene and gene-gene interactions. We used six datasets to create evaluation groups, each consisting of a related set and an unrelated set of genes. The relatedness was determined using the Comparative Toxicogenomics Database (CTD) for drug-gene interactions and MSigDB for gene sets based on various perspectives such as human chromosomes and gene ontologies. The effectiveness of the concept embeddings was quantified by measuring the average cosine similarity within the related and unrelated sets. A higher similarity difference between the positive and negative sets indicated better performance.\n\nWe compared BioConceptVec (cbow) with two baselines: BioAvgWord (cbow), which averages word vectors based on bio-concept names, and the concept embedding provided by Yu et al. We also evaluated BioConceptVec under different hyperparameter settings and using different embedding methods (skip-gram, GloVe, and fastText) to ensure robustness and versatility.\n\nExtrinsic evaluations involved protein-protein interaction (PPI) predictions on the STRING database. We assessed the classification results using metrics such as precision, recall, F1 score, and AUC. BioConceptVec (cbow) demonstrated superior performance compared to the baselines, achieving the highest F1 score and AUC on both datasets. Additionally, BioConceptVec (fastText) showed the best overall performance for this task, although BioConceptVec (cbow) and BioConceptVec (skip-gram) were very close in performance.\n\nIn summary, our evaluation method involved a comprehensive assessment using both intrinsic and extrinsic evaluations, ensuring that BioConceptVec effectively captures biomedical concepts and performs well in practical applications.",
  "evaluation/measure": "In our evaluation, we primarily report the average group similarity difference in percentage as our key intrinsic evaluation metric. This metric assesses how well the concept embeddings distinguish between positive and negative sets within a group, with higher values indicating better performance. We also report precision, recall, F1 score, and the area under the receiver operating characteristic curve (AUC) for extrinsic evaluations, such as protein-protein interaction (PPI) predictions. These metrics are widely used in the literature and provide a comprehensive view of the model's performance.\n\nThe F1 score is particularly useful as it balances precision and recall, giving a single metric that represents the overall effectiveness of the embeddings in classification tasks. The AUC provides insight into the model's ability to distinguish between positive and negative classes across all threshold levels. These metrics are representative of those commonly used in the field of bioinformatics and text mining, ensuring that our evaluation is comparable to other studies.\n\nFor intrinsic evaluations, we focus on the average group similarity difference because it directly measures the effectiveness of the embeddings in capturing the semantic relationships between concepts. This metric is crucial for tasks that rely on the similarity between concepts, such as gene set enrichment analysis and drug repurposing.\n\nIn summary, the reported metrics are well-established in the literature and provide a thorough assessment of the embeddings' performance in both intrinsic and extrinsic evaluations.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison with publicly available methods on benchmark datasets. We specifically chose to compare with concept embeddings trained on PubMed, as this aligns with our training data. For a fair comparison, we used the exact hyperparameter values from a previous study as our default settings. This allowed us to directly compare our method, BioConceptVec, with existing concept embeddings.\n\nWe also performed comparisons with simpler baselines to establish a strong foundation. One such baseline was a common continuous bag of words (cbow) word embedding trained on PubMed abstracts. Since common word embeddings do not contain vectors for normalized bio-concepts, we averaged the word vectors based on the bio-concept name to represent the concept vector. This approach, which we refer to as BioAvgWord (cbow), has been shown to be effective in various embedding-related tasks.\n\nAdditionally, we trained and assessed BioConceptVec under different parameter settings while keeping certain values constant to ensure a consistent vocabulary and optimization procedure. We selected representative values for hyperparameters based on previous studies on embeddings. This allowed us to evaluate the effectiveness of BioConceptVec under various conditions.\n\nTo ensure a comprehensive evaluation, we also trained BioConceptVec using different embedding methods, including skip-gram, GloVe, and fastText, using the same default setups. This approach provides users with multiple versions of BioConceptVec to experiment with and choose from for their specific tasks. All four versions of BioConceptVec are publicly available.\n\nIn summary, our evaluation included comparisons with publicly available methods on benchmark datasets and simpler baselines, providing a robust assessment of BioConceptVec's performance.",
  "evaluation/confidence": "The evaluation of BioConceptVec includes both intrinsic and extrinsic assessments, providing a comprehensive view of its performance. For intrinsic evaluations, the average group similarity difference is used as the metric, with BioConceptVec (cbow) consistently outperforming baseline methods across six datasets. The differences in performance are statistically significant, with BioConceptVec (cbow) showing an average improvement of 4 percentage points over Yu et al. and 7 percentage points over average word embeddings.\n\nIn extrinsic evaluations, BioConceptVec (cbow) demonstrates superior performance in protein-protein interaction predictions on the STRING database, achieving the highest F1 score and AUC. The results are consistent across different hyperparameters, indicating robustness. Additionally, BioConceptVec (fastText) shows the best overall performance for this task, though the differences between BioConceptVec (cbow) and BioConceptVec (skip-gram) are minimal.\n\nFor drug-drug interaction extraction, BioConceptVec (cbow) significantly improves the F1-score when added to the SEN model, outperforming baseline approaches. The performance is consistent across different hyperparameters, and qualitative analysis shows that BioConceptVec helps in classifying challenging cases by providing additional information from PubMed abstracts.\n\nWhile specific confidence intervals are not explicitly mentioned, the consistent and significant improvements across various tasks and datasets suggest high confidence in the superior performance of BioConceptVec. The results are statistically significant, supporting the claim that BioConceptVec is superior to other methods and baselines.",
  "evaluation/availability": "The raw evaluation files are publicly available. All embeddings and evaluation datasets can be downloaded via a provided GitHub repository. Additionally, a Jupyter notebook containing code examples is available to help users get started. The resources are made freely accessible to facilitate further research and application in the community."
}