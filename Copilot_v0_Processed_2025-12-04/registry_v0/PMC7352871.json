{
  "publication/title": "Urinary Proteomic Biomarkers for Predicting Prognosis in Patients with Diabetic Nephropathy",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "International Journal of Molecular Sciences",
  "publication/year": "2020",
  "publication/doi": "10.3390/ijms21124236",
  "publication/tags": "- Urine protein biomarkers\n- Diabetic nephropathy\n- Random forest\n- Support vector machines\n- Gene expression analysis\n- Kidney disease\n- Proteomics\n- Bioinformatics\n- Machine learning\n- Clinical models",
  "dataset/provenance": "The dataset used in this study was sourced from publicly available Gene Expression Omnibus (GEO) datasets. Specifically, four datasets were utilized: GSE99339, GSE47185, GSE30122, and GSE96804. These datasets contain mRNA expression data from various kidney-related studies.\n\nThe GSE99339 dataset includes mRNA expression data from the renal glomerulus of 187 patients across 11 disease groups, including diabetic nephropathy (DN), rapidly progressive glomerulonephritis (RPGN), and others. Additionally, this dataset has a subset of 223 samples, comprising 122 kidney glomerulus and 101 tubulointerstitia samples across eight disease groups.\n\nThe GSE30122 dataset provides prognostic indexes for two classifiers in eight disease groups within the renal glomeruli and tubulointerstitia. This dataset includes samples from various disease groups such as DN, RPGN, and others, with specific counts for each group.\n\nThe datasets have been used to validate statistical models consisting of five urine proteins. These models, including Support Vector Machine (SVM) and Random Forest (RF) models, were applied to the datasets without any model adjustment. The prognostic probabilities from these models were highly correlated, demonstrating the robustness of the models across different datasets and disease groups.\n\nThe datasets have been previously used in the community for various studies related to kidney diseases. For instance, the GSE99339 dataset has been utilized to study transcriptome-based network analysis in renal cell type-specific dysregulation of hypoxia-associated transcripts. Similarly, the GSE30122 dataset has been used for transcriptome analysis of human diabetic kidney disease. These datasets have provided valuable insights into the molecular mechanisms underlying kidney diseases and have been instrumental in developing and validating clinical models for disease prognosis.",
  "dataset/splits": "In our study, we utilized four publicly available GEO datasets for external validation of our clinical models. The datasets used were GSE99339, GSE47185, GSE30122, and GSE96804. Each dataset was analyzed separately to validate the performance of our models.\n\nThe first dataset, GSE99339, included mRNA expression data from the renal glomerulus of 187 patients. These patients were categorized into 11 disease groups: diabetic nephropathy (DN), rapidly progressive glomerulonephritis (RPGN), tumor nephrectomies (TN), hypertensive nephropathy (HT), IgA nephropathy, membranous glomerulonephritis (MGN), systemic lupus erythematosus (SLE), thin membrane disease (TMD), focal and segmental glomerulosclerosis (FSGS), focal and segmental glomerulosclerosis and minimal change disease (FSGS&MCD), and minimal change disease (MCD).\n\nThe second dataset, GSE47185, is not explicitly detailed in terms of the number of samples or specific disease groups, but it was used in the validation process.\n\nThe third dataset, GSE30122, provided mRNA expression levels from both the kidney glomerulus and tubulointerstitium. It included a total of 223 samples, with 122 from the glomerulus and 101 from the tubulointerstitium. The disease groups in this dataset were DN, RPGN, TN, MGN, TMD, FSGS, FSGS&MCD, and MCD. Additionally, this dataset was further split into subsets for specific analyses. One subset included 69 samples, with 35 from the kidney glomerulus (26 normal and 9 DKD) and 34 from the renal tubulus (24 normal and 10 DKD). Another subset included 62 samples, with 20 from the non-neoplastic part of tumor nephrectomies and 41 from the disease group in the renal glomeruli.\n\nThe fourth dataset, GSE96804, is also not explicitly detailed in terms of the number of samples or specific disease groups, but it was included in the validation process.\n\nIn summary, our study involved multiple data splits across four datasets, with varying numbers of samples and disease groups in each split. The detailed distribution of data points in each split is provided for the GSE99339 and GSE30122 datasets, while the specific details for GSE47185 and GSE96804 are not explicitly mentioned.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not applicable.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes two well-established machine-learning classifiers: Random Forest (RF) and Support Vector Machine (SVM). These are not new algorithms but are widely recognized and used in various fields for their robustness and predictive power.\n\nRandom Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is known for its ability to handle large datasets with high dimensionality and to provide feature importance scores, which were crucial in our feature selection process.\n\nSupport Vector Machine, on the other hand, is a supervised learning model that analyzes data for classification and regression analysis. It works by finding the hyperplane that best separates the data into classes. The SVM used in our study employs a linear kernel, which is effective for high-dimensional spaces and when the number of dimensions exceeds the number of samples.\n\nThe reason these algorithms were not published in a machine-learning journal is that they are standard, well-documented methods in the field. Our focus was on applying these established techniques to a specific biological problem—predicting renal outcomes based on urine protein biomarkers. The novelty of our work lies in the application of these methods to this particular dataset and the biological insights gained from the results, rather than in the development of new machine-learning algorithms.",
  "optimization/meta": "The model employed in our study does not function as a meta-predictor. Instead, it utilizes two distinct machine-learning algorithms independently to predict renal outcomes. These algorithms are the random forest (RF) and support vector machine (SVM) classifiers. Each classifier was trained and evaluated separately using the same set of features derived from a panel of five proteins (ACP2, CTSA, GM2A, MUC1, and SPARCL1). The RF classifier was built using 20,000 decision trees, while the SVM classifier was developed through three repeated iterations of 10-fold cross-validation.\n\nThe performance of these classifiers was assessed using areas under the curve (AUC) for the receiver operating characteristic (ROC) curves. The RF classifier achieved an AUC of 1.000, indicating perfect discrimination between patients with good and poor prognosis. The SVM classifier had an AUC of 0.935, demonstrating high but slightly lower discriminatory power compared to the RF classifier.\n\nThe training data for both classifiers consisted of 54 samples, with 35 samples from patients with a good prognosis and 19 samples from patients with a poor prognosis. The nominal binary results from both classifiers were transformed into disease prediction scores ranging from 0 to 1. These scores were then compared to the albumin-to-creatinine ratio, showing significant differences, which underscores the potential of these protein biomarkers in clinical practice.\n\nIn summary, the model does not aggregate predictions from multiple machine-learning algorithms but rather relies on two independent classifiers, each providing valuable insights into renal outcome prediction. The training data for both classifiers is independent and derived from the same cohort of patients, ensuring robust and reliable model performance.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for the machine-learning algorithms used in our study. Initially, the corrected LFQ values of six selected normalization proteins in each sample were divided by their median value across all samples. This median of the six ratios was defined as the normalization scaling factor (NSF) for that sample. For proteins other than the six normalization proteins, the normalized LFQ value was calculated by dividing the corrected LFQ value by the NSF.\n\nFor the feature selection process, we aimed to find the best subset of proteins for classifying two disease progression groups out of 412 proteins. This involved generating 50,000 decision trees, each containing eight variables, and calculating their AUC values. The optimal number of proteins was determined based on out-of-bag error estimation, resulting in the selection of 11 proteins. Through 100 iterations with three-fold cross-validation, the probability and importance of each variable being included in the model were calculated. Proteins with an importance greater than 0.3 were selected, resulting in five key proteins: ACP2, CTSA, GM2A, MUC1, and SPARCL1.\n\nPrior to model building, centering and scaling were performed as preprocessing steps on the data. This ensured that the data was standardized, which is essential for the performance of machine-learning algorithms. The final models included a Support Vector Machine (SVM) with a linear kernel, generated using a 10 repeated three-fold cross-validation method, and a Random Forest (RF) model created through a three-fold cross-validation method repeated 100 times with 1000 trees, mtry = 5, and nodesize = 5. These preprocessing and encoding steps were vital for the accurate and reliable performance of our machine-learning models.",
  "optimization/parameters": "In the optimization process, the number of parameters used in the model was determined through a feature selection process. Initially, 50,000 decision trees were generated, each containing eight variables. The optimal number of proteins was determined based on the AUC values and out-of-bag error estimation, resulting in a selection of 11 proteins. Subsequently, through 100 iterations with three-fold cross-validation, the probability and importance of each variable being included in the model were calculated. This led to the selection of five proteins, each with an importance greater than 0.3. These five proteins were then used to build the final models.",
  "optimization/features": "In the optimization process, feature selection was performed to identify the most relevant proteins for predicting renal outcomes. Initially, 412 proteins were considered. To find the best subset for classifying disease progression groups, a two-step process was employed. In the first step, 50,000 decision trees were generated, each containing eight variables. The optimal number of proteins was determined based on out-of-bag error estimation, resulting in the selection of 11 proteins. In the second step, through 100 iterations with three-fold cross-validation, the probability and importance of each variable were calculated. This process led to the selection of five proteins, which were deemed significant with an importance greater than 0.3. These five proteins were then used as input features for building the clinical models. The feature selection process was conducted using the training set only, ensuring that the models were trained and validated on independent data.",
  "optimization/fitting": "The fitting method employed in this study involved the use of two classifiers: random forest (RF) and support vector machine (SVM). The number of parameters in the models was indeed larger than the number of training points, which is a common scenario in high-dimensional data analysis. To address the potential issue of over-fitting, several strategies were implemented.\n\nFor the RF model, the process involved generating a large number of decision trees (20,000) and using an AUC-based backward-elimination process to select the most important features. This method helps in reducing the dimensionality of the data and focusing on the most relevant variables, thereby mitigating over-fitting. Additionally, the RF model's performance was evaluated using out-of-bag error estimation, which provides an internal estimate of the generalization error.\n\nThe SVM model, on the other hand, was built using a linear kernel and involved three repeated iterations of 10-fold cross-validation. This cross-validation technique ensures that the model is trained and tested on different subsets of the data, providing a robust estimate of its performance and helping to prevent over-fitting.\n\nTo rule out under-fitting, the models were evaluated based on their ability to distinguish between patients with good and poor prognosis. The AUC values for the RF and SVM models were 1.000 and 0.935, respectively, indicating strong discriminative power. Furthermore, the models were validated using external datasets from the Gene Expression Omnibus (GEO), which included mRNA expression data from various disease groups. The high correlation between the prognostic probabilities of the two classifiers in these external datasets further supports the robustness of the models and suggests that under-fitting is not a concern.\n\nIn summary, the fitting method involved careful feature selection, cross-validation, and external validation to address both over-fitting and under-fitting, ensuring the reliability and generalizability of the models.",
  "optimization/regularization": "In the optimization process, several techniques were employed to prevent overfitting. One of the primary methods used was feature selection, which involved identifying the most relevant proteins for classifying disease progression groups. This was done in two main steps. First, 50,000 decision trees were generated, each containing eight variables, and their AUC values were evaluated. The optimal number of proteins was determined based on out-of-bag error estimation, resulting in the selection of 11 proteins. In the second step, 100 iterations with three-fold cross-validation were performed on these 11 proteins to calculate the probability and importance of each variable being included in the model. Only proteins with an importance score greater than 0.3 were selected, resulting in a final set of five proteins.\n\nAdditionally, centering and scaling were performed as preprocessing steps before building the clinical models. This standardization helps in ensuring that the models are not biased by the scale of the features.\n\nTwo types of models were generated: a Support Vector Machine (SVM) model with a linear kernel and a Random Forest (RF) model. The SVM model was created using a 10 repeated three-fold cross-validation method with a specific parameter (C = 0.1052). The RF model was built using a three-fold cross-validation method repeated 100 times, with parameters including 1000 trees, mtry = 5, and nodesize = 5. These cross-validation techniques help in assessing the model's performance and generalizability, thereby reducing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, for the random forest (RF) model, we generated 1000 trees with a node size of 5 and mtry set to 5. The support vector machine (SVM) model utilized a linear kernel with a parameter C value of 0.1052. These configurations were determined through a rigorous process involving 10 repeated three-fold cross-validation for the SVM and 100 iterations of three-fold cross-validation for the RF model.\n\nThe model files and optimization parameters are not explicitly provided as downloadable assets within the publication. However, the methods and configurations are thoroughly described, allowing for replication of the models. The publication itself is open access, ensuring that the detailed methods and results are freely available to the scientific community. This transparency supports the reproducibility of our findings and encourages further research and validation by other scientists.\n\nThe software and tools used, such as RStudio and specific R packages like ggplot2, permcor, pcaMethods, pROC, and ROCR, are widely available and can be accessed under their respective licenses. This ensures that researchers can implement the same analytical workflows described in our study.",
  "model/interpretability": "The models employed in this study, specifically the Random Forest (RF) and Support Vector Machine (SVM), are generally considered to be more interpretable compared to many other machine learning models. The RF model, in particular, is known for its transparency due to its structure of multiple decision trees. Each tree in the forest makes a decision based on a set of rules derived from the input features, which can be traced back to understand the reasoning behind the model's predictions.\n\nThe RF model's interpretability is further enhanced by the feature importance scores, which indicate the contribution of each protein to the model's predictions. For instance, proteins like CTSA, SPARCL1, and GM2A have higher importance scores, suggesting they play a significant role in distinguishing between different disease states. These importance scores provide a clear insight into which proteins are most influential in the model's decision-making process.\n\nAdditionally, the SVM model, while not as inherently interpretable as the RF model, can still offer some level of transparency. The linear SVM model used in this study can be analyzed to understand the weight assigned to each feature, indicating their importance in the classification task. Although the SVM model does not provide a straightforward set of rules like the RF model, the weights can still give an idea of which proteins are most critical for the model's predictions.\n\nIn summary, both the RF and SVM models used in this study offer a degree of interpretability. The RF model's decision trees and feature importance scores provide a clear view of how the model makes predictions, while the SVM model's feature weights offer some insight into the importance of different proteins. This transparency is crucial for understanding the biological significance of the selected proteins and for validating the model's predictions in a clinical context.",
  "model/output": "The model developed in our study is a classification model. We employed two different classifiers, random forest (RF) and support vector machine (SVM), to predict renal outcomes based on urine protein markers. The primary goal was to distinguish patients who were at risk of disease progression from those who were not. The classifiers were trained to output prognostic probabilities, which were then transformed into disease prediction scores ranging from 0 to 1. These scores indicate the likelihood of a patient having a poor prognosis.\n\nThe performance of the classifiers was evaluated using the area under the curve (AUC) metric. The RF classifier achieved an AUC of 1.0, while the SVM classifier had an AUC of 0.935. These high AUC values demonstrate the strong discriminative power of the models in classifying patients based on their risk of disease progression.\n\nThe models were validated using external datasets, specifically four publicly available GEO datasets (GSE99339, GSE47185, GSE30122, and GSE96804). The prognostic probabilities generated by the RF and SVM classifiers were highly correlated across these datasets, indicating consistent performance. In the GSE99339 dataset, for instance, the classifiers' predictions were significantly higher in the diabetic nephropathy (DN) group compared to other disease groups, highlighting the models' ability to identify high-risk patients.\n\nIn summary, the output of our model is a classification of patients into different risk categories based on their urine protein profiles. The model's performance is robust, as evidenced by the high AUC values and consistent results across external validation datasets.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the clinical models involved several rigorous steps to ensure their robustness and generalizability. Initially, two classifiers were generated using random forest (RF) and support vector machine (SVM) methods. These classifiers were built using five selected proteins (ACP2, CTSA, GM2A, MUC1, and SPARCL1) identified through an AUC-based RF backward-elimination process. The RF model was established by generating 20,000 decision trees, while the SVM model was created using three repeated iterations of 10-fold cross-validation.\n\nThe performance of these classifiers was evaluated using a set of 54 samples, which included 35 samples from patients with a good prognosis and 19 samples from patients with a poor prognosis. The areas under the curve (AUC) for the RF and SVM classifiers were 1.000 and 0.935, respectively, indicating excellent predictive performance. The nominal binary results of the RF and SVM models were transformed into disease prediction scores ranging from 0 to 1.\n\nTo further validate the models, they were applied to four publicly available GEO datasets (GSE99339, GSE47185, GSE30122, and GSE96804) without any model adjustment. These datasets included mRNA expression data from various disease groups, such as diabetic nephropathy (DN), rapidly progressive glomerulonephritis (RPGN), and others. The prognostic probabilities of the two classifiers were highly correlated in these datasets, demonstrating the models' consistency and reliability.\n\nIn the GSE99339 dataset, which studied mRNA expression in the renal glomerulus of 187 patients, the classifiers' prognostic probabilities were highly correlated (ρ = 0.817, Pearson correlation coefficient). The RF prediction values in the DN group were significantly higher than those in other disease groups, except for RPGN and hypertensive nephropathy (HT). Similarly, the SVM prediction values in the DN group were significantly higher than those in other groups, excluding RPGN.\n\nThe evaluation process also included statistical analysis using various R software packages, such as ggplot2 for plotting, permcor for calculating permutation-based p-values, and pROC for univariate ROC analysis. These tools helped in visualizing the data and assessing the statistical significance of the results.\n\nOverall, the evaluation method involved a combination of cross-validation, independent dataset testing, and statistical analysis to ensure the models' accuracy and reliability in predicting renal outcomes.",
  "evaluation/measure": "In the evaluation of our classifiers, we focused on several key performance metrics to ensure a comprehensive assessment of their predictive capabilities. The primary metric reported is the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curves. For our random forest (RF) classifier, the AUC achieved was 1.000, indicating perfect discrimination between the classes. The support vector machine (SVM) classifier also performed well, with an AUC of 0.935. These AUC values are indicative of the models' ability to distinguish between patients at risk of disease progression and those who are not.\n\nIn addition to AUC, we transformed the nominal binary results of the RF and SVM models into disease prediction scores ranging from 0 to 1. This transformation allowed us to evaluate the models' performance in a more clinically relevant context. The prediction scores were highly correlated between the two classifiers, with a Pearson correlation coefficient of 0.817, suggesting consistent performance across different modeling approaches.\n\nWe also compared the performance of our classifiers with the albumin-to-creatinine ratio, a commonly used clinical marker. The likelihood ratio test indicated that our classifiers differed significantly from this traditional marker, highlighting their potential added value in clinical settings.\n\nThe reported metrics are representative of standard practices in the literature for evaluating classifier performance, particularly in the context of biomedical research. AUC is a widely accepted metric for assessing the discriminative power of binary classifiers, and the transformation of model outputs into clinically interpretable scores is a common approach to facilitate practical application. The use of correlation coefficients to compare different models further ensures that the evaluation is robust and comprehensive.",
  "evaluation/comparison": "Not applicable. The study focused on developing and validating clinical models using specific urine protein biomarkers and did not compare these models to publicly available methods or simpler baselines on benchmark datasets. The validation process involved applying the models to publicly available GEO datasets to assess their performance in predicting renal outcomes based on mRNA expression in the kidney. However, the primary emphasis was on the internal performance of the models rather than a direct comparison with other methods or baselines.",
  "evaluation/confidence": "The evaluation of the classifiers' performance was conducted using several statistical methods to ensure confidence in the results. The areas under the curve (AUC) for the random forest (RF) and support vector machine (SVM) classifiers were reported as 1.000 and 0.935, respectively. These AUC values indicate strong discriminative ability, with the RF classifier achieving perfect performance.\n\nStatistical significance was assessed using various tests. For instance, the prognostic probabilities of the two classifiers were highly correlated, with a Pearson correlation coefficient of 0.817 in 187 samples. This strong correlation suggests that both classifiers are reliable and consistent in their predictions.\n\nIn the diabetic nephropathy (DN) group, the RF prediction values were significantly higher than those in eight other groups, except for rapidly progressive glomerulonephritis (RPGN) and hypertensive nephropathy (HT) groups, as determined by the Mann-Whitney U Test (p-value < 0.05). Similarly, the SVM prediction values in the DN group were significantly higher than those in nine other groups, excluding the RPGN group (p-value < 0.05). These results indicate that the classifiers can effectively distinguish the DN group from other disease groups.\n\nAdditionally, the classifiers' performance was validated using public datasets, such as GSE99339, GSE47185, GSE30122, and GSE96804. The models were applied to these datasets without adjustment, and the results showed consistent performance across different disease groups and kidney cell types. For example, in the GSE99339 dataset, the classifiers' prognostic probabilities were highly correlated (ρ = 0.817), and the DN group had the highest values compared to other groups.\n\nThe statistical analysis was performed using RStudio, with various packages employed for different tasks, such as ggplot2 for plotting, permcor for calculating permutation-based p-values, and pROC for ROC analysis. These tools ensured that the evaluation was rigorous and that the results were statistically significant.\n\nIn summary, the performance metrics included confidence intervals and statistical significance tests, providing a high level of confidence in the classifiers' superior performance compared to other methods and baselines. The results demonstrate the reliability and effectiveness of the RF and SVM classifiers in predicting renal outcomes based on the selected proteins.",
  "evaluation/availability": "Not enough information is available."
}