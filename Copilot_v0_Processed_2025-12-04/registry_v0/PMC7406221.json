{
  "publication/title": "Global RNA Decay in Cyanobacteria",
  "publication/authors": "The authors who contributed to this article are Gordon C. G., J. Cameron C., S. T. P. G., J. L. R., and B. F. P. G. C. G. and J. C. C. were responsible for designing the overall study, collecting RNA samples, and analyzing the half-life data. G. C. G. also performed the motif search and functional analysis. S. T. P. G. and J. L. R. designed and performed the machine learning analysis. All authors contributed to writing, data analysis, and editing the manuscript.",
  "publication/journal": "msystems.asm.org",
  "publication/year": "2020",
  "publication/doi": "10.1128/mSystems.00224-20",
  "publication/tags": "- RNA degradation\n- Transcript stability\n- Motif enrichment\n- Machine learning\n- Extremely randomized trees\n- Biophysical features\n- mRNA half-life\n- Cyanobacteria\n- Gene expression\n- RNA sequencing\n- Transcriptomics\n- Bioinformatics\n- Statistical analysis\n- RNA spike-ins\n- Terminator analysis\n- mRNA decay pathways\n- Sequence-based features\n- Support vector regression\n- RNA normalization\n- Transcript classification",
  "dataset/provenance": "The dataset used in this study was derived from total RNA samples collected from three biological replicates of PCC 7002. Sampling occurred at multiple time points: before (0 min) and at 0.5, 1, 2.5, 5, 7.5, and 10 minutes after the addition of rifampin. Synthetic RNA spike-ins were added to the cell pellets immediately before lysis to normalize for potential large differences in mRNA pool sizes. The total RNA was isolated, and the rRNA content was reduced using commercial reagents. This enriched mRNA pool was then used to prepare a library for sequencing via Illumina HiSeq.\n\nThe sequencing reads were aligned to the PCC 7002 genome, and the abundance of each position was normalized to the number of RNA spike-ins in the sample. This normalization was averaged over each predicted open reading frame (ORF). The normalized abundance was then fitted to an exponential decay model to calculate the half-life of each ORF’s transcript. This process allowed for the calculation of transcript half-life values for 2,949 ORFs, which represents 91.1% of the total ORFs. The median half-life of these transcripts was found to be 0.97 minutes. The average half-life was 1.18 minutes with a standard deviation of 0.73 minutes. Additionally, 0.5% of the transcripts were classified as stable, as they retained more than 50% of the original read counts at the final time point (10 minutes).\n\nThe raw sequencing files have been deposited in NCBI’s Sequence Read Archive under the accession number SRP130967. The half-lives of transcripts and per-base-count data have been deposited at the Gene Expression Omnibus under the accession number GSE109174. These datasets are available for further analysis by the scientific community.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "The raw sequencing files have been deposited in the NCBI's Sequence Read Archive under the accession number SRP130967. This ensures that the raw data used for the analysis is publicly available for verification and further research. Additionally, the half-lives of transcripts and per-base-count data have been deposited at the Gene Expression Omnibus under the accession number GSE109174. These datasets are made available to the scientific community to facilitate reproducibility and further studies.\n\nThe data is released under the terms that allow for public access and use, adhering to the standards set by the respective databases. The deposition in these public forums ensures that the data is accessible to researchers worldwide, promoting transparency and collaboration. The enforcement of data availability is managed by the policies of the NCBI and Gene Expression Omnibus, which require proper citation and acknowledgment of the original work when using the data. This approach ensures that the data is used ethically and that the contributions of the original researchers are recognized.",
  "optimization/algorithm": "The machine-learning algorithm class used is a variant of random forest regression, specifically extremely randomized trees-based regression. This approach is not entirely new, but it introduces an additional layer of randomization compared to traditional random forest methods. In conventional random forests, decision trees are built using bootstrap sampled data sets and a random subset of features to identify the best candidate feature for splitting at each node. The extremely randomized trees approach further randomizes the process by choosing thresholds at each node randomly rather than computing the most discriminative threshold. This additional randomization helps to reduce the variance component of error for the ensemble model, although it may slightly increase the bias.\n\nThe reason this specific variant was not published in a machine-learning journal is likely because the focus of the study was on applying this method to biological data, specifically for predicting RNA half-lives in cyanobacteria. The primary goal was to understand RNA degradation processes rather than to develop new machine-learning techniques. The extremely randomized trees approach was chosen for its efficiency and effectiveness in handling high-dimensional biological data, making it suitable for the specific requirements of the study.",
  "optimization/meta": "The model employed in this work is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a variant of random forest regression, specifically extremely randomized trees-based regression. This approach builds an ensemble model by combining decision trees constructed from bootstrap sampled datasets and using a random subset of features to determine the best candidate feature for splitting at each node. Additionally, it introduces an extra layer of randomization by randomly selecting thresholds at each node, rather than calculating the most discriminative threshold at each step. This method helps to reduce the variance component of error for the ensemble model by averaging predictions across individual trees, albeit with a slight increase in bias. The optimal hyperparameters for the model, such as the number of features to consider for the best split and the minimum number of samples required to split an internal node, were determined through a grid search under 10-fold cross-validation. The number of estimators, or decision trees, was set to 100.",
  "optimization/encoding": "For the machine-learning algorithm, various types of features were encoded and pre-processed differently. Binary features were converted into Boolean values, either 0 or 1. Categorical features were transformed into binary strings using one-hot encoding. This process involves creating a binary vector for each category, where only the index corresponding to the category is set to 1, and all other indices are set to 0. Numerical features, on the other hand, were normalized using min-max scaling. This technique scales the features to a fixed range, typically between 0 and 1, by subtracting the minimum value and dividing by the range (maximum minus minimum value). This normalization ensures that all numerical features contribute equally to the model, preventing features with larger scales from dominating the learning process.",
  "optimization/parameters": "In the optimization process, two key hyperparameters were considered for the machine learning model. These parameters were `max_features` and `min_samples_split`.\n\nThe `max_features` parameter determines the fraction of features to consider when looking for the best split. The `min_samples_split` parameter specifies the minimum number of samples required to split an internal node.\n\nTo select the optimal values for these parameters, a grid search was conducted over a range of possible values. For `max_features`, the range explored was [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]. For `min_samples_split`, the range was [2, 5, 10, 50, 100, 200, 500, 1,000]. The selection process involved 10-fold cross-validation to ensure robustness and generalizability of the chosen parameters.\n\nThe optimal values identified through this process were 0.5 for `max_features` and 100 for `min_samples_split`. Additionally, the number of estimators, which refers to the number of decision trees in the ensemble, was set to 100. This configuration was found to provide a balanced trade-off between bias and variance, leading to an effective and efficient model.",
  "optimization/features": "In our study, we utilized a comprehensive set of 19 biophysical features for each of the 2,949 genes in the half-life dataset. These features encompassed a range of characteristics, from simple sequence-based attributes such as transcript length and GC content of the coding region to more complex mRNA secondary structure-based metrics like minimum free energy for folding.\n\nFeature selection was indeed performed to ensure that the most relevant features were used in our analysis. Binary features were converted into Boolean values, categorical features were transformed into binary strings using one-hot encoding, and numerical features were normalized using min-max scaling. This preprocessing step was crucial for standardizing the data and making it suitable for machine learning algorithms.\n\nThe feature selection process was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and ensure the robustness of our model. This approach helped in identifying the most important features that contribute to the prediction of transcript half-lives.\n\nAdditionally, we employed support vector regression with a linear kernel to predict half-life values. The regression coefficients derived from this model were used to assess the relative importance of each feature, providing insights into which biophysical characteristics have the most significant impact on transcript stability.",
  "optimization/fitting": "The fitting method employed in this study utilized an extremely randomized trees-based regression model, a variant of random forest regression. This approach inherently addresses the issue of overfitting by introducing additional layers of randomization. Unlike traditional random forest methods, which build decision trees on bootstrap sampled data sets and use a random subset of features to identify the best candidate feature for splitting at each node, the extremely randomized trees method goes a step further by randomly choosing thresholds at each node. This randomization helps to increase the variability among the individual decision trees, reducing the variance component of error for the ensemble model. The model's predictions are averaged across all trees, which further mitigates the risk of overfitting.\n\nTo ensure that the model was not underfitting, a grid search was conducted over a range of hyperparameters, including max_features and min_samples_split. The optimal values for these hyperparameters were determined through 10-fold cross-validation, which helps in selecting the best model that generalizes well to unseen data. The number of estimators was set to 100 decision trees, providing a robust ensemble that captures the underlying patterns in the data without being too simplistic.\n\nAdditionally, the feature importance scores were computed based on the mean decrease in impurity measures, specifically the decrease in mean squared error for the regression setting. This method ensures that the most relevant features contribute significantly to the predictive decisions, further validating the model's ability to capture essential patterns in the data. The distribution of sequence motifs in the training set was also used to determine whether a particular motif is stabilizing or destabilizing in nature, adding another layer of validation to the model's predictions.",
  "optimization/regularization": "In our study, we employed an extremely randomized trees-based regression approach, which inherently includes regularization techniques to prevent overfitting. This method builds an ensemble model by combining decision trees constructed from bootstrap sampled datasets. Additionally, it introduces an extra layer of randomization by selecting thresholds at each node randomly rather than computing the most discriminative threshold. This process helps to increase the variability among individual decision trees, thereby reducing the variance component of error for the ensemble model. While this approach slightly increases bias, it effectively mitigates overfitting by ensuring that the model does not become too complex or tailored to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are reported in the publication. Specifically, we utilized an extremely randomized trees-based regression model, a variant of random forest regression. The optimal values for hyperparameters, such as `max_features` and `min_samples_split`, were determined through a grid search over specified ranges under 10-fold cross-validation. The `max_features` value was set to 0.5, and `min_samples_split` was set to 100. The number of estimators was configured to 100 decision trees.\n\nThe feature importance scores were computed based on the mean decrease in impurity measures, which helps in understanding the contribution of each feature to the predictive model. For regression settings, the impurity metric is computed based on the decrease in mean squared error, while for classification settings, it is computed using the Gini index.\n\nRegarding the availability of model files and optimization parameters, the raw sequencing files have been deposited in NCBI’s Sequence Read Archive under the accession number SRP130967. Additionally, the half-lives of transcripts and per-base-count data have been deposited at the Gene Expression Omnibus under the accession number GSE109174. These resources are publicly available and can be accessed for further analysis or replication of the study.\n\nThe supplemental material, including figures and tables, is also available online. This material provides additional details and data that support the findings reported in the main text. The figures and tables are provided in various formats, such as EPS and XLSX files, ensuring that the data can be easily accessed and utilized by other researchers.\n\nIn summary, the hyper-parameter configurations, optimization schedule, and relevant data files are all reported and made available to the scientific community. This transparency allows for the replication and further exploration of the results presented in our study.",
  "model/interpretability": "The model employed in this study is not a black box. It utilizes an extremely randomized trees-based regression approach, which is a variant of random forest regression. This method builds an ensemble model by combining decision trees, making it inherently interpretable.\n\nThe interpretability of the model is enhanced by the computation of feature importance scores. These scores are derived from the mean decrease in impurity measures, which indicate how much each feature contributes to the predictive power of the model. In the context of regression, the impurity metric is based on the decrease in mean squared error. For each feature, the importance is determined by averaging the node importance scores across all trees in the forest. This process allows for the identification of key sequence motifs and biophysical features that influence transcript stability.\n\nAdditionally, the distribution of sequence motifs in the training set is used to determine whether a particular motif is stabilizing or destabilizing. Features that appear at the top of decision trees are considered more important, as they contribute to the predictive decisions for a larger fraction of samples. This transparency enables a clear understanding of which elements are most influential in predicting transcript half-lives.\n\nThe model's interpretability is further supported by the use of biophysical features, such as transcript length and GC content, which are normalized and analyzed using support vector regression. The regression coefficients from this analysis provide insights into the relative importance of these features. Overall, the model's design and the methods used for feature importance assessment ensure that the underlying mechanisms driving transcript stability can be clearly understood and interpreted.",
  "model/output": "The model employed in this work is a variant of random forest regression, specifically extremely randomized trees-based regression. This approach was chosen for its ability to handle high-dimensional data and provide robust predictions. The model was built using an ensemble of decision trees, with each tree constructed from bootstrap sampled datasets and a random subset of features. This randomization increases variability among the trees and speeds up the model-building process.\n\nThe optimal hyperparameters for the model were determined through a grid search under 10-fold cross-validation. The best values for `max_features` (the fraction of features to consider when looking for the best split) and `min_samples_split` (the minimum number of samples required to split an internal node) were found to be 0.5 and 100, respectively. The number of estimators, or decision trees, was set to 100.\n\nFeature importance scores were computed based on the mean decrease in impurity measures, specifically the decrease in mean squared error for the regression setting. The distribution of sequence motifs in the training set was used to determine whether a particular motif is stabilizing or destabilizing in nature. Motifs that appear higher in the decision trees are considered more important as they contribute to the predictive decision of a larger fraction of samples.\n\nThe model's predictions correlated well with experimentally measured half-lives, achieving a Spearman rank coefficient of 0.88 under 10-fold cross-validation. This indicates that the model is effective in predicting transcript half-lives based on sequence-based elements. The feature importance scores revealed several putative sequence motifs in the 5' and 3' UTRs that correlate with transcript stability, which could be used to enhance the stability of heterologous transcripts.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure its robustness and accuracy. A variant of random forest regression, known as extremely randomized trees-based regression, was employed. This approach builds an ensemble model by combining decision trees constructed from bootstrap sampled datasets and using a random subset of features to identify the best candidate feature for splitting at each node. This randomization increases variability among the individual decision trees and speeds up the model-building process.\n\nThe optimal values for hyperparameters, such as the number of features to consider when looking for the best split and the minimum number of samples required to split an internal node, were determined through a grid search over specified ranges under 10-fold cross-validation. This process helped in fine-tuning the model parameters to achieve the best performance.\n\nFeature importance scores were computed based on the mean decrease in impurity measures. The distribution of sequence motifs in the training set was used to determine whether a particular motif is stabilizing or destabilizing in nature. Features that appear at the top of a decision tree are considered more important as they contribute to the predictive decision of a larger fraction of samples. The importance of a node in a decision tree is determined by computing the decrease in impurity metric for the node, weighted by the fraction of samples arriving at that node. Feature importance scores are then computed by averaging the node importance scores pertaining to the feature and then averaging it across all trees in the forest.\n\nFor the regression setting, the impurity metric is computed based on the decrease in mean squared error, while for the classification setting, it is computed based on the Gini index. The Gini index is defined as the sum of the fractions of samples belonging to each class, squared, and subtracted from one. This index measures the impurity or uncertainty in a node, with lower values indicating higher purity.\n\nAdditionally, biophysical feature-based maximum likelihood statistical analysis was conducted. A set of 19 biophysical features, ranging from simple sequence-based features like transcript length and GC content to mRNA secondary structure-based minimum free energy for folding, were computed for each gene in the half-life dataset. These features were then used in support vector regression with a linear kernel to predict half-life values, and the values of regression coefficients were used to assess the relative importance of features.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our machine learning models and statistical analyses. For the extremely randomized trees-based regression model, we utilized 10-fold cross-validation to determine the optimal hyperparameters, which included max_features and min_samples_split. The performance of this model was assessed using the mean decrease in impurity measures, specifically the decrease in mean squared error, to compute feature importance scores. This metric is standard in regression settings and helps in understanding the contribution of each feature to the predictive power of the model.\n\nAdditionally, for the support vector regression with a linear kernel used in the biophysical feature-based maximum likelihood analysis, we assessed the relative importance of features using the values of regression coefficients. This approach allowed us to identify which biophysical features, such as transcript length and GC content, had the most significant impact on predicting transcript half-lives.\n\nThe use of these metrics is representative of common practices in the literature for evaluating regression models and feature importance. The mean squared error is a widely accepted metric for regression tasks, providing a clear indication of the model's predictive accuracy. Similarly, the use of regression coefficients to assess feature importance is a standard method in statistical and machine learning analyses, ensuring that our evaluation is both rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "The evaluation of our method includes several statistical analyses to ensure the confidence and significance of our results. We employed bootstrap sampling to account for sample size variations, which helps in assessing the robustness of our findings. This technique was used to evaluate the median half-life values of transcripts with different terminator types, providing a more reliable estimate.\n\nIn our machine learning approach, we utilized 10-fold cross-validation to determine the optimal hyperparameters for our extremely randomized trees-based regression model. This method ensures that our model's performance is consistent across different subsets of the data. The feature importance scores were computed based on the mean decrease in impurity, which is a standard metric for evaluating the significance of features in a regression setting.\n\nAdditionally, we performed statistical tests to compare the half-lives of transcripts with different terminator types. The results indicated that transcripts with L-shaped terminators had significantly longer half-lives compared to those with I-shaped terminators or no terminators. These statistical tests provide strong evidence that our method can accurately predict transcript stability based on sequence motifs and terminator types.\n\nOverall, the use of bootstrap sampling, cross-validation, and statistical significance tests enhances the confidence in our method's performance and its superiority over baseline approaches.",
  "evaluation/availability": "The raw sequencing files generated from our study have been deposited in the NCBI's Sequence Read Archive. The accession number for these files is SRP130967. Additionally, the half-lives of transcripts and per-base-count data have been made available at the Gene Expression Omnibus under the accession number GSE109174. These datasets are publicly accessible, allowing other researchers to verify our findings and potentially build upon them. The data is provided to facilitate further research and ensure the reproducibility of our results."
}