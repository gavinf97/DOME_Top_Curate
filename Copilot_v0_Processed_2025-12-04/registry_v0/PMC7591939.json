{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2020",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Bioinformatics\n- Machine Learning\n- Computational Biology\n- Metabolomics\n- Protein Analysis\n- Sequence-Based Prediction\n- Support Vector Machines\n- Drug Discovery\n- Genomics\n- Molecular Biology",
  "dataset/provenance": "The dataset used in this study consists of protein sequences, specifically focusing on vesicle transporters and non-vesicle transporters. The final training set comprises 4428 sequences, while the test set includes 1832 sequences. The dataset is composed of 2533 vesicle transporters and 9086 non-vesicle transporters in the original dataset. For the original training set, there are 2214 vesicle transporters and 7573 non-vesicle transporters. To balance the dataset, undersampling was applied to the original training set, resulting in a training set with 2214 vesicle transporters and 2214 non-vesicle transporters. The test set contains 319 vesicle transporters and 1513 non-vesicle transporters.\n\nThe dataset composition is detailed in a table, providing a clear breakdown of the sequences used for training and testing. This balanced approach ensures that the model can effectively learn to distinguish between vesicle and non-vesicle transporters. The dataset has been carefully curated to include a diverse range of sequences, which is crucial for building a robust prediction model. The use of undersampling helps to mitigate class imbalance, which is a common challenge in machine learning. This balanced dataset is essential for achieving accurate and reliable classification results.",
  "dataset/splits": "There are two main data splits: the training set and the test set. The final training set consists of 4428 sequences, while the test set contains 1832 sequences. The training set was initially composed of 2214 vesicle transporters and 7573 non-vesicle transporters. However, an undersampling technique was applied to balance the dataset, resulting in a training set with 2214 vesicle transporters and 2214 non-vesicle transporters. The test set includes 319 vesicle transporters and 1513 non-vesicle transporters.",
  "dataset/redundancy": "The dataset used in this study consists of protein sequences, specifically focusing on vesicle transporters and non-vesicle transporters. The final training set contains 4428 sequences, while the test set comprises 1832 sequences. The original dataset was split into training and test sets to ensure independence between them. The training set initially had 2214 vesicle transporters and 7573 non-vesicle transporters. To balance the dataset, undersampling was applied to the training set, resulting in an equal number of vesicle and non-vesicle transporters, each with 2214 sequences. The test set, which was not modified, contains 319 vesicle transporters and 1513 non-vesicle transporters.\n\nThis splitting ensures that the training and test sets are independent, preventing data leakage and providing a more robust evaluation of the model's performance. The distribution of vesicle and non-vesicle transporters in the test set reflects a more realistic scenario, where the number of non-vesicle transporters is significantly higher than that of vesicle transporters. This approach aligns with previously published machine learning datasets that emphasize the importance of independent training and test sets for reliable model evaluation.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used is Support Vector Machines (SVM), specifically utilizing the radial basis function (RBF) kernel. This is a well-established method in the field of machine learning and is not new. The choice of SVM with an RBF kernel is common for classification tasks due to its effectiveness in handling non-linear data.\n\nThe reason this algorithm is discussed in a computational biology journal rather than a machine-learning journal is that the focus of the study is on applying machine learning techniques to solve specific biological problems, particularly in the identification of vesicle transporters. The optimization and evaluation of the SVM parameters, such as the cost (c) and gamma (g), are tailored to this biological context. The study aims to demonstrate the practical application of SVM in bioinformatics, showcasing how dimensionality reduction techniques like MRMD can enhance the classification performance for biological data.\n\nThe use of cross-validation, specifically 5-fold cross-validation, is employed to objectively evaluate the performance of the prediction model. This method is crucial for ensuring that the model generalizes well to unseen data, which is particularly important in biological studies where data can be noisy and complex. The parameters c and g are optimized using the grid.py program in the LibSVM toolkit, automating the process of finding the best values for these parameters. This automation is essential for handling the large and complex datasets typical in bioinformatics.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model uses a single machine-learning method, specifically the LibSVM classifier, for the final prediction of vesicle transporters. The feature extraction process involves using the CTDC method from the iLearn toolkit, but this is not considered a meta-predictor approach. The training data is independent, as it is divided into training and test sets, with the training set further undergoing undersampling to balance the classes. The final prediction model is built using the LibSVM classification method after dimensionality reduction with MRMD.",
  "optimization/encoding": "The data encoding process involved extracting features from protein sequences using the CTDC method within the iLearn toolkit. This toolkit, developed by Chen et al., is a comprehensive platform that integrates various feature extraction and analysis methods. The CTDC method is part of the CTD feature extraction technique, which utilizes the first of three descriptors to encode the sequences.\n\nThe dataset consisted of 4428 sequences in the final training set and 1832 sequences in the test set. To balance the dataset, undersampling was performed on the original training set, resulting in an equal number of vesicle transporters and non-vesicle transporters. This preprocessing step ensured that the training set had 2214 vesicle transporters and 2214 non-vesicle transporters, while the test set contained 319 vesicle transporters and 1513 non-vesicle transporters.\n\nThe feature extraction process was crucial for constructing an effective prediction model. The iLearn toolkit provided a robust framework for this task, allowing for the integration of extracted features into the subsequent classification steps. The CTDC method specifically focused on capturing relevant descriptors from the protein sequences, which were then used to train the LibSVM classifier. This approach ensured that the model could accurately identify vesicle transporters based on the encoded features.",
  "optimization/parameters": "In our study, we utilized a Support Vector Machine (SVM) model with a radial basis function (RBF) kernel. The RBF kernel introduces two key parameters that need to be optimized: the cost parameter (c) and the gamma parameter (g). These parameters significantly influence the model's performance.\n\nThe selection of these parameters was not done arbitrarily. Instead, we employed a systematic approach to determine their optimal values. We allowed c and g to take values within a specified range and then varied these parameters during the training process. To evaluate the effectiveness of different combinations of c and g, we used cross-validation. This method involves dividing the dataset into multiple subsets, training the model on some subsets, and validating it on the remaining subsets. By comparing the classification accuracy across different combinations, we identified the pair of c and g that yielded the best performance.\n\nFor the dataset with all 39 features, the optimal values for c and g were found to be 2048 and 0.5, respectively. After applying dimensionality reduction using the MRMD algorithm, which reduced the feature set to 21 dimensions, the optimal parameters changed to c = 128 and g = 2. This adjustment reflects the impact of dimensionality reduction on the model's parameter requirements.\n\nThe process of parameter optimization was automated using the LibSVM toolkit, specifically the grid.py program, which streamlined the search for the best parameters. This automated approach ensured that the selected parameters were objectively determined based on the dataset's characteristics and the model's performance metrics.",
  "optimization/features": "The input features used in the study are extracted using the CTDC method from the iLearn toolkit. Initially, 39 features are extracted from the protein sequences. Feature selection is performed using the MRMD algorithm, which reduces the dimensionality of the feature set. This process involves evaluating the contribution of each feature to the classification and then selecting the most relevant features. The final prediction model uses only 21 features after dimensionality reduction. The feature selection process is conducted using the training set only, ensuring that the test set remains independent and unbiased. This approach helps in identifying the most significant features that contribute to the classification of vesicle transporters.",
  "optimization/fitting": "In our study, we employed a Support Vector Machine (SVM) with a radial basis function (RBF) kernel for classification. The SVM model involves optimizing parameters ω, b, and ξ, where ξ represents relaxation variables for each sample point. The penalty parameter C is manually set according to the actual situation to balance the trade-off between achieving a low training error and a low testing error.\n\nTo address the potential issue of overfitting, we used a soft margin approach, which allows for some misclassifications by introducing slack variables ξ. This method helps in finding a balance between maximizing the margin and minimizing the classification error. Additionally, we utilized cross-validation to tune the parameters c (penalty coefficient) and g (gamma parameter of the RBF kernel). By systematically varying these parameters and evaluating the model's performance through cross-validation, we selected the optimal values that minimized overfitting.\n\nThe parameter c controls the trade-off between achieving a low training error and a low testing error. A higher value of c increases the penalty for misclassifications, which can lead to overfitting if not properly managed. We carefully selected c and g using grid search and cross-validation to ensure that the model generalizes well to unseen data.\n\nFurthermore, we implemented dimensionality reduction using the max-relevance-max-distance (MRMD) algorithm. This algorithm evaluates the contribution of each feature to the classification and selects the most relevant features, reducing the dimensionality of the dataset from 39 to 21. By focusing on the most informative features, we mitigated the risk of overfitting and improved the model's interpretability.\n\nTo rule out underfitting, we ensured that the model had sufficient capacity to capture the underlying patterns in the data. The use of the RBF kernel allowed the model to handle non-linear relationships, and the optimization of parameters through cross-validation ensured that the model was neither too simple nor too complex. The classification accuracy of 66.84% on the training set and 71.77% on the test set indicates that the model effectively learned from the data without underfitting.\n\nIn summary, we employed a combination of soft margin SVM, parameter optimization through cross-validation, and dimensionality reduction to address both overfitting and underfitting. These techniques ensured that our model achieved a good balance between bias and variance, leading to robust and generalizable classification performance.",
  "optimization/regularization": "In our study, we employed a regularization method to prevent overfitting. Specifically, we used a soft margin approach in our support vector machine (SVM) model. This method allows for some misclassifications by introducing slack variables, which help to create a more flexible decision boundary. The penalty parameter, denoted as C, controls the trade-off between maximizing the margin and minimizing the classification error. A higher value of C increases the penalty for misclassifications, potentially leading to overfitting, while a lower value may result in underfitting. We optimized this parameter using cross-validation to find the best balance.\n\nAdditionally, we utilized the radial basis function (RBF) as the kernel function in our SVM model. The RBF kernel has a parameter, gamma (g), which influences the model's ability to fit the training data. A higher gamma value makes the model more sensitive to the training data, which can lead to overfitting. Conversely, a lower gamma value makes the model more generalized but may underfit the data. We also optimized this parameter through cross-validation to ensure an appropriate level of regularization.\n\nBy carefully tuning these parameters, we aimed to achieve a model that generalizes well to unseen data, thereby preventing overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and can be accessed through the LibSVM toolkit. Specifically, we utilized the grid.py program within the LibSVM tool folder to automatically determine the optimal parameters, which eliminates the need for manual adjustment. The optimal values for the cost (c) and gamma (g) parameters were found to be 2048 and 0.5, respectively, for the dataset without dimensionality reduction. For the dataset processed with dimensionality reduction, the optimal parameters were 128 and 2, respectively.\n\nThe experimental data, including the datasets used for training and testing, can be obtained from the GitHub repository at https://github.com/taozhy/identifying-vesicle-transport-proteins. Additionally, direct inquiries can be made to the corresponding author via email at 1765145064@qq.com.\n\nThe MRMD algorithm, which was employed for dimensionality reduction, is also available for download at https://github.com/heshida01/mrmd/tree/master/mrmdjar. This algorithm helps in evaluating the contribution of each feature to the classification process and facilitates the selection of the most relevant features.\n\nThe study was conducted using the Weka tool for classification and evaluation metrics such as recall, precision, MCC, and accuracy. The results, including the classification accuracy for both the training and test sets, are reported in the publication. The use of 5-fold cross-validation ensures that the results are objectively evaluated and that the model's performance is robust.\n\nIn summary, all necessary configurations, optimization parameters, and datasets are available either through the provided GitHub repositories or by direct contact with the authors. This ensures reproducibility and transparency in our research findings.",
  "model/interpretability": "The model employed in this study is not a black box, as it provides clear insights into the classification process. Unlike deep learning models such as convolutional neural networks (CNNs) or deep neural networks (DNNs), which often suffer from overfitting and poor interpretability, the approach used here focuses on transparency and interpretability.\n\nThe method utilized involves dimensionality reduction techniques, specifically the MRMD algorithm, which not only reduces the number of features used in classification but also ensures that the prediction effect is not negatively impacted. This reduction process helps in identifying the most relevant features for classification, making the model more interpretable.\n\nFor instance, features like charge, G2, and hydrophobicity_Casg920101.G1 were found to be crucial in recognizing vesicle transporters. These physicochemical properties play a key role in the classification process, and their importance can be clearly understood and explained. The selection of the top 21 features for classification further supports the idea that these features have significant differences between positive and negative samples, while the remaining features do not contribute as much to the classification task.\n\nMoreover, the use of classical sequence features, such as those extracted by the CTD method, provides additional interpretability. The composition descriptor, which represents the ratio of specific amino acids or sequence fragments with certain properties, helps in understanding the underlying rules in the arrangement of amino acids. This information can be useful for scholars in determining whether an unknown protein is a vesicular transporter.\n\nIn summary, the model is designed to be transparent, with clear examples of how specific features contribute to the classification of vesicle transporters. This transparency is achieved through the use of dimensionality reduction techniques and the focus on classical sequence features, making the model more interpretable compared to black-box deep learning models.",
  "model/output": "The model employed in this study is a classification model. Specifically, it utilizes the Support Vector Machine (SVM) method, which is a type of generalized linear classifier that relies on supervised learning. The primary goal of this model is to form a hyperplane in a multidimensional feature space to approximate the separation of positive and negative samples. In the context of this experiment, the positive samples represent vesicular transporters, while the negative samples represent nonvesicular transporters.\n\nThe SVM model is implemented using LibSVM, a widely-used library for support vector machines, integrated within the Weka data mining platform. This combination allows for flexible and efficient classification tasks, including the automatic performance of cross-validation during the classification process.\n\nThe model's performance is evaluated using several metrics, including recall, precision, Matthews correlation coefficient (MCC), and accuracy. These metrics provide a comprehensive assessment of the model's ability to correctly classify samples into their respective categories. The evaluation is conducted using 5-fold cross-validation, a method that involves dividing the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once.\n\nThe model's output includes the classification accuracy for both the training and test sets, as well as the number of misclassified samples. Additionally, the model provides insights into the contribution of each feature to the classification process, highlighting which features have the greatest impact on distinguishing between vesicular and nonvesicular transporters. This information is crucial for understanding the underlying mechanisms that differentiate these two types of transporters and for improving the model's predictive accuracy.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the support vector machine (SVM) library used in this study, known as LibSVM, is publicly available. It was developed by Professor Lin and his team in 2001 and can be accessed at https://www.csie.ntu.edu.tw/~cjlin/. This library is open-source, allowing for easy expansion and modification. It is widely used in bioinformatics and has the advantages of being a small, flexible program with fewer input parameters.\n\nAdditionally, the Weka platform, which supports LibSVM classification since version 3.5, is a free and non-commercial mining platform. Weka provides a series of functional modules that meet various needs in data analysis, including different classification and regression algorithms and performing cross-validation during classification automatically. Weka is also open-source and can be accessed freely for use in research and development.\n\nFor running the algorithm, users can utilize the LibSVM library within the Weka platform. This integration allows for seamless execution of SVM-based classification tasks. The combination of LibSVM and Weka provides a robust and accessible toolset for researchers in the field of bioinformatics and beyond.",
  "evaluation/method": "To evaluate the experimental results objectively, cross-validation (CV) was adopted. This is a classic, analytical method used to judge the performance of a prediction model. The core idea involves using most of the samples in a given dataset to build a classification model, while a small portion of the samples is set aside for prediction. The forecast errors of these small samples are then calculated and their sum of squares is recorded. This process continues until all samples have been predicted once and only once.\n\nThere are three common CV methods: the hold-out method, K-fold cross-validation (K-CV), and leave-one-out cross-validation (LOO-CV). The K-fold cross-validation approach was chosen. In this method, the initial data is divided into k groups of subdatasets. One group is retained as the validation data, while the remaining k-1 subdatasets are used for training. This results in k models, each taking into account the prediction results of the classifier. Typically, the average value of each index from the k models is calculated to evaluate the classification results.\n\nThe value of K can be set according to the actual situation, and in this case, it was set to 5. After performing 5-fold cross-validation, several indexes were used to evaluate the classification results. The metrics employed include recall, precision, Matthews correlation coefficient (MCC), and accuracy. These metrics provide a comprehensive evaluation of the model's performance.",
  "evaluation/measure": "In our evaluation of classification results, we employed several key performance metrics to objectively assess the effectiveness of our prediction model. These metrics include recall, precision, accuracy, and the Matthews correlation coefficient (MCC). These metrics are widely recognized and used in the literature for evaluating classification models, ensuring that our evaluation is both comprehensive and comparable to other studies in the field.\n\nRecall, also known as sensitivity, measures the proportion of actual positives that are correctly identified by the model. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). Precision, on the other hand, assesses the proportion of predicted positives that are actually correct. It is determined by the ratio of true positives to the sum of true positives and false positives (FP). Accuracy provides an overall measure of the model's performance by calculating the proportion of correctly predicted instances (both true positives and true negatives) out of the total number of instances. Finally, the Matthews correlation coefficient (MCC) offers a balanced measure that takes into account all four categories of the confusion matrix (TP, TN, FP, FN), providing a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation.\n\nBy reporting these metrics, we aim to provide a thorough evaluation of our model's performance, highlighting its strengths and areas for potential improvement. These metrics are not only representative of standard practices in the literature but also offer a nuanced understanding of the model's predictive capabilities.",
  "evaluation/comparison": "Not applicable.",
  "evaluation/confidence": "The evaluation of our classification results was conducted using cross-validation, specifically the K-fold cross-validation method. This approach ensures that the performance metrics are robust and not dependent on a particular split of the data. We used K=5, which means the data was divided into five subsets, and the model was trained and tested five times, each time using a different subset as the test set and the remaining four as the training set.\n\nThe performance metrics we reported, including recall, precision, Matthews correlation coefficient (MCC), and accuracy, were calculated for each of the five folds. The final values presented are the averages of these metrics across all folds. This averaging process helps to mitigate the variability that can occur with a single train-test split and provides a more reliable estimate of the model's performance.\n\nHowever, while we reported the average values of these metrics, we did not provide confidence intervals. Confidence intervals would give a range within which the true performance metric is likely to fall, providing additional insight into the reliability of our results. Without confidence intervals, it is more challenging to assess the statistical significance of our findings.\n\nIn terms of statistical significance, we did not explicitly compare our method to other baselines using statistical tests. To claim that our method is superior, future work could include statistical tests such as paired t-tests or Wilcoxon signed-rank tests to compare the performance metrics of our method against those of other baselines. This would provide stronger evidence of the superiority of our approach.\n\nIn summary, while our evaluation provides a robust estimate of our model's performance, the lack of confidence intervals and statistical comparisons to baselines means that the confidence in our results is somewhat limited. Future work should address these aspects to provide a more comprehensive evaluation.",
  "evaluation/availability": "Not enough information is available."
}