{
  "publication/title": "A new efficient method to detect genetic interactions for lung cancer GWAS",
  "publication/authors": "The authors who contributed to this article are:\n\n- J. Luyapan\n- C. I. Amador\n- J. Gui\n- X. Jia\n- T. A. M.  Marsit\n- S. Liu\n- X. Xu\n- D. Zhang\n- E. J. Duell\n- D. C. Christiani\n- M. B. Schottenfeld\n- S. M. Adams\n- S. Zhang\n- H. Brenner\n- O. Mikkelsen\n- M. D. Thun\n- J. Gui\n\nJ. Gui and C. I. Amador contributed to the project design and oversight. J. Gui contributed to the method development. J. Gui and J. Luyapan contributed to the implementation and benchmarking. J. Luyapan and X. Jia wrote the manuscript. J. Gui, C. I. Amador, J. Luyapan, X. Jia, T. A. M. Marsit, and S. Liu contributed to the data analysis and discussions. X. Jia, X. Xu, and J. Luyapan contributed to the genetic annotation of identified genes. J. Gui, C. I. Amador, and J. Luyapan contributed to the data interpretation. J. Luyapan, X. Jia, S. Liu, X. Xu, D. Zhang, E. J. Duell, D. C. Christiani, M. B. Schottenfeld, S. M. Adams, S. Zhang, H. Brenner, O. Mikkelsen, M. D. Thun, T. A. M. Marsit, C. I. Amador, and J. Gui contributed to the data preparation, manuscript editing, and discussion. All authors read and approved the final manuscript.",
  "publication/journal": "BMC Medical Genomics",
  "publication/year": "2020",
  "publication/doi": "10.1186/s12920-020-00807-9",
  "publication/tags": "- Genetic interactions\n- Machine learning\n- Genome-wide association study\n- Lung cancer\n- Survival analysis\n- Single nucleotide polymorphism\n- Multifactor Dimensionality Reduction\n- Computational efficiency\n- Age of disease-onset\n- Martingale Residuals",
  "dataset/provenance": "The dataset used in this study is sourced from the OncoArray-TRICL Consortium, a population-based study that includes individuals from various regions across North America, Europe, and Asia. The genotyping was performed using the Illumina OncoArray-500K BeadChip Platform, which covers a genome-wide backbone and specific loci known to be associated with cancer phenotypes.\n\nThe dataset consists of 533,631 single nucleotide polymorphisms (SNPs) from 57,775 individuals. However, after applying quality control measures, the study focused on 14,935 lung cancer cases and 12,787 controls. These quality control steps excluded participants lacking critical information such as lung cancer status, smoking status, age, and gender, as well as those with non-European ancestry, low-quality DNA, or low genotype call rates.\n\nThe OncoArray-TRICL data has been utilized in previous research, and the quality control procedures followed in this study are consistent with those described in earlier publications. The dataset is part of a broader effort to identify genetic variants associated with lung cancer, leveraging a large and diverse population to enhance the robustness and generalizability of the findings.",
  "dataset/splits": "In our study, we utilized two-fold cross-validation for dataset splits. This approach was chosen to ensure that there was no overlap between training sets, and that all predicted values were independent of each other. Each fold contained an equal number of data points, with the sample sizes varying across different simulations. Specifically, we generated models with sample sizes of 400, 800, and 1600. For each sample size, we created 70 models with varying allele-heritability frequency combinations, which were replicated five times. This resulted in a total of 7,000 data sets across all simulations. The data sets were split into training and testing sets, each containing an equal number of data points to maintain the independence and robustness of our analysis.",
  "dataset/redundancy": "In our study, we employed a two-fold cross-validation approach to evaluate the optimal one-way and two-way interaction models. This method was chosen to ensure that the training folds were mutually independent with no overlap, which is crucial for hypothesis testing. The rationale behind using two-fold cross-validation is that it allows for the training and testing sets to be completely independent, thereby avoiding any bias that might arise from overlapping data.\n\nThe two-fold cross-validation process involved splitting the dataset into two parts. The model was trained on one part and tested on the other, and this process was repeated with the roles of the two parts reversed. This ensured that each data point was used for both training and testing, but never for both in the same iteration, maintaining the independence of the training and test sets.\n\nThis approach is different from the ten-fold cross-validation method used in some previous studies, which can introduce extra variation due to overlapping training sets. By using two-fold cross-validation, we aimed to achieve a more robust and unbiased evaluation of our models.\n\nThe distribution of our datasets, particularly the OncoArray-TRICL dataset, is notable for its large size and the diversity of the population studied. The dataset includes 533,631 SNPs from 57,775 individuals across multiple studies in North America, Europe, and Asia. After rigorous quality control measures, 14,935 lung cancer cases and 12,787 controls were included in the final analysis. This extensive dataset allowed for a comprehensive analysis of genetic interactions related to lung cancer, providing a robust foundation for our predictive models.",
  "dataset/availability": "The datasets generated and analyzed during the current study are publicly available in the NCBI dbGaP repository. The specific study accession number is phs001273.v3.p2. This repository can be accessed via the provided website link. The data is released under the Creative Commons Public Domain Dedication waiver, which allows for broad use and distribution. This waiver ensures that the data can be freely used, modified, and shared by anyone, subject to proper citation. The enforcement of this waiver is managed through the terms specified in the waiver itself, ensuring compliance with open access principles.",
  "optimization/algorithm": "The optimization algorithm presented in this study is centered around the Efficient Survival Multifactor Dimensionality Reduction (ES-MDR) method. This approach falls under the class of machine learning algorithms known as Multifactor Dimensionality Reduction (MDR) methods. MDR methods are designed to handle high-dimensional genetic data by reducing multiple genetic loci into a single variable, facilitating the classification of genotypes into high-risk and low-risk categories.\n\nThe ES-MDR algorithm is a novel extension of the traditional MDR method. It was developed to address specific challenges in analyzing survival data, particularly in the context of genetic interactions associated with the age of disease onset. The primary motivation for creating ES-MDR was to improve computational efficiency and to allow for the adjustment of covariates, which is crucial for controlling confounding factors in genetic studies.\n\nThe decision to publish this work in a genomics journal rather than a machine-learning journal is driven by the specific application and context of the research. The focus of this study is on the application of machine learning techniques to genetic data, particularly in the context of lung cancer research. The ES-MDR method was developed to meet the unique needs of genetic epidemiology, where the ability to handle survival data and adjust for covariates is essential. Therefore, the publication in a genomics journal ensures that the method is presented within the relevant scientific community and highlights its practical applications in genetic research.",
  "optimization/meta": "The model leverages predictive scores from various machine-learning algorithms as input. Specifically, it utilizes the Cox Lasso regression to identify tuning parameters and then evaluates the predictive performance of different models, including those that incorporate smoking status and various numbers of single nucleotide polymorphisms (SNPs). The models assessed include smoking only and smoking plus 2 SNPs, 4 SNPs, 13 SNPs, 19 SNPs, 29 SNPs, and 183 SNPs.\n\nThe meta-predictor approach involves using the estimated area under the curve (AUC) from these models to determine their predictive performance. The AUCs are averaged over the OncoArray-TRICL data using predictive scores from an independent left-out test dataset. This ensures that the training data for each model is independent, as the test data is not used in the training process.\n\nThe machine-learning methods constituting the whole include Cox Lasso regression for variable selection and model tuning, as well as the evaluation of predictive performance using AUC. The models are generated using the ES-MDR method, which is designed to capture non-linear and high-order interactions for time-to-event analysis. The use of two-fold cross-validation further ensures that the training folds are mutually independent with no overlap, enhancing the reliability of the model selection process.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the efficiency and accuracy of our machine-learning algorithm, specifically the Efficient Survival Multifactor Dimensionality Reduction (ES-MDR) method. We began by selecting a subset of K single nucleotide polymorphisms (SNPs) from the entire dataset. For each combination of these K SNPs, we created a contingency table that captured all possible genotype combinations.\n\nTo handle the survival data, we utilized Martingale Residuals as a continuous score. This involved replacing the event time and status with Martingale Residuals, which were adjusted for confounding covariates. This step was essential for transforming the survival data into a format suitable for analysis with the Quantitative MDR (QMDR) algorithm, which is designed for continuous outcomes.\n\nIn the contingency table, each cell represented a specific multi-locus genotype combination. We then summed the Martingale Residuals for samples with and without each genotype combination. Cells with a positive sum of Martingale Residuals were labeled as \"high-risk,\" while those with a negative sum were labeled as \"low-risk.\" This labeling process effectively reduced the high-dimensional genetic data into a one-dimensional binary variable, making it easier to classify individuals into high and low-risk groups.\n\nThe use of Martingale Residuals allowed us to incorporate survival analysis into the QMDR framework, enabling us to adjust for covariates and improve the computational efficiency of the algorithm. This approach provided a reliable and efficient method for identifying genetic interactions associated with survival outcomes, particularly in the context of age-of-onset for diseases like lung cancer.",
  "optimization/parameters": "In our study, we explored various models with different numbers of single nucleotide polymorphisms (SNPs) as input parameters. Specifically, we examined models that included smoking only and smoking plus 2 SNPs, 4 SNPs, 13 SNPs, 19 SNPs, 29 SNPs, and 183 SNPs. The selection of these parameters was guided by the goal of assessing the predictive performance of our models in relation to lung cancer onset.\n\nThe choice of the number of SNPs was influenced by the need to balance model complexity and predictive power. We found that models with more SNPs generally showed better predictive performance, as indicated by the area under the curve (AUC) estimates. However, the AUC for models with more SNPs tended to plateau for ages between 40 and 80 years, suggesting that a moderate number of SNPs might be sufficient for accurate predictions within this age range.\n\nThe selection of the number of SNPs was also informed by the results of our simulations. In simulation I, we determined that the type I error rate converged to 5% with sample sizes greater than approximately 12,800. This finding guided our choice of sample sizes in subsequent analyses. In simulation II, we estimated the power of our models using a dataset that included quantitative outcome variables and a pair of functional interacting SNPs along with 18 non-interacting SNPs. This simulation helped us to understand the performance of our models under different conditions and to select the optimal number of SNPs for our analyses.\n\nOverall, the number of SNPs used in our models was selected based on a combination of empirical evidence from our simulations and the need to achieve a balance between model complexity and predictive accuracy.",
  "optimization/features": "In our study, we utilized a range of input features to predict lung cancer onset. Initially, we considered a large set of single nucleotide polymorphisms (SNPs) from the OncoArray-TRICL data, which included 533,631 SNPs. However, we performed feature selection to filter out non-significant and highly correlated SNPs. This process was crucial, especially given that our sample size was smaller than the number of SNP predictors.\n\nWe employed the Lasso regression method for feature selection, which is similar to the forward stepwise method. Lasso provides coefficient shrinkage and variable selection by driving nonsignificant coefficients to zero. This approach helped us to identify SNPs that were not associated with the outcome or were highly correlated with other SNPs.\n\nThe feature selection was conducted using the training set only, ensuring that the testing set remained independent for evaluating the model's performance. This step was essential to avoid overfitting and to ensure that our models generalized well to new data.\n\nAfter feature selection, we examined the predictive performance of models with various numbers of SNPs, ranging from smoking only to smoking plus 2 SNPs, 4 SNPs, 13 SNPs, 19 SNPs, 29 SNPs, and up to 183 SNPs. This allowed us to assess how the inclusion of different numbers of SNPs affected the model's predictive accuracy.\n\nIn summary, we started with a large set of SNPs and performed feature selection using Lasso regression on the training set. This process helped us to identify the most relevant SNPs for predicting lung cancer onset, ensuring that our models were robust and generalizable.",
  "optimization/fitting": "The fitting method employed in this study involved a two-fold cross-validation approach, which was chosen to ensure that the training folds were mutually independent with no overlap. This method was used to evaluate the optimal one-way and two-way interaction models, as well as the overall best model.\n\nThe use of two-fold cross-validation helped to mitigate the risk of over-fitting. By ensuring that the training and testing sets were independent, the model's performance could be more reliably assessed. This approach also helped to reduce the extra variation introduced by overlapping training sets, which can lead to a slight right skew in the empirical distributions of testing scores.\n\nTo further guard against over-fitting, a threshold was set using the 95th percentile of the testing score from the null models. This threshold helped to remove any non-significant findings and ensured that the results were statistically significant at the 0.05 level.\n\nThe study also addressed the potential for under-fitting by using a sufficiently large sample size. The central limit theorem was applied, assuming a large sample size (n > 50) from a population with a finite level of variance. This allowed for the expectation that the testing scores with 400 samples from the simulation study would follow a standard normal distribution.\n\nAdditionally, the power of the proposed method was estimated by running it on each of the 7000 data sets and searching for the best model over all possible one-, two-, and three-way interaction models. This comprehensive approach helped to ensure that the model was not under-fitted and could accurately capture the interactions between SNPs.\n\nIn summary, the fitting method used in this study involved a two-fold cross-validation approach, a threshold set using the 95th percentile of the testing score from the null models, and a sufficiently large sample size. These measures helped to rule out both over-fitting and under-fitting, ensuring the reliability and accuracy of the results.",
  "optimization/regularization": "In our study, we employed regularization techniques to prevent overfitting and enhance the model's predictive performance. Specifically, we utilized the Lasso (Least Absolute Shrinkage and Selection Operator) method. Lasso is particularly useful in scenarios where the number of predictors exceeds the sample size, as it performs both variable selection and regularization by shrinking the coefficients of less important variables to zero. This approach helps in filtering out single nucleotide polymorphisms (SNPs) that are not significantly associated with the outcome or are highly correlated with other SNPs. By doing so, Lasso aids in creating a more parsimonious model that generalizes better to new data.\n\nAdditionally, we incorporated cross-validation techniques to further mitigate overfitting. We opted for two-fold cross-validation instead of the more common ten-fold cross-validation. This choice was driven by the need to ensure that the training folds were mutually independent with no overlap, which is crucial for reliable hypothesis testing. The two-fold cross-validation helped in evaluating the optimal one-way and two-way interaction models, as well as the overall best model, by providing a more robust estimate of model performance.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are not explicitly detailed in the publication. However, the methods and models employed, such as ES-MDR and Cox Lasso regression, are described in sufficient detail to allow replication of the analyses. The datasets generated and analyzed during the current study are available in the NCBI dbGaP repository, under the study accession phs001273.v3.p2. This repository provides access to the data used for our analyses, which can be utilized to reproduce the results and explore further optimizations.\n\nThe specific configurations and parameters for the models, such as the tuning parameters identified from Cox Lasso regression and the settings for ES-MDR, are not provided in a separate file or repository. Instead, the descriptions within the publication offer insights into the methodological approaches taken. For instance, the use of two-fold cross-validation for evaluating optimal one-way and two-way interaction models is mentioned, which is a crucial aspect of the optimization process.\n\nWhile the exact hyper-parameter configurations and optimization schedules are not made available in a downloadable format, the methodological framework is thoroughly explained. This includes the use of Martingale Residuals in the ES-MDR method and the generation of simulated data sets for power estimation. Researchers interested in replicating or building upon our work can refer to these detailed descriptions to guide their own optimizations.\n\nFor those seeking to access the data and replicate the analyses, the NCBI dbGaP repository is the primary resource. The data is made available under the terms and conditions set by the repository, which typically include requirements for ethical approval and data usage agreements. This ensures that the data is used responsibly and in accordance with the guidelines set by the contributing institutions and ethical review boards.",
  "model/interpretability": "The model we developed, ES-MDR, is designed to be more interpretable compared to traditional black-box models. It focuses on identifying high-order genetic interactions, which are crucial for understanding complex diseases like lung cancer. Unlike black-box models that provide predictions without clear insights into the underlying mechanisms, ES-MDR explicitly identifies combinations of single nucleotide polymorphisms (SNPs) that contribute to disease risk.\n\nOne of the key features of ES-MDR is its use of Martingale Residuals to classify multi-locus genotype combinations into high-risk and low-risk groups. This approach allows us to directly observe how different combinations of SNPs influence the hazard rate, making the model's decisions more transparent. For instance, if a particular combination of SNPs is consistently associated with a higher hazard rate, ES-MDR will classify individuals with this combination as high-risk. This provides a clear, interpretable link between genetic factors and disease outcomes.\n\nAdditionally, ES-MDR uses a T-statistic testing score to identify the best models, which include the most significant interacting SNPs. This statistical measure helps in ranking the importance of different SNP interactions, further enhancing the model's interpretability. For example, in our study, we identified top one-way models associated with lung cancer risk, such as SNPs rs12358150 and rs149743903, which had significant log-rank test scores and hazard ratios. These findings not only improve our understanding of the genetic basis of lung cancer but also provide actionable insights for clinical applications.\n\nMoreover, the use of time-dependent receiver operating characteristic (ROC) curves and area under the curve (AUC) allows us to evaluate the predictive performance of our models at different age intervals. This temporal aspect of the model adds another layer of interpretability, as it shows how the predictive power of the model changes with age. For example, we observed that the AUC for models with more SNPs increased with age, indicating that these models are more effective in predicting lung cancer onset in older individuals.\n\nIn summary, ES-MDR is a transparent model that provides clear insights into the genetic interactions associated with disease risk. By using Martingale Residuals, T-statistic testing scores, and time-dependent ROC curves, we can identify and interpret the most significant SNP interactions, making the model's decisions more understandable and actionable.",
  "model/output": "The model discussed in this publication is primarily focused on survival analysis, which is a type of regression model. It aims to predict the time to an event, specifically the onset of lung cancer, based on genetic factors and other covariates like smoking status. The model uses the Cox proportional hazards (Cox ph) model to generate survival times and incorporates Martingale Residuals to classify genotype combinations into high-risk and low-risk groups. The output of the model includes predictive scores that are used to estimate the area under the curve (AUC) for different age intervals, indicating the model's predictive performance. Additionally, Kaplan-Meier plots are generated to visualize the differences in the age of lung cancer onset between high-risk and low-risk groups based on identified SNPs. The model's performance is evaluated using time-dependent receiver operating characteristic (ROC) curves and AUC, which measure the predictability of time-to-event at specific times.",
  "model/duration": "The execution time for the models was compared between two methods: ES-MDR and Surv-MDR. For 100 simulated data sets, with one-, two-, and three-way interactions, and using ten-fold cross-validation, the computing time for Surv-MDR was significantly longer at 734.5 minutes. In contrast, ES-MDR completed the same tasks in just 2.25 minutes. Both methods were run on a high-performance computing cluster called Discovery, which features AMD 3.1 GHz CPUs and 64 GB of memory. This cluster is part of a larger system with 160 computing nodes, over 3000 cores, and 12.5 TB of memory, available to the Dartmouth research community. The substantial difference in execution time highlights the efficiency of ES-MDR, making it a more practical choice for large-scale genetic interaction analyses.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved a comprehensive simulation study designed to assess the performance and accuracy of the ES-MDR algorithm. The study focused on evaluating the null distribution of the testing score to determine the type I error rate and to analyze the power of the method.\n\nTwo simulations were conducted. The first simulation aimed to estimate the 5% type I error threshold by evaluating an empirical null distribution with independent non-interacting SNPs and quantitative outcome values. This involved creating sets of SNPs with varying numbers and sample sizes, ensuring that there were no associations between the SNPs and the outcome. The simulation generated 24,000 data sets to analyze the percentage of times that ES-MDR randomly identified two interacting SNPs from a null data set.\n\nThe second simulation was used to compare the power of ES-MDR with Surv-MDR. The power was estimated as the percentage of time ES-MDR correctly included the two functional interacting SNPs in the best model out of each set of 7000 data sets. A significant threshold for the results was set at the 0.05 level.\n\nFor the evaluation, a two-fold cross-validation method was employed. This method was chosen because it ensures that there is no overlap between training sets, making all predicted values independent of each other. The best model was selected based on the smallest prediction error and the largest consistency in including the two functional interacting SNPs.\n\nAdditionally, the 95th percentile of the testing score from the null models was used as a threshold to guard against any non-significant findings. This rigorous evaluation process ensured that the method's performance was thoroughly assessed and validated.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models in predicting lung cancer onset. One of the primary metrics used was the Area Under the Curve (AUC) of the time-dependent Receiver Operating Characteristic (ROC) curve. The ROC curve plots sensitivity against 1-specificity at various threshold settings, and the AUC provides a single scalar value that summarizes the overall ability of the model to discriminate between high-risk and low-risk groups across different time points. A larger AUC indicates better model performance in predicting the time-to-event.\n\nWe also utilized the Kaplan-Meier method to generate survival plots, which visually depicted the differences in the age of lung cancer onset between high-risk and low-risk groups based on the top identified SNPs associated with lung cancer risk. This method is widely used in survival analysis and provides a clear visual representation of the survival probabilities over time.\n\nAdditionally, we adjusted for additional factors related to patient survival using the Cox proportional hazards regression model, which included smoking status as a covariate. This adjustment helped to control for confounding variables and provided a more accurate assessment of the model's performance.\n\nThe use of these metrics is representative of standard practices in the literature for evaluating predictive models in survival analysis. The AUC and ROC curve are commonly used to assess the discriminative ability of models, while the Kaplan-Meier method and Cox regression are standard tools for survival analysis. Together, these metrics provide a comprehensive evaluation of our models' performance in predicting lung cancer onset.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed method, ES-MDR, with existing methods, particularly Surv-MDR. This comparison was essential to demonstrate the strengths and improvements of ES-MDR over previous approaches.\n\nWe performed simulations to evaluate the performance of ES-MDR against Surv-MDR. These simulations included generating datasets with varying numbers of SNPs and sample sizes to assess how well ES-MDR could identify interacting SNPs compared to Surv-MDR. The simulations were designed to test the type I error rate and power of ES-MDR, ensuring that it did not inflate the type I error rate while maintaining or improving power.\n\nIn addition to Surv-MDR, we also compared ES-MDR to simpler baselines, such as the Cox proportional hazards model. This comparison helped us understand the advantages of ES-MDR in handling high-order interactions and non-linear relationships in survival data. The Cox model, while widely used, does not inherently capture interactions between multiple SNPs, making it a suitable baseline for comparison.\n\nOur results showed that ES-MDR outperformed Surv-MDR in terms of power and efficiency, especially when dealing with high-order interactions. The use of Martingale Residuals in ES-MDR provided a more efficient way to classify genotype combinations into high-risk and low-risk groups, which is crucial for survival analysis.\n\nOverall, the comparison to publicly available methods and simpler baselines demonstrated that ES-MDR is a robust and efficient tool for identifying genetic interactions in survival data. This comparison was crucial in validating the improvements and unique advantages of ES-MDR over existing methods.",
  "evaluation/confidence": "In our study, we employed rigorous statistical methods to evaluate the performance of our models. We utilized two-fold cross-validation to assess the optimal one-way and two-way interaction models, ensuring that the training folds were mutually independent with no overlap. This approach helped in maintaining the integrity of our hypothesis testing.\n\nWe also conducted simulation studies to evaluate the type I error rate and power of our method. In Simulation I, we determined that the type I error rate was close to the expected value of 0.05 when there were no SNP interaction effects. The null distributions for the one- and two-way models followed the normal distribution quite closely, indicating the reliability of our testing scores. For the three-way model, although there was a slight right skew, the upper right tail regions overlapped with the normal distribution, making the 95th quantile a suitable threshold for removing false positives.\n\nIn Simulation II, we estimated the power of our method using a dataset that included quantitative outcome variables and a pair of functional interacting SNPs. The results showed that our method had a high power to detect true interactions, further validating its effectiveness.\n\nAdditionally, we used the area under the curve (AUC) to evaluate the predictive performance of our models. The AUC estimates for each model were plotted, and we observed that the AUC peaked around age-of-onset less than 30 and greater than 90 years old. This was likely due to the limited number of lung cancer cases at these ages. However, for the 40â€“80 years old range, the AUC averages plateaued, indicating good estimates for age of onset for all models, which was likely due to the larger sample size for evaluation.\n\nOverall, our performance metrics included confidence intervals and statistical significance tests, ensuring that our claims about the superiority of our method over others and baselines are robust and reliable.",
  "evaluation/availability": "The datasets generated and analyzed during the current study are available in the NCBI dbGaP repository. The specific study accession is phs001273.v3.p2. This repository can be accessed via the provided website link. The data made available in this article is under the Creative Commons Public Domain Dedication waiver, unless otherwise stated in a credit line to the data. This waiver allows for the free use and distribution of the data, facilitating further research and validation of the findings presented in the study."
}