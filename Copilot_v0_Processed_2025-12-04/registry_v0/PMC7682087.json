{
  "publication/title": "Mal-Prec: computational prediction of protein Malonylation sites via machine learning based feature integration",
  "publication/authors": "The authors who contributed to the article are:\n\n- **Xin Liu**: Proposed the core ideas of the project and contributed to the writing of the manuscript.\n- **Liang Wang**: Collected and processed the data, performed the experiments, and contributed to the writing of the manuscript.\n- **Jian Li**: Not sure.\n- **Junfeng Hu**: Not sure.\n- **Xiao Zhang**: Proposed the core ideas of the project.",
  "publication/journal": "BMC Genomics",
  "publication/year": "2020",
  "publication/doi": "10.1186/s12864-020-07166-w",
  "publication/tags": "- Post-translational modification\n- Malonylation\n- Machine learning\n- Principal component analysis\n- Support vector machine\n- Protein sequence analysis\n- Bioinformatics\n- Computational biology\n- Lysine malonylation\n- Predictive modeling\n- Feature extraction\n- Sequence-based prediction\n- Protein modification\n- Data preprocessing\n- Model optimization",
  "dataset/provenance": "The dataset used in this study was collected from literature and NCBI websites. A total of 1768 sequence fragments from 934 human proteins were initially gathered. To ensure the quality of the data, redundancy was reduced by employing CD-HIT to remove sequences with equal to or more than 40% similarity. This process resulted in a dataset consisting of 1735 sequence fragments from 931 human proteins, which were selected as the positive dataset. The negative dataset was constituted by sequence fragments around lysine that were not included in the positive dataset, resulting in 45,607 negative samples. Due to the imbalance in the dataset, a down-sampling method was used to construct a balanced dataset, which contains 3470 sets. Additionally, 20% of the dataset, amounting to 695 samples, was set aside as an independent dataset to validate the performance of the predictor, while the remaining 2775 samples were used as the training dataset.",
  "dataset/splits": "The dataset was divided into two main splits: the training dataset and the independent dataset. The independent dataset consisted of 695 data points, which accounted for 20% of the total dataset. The remaining 80% of the dataset, totaling 2775 data points, was used as the training dataset. The training dataset was further divided using a 5-fold cross-validation approach, where the data was randomly split into five subsets of roughly equal size. In each fold, one subset was used as the test set, and the remaining four subsets were used to train the classifier. This process was repeated five times, with each subset serving as the test set once. The distribution of data points in each fold was approximately equal, ensuring a balanced evaluation of the model's performance.",
  "dataset/redundancy": "The dataset used in this study was collected from literature and consisted of 1768 sequence fragments from 934 human proteins. To ensure the quality and reliability of the dataset, redundancy was reduced by employing CD-HIT to remove sequences with 40% or more similarity. This step was crucial to avoid artificial bias and to ensure that the model generalizes well to unseen data.\n\nAfter preprocessing, the sequences were truncated into 17-residue long segments with lysine (K) located at the center. This resulted in 1735 sequence fragments from 931 human proteins being selected as the positive dataset. The negative dataset was constituted by sequence fragments around lysine that were not included in the positive dataset, resulting in 45,607 negative samples. To address the imbalance in the dataset, a down-sampling method was used to construct a balanced dataset containing 3470 sets of data, with an equal number of positive and negative samples.\n\nTo validate the performance of the predictor, the dataset was split into an independent dataset and a training dataset. Specifically, 20% of the dataset (695 samples) was set aside as the independent dataset, while the remaining 80% (2775 samples) was used as the training dataset. This split ensures that the training and test sets are independent, providing an unbiased evaluation of the model's performance.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field. The use of a balanced dataset and the independent test set helps to mitigate overfitting and ensures that the model's performance is robust and generalizable. The preprocessing steps, including redundancy reduction and sequence truncation, are standard practices in the field and help to ensure the quality and reliability of the dataset.",
  "dataset/availability": "The data used in this study were collected from literature and NCBI websites. The dataset consists of 1735 sequence fragments from 931 human proteins as the positive dataset, and 45,607 negative samples were constituted from lysine residues not included in the positive dataset. To address the imbalance in the dataset, a down-sampling method was employed, resulting in a balanced dataset containing 3470 sets of data, with an equal number of positive and negative samples.\n\nThe dataset was split into training and independent datasets. 20% of the dataset, comprising 695 samples, was set aside as the independent dataset for validation purposes. The remaining 80%, totaling 2775 samples, was used as the training dataset.\n\nThe specific details about the public availability of the dataset, including the data splits used, are not provided. Therefore, it is not clear whether the data is released in a public forum or under what license. Additionally, there is no information on how the enforcement of data availability was ensured.",
  "optimization/algorithm": "The machine-learning algorithm class used is Support Vector Machine (SVM). This is a well-established and widely used algorithm in the field of machine learning, particularly for classification tasks.\n\nThe SVM algorithm employed is not new. It is a classical algorithm that has been extensively studied and applied in various domains, including bioinformatics.\n\nThe reason it was not published in a machine-learning journal is that the focus of the study is on the application of SVM in predicting malonylation sites in human proteins, rather than on the development of a new machine-learning algorithm. The research highlights the effectiveness of SVM in combination with principal component analysis (PCA) and specific feature representations (CKSAAP, AAindex, and one-hot encoding) for this particular biological problem. The innovation lies in the application and optimization of these techniques for the specific task of malonylation site prediction, rather than in the creation of a new algorithm.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the dataset for the machine-learning algorithm. We began by collecting sequence fragments from human proteins, resulting in a total of 1768 fragments from 934 proteins. To minimize redundancy and avoid bias, we employed the CD-HIT tool to remove sequences with 30% or more similarity.\n\nFor feature representation, we utilized three distinct methods: CKSAAP, AAindex, and one-hot encoding. CKSAAP (Composition of k-spaced amino acid pairs) captures the pairwise amino acid composition within a peptide, providing insights into local sequence patterns. AAindex represents the physicochemical properties of amino acids, offering a more abstract view of the peptide's characteristics. One-hot encoding, also known as binary encoding, transforms amino acids into orthogonal numeric vectors, allowing for straightforward machine learning applications.\n\nTo create the dataset, we applied a sliding window approach to select peptides of length 17, with lysine positioned at the center. We constructed balanced positive and negative datasets using a down-sampling method, ensuring equal quantities in each set.\n\nGiven the high dimensionality of the combined features, we employed Principal Component Analysis (PCA) for dimensionality reduction. This step was essential to mitigate the curse of dimensionality and enhance the algorithm's performance. Through cross-validation, we determined that reducing the dimensions to 100 yielded the best results, with significant improvements in accuracy, sensitivity, specificity, F1 score, and Matthews correlation coefficient (MCC).\n\nIn summary, our data encoding and preprocessing pipeline involved sequence collection, redundancy reduction, feature representation using CKSAAP, AAindex, and one-hot encoding, and dimensionality reduction via PCA. These steps collectively prepared the data for effective machine learning, enabling our predictor to achieve robust performance in identifying malonylation sites.",
  "optimization/parameters": "In our study, we utilized the Support Vector Machine (SVM) algorithm for classification, which involves two crucial parameters: the penalty parameter (c) and the kernel parameter (g). These parameters were optimized using the grid search method.\n\nThe grid search method systematically works through multiple combinations of parameter tunes to determine the optimal values. For our model, we set c to 10 and g to 2. These values were chosen based on the results of the grid search, which aimed to maximize the performance metrics such as accuracy, sensitivity, specificity, F1 score, and Matthews correlation coefficient (MCC).\n\nAdditionally, we employed Principal Component Analysis (PCA) to reduce the dimensionality of our feature set. The optimal number of dimensions for PCA was determined by evaluating the model's performance at different dimensionalities (50, 100, 150, 200, 250, and 300). Through this evaluation, we found that reducing the dimensions to 100 yielded the best performance across various metrics.\n\nIn summary, our model uses two primary parameters (c and g) for the SVM classifier, and the dimensionality reduction through PCA was optimized to 100 dimensions. These parameters were selected based on extensive grid search and cross-validation to ensure the best possible performance of our predictive model.",
  "optimization/features": "The input features used in our study are a combination of CKSAAP, AAindex, and one-hot encoding. Specifically, we utilized the first four CKSAAP features, which resulted in a total of 1764 dimensions (441 dimensions per CKSAAP feature). Feature selection was performed by analyzing the performance of different combinations of CKSAAP features, both individually and in combination. This process involved using single K values ranging from 0 to 6 and evaluating their combined effects. The optimal combination was determined to be the first four CKSAAP features, along with AAindex and one-hot encoding. This selection was done using the training set only, ensuring that the feature selection process did not introduce any bias from the test set.",
  "optimization/fitting": "The fitting method employed in this study involved a comprehensive approach to ensure both over-fitting and under-fitting were effectively managed. The model utilized a Support Vector Machine (SVM) with a radial basis function kernel, which is known for its ability to handle high-dimensional spaces and complex decision boundaries.\n\nTo address the potential issue of over-fitting, given the high-dimensional feature space, Principal Component Analysis (PCA) was used for dimensionality reduction. This technique transformed the original data into a set of linearly independent representations, extracting the main feature components and reducing the dimensionality to manageable levels. Specifically, the dimensions were reduced to 100, which was found to yield the best performance metrics, including accuracy, sensitivity, specificity, F1 score, and Matthews correlation coefficient (MCC).\n\nAdditionally, 5-fold cross-validation was executed for 50 iterations to optimize the parameters in the training model. This method involved dividing the entire training dataset into five subsets, with each subset being used as a test set while the remaining four subsets were used for training. This process ensured that the model's performance was evaluated on different portions of the data, providing a robust assessment of its generalization capability.\n\nThe parameters c and g in the SVM were optimized using grid search, which systematically worked through multiple combinations of parameter tunes to determine the best settings. This approach helped in fine-tuning the model to achieve optimal performance without over-fitting to the training data.\n\nTo rule out under-fitting, the model's performance was thoroughly evaluated using multiple metrics, including accuracy, sensitivity, specificity, F1 score, and MCC. The results showed that the model achieved high performance across these metrics, indicating that it was capable of capturing the underlying patterns in the data without being too simplistic.\n\nFurthermore, the effectiveness of PCA was analyzed by comparing the performance of the model with and without PCA. The results demonstrated that the use of PCA significantly improved the model's performance, confirming its role in mitigating over-fitting and enhancing the model's generalization capability.",
  "optimization/regularization": "To prevent overfitting, we employed the 5-fold cross-validation technique. This method involves dividing the entire training dataset into five subsets of roughly equal size. Each subset is then used once as a test set, while the remaining four subsets are used to train the classifier. This process is repeated five times, with each subset serving as the test set once. By doing so, we ensure that every data point gets to be in the test set exactly once, providing a more robust evaluation of the model's performance and helping to avoid overfitting. Additionally, we optimized the parameters of our Support Vector Machine (SVM) model using grid search, which systematically works through multiple combinations of parameter tunes to determine the optimal settings. This approach further helps in preventing overfitting by ensuring that the model generalizes well to unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The specific parameters used for the Support Vector Machine (SVM) model, such as the penalty parameter (c) and the kernel parameter (g), were optimized using a grid search method. The final values set for these parameters were c = 10 and g = 2 when using CKSAAP features alone, and c = 1.9 and g = 0.07 when combining all features (CKSAAP, one-hot encoding, AAindex). The radial basis function was adopted as the kernel function.\n\nThe optimization process involved 5-fold cross-validation executed 50 times to ensure robust parameter tuning. The results of these optimizations are detailed in the supplementary tables provided with the publication.\n\nRegarding model files and optimization parameters, the specific files are not directly mentioned as being available for download. However, the LIBSVM tool used for the SVM implementation is publicly available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/. This tool can be used to replicate the experiments and optimize the parameters as described in the paper.\n\nThe publication does not explicitly state the license under which the supplementary materials or the LIBSVM tool are provided. However, the LIBSVM tool is generally available under a permissive license that allows for academic and commercial use, subject to the terms specified in its documentation. For the supplementary materials, it is typical in academic publishing to make them available under a Creative Commons license, but this would need to be verified through the specific journal's policies or the supplementary information provided with the article.",
  "model/interpretability": "The Mal-Prec model, while highly effective in predicting malonylation sites, is not entirely transparent and can be considered somewhat of a black box. The model utilizes a support vector machine (SVM) classifier, which is known for its robustness and accuracy but is not inherently interpretable. The SVM operates by finding a hyperplane that best separates the data into different classes, which does not provide straightforward insights into the decision-making process.\n\nHowever, there are aspects of the model that offer some interpretability. For instance, the use of principal component analysis (PCA) to reduce the dimensionality of the feature set helps in identifying the most significant features that contribute to the prediction. This dimensionality reduction process can be seen as a form of feature selection, highlighting which features are most important for the classification task.\n\nAdditionally, the analysis of sequence occurrence frequency using Two Sample Logo with t-test provides insights into the sequence preferences of malonylation and non-malonylation peptides. This analysis reveals that certain amino acids, such as Glycine (G), Leucine (L), Alanine (A), and Valine (V), are significantly richer in malonylation peptides, while Lysine (K) and Glutamic acid (E) are more abundant in non-malonylation peptides. This information can be used to understand the underlying biological mechanisms and to distinguish between the two types of peptides.\n\nFurthermore, the performance comparison of different feature combinations and the role of various features in the model provide some level of interpretability. The analysis shows that the combination of CKSAAP, AAindex, and one-hot encoding features, along with PCA, achieves the best performance. This indicates that these features are crucial for the model's predictive power.\n\nIn summary, while the Mal-Prec model is not entirely transparent and relies on complex algorithms like SVM and PCA, there are elements within the model that offer some interpretability. The feature selection process through PCA and the sequence occurrence frequency analysis provide valuable insights into the factors influencing the prediction of malonylation sites.",
  "model/output": "The model is a classification model. It employs a support vector machine (SVM) as the classifier. The SVM is used to predict malonylation sites in human proteins. The model's performance is evaluated using metrics such as Accuracy, Sensitivity, Specificity, F1 score, and Matthews correlation coefficient (MCC). These metrics are commonly used to assess the quality of classification models. The model was trained and validated using 5-fold cross-validation, which is a technique used to ensure the model's robustness and generalizability. Additionally, the model's performance was tested on an independent dataset to provide an objective evaluation. The use of SVM and the evaluation metrics indicate that the model is designed for classification tasks.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the algorithm is not released.\n\nThe LIBSVM tool, which was used for the implementation of the Support Vector Machine (SVM) classifier, is publicly available. It can be accessed at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/. The LIBSVM tool is released under a BSD license, which allows for free use, modification, and distribution.\n\nThe method to run the algorithm is not explicitly provided. However, the use of the LIBSVM tool suggests that the algorithm can be run using a compatible environment that supports the LIBSVM tool. The specific details of the environment and any additional dependencies required to run the algorithm are not provided.",
  "evaluation/method": "The method was evaluated using a combination of 5-fold cross-validation and an independent dataset to ensure robust and unbiased performance assessment.\n\n5-fold cross-validation was employed to conduct model selection. This technique involves dividing the entire training dataset into five subsets of roughly equal size. Each subset is then used in turn as a test set, while the remaining four subsets are used to train the classifier. This process helps to effectively avoid overfitting and underfitting, providing more persuasive results.\n\nIn addition to cross-validation, the method was evaluated on an independent dataset that was truly blind to the training data. This approach ensures an objective performance comparison. The independent dataset evaluation showed that the proposed method achieved high performance metrics, including accuracy, sensitivity, specificity, F1 score, and Matthews correlation coefficient.\n\nThe use of these evaluation techniques demonstrates the reliability and generalizability of the proposed method for predicting malonylation sites in human proteins.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the prediction quality of our model. These metrics include Accuracy (Acc), Sensitivity (Sen), Specificity (Spec), F1 score, and Matthews correlation coefficient (MCC). These metrics are widely recognized and used in the literature for evaluating classification models, ensuring that our evaluation is representative and comparable to other studies.\n\nAccuracy measures the overall correctness of the model by calculating the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, evaluates the model's ability to identify positive samples correctly. Specificity assesses the model's capability to correctly identify negative samples. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. The Matthews correlation coefficient offers a balanced measure that takes into account true and false positives and negatives, providing a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation.\n\nThese metrics collectively provide a thorough evaluation of our model's performance, covering various aspects of prediction quality. By using these established metrics, we ensure that our results are both rigorous and comparable to other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our proposed method, Mal-Prec, against several classical classifiers and state-of-the-art approaches to ensure its effectiveness and robustness. We compared Mal-Prec with Random Forest (RF), K-nearest neighbors (KNN), an Ensemble of decision trees, and Naive Bayes (NB) on the training datasets. The KNN algorithm used Euclidean distance with 2 neighbors, while the RF and Ensemble methods utilized 20 and 50 decision trees, respectively. We conducted 5-fold cross-validation 50 times for each classifier to ensure reliable performance metrics.\n\nThe results, presented in a performance comparison table, demonstrated that Mal-Prec outperformed all classical classifiers across various metrics, including Accuracy (Acc), Sensitivity (Sen), Specificity (Spec), F1 score, and Matthews Correlation Coefficient (MCC). This comparison highlighted the superior performance of Mal-Prec in handling the complexity of the data and achieving higher predictive accuracy.\n\nAdditionally, we compared Mal-Prec with state-of-the-art approaches specifically designed for predicting malonylation sites, such as Mal-Lys, MaloPred, iLMS, LEMP, and SPRINT-Mal. The comparison showed that Mal-Prec achieved the best performance in terms of Acc, Sen, Spec, F1, and MCC, confirming its effectiveness and reliability.\n\nFurthermore, we analyzed the role of Principal Component Analysis (PCA) in our method by comparing the performance with and without PCA. The results indicated that using PCA to reduce the dimensionality of the feature combination significantly improved the performance metrics, underscoring the importance of dimensionality reduction in enhancing predictive accuracy.\n\nOverall, the comprehensive comparison with both classical classifiers and state-of-the-art methods on benchmark datasets validated the superiority of Mal-Prec in predicting malonylation sites.",
  "evaluation/confidence": "The evaluation of our method, Mal-Prec, includes a detailed analysis of performance metrics using 5-fold cross-validation. This approach helps to ensure that the results are robust and not due to overfitting or underfitting. The performance metrics reported include Accuracy (Acc), Sensitivity (Sen), Specificity (Spec), F1 score, and Matthews Correlation Coefficient (MCC). These metrics are crucial for assessing the effectiveness of our model.\n\nFor the 5-fold cross-validation, the average values of these metrics are provided, along with their standard deviations. For instance, when the dimensions are set to 100, the average Acc, Sen, Spec, F1, and MCC are 91.24%, 91.71%, 90.83%, 91.18%, and 84.03%, respectively. The standard deviations for these metrics are 1.24%, 2.50%, 2.10%, 1.43%, and 2.09%, respectively. These standard deviations indicate the variability of the performance metrics across the different folds, providing a measure of confidence in the reported averages.\n\nAdditionally, the Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC) values are calculated. The average AUC value is 96.47%, which further supports the high performance of our method.\n\nTo analyze the role of Principal Component Analysis (PCA) in our method, we also evaluated the performance without PCA. The results show a significant drop in performance metrics when PCA is not used, highlighting the importance of dimensionality reduction in improving the model's accuracy and robustness.\n\nComparisons with other classical classifiers, such as Random Forest (RF), K-nearest neighbors (KNN), Ensemble of decision trees, and Naive Bayes (NB), were also conducted. The performance of Mal-Prec (SVM) is superior to these classifiers across all metrics, demonstrating its effectiveness and reliability.\n\nIn summary, the performance metrics of our method are supported by statistical measures such as standard deviations and AUC values, providing a high level of confidence in the reported results. The comparisons with other classifiers further validate the superiority of our approach.",
  "evaluation/availability": "Not enough information is available."
}