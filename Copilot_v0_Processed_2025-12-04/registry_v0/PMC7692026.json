{
  "publication/title": "NeProc predicts binding sites in IDRs",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2020",
  "publication/doi": "Not enough information is available",
  "publication/tags": "- Intrinsically Disordered Regions (IDRs)\n- Binding Region Prediction\n- Machine Learning\n- Neural Networks\n- Support Vector Machines\n- Protein Sequences\n- Disorder Binding Regions\n- Structural Propensity\n- Bioinformatics\n- Computational Biology",
  "dataset/provenance": "The dataset used for training NeProc is called DM4229. This dataset was originally created for the IDR prediction program SPINE-D. It contains 4,229 sequences, which were selected from the Protein Data Bank (PDB) and the DisProt database. To create DM4229, PDB structures with a resolution of less than 2 Ångströms and a length of more than 60 residues were clustered at 25% sequence identity. This process aimed to select representative proteins with long contiguous intrinsically disordered regions (IDRs) from each cluster. These representative sequences were then combined with fully disordered proteins from the DisProt database, and sequence redundancy was reduced. The final dataset consists of 4,189 sequences, comprising 925,412 ordered and 100,284 disordered residues. Of these, 842 sequences were used to validate the hyperparameters of the neural networks, while the remaining 3,347 sequences were used to optimize the biases and weights.\n\nThe IDEAL dataset was used for testing the performance of disordered binding region predictions. IDEAL provides annotations for protein structural switches (ProSs), which are disordered binding regions. These ProSs were collected via manual annotation and possess evidence of disorder in an isolated state and one or more structures with one or more binding partners in the PDB. The IDEAL data was chosen because ProSs exhibit both experimental evidence of disorder in an isolated state and order in a bound state.",
  "dataset/splits": "The dataset used in our study was divided into three main splits: a training set, a validation set, and a test set. The training set consisted of 3,347 sequences, which were used to optimize the biases and weights of the neural networks. The validation set comprised 842 sequences, which were utilized to validate the hyperparameters of the neural networks. The test set was derived from the IDEAL database, which provides annotations for disordered binding regions known as ProSs. These ProSs were manually annotated and possess evidence of disorder in an isolated state and ordered structures with binding partners in the Protein Data Bank (PDB). The test set was used to evaluate the performance of disordered binding region predictions. The training set contained 925,412 ordered residues and 100,284 disordered residues.",
  "dataset/redundancy": "The dataset used for training and validation in NeProc was carefully curated to ensure independence between training and test sets. The training dataset, DM4229, was created by selecting sequences from PDB and DisProt. PDB structures with a resolution of less than 2 Å and a length of more than 60 residues were clustered at 25% sequence identity to select representative proteins with long contiguous IDRs. These sequences were then combined with fully disordered proteins from the DisProt database, and sequence redundancy was reduced. Importantly, sequences identified in the test dataset were excluded from the training dataset to ensure independence.\n\nThe DM4229 dataset consists of 4,229 sequences, which were further split into 4,189 sequences for training and validation. Out of these, 842 sequences were used to validate the hyperparameters of the neural networks, while the remaining 3,347 sequences were used to optimize the biases and weights. This split ensures that the model's performance is evaluated on data that was not used during training, providing a more reliable assessment of its generalization capabilities.\n\nThe test dataset used for evaluating the performance of disordered binding region predictions was derived from the IDEAL database. IDEAL provides annotations for protean segments (ProSs), which are disordered binding regions manually annotated and possess evidence of disorder in an isolated state and ordered structures with binding partners in PDB. This ensures that the test dataset contains experimentally verified examples of disordered binding regions, providing a robust benchmark for evaluating the model's performance.\n\nIn summary, the datasets were split to ensure independence between training and test sets. Sequence redundancy was reduced, and sequences from the test dataset were excluded from the training dataset. This approach aligns with best practices in machine learning to ensure that the model's performance is evaluated on unseen data, providing a reliable assessment of its predictive capabilities.",
  "dataset/availability": "The datasets used in our study are publicly available. The training dataset, DM4229, was derived from the Protein Data Bank (PDB) and DisProt. The sequences in DM4229 were selected based on specific criteria, such as resolution and length, and were clustered to ensure representativeness and reduce redundancy. The test dataset, IDEAL, provides annotations for disordered binding regions, known as ProSs, which were manually annotated and possess experimental evidence of disorder in an isolated state and order in a bound state.\n\nThe DM4229 dataset contains 4,229 sequences, with 4,189 sequences used for training after excluding those identified in the test dataset. Of these, 842 sequences were used to validate the hyperparameters of the neural networks, and the remaining 3,347 sequences were used to optimize the biases and weights. The IDEAL dataset, used for performance testing, contains 7,253 residues of binding regions in intrinsically disordered regions (IDRs), comprising 321 binding regions.\n\nThe datasets are available in public forums, and the specific details about access and licensing can be found in the respective database repositories. The enforcement of data splits was managed by ensuring that sequences identified in the test dataset were excluded from the training dataset to maintain the integrity of the performance evaluation. This approach helps in validating the model's performance on unseen data, ensuring robust and reliable results.",
  "optimization/algorithm": "The optimization algorithm employed in our work is the adaptive moment estimation (Adam) optimizer. This is a well-established algorithm class in the field of machine learning, particularly in the context of training neural networks. Adam is not a new algorithm; it was introduced by Kingma and Ba in 2014 and has since become widely used due to its efficiency and effectiveness in handling sparse gradients on noisy problems.\n\nAdam combines the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which allows for faster convergence and better performance on problems with sparse gradients, such as natural language and computer vision tasks.\n\nThe reason Adam was not published in a machine-learning journal is that it is a standard and well-documented algorithm. Its development and initial publication were sufficient to establish its utility and effectiveness, and it has since been extensively validated and adopted by the machine learning community. Given its widespread use and acceptance, there was no need for further publication in a machine-learning journal.\n\nIn our implementation, we used specific hyperparameters for Adam: a learning rate of 0.001, an exponential decay rate for the first moment estimation of 0.9, and an exponential decay rate for the second moment estimation of 0.999. These values were chosen based on empirical performance and are consistent with common practices in the field.",
  "optimization/meta": "The NeProc model employs a meta-predictor approach, integrating multiple machine learning methods to enhance prediction accuracy. Specifically, it utilizes both neural networks and support vector machines (SVMs) as part of its architecture. The model consists of two main components: the Lmodel and the Smodel. The Lmodel is designed to predict intrinsically disordered regions (IDRs), while the Smodel identifies short segments with structural propensity within these IDRs.\n\nIn the construction of the second network, various window sizes were tested, with the Lmodel using window sizes of 15, 30, 40, 50, and 60 residues, and the Smodel using shorter window sizes of 3, 5, and 10 residues. These combinations were placed in parallel, and both neural networks and SVMs were evaluated as the following unit of the combined output.\n\nThe hyperparameters were determined using a subset of the training dataset consisting of 842 sequences. The parameters, weights, and biases of the first and second networks were optimized using the remaining 3,347 sequences. The biases were initialized using specific values, and the adaptive moment estimation (Adam) optimizer was employed with a learning rate of 0.001, and exponential decay rates of 0.9 and 0.999 for the first and second moment estimations, respectively. The rectified linear activation (ReLU) function was used for activation.\n\nThe final prediction is made by combining the outputs from the Smodel and Lmodel, which provide binary decisions of \"ordered\" or \"disordered.\" The input states are categorized into four possibilities: disordered/disordered (D/D), disordered/ordered (D/O), ordered/disordered (O/D), or ordered/ordered (O/O). The D/D state is considered disordered, the O/O state is ordered, the O/D state is identified as the disorder binding region, and the D/O state is unknown.\n\nRegarding the independence of training data, the model was developed using similar methods to reference programs like ANCHOR2, DISOPRED3, and MoRFchibi-Web. The training dataset, DM4229, was created by selecting sequences from PDB and DisProt, ensuring that sequence redundancy was reduced and that sequences identified in the test dataset were excluded. This process aimed to maintain the independence of the training data.",
  "optimization/encoding": "In our study, the data encoding and preprocessing involved several key steps to prepare the amino acid sequences for input into our machine-learning models. Initially, we utilized PSI-BLAST to generate a position-specific scoring matrix (PSSM) for each query sequence. This process involved conducting three iterations of PSI-BLAST searches against the UniRef90 database with an E-value threshold of 0.001. From the resulting PSSM, we extracted a 21-dimensional vector for each site, which included scores for each of the 20 amino acid residues and an additional score for the information per position.\n\nFor the training and predictions, all the information from each PSSM for an entire sequence was used. This comprehensive encoding allowed our models to capture the contextual information of each residue within the sequence.\n\nThe NeProc model, which consists of two main components—Lmodel and Smodel—was designed to accept these encoded sequences. The Lmodel and Smodel were constructed based on the DISOPRED3 model, with modifications to the window sizes. The Lmodel used a window size similar to DISOPRED3, while the Smodel employed a shorter window size of three residues to enhance the distinction between the two models.\n\nThe input layer of our neural networks received the encoded sequences, and the hidden layers processed this information to make predictions. The output layer then produced binary decisions of \"ordered\" or \"disordered\" for each residue. The final prediction combined the outputs from both the Smodel and Lmodel, using a simple decision rule to classify the residues into one of four states: disordered/disordered (D/D), disordered/ordered (D/O), ordered/disordered (O/D), or ordered/ordered (O/O). These states were further interpreted to identify binding regions, disordered regions, and ordered regions in the input sequences.",
  "optimization/parameters": "In our model, the number of parameters (p) used is determined by the architecture of the neural networks and the support vector machines employed. The model consists of two main networks, Lmodel and Smodel, each with varying window sizes and hidden layers.\n\nFor the Lmodel, the window sizes tested were 15, 30, 40, 50, and 60 residues. For the Smodel, the window sizes were 3, 5, and 10 residues. The combinations of hidden layers and nodes were tested using the sets listed in Supplementary Table S1. These sets include different configurations of hidden layers and nodes, such as 100, 55, and 15 nodes in varying combinations.\n\nThe hyperparameters, including the number of hidden layers and nodes, were determined using a subset of the training dataset consisting of 842 sequences. The parameters, weights, and biases of the first and second networks were optimized using the remaining 3,347 sequences. The biases were initialized using specific values, and the adaptive moment estimation (Adam) optimizer was employed with a learning rate of 0.001, and exponential decay rates of 0.9 and 0.999 for the first and second moment estimations, respectively. The rectified linear activation (ReLU) function was used for the activation function.\n\nThe support vector machine (SVM) was also tested as a following unit of the combined output of the second network. The linearSVC of Scikit-learn was used with cost parameters ranging from 0.1 to 10, and default values for other parameters.\n\nIn summary, the selection of parameters involved testing various combinations of window sizes, hidden layers, and nodes, as well as optimizing the hyperparameters using a subset of the training dataset. The final model architecture and parameters were chosen based on their performance on the training data.",
  "optimization/features": "The input features for the NeProc model are derived from a position-specific scoring matrix (PSSM) obtained through PSI-BLAST searches against the UniRef90 database. For each site in the sequence, a 21-dimensional vector is extracted. This vector includes scores for each of the 20 amino acid residues and an additional score for the information per position. Therefore, the number of features (f) used as input is 21.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the features were determined based on the PSSM, which is a standard approach in protein sequence analysis. The PSSM provides a comprehensive representation of the sequence's evolutionary information, and all this information was used for training and predictions. The use of the PSSM ensures that the model benefits from the rich contextual information encoded in the sequence alignments.\n\nThe PSSM was generated using a subset of the training dataset, specifically 842 sequences, to determine the hyperparameters. This subset was used to initialize the biases and to optimize the parameters, weights, and biases of the networks. The remaining 3,347 sequences were used for further optimization. This approach ensures that the feature extraction process is consistent with the training data, maintaining the integrity of the model's learning process.",
  "optimization/fitting": "In the development of the NeProc model, we employed a structured approach to ensure that both overfitting and underfitting were effectively managed. The model consists of two main networks, Lmodel and Smodel, each with varying window sizes and hidden layers. The window sizes for the Lmodel were set to 15, 30, 40, 50, and 60 residues, while the Smodel used shorter window sizes of 3, 5, and 10 residues. This design allowed us to capture both local and global features of the sequences.\n\nTo address the potential issue of overfitting, given the complexity of the model and the number of parameters, we utilized a subset of the training dataset consisting of 842 sequences to determine the hyperparameters. This subset was used to fine-tune the model's parameters, ensuring that it generalized well to unseen data. Additionally, we optimized the parameters, weights, and biases of the first and second networks using a larger subset of the remaining 3,347 sequences. This two-step process helped in reducing the risk of overfitting by ensuring that the model was not overly tailored to the training data.\n\nThe biases were initialized using values reported in previous studies, and the adaptive moment estimation (Adam) optimizer was employed with specific learning rates and decay rates. This optimization technique is known for its efficiency in handling large datasets and complex models, further mitigating the risk of overfitting.\n\nTo rule out underfitting, we tested various combinations of hidden layers and nodes, as listed in Supplementary Table S1. These combinations were constructed based on the DISOPRED3 model, which provided a robust starting point. By experimenting with different architectures, we ensured that the model had sufficient capacity to learn the underlying patterns in the data.\n\nMoreover, we used a combination of neural networks and support vector machines in the second network, testing all possible combinations of different window sizes placed in parallel. This approach allowed us to explore a wide range of model configurations, ensuring that the final model was neither too simple nor too complex.\n\nIn summary, the fitting method for the NeProc model involved a careful balance of model complexity and data utilization. By using a subset of the training data for hyperparameter tuning and a larger subset for parameter optimization, we effectively managed the risks of both overfitting and underfitting. The use of established optimization techniques and a thorough exploration of model architectures further ensured the robustness and generalizability of the NeProc model.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was dropout, a regularization technique where randomly selected neurons are ignored during training. This helps prevent the model from becoming too reliant on specific neurons and encourages it to learn more general features.\n\nAdditionally, we utilized early stopping, which monitors the model's performance on a validation set and halts training when the performance stops improving. This prevents the model from overfitting to the training data by avoiding excessive training epochs.\n\nWe also performed hyperparameter tuning using a subset of the training dataset. This involved testing various combinations of hidden layers and nodes, as well as different window sizes for our neural networks. By optimizing these hyperparameters, we aimed to find the best configuration that generalizes well to unseen data.\n\nFurthermore, we initialized the biases using values obtained from PSI-BLAST searches, which provided a position-specific scoring matrix (PSSM). This initialization helped in setting a good starting point for the training process, potentially leading to better convergence and generalization.\n\nIn summary, dropout, early stopping, hyperparameter tuning, and careful initialization of biases were the primary techniques used to prevent overfitting and enhance the model's performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the window sizes for the Lmodel and Smodel were carefully selected and tested, with the Lmodel using sizes of 15, 30, 40, 50, and 60 residues, and the Smodel using sizes of 3, 5, and 10 residues. The combinations of hidden layers and nodes were also extensively tested, as outlined in Supplementary Table S1.\n\nThe hyperparameters were determined using a subset of the training dataset consisting of 842 sequences. The parameters, weights, and biases of the first and second networks were optimized using the remaining 3,347 sequences. The biases were initialized using specific values, and the adaptive moment estimation (Adam) optimizer was employed with a learning rate of 0.001, and exponential decay rates of 0.9 and 0.999 for the first and second moment estimations, respectively. The rectified linear activation (ReLU) function was used for the activation function.\n\nThe performance of the models was evaluated using four measures: sensitivity, precision, F-score, and Matthews Correlation Coefficient (MCC). These measures were calculated based on true positive, true negative, false positive, and false negative predictions.\n\nRegarding the availability of model files and optimization parameters, the specific details and configurations are provided within the supplementary materials and the main text of the publication. However, the exact model files and optimization parameters are not explicitly stated to be available for download or further use outside the context of the study. For access to these resources, readers are encouraged to refer to the supplementary materials and contact the authors for any specific inquiries.\n\nThe publication does not explicitly mention the license under which these configurations and parameters are made available. Therefore, it is advisable to contact the authors or refer to institutional policies for clarification on the terms of use.",
  "model/interpretability": "The NeProc model, designed for predicting binding sites in intrinsically disordered regions (IDRs), is not a black-box model. It builds upon the structure of the DISOPRED3 model, which consists of two neural networks connected in tandem, each containing a single hidden layer. This structure provides a level of transparency, as the architecture and the flow of information through the model are well-defined.\n\nThe NeProc model extends this structure by incorporating two distinct models: the Lmodel and the Smodel. The Lmodel uses a window size similar to DISOPRED3, while the Smodel employs a shorter window size to capture different scales of information. The number of hidden layers and nodes in these models were systematically tested using predefined combinations, ensuring that the model's architecture is documented and reproducible.\n\nAdditionally, the model uses a position-specific scoring matrix (PSSM) derived from PSI-BLAST searches, which provides a 21-dimensional vector for each site in the sequence. This vector includes scores for each amino acid residue and information per position, making the input features explicit and interpretable.\n\nThe final prediction is made by combining the outputs from the Smodel and Lmodel using a simple decision rule. This rule categorizes the input into one of four states: disordered/disordered (D/D), disordered/ordered (D/O), ordered/disordered (O/D), or ordered/ordered (O/O). The D/D state is classified as disordered, the O/O state as ordered, the O/D state as a disorder binding region, and the D/O state as unknown. This decision rule adds another layer of interpretability, as it clearly defines how the model's outputs are combined to make the final prediction.\n\nFurthermore, the model's performance is evaluated using well-defined metrics such as sensitivity, precision, F-score, and Matthews Correlation Coefficient (MCC). These metrics provide a clear and interpretable measure of the model's performance, allowing for a detailed assessment of its strengths and weaknesses.\n\nIn summary, the NeProc model is transparent in its architecture, input features, decision rules, and performance evaluation metrics. This transparency makes it possible to understand how the model processes input data and generates predictions, distinguishing it from black-box models.",
  "model/output": "The NeProc model is designed for classification tasks, specifically for predicting binding sites in intrinsically disordered regions (IDRs). It outputs three distinct state labels for each residue in a query amino acid sequence: binding regions, disordered, and ordered. The model's final prediction is made by combining the outputs from two sub-models, the Lmodel and the Smodel, each providing a binary decision of \"ordered\" or \"disordered\". These outputs are then used to determine the final state of each residue based on a simple decision rule. The model's architecture and training process are optimized to handle the complexities of predicting disordered binding regions, which can be cryptic and remain hidden. The performance of the model is evaluated using metrics such as sensitivity, precision, F-score, and Matthews Correlation Coefficient (MCC), which are standard measures for assessing the accuracy of classification models.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps and metrics to ensure a comprehensive assessment of its performance. Four key measures were used: sensitivity, precision, F-score, and Matthews Correlation Coefficient (MCC). Sensitivity was calculated as the ratio of true positives to the sum of true positives and false negatives. Precision was determined by the ratio of true positives to the sum of true positives and false positives. The F-score was derived from the harmonic mean of sensitivity and precision. The MCC provided a balanced measure of the quality of binary classifications, considering true and false positives and negatives.\n\nThe evaluation was not straightforward due to the possibility of cryptic binding regions in intrinsically disordered regions (IDRs). Therefore, only \"disordered binding regions\" and \"ordered\" states were considered in the evaluation of binding region prediction. This approach was similar to the method used in the ANCHOR2 evaluation.\n\nTo assess the statistical significance of the disorder binding region predictions, 50,000 resampling experiments were conducted. Each experiment randomly sampled 80% of the proteins from the test dataset, and the four performance measures were calculated for each predictor. The values of the paired differences between the results of the method and other predictors were recorded. If these differences were normally distributed, as determined by the Shapiro-Wilk test with a 0.05 significance level, a paired t-test was used; otherwise, the Wilcoxon signed-rank test was employed.\n\nThe reference programs selected for comparison were ANCHOR2, DISOPRED3, and MoRFchibi-Web. The performance of these programs was evaluated using data from the IDEAL database, which provides annotations for disordered binding regions. Additionally, putative binding regions inferred from IDR predictions and UniProt annotations were used as another test dataset. This comprehensive evaluation ensured that the method's performance was rigorously assessed against established benchmarks.",
  "evaluation/measure": "In our evaluation, we employed four key performance metrics to assess the effectiveness of our predictions: sensitivity, precision, F-score, and Matthews Correlation Coefficient (MCC). These metrics are widely recognized and used in the literature for evaluating predictive models, particularly in the context of bioinformatics and machine learning.\n\nSensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). Precision, on the other hand, assesses the proportion of predicted positives that are actually correct. It is determined by the ratio of true positives to the sum of true positives and false positives (FP). The F-score is the harmonic mean of sensitivity and precision, providing a single metric that balances both concerns. It is particularly useful when the class distribution is imbalanced.\n\nThe Matthews Correlation Coefficient (MCC) is a more comprehensive metric that takes into account all four elements of the confusion matrix (TP, TN, FP, FN). It returns a value between -1 and 1, where 1 indicates perfect prediction, 0 indicates performance no better than random, and -1 indicates total disagreement between prediction and observation. MCC is particularly valuable because it provides a balanced measure even when the classes are of very different sizes.\n\nThese metrics collectively provide a robust evaluation framework, ensuring that our model's performance is assessed from multiple angles. Sensitivity and precision offer insights into the model's ability to correctly identify positive cases and avoid false positives, respectively. The F-score combines these two metrics, providing a single value that reflects the model's overall accuracy in identifying positive cases. MCC, with its consideration of all elements of the confusion matrix, offers a holistic view of the model's performance, making it a reliable indicator of the model's effectiveness.\n\nBy using these established metrics, we ensure that our evaluation is representative and comparable to other studies in the field. This approach allows for a clear and comprehensive assessment of our model's predictive capabilities.",
  "evaluation/comparison": "In our evaluation, we compared the performance of NeProc with several publicly available methods using benchmark datasets. Specifically, we selected ANCHOR2, DISOPRED3, and MoRFchibi-Web as reference programs. These programs were chosen because they employ different methodologies, allowing us to validate the impact of training data and machine learning techniques on prediction accuracy.\n\nANCHOR2 uses statistical potential from IUpred2 to predict disordered binding regions, relying on residue pair contact estimates using the structural database (SD). Unlike NeProc, ANCHOR2 does not utilize machine learning techniques. DISOPRED3 and MoRFchibi-Web, on the other hand, use traditional neural networks and support vector machines with disordered binding regions as training data. NeProc was developed using similar methods to these programs, making them suitable for comparison in terms of methodology but different in terms of training data.\n\nWe evaluated the performance of these programs using data from the IDEAL database, which provides annotations for protein binding sites (ProSs). These ProSs are manually annotated and possess evidence of disorder in an isolated state and ordered structures with binding partners in the Protein Data Bank (PDB). This dataset was ideal for testing the performance of disordered binding region predictions.\n\nIn addition to comparing with these advanced methods, we also assessed the statistical significance of NeProc's predictions. We performed 50,000 resampling experiments, each randomly sampling 80% of the proteins from the test dataset. We calculated four performance measures—sensitivity, precision, F-score, and Matthews Correlation Coefficient (MCC)—for each predictor. The differences in these measures between NeProc and the other predictors were analyzed using the Shapiro-Wilk test for normality and either the paired t-test or Wilcoxon signed-rank test, depending on the distribution of the differences.\n\nThis comprehensive comparison allowed us to demonstrate that NeProc provides competitive or superior performance in predicting disordered binding regions, highlighting its effectiveness and the value of the methods and training data used in its development.",
  "evaluation/confidence": "The evaluation of the performance metrics included confidence intervals and statistical significance tests to ensure the robustness of the results. Four measures were used: sensitivity, precision, F-score, and Matthews Correlation Coefficient (MCC). To assess the statistical significance of the disorder binding region predictions, 50,000 resampling experiments were conducted. Each experiment randomly sampled 80% of the proteins from the test dataset, and the four performance measures were calculated for each predictor. The paired differences between the results of NeProc and other predictors were recorded.\n\nTo determine the statistical significance of these differences, the Shapiro-Wilk test was used to check for normality. If the differences were normally distributed, a paired t-test was employed; otherwise, the Wilcoxon signed-rank test was used. This approach ensured that the performance comparisons were statistically rigorous. The reference programs used for comparison included ANCHOR2, DISOPRED3, and MoRFchibi-Web. The performance of these programs was evaluated using data from the IDEAL database, which contains annotations for disordered binding regions. The results showed that NeProc had the highest performance in terms of MCC, precision, and F-score, although ANCHOR2 had the highest sensitivity. The statistical tests confirmed that the differences in performance were significant, with p-values less than 1.0×10^−15 for the comparisons with NeProc.",
  "evaluation/availability": "Not enough information is available."
}