{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Pulliam, who is the first author and has contributed significantly to the research and writing of the paper.\n- Other authors who have contributed to the study but their specific contributions are not detailed in the provided information.",
  "publication/journal": "J Neurovirol",
  "publication/year": "2021",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- HIV\n- Cognitive impairment\n- Machine learning\n- Biomarkers\n- Neurodegeneration\n- Neuroinflammation\n- Extracellular vesicles\n- Support Vector Machines\n- K-Nearest Neighbors\n- AdaBoost\n- Neuropsychological testing\n- Antiretroviral therapy\n- Plasma HIV viral RNA\n- Neurovirology\n- Alzheimer's disease\n- HIV-associated neurocognitive disorders\n- Data analysis\n- Statistical methods\n- Clinical variables\n- Protein markers",
  "dataset/provenance": "The dataset used in this study consists of plasma samples obtained from individuals with HIV infection. Specifically, 60 plasma samples were collected from 38 women and 22 men. Among these participants, 40 individuals were diagnosed with HIV-associated neurocognitive disorder (HAND). All participants underwent neuropsychological testing to assess their cognitive status.\n\nThe plasma samples were sourced from several shared resources funded by the National Institute of Mental Health (NIMH) and the National Institute of Neurological Disorders and Stroke (NINDS). These resources include the Manhattan HIV Brain Bank (MHBB), the Texas NeuroAIDS Research Center (TNRC), the National Neurological AIDS Bank (NNAB), and the California NeuroAIDS Tissue Network (CNTN). Additionally, samples were provided by the HIV Neurobehavioral Research Center (HNRC), which is supported by public funding from the National Institutes of Health (NIH), the State of California, and other sources.\n\nThe dataset includes both clinical data and neuronal extracellular vesicle (nEV) proteins. The nEVs were isolated using an immunoadsorption method with a neuron-specific L1CAM antibody. Key proteins quantified in the nEVs include high-mobility group box 1 (HMGB1), neurofilament light (NFL), and phosphorylated tau-181 (p-T181-tau). These proteins were measured using enzyme-linked immunosorbent assays (ELISAs).\n\nThe clinical data encompassed various demographic and health-related variables, such as sex, ethnicity, age, education level, CD4 count, and Global Deficit Score (GDS). The GDS was found to be significantly higher in individuals with neurocognitive impairment compared to those who were neuropsychologically normal.\n\nThis dataset builds upon previous research that has utilized nEV markers and machine learning algorithms to predict cognitive impairment in HIV-infected individuals. For instance, a study by Kapogiannis et al. used machine learning with nEV targets to predict Alzheimer's disease, achieving an area under the curve (AUC) of 0.8. Another study by Tu et al. employed a random forest algorithm to predict HAND, with an AUC of 0.87. The current study aims to differentiate HAND from other age-defining illnesses and to identify more nEV protein targets to assess neuroinflammation and neuronal health.",
  "dataset/splits": "We employed repeated 6-fold cross-validation for model evaluation. This approach involved creating 10 different splits of the data, each with 6 folds. In each split, the data was divided into 6 parts, where 5 parts were used for training and 1 part for testing. This process was repeated 10 times to ensure a thorough evaluation of model performance. The distribution of data points in each split was balanced to maintain an optimal ratio between training and testing sizes, given our sample size of 60 participants. This method allowed for a comprehensive assessment of the models' predictive capabilities by providing multiple perspectives on the data.",
  "dataset/redundancy": "To ensure the robustness and reliability of our machine learning models, we employed repeated 6-fold cross-validation. This method involved splitting the dataset into six folds, where five folds were used for training and one fold for testing. This process was repeated ten times, with each fold serving as the test set once. This approach allowed for a more exhaustive estimation of model performance by providing multiple perspectives on the data.\n\nThe independence of the training and test sets was strictly maintained within the main cross-validation loop. Data leakage between test folds was precluded, ensuring that the models were evaluated on unseen data during each iteration. This rigorous procedure helped to prevent overfitting and provided a more accurate assessment of the models' generalizability.\n\nRegarding the distribution of our dataset, it is important to note that our study focused on a specific population of HIV-positive individuals, with a particular emphasis on those with and without neuropsychological impairment. The dataset included clinical variables such as CD4 counts, age, ethnicity, sex, and education, as well as biomarkers like HMGB1 and NFL. The distribution of these variables was carefully analyzed and compared across different groups to ensure that the models were trained on representative data.\n\nWhile we cannot directly compare our dataset to previously published machine learning datasets due to differences in study design and population characteristics, our approach to data splitting and validation is consistent with best practices in the field. The use of repeated cross-validation provides a robust framework for evaluating model performance and ensuring the reliability of our findings.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithms used in this study belong to the supervised-learning class. Specifically, we employed tree-based methods, clustering, ensembling, and support vector machines. These algorithms are well-established and have proven success in various classification tasks.\n\nThe algorithms used are not new; they are widely recognized and have been extensively studied and applied in scientific research, particularly in \"omics\" type data. The choice of these algorithms was driven by their demonstrated performance in similar contexts.\n\nThe reason these algorithms were not published in a machine-learning journal is that the focus of this study was on applying these methods to a specific biological problem—predicting cognitive impairment in HIV-positive individuals using clinical data and neuronal extracellular vesicle (nEV) protein markers. The primary goal was to evaluate the predictive power of these biomarkers in combination with clinical features, rather than to introduce novel machine-learning techniques. The algorithms were selected for their robustness and effectiveness in handling the type of data and the complexity of the relationships within the dataset.",
  "optimization/meta": "In our study, we employed several machine learning algorithms to classify between NPI and NPN samples. Among these, ensemble methods were used, which can be considered a form of meta-predictor. These ensemble methods combine the predictions of multiple base models to improve overall performance.\n\nOne of the ensemble methods we utilized was AdaBoost, which leverages an ensemble of weak learners. In our implementation, AdaBoost used decision trees as the weak learners. The process involves building weak learners sequentially, with each subsequent model learning from the misclassifications of the previous one. This iterative process aims to create a robust estimator by focusing on the instances that were misclassified by earlier models.\n\nAnother ensemble method we employed was an ensemble version of K-Nearest Neighbors (KNN). This approach leverages the high variance of low K, KNN models (K=1) by using individual KNN models as weak learners to build a more robust estimator. The ensemble KNN method was specifically used to capitalize on the high variance predictions of individual KNN models.\n\nFor tree-based algorithms, we tested Random Forests and Adaptive boosting of decision trees. Adaptive boosting, also known as AdaBoost, yielded better results for our dataset. This method involves combining multiple decision trees to create a stronger predictive model.\n\nIt is important to note that the training data for each base model within the ensemble methods was independent. This independence ensures that the models do not overfit to the training data and can generalize better to new, unseen data. The ensemble methods aggregate the predictions from these independent models to make a final prediction, thereby enhancing the overall accuracy and robustness of the classification.\n\nIn summary, our study utilized ensemble methods, including AdaBoost and ensemble KNN, which can be considered meta-predictors. These methods combine the predictions of multiple base models, such as decision trees and KNN, to improve classification performance. The training data for each base model was independent, ensuring robust and generalizable predictions.",
  "optimization/encoding": "For the machine-learning algorithms, several data transformations were performed to ensure proper processing. Education levels were trichotomized into three categories: 0 for 6–10 years, 1 for 11–15 years, and 2 for 16–20 years. Age was grouped into two categories: young (32–44 years) and old (49–69 years). Categorical features such as age-group, sex, and ethnicity were encoded using one-hot encoding. This method converts categorical variables into a form that could be provided to machine-learning algorithms to improve predictions.\n\nMissing values in the dataset were handled using the K-Nearest Neighbors (KNN) imputation method. This technique estimates missing values based on the values of the k-nearest neighbors in the feature space. Clinical variables, including CD4 count, age-group, ethnicity, sex, and education, were included in the models. The Geriatric Depression Scale (GDS) was omitted due to its colinearity with the prediction labels, which could lead to overfitting and misleading results.\n\nThe data was then split into training and testing sets using repeated (10x) 6-fold cross-validation. This approach ensures that each data point gets to be in the training set multiple times and in the test set multiple times, providing a more robust evaluation of the model's performance. Within the main cross-validation loop, data leakage between test folds was precluded to maintain the integrity of the evaluation process.",
  "optimization/parameters": "In our study, we utilized a range of parameters to build and evaluate our models. The total number of parameters (p) varied depending on the specific model configuration. We initially considered seven different models, each incorporating a combination of clinical features and biomarkers.\n\nThe clinical features included in all models were sex, ethnicity, age, education, and CD4 count. These features were chosen based on their relevance to cognitive impairment in HIV+ subjects and their availability in our dataset. Age was grouped into two categories: young (32–44 years) and old (49–69 years), and education was trichotomized into three levels (6–10, 11–15, and 16–20 years). Categorical features were encoded using one-hot encoding.\n\nIn addition to the clinical features, we included biomarkers such as HMGB1, NFL, and p-T181-tau in various combinations across the models. Model 1 (M1) used only clinical parameters. Model 2 (M2) added NFL to the clinical parameters. Model 3 (M3) included HMGB1 with the clinical parameters. Model 4 (M4) combined clinical parameters with both NFL and HMGB1. Models 5, 6, and 7 included p-T181-tau in different combinations with the clinical parameters and other biomarkers, but these models were ultimately dropped due to their lack of improvement in performance.\n\nThe selection of parameters was guided by exploratory data analysis and domain knowledge. We performed feature importance analysis to understand the contribution of each variable in the models. This analysis showed that CD4, HMGB1, and NFL each contributed significantly to the model's performance. The final models focused on the combinations that provided the best predictive accuracy, particularly Model 3 (M3) with SVM, which achieved an AUC of 0.82 (±0.16).\n\nIn summary, the number of parameters varied by model, but the key parameters included clinical features and specific biomarkers that were selected based on their relevance and contribution to predictive performance.",
  "optimization/features": "In the optimization process, several features were considered as inputs for the machine learning models. Initially, clinical variables such as CD4 count, age-group, ethnicity, sex, and education were included. Additionally, biomarkers like HMGB1, NFL, and p-T181-tau were evaluated. However, p-T181-tau was omitted from the final models due to its lack of predictive value.\n\nFeature selection was implicitly performed by evaluating different combinations of these features. Seven models were compared, each including a different subset of the available features. The models were as follows:\n\n* Model 1 (M1): Clinical parameters only.\n* Model 2 (M2): Clinical parameters plus NFL.\n* Model 3 (M3): Clinical parameters plus HMGB1.\n* Model 4 (M4): Clinical parameters plus NFL and HMGB1.\n* Model 5: Clinical parameters plus p-T181-tau.\n* Model 6: Clinical parameters plus HMGB1 and p-T181-tau.\n* Model 7: Clinical parameters plus HMGB1, NFL, and p-T181-tau.\n\nModels 5, 6, and 7 were dropped because adding p-T181-tau did not improve performance. Therefore, the final models focused on M1, M2, M3, and M4.\n\nThe best-performing model used Support Vector Machines (SVM) with M3, which included clinical parameters plus HMGB1. This model achieved an Area Under the Curve (AUC) of 0.82 (±0.16). Feature importance analysis showed that CD4, HMGB1, and NFL each contributed over 10% to the model's performance.\n\nThe feature selection process was conducted using cross-validation, ensuring that the selection was done using the training set only. This approach helped to prevent data leakage and provided a more robust estimation of model performance.",
  "optimization/fitting": "The fitting method employed in this study involved several machine learning algorithms, including Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and AdaBoost. The number of parameters in these models was not excessively large compared to the number of training points, which helped mitigate the risk of overfitting.\n\nTo further ensure that overfitting was not an issue, repeated 10-fold cross-validation was used. This technique involved splitting the data into 6 folds, training the model on 5 folds, and testing it on the remaining fold. This process was repeated 10 times with different splits, providing a robust estimate of model performance and helping to generalize the results to unseen data.\n\nAdditionally, feature importance analysis was conducted to understand the contribution of each variable in the model. This analysis helped in identifying the most significant features, such as CD4, HMGB1, and NFL, which contributed over 10% to the model's performance. By focusing on these key features, the model's complexity was kept in check, reducing the risk of overfitting.\n\nUnderfitting was addressed by using ensemble methods, which combined multiple weak learners to create a more robust estimator. For instance, an ensemble version of KNN was used to leverage the high variance of low K, KNN models. This approach ensured that the model was complex enough to capture the underlying patterns in the data without being too simplistic.\n\nMoreover, the use of different algorithms, including tree-based methods like Random Forests and Adaptive Boosting, provided a diverse set of models. This diversity helped in capturing different aspects of the data, ensuring that the models were not underfitting. The comparison of these models using receiver-operating characteristics (ROC) curves and area under the curve (AUC) further validated their performance and ensured that they were not too simplistic.\n\nIn summary, the fitting method involved careful consideration of model complexity, the use of cross-validation to prevent overfitting, and ensemble methods to avoid underfitting. These techniques ensured that the models were well-suited to the data and provided reliable predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure robust model performance. One of the key methods used was repeated cross-validation. Specifically, we utilized repeated (10x) 6-fold cross-validation. This approach allowed us to balance the training and test sizes effectively, ensuring that our models were adequately trained and evaluated. The repeated cross-validation provided a more exhaustive estimation of model performance by offering multiple perspectives on the data.\n\nAdditionally, we used ensemble methods, which are known for their ability to reduce overfitting. For instance, we employed an ensemble version of K-Nearest Neighbors (KNN) that leverages the high variance of low K, KNN models (K=1). This ensemble approach uses individual KNN models as weak learners to build a more robust estimator. Similarly, we used AdaBoost, which sequentially builds weak learners, with each subsequent model learning from the misclassifications of the previous one. This method helps in improving the overall performance and generalization of the model.\n\nFurthermore, we performed feature importance analysis to understand the contribution of each variable in the model. This analysis helped in identifying the most significant features, such as CD4, HMGB1, and NFL, which contributed over 10% of the performance. By focusing on these important features, we could reduce the complexity of the model and prevent overfitting.\n\nIn summary, our study incorporated repeated cross-validation, ensemble methods, and feature importance analysis to prevent overfitting and ensure that our models generalized well to new data.",
  "optimization/config": "Not enough information is available.",
  "model/interpretability": "The models employed in our study are not entirely black-box, as we have taken steps to ensure interpretability. We utilized several machine learning algorithms, including Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and AdaBoost, each of which has its own level of transparency.\n\nSVM, for instance, is relatively interpretable because it relies on support vectors—specific data points that are critical for defining the decision boundary. By examining these support vectors, one can gain insights into which data points are most influential in the model's decisions. Additionally, we performed feature importance analysis to understand the contribution of each variable in the model. This analysis revealed that CD4, HMGB1, and NFL each contributed over 10% to the model's performance, indicating their significance in predicting cognitive impairment.\n\nFor the KNN algorithm, while it is generally considered less interpretable due to its reliance on distance metrics, we used an ensemble version that leverages multiple weak learners. This approach helps in understanding how different subsets of the data contribute to the overall prediction. AdaBoost, on the other hand, builds an ensemble of weak learners sequentially, where each subsequent model learns from the misclassifications of the previous one. This iterative process provides a clearer understanding of which features are most important at each step.\n\nFurthermore, we generated partial dependency plots to visualize the relationship between the response variable (probability of neurocognitive impairment) and the feature values of CD4, HMGB1, NFL, and p-T181-tau. These plots showed that the probability of neurocognitive impairment varies nonlinearly with both HMGB1 and NFL, with steep inflections at specific concentration levels. This visual representation helps in interpreting how changes in these features affect the model's predictions.\n\nIn summary, while our models are not entirely transparent, we have employed techniques such as feature importance analysis and partial dependency plots to enhance their interpretability. These methods provide valuable insights into the factors driving the model's decisions, making it more understandable and trustworthy.",
  "model/output": "The model employed in our study is a classification model. It is designed to distinguish between neurocognitive impairment (NPI) and no neurocognitive impairment (NPN) samples. Various supervised-learning methods were utilized, including tree-based algorithms, clustering, ensembling, and support vector machines. These methods were chosen for their proven success in classification tasks. The performance of the models was evaluated using the area under the curve (AUC) of the receiver-operating characteristic (ROC) curve, which is a common metric for assessing the effectiveness of classification models. The best-performing model, which used a support vector machine (SVM) with clinical parameters plus HMGB1, achieved an AUC of 0.82 (±0.16). This indicates that the model is effective in classifying the presence of neurocognitive impairment.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "To evaluate the performance of our machine learning algorithms, we employed repeated (10x) 6-fold cross-validation. This method was chosen to ensure an optimal balance between training and test sizes, given our sample size of 60 participants. The repeated cross-validation allowed for a more comprehensive estimation of model performance by providing multiple perspectives.\n\nWithin the main cross-validation loop, we took precautions to prevent data leakage between test folds, ensuring that each model was evaluated on unseen data. This approach helped to mitigate overfitting and provided a more reliable assessment of each model's generalizability.\n\nWe evaluated the models using the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve for each cross-validation instance. To further enhance the robustness of our performance estimates, we calculated confidence intervals via bootstrap aggregation of the cross-validated AUCs.\n\nWe compared several models, each incorporating different combinations of clinical parameters and biomarkers. The models included:\n\n* Model 1 (M1): Clinical parameters only (sex, ethnicity, age, education, CD4 count)\n* Model 2 (M2): M1 + NFL\n* Model 3 (M3): M1 + HMGB1\n* Model 4 (M4): M1 + NFL + HMGB1\n\nWe also initially tested models that included p-T181-tau, but these did not improve performance and were subsequently dropped from further analysis.\n\nThe best-performing model was identified as the Support Vector Machine (SVM) using Model 3, which included clinical parameters plus HMGB1, achieving an AUC of 0.82 (±0.16). While adding NFL in Model 4 did not significantly improve the SVM's performance, it did enhance the predictive value in other algorithms such as Ensemble KNN and AdaBoost.\n\nTo understand the contribution of each variable, we performed a feature importance analysis. This analysis revealed that CD4, HMGB1, and NFL each contributed over 10% to the model's performance. The probability of cognitive impairment varied nonlinearly with both HMGB1 and NFL concentrations, with steep inflections observed at specific threshold levels. Higher CD4 counts were generally associated with a lower probability of cognitive impairment, although this relationship showed high variability.",
  "evaluation/measure": "In our evaluation of the machine learning models, we primarily focused on the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve as our key performance metric. The AUC provides a comprehensive measure of the model's ability to distinguish between the presence and absence of neurocognitive impairment (NPI) across all possible classification thresholds. This metric is widely used in the literature for evaluating binary classification problems, particularly in medical and biological studies, making it a representative and reliable choice for our analysis.\n\nWe employed repeated (10x) 6-fold cross-validation to ensure robust and unbiased performance estimates. This approach allowed us to aggregate the AUC values across multiple validation instances, providing a more stable and generalizable assessment of model performance. Additionally, we calculated confidence intervals via bootstrap aggregation of the cross-validated AUCs to quantify the uncertainty associated with our performance estimates.\n\nTo further understand the contribution of each variable in the models, we performed feature importance analysis. This analysis helped us identify which clinical and biomarker features were most influential in predicting neurocognitive impairment. Specifically, we found that CD4 count, HMGB1, and NFL each contributed significantly to the model's performance, highlighting their importance in the predictive process.\n\nWhile the AUC was our primary performance metric, we also considered the non-linear relationships between the biomarkers and the probability of neurocognitive impairment. For instance, we observed that higher concentrations of HMGB1 and NFL increased the probability of NPI, with steep inflections at specific concentration levels. This detailed analysis provided insights into how individual features interact with the outcome variable, enhancing our understanding of the underlying biological processes.\n\nIn summary, our use of AUC as the primary performance metric, along with cross-validation and feature importance analysis, ensures that our evaluation is both rigorous and representative of established practices in the field. These measures collectively provide a comprehensive assessment of our models' predictive capabilities and the significance of the features involved.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of different machine learning algorithms and models specifically tailored to our dataset, which included clinical parameters and neuronal extracellular vesicle (nEV) protein markers.\n\nWe compared several machine learning algorithms, including K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and AdaBoost. Each of these algorithms was evaluated using repeated 10x 6-fold cross-validation to ensure a robust estimation of model performance. This approach allowed us to assess the diagnostic performance of different models in predicting neurocognitive impairment (NPI) in HIV-positive individuals.\n\nFor the KNN algorithm, we used an ensemble version that leverages the high variance of low K, KNN models (K=1). This ensemble method aims to build a more robust estimator by using individual KNN models as weak learners.\n\nSVM was chosen for its proven performance on smaller datasets and its success in scientific studies involving \"omics\" type data. We utilized SVM to determine the best separating boundary for classifying data points, employing kernel functions to handle non-linear boundaries or higher-dimensional data.\n\nAdaBoost was employed to leverage an ensemble of weak learners, specifically decision trees, to sequentially build models that learn from the misclassifications of previous models. This method aims to optimize the estimation by iteratively improving the model's performance.\n\nIn addition to these algorithms, we compared different models that included various combinations of clinical features and nEV protein markers. The models evaluated were:\n\n* Model 1 (M1): Clinical parameters only\n* Model 2 (M2): M1 + Neurofilament Light (NFL)\n* Model 3 (M3): M1 + High-Mobility Group Box 1 (HMGB1)\n* Model 4 (M4): M1 + NFL + HMGB1\n\nWe found that the combined models (M2, M3, and M4) generally outperformed the clinical model (M1) in predicting NPI. Specifically, the SVM algorithm with Model 3 (clinical parameters plus HMGB1) showed the best performance with an Area Under the Curve (AUC) of 0.82 (±0.16). Adding NFL in Model 4 did not significantly improve the prediction value for SVM but did add predictive value when using Ensemble KNN and AdaBoost.\n\nIn summary, while we did not compare our methods to publicly available benchmarks, we conducted a thorough comparison of different machine learning algorithms and models within our dataset. This approach allowed us to identify the most effective combination of algorithms and features for predicting neurocognitive impairment in HIV-positive individuals.",
  "evaluation/confidence": "Confidence intervals were calculated for the performance metrics using bootstrap aggregation of cross-validated AUCs. This approach provides a range within which the true AUC value is expected to lie, giving an indication of the uncertainty around the point estimate.\n\nStatistical significance was assessed using paired t-tests to compare the performance of different models. The combined models, which included both clinical parameters and biomarkers, outperformed the clinical model alone in predicting neurocognitive impairment. This difference was statistically significant with a p-value of less than 0.05. This suggests that the inclusion of biomarkers such as HMGB1 and NFL significantly improves the predictive ability of the models compared to using clinical parameters alone.",
  "evaluation/availability": "Not enough information is available."
}