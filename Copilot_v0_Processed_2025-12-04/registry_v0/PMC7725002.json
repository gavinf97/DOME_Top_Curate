{
  "publication/title": "Functional Connectivity Combined With a Machine Learning Algorithm Can Classify High-Risk First-Degree Relatives of Patients With Schizophrenia and Identify Correlates of Cognitive Impairments",
  "publication/authors": "The authors who contributed to the article are:\n\nYZ and HW contributed to the manuscript revision. All authors read and approved the submitted version.",
  "publication/journal": "Frontiers in Neuroscience",
  "publication/year": "2020",
  "publication/doi": "10.3389/fnins.2020.577568",
  "publication/tags": "- Functional Connectivity\n- Schizophrenia\n- First-Degree Relatives\n- Support Vector Machine\n- Classification\n- Cognitive Function\n- Resting-State fMRI\n- Machine Learning\n- Neuroimaging\n- Biomarkers",
  "dataset/provenance": "The dataset used in this study was collected from the First Affiliated Hospital (Xijing Hospital) of Air Force Medical University. The study sample consisted of 40 first-episode schizophrenia (SCZ) patients, 36 first-degree relatives (FDRs) of patients with SCZ, and 40 healthy controls (HCs). However, after applying exclusion criteria based on head motion during MRI scans, the final dataset included 38 SCZ patients, 33 FDRs, and 38 HCs.\n\nThe MRI data were acquired using a 3.0-T Siemens Magnetom Trio Tim scanner. Resting-state functional magnetic resonance imaging (RS-fMRI) scans were obtained with specific parameters, including a repetition time (TR) of 2,000 ms, echo time (TE) of 30 ms, and a total of 240 volumes over 7 minutes. Additionally, a high-resolution T1 image was acquired for anatomical reference.\n\nThe dataset has been used in previous studies to investigate functional brain connectivity patterns for classifying SCZ patients from healthy controls. For instance, previous research has shown that functional brain networks derived from RS-fMRI can achieve an accuracy of up to 85.5% in distinguishing SCZ patients from HCs. The current study builds on this foundation by exploring the classification efficiency of functional connectivity (FC) obtained from RS-fMRI for distinguishing SCZ patients from HCs and applying these classification models to determine similarities between FDRs and SCZ patients or HCs.",
  "dataset/splits": "In our study, we employed a leave-one-out cross-validation (LOOCV) strategy. This means that for each iteration of the cross-validation process, one subject's data was used as the test set, while the remaining data was used to train the classifier. Given that we had 38 schizophrenia (SCZ) patients, 33 first-degree relatives (FDRs), and 38 healthy controls (HCs), this resulted in 109 iterations of LOOCV, as each subject's data was used once as the test set.\n\nIn each iteration, the features were ranked based on their F-scores, and the top 644 features were selected to build the classifier. This process ensured that the classifier was trained and tested on different subsets of the data, providing a robust evaluation of its performance.\n\nThe distribution of data points in each split was as follows:\n\n* 38 splits with 37 SCZ patients and 71 other subjects (33 FDRs and 38 HCs).\n* 33 splits with 38 SCZ patients, 32 FDRs, and 38 HCs.\n* 38 splits with 37 SCZ patients, 33 FDRs, and 37 HCs.\n\nThis approach allowed us to leverage all available data for both training and testing, maximizing the use of our dataset while ensuring that each subject's data was used for validation exactly once.",
  "dataset/redundancy": "In our study, we employed a leave-one-out cross-validation (LOOCV) strategy to evaluate the performance of our classifier. This approach ensures that each subject's data is used once as the test set while the classifier is trained on the remaining dataset. This method is particularly useful when dealing with a relatively small sample size, as it maximizes the use of available data.\n\nTo enforce independence between the training and test sets, each iteration of the LOOCV involved a different subject as the test data. This means that for each of the 76 iterations (corresponding to the 76 subjects in the combined SCZ and HC groups), a unique subject was set aside as the test subject, and the classifier was trained on the data from the remaining 75 subjects. This process ensures that the test data is never used in the training phase, maintaining the independence required for a robust evaluation.\n\nThe distribution of our dataset compares favorably with previously published machine learning datasets in the field of neuroscience. Our sample consisted of 38 patients with schizophrenia (SCZ), 33 first-degree relatives (FDRs) of SCZ patients, and 38 healthy controls (HCs). This distribution allows for a comprehensive analysis of functional connectivity patterns and their potential to distinguish between SCZ patients and HCs, as well as to identify similar patterns in FDRs.\n\nThe demographic and clinical features of the participants were carefully matched to minimize confounding variables. There were no significant differences in age, gender distribution, education level, and frame-wise displacement across the three groups. However, significant differences were found in semantic fluency scores, which is a critical measure in the context of SCZ research. This careful matching and the use of LOOCV ensure that our findings are robust and generalizable.",
  "dataset/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This means that the dataset, including the data splits used, is not publicly available at the moment, but it can be obtained by requesting it from the authors. The authors have committed to sharing the data, ensuring that other researchers can verify the findings and potentially build upon them. However, the specifics of how the data will be shared, such as the format and any potential licensing agreements, have not been detailed. The enforcement of this data sharing policy relies on the authors' commitment to providing the data upon request.",
  "optimization/algorithm": "The machine-learning algorithm used in our study is the Support Vector Machine (SVM). This algorithm is well-established and widely used in various fields, including neuroimaging studies. It was chosen for its efficiency, particularly when dealing with relatively small sample sizes. The SVM algorithm, specifically a linear kernel SVM, was implemented using the LIBSVM toolbox. This choice was made to avoid overfitting and to allow for the direct extraction of feature weights, which are crucial for understanding the contributions of different brain regions to the classification.\n\nThe SVM algorithm is not new; it has been extensively studied and applied in numerous research areas. The reason it was not published in a machine-learning journal is that our focus is on its application in neuroimaging, specifically in distinguishing schizophrenia patients from healthy controls and identifying functional connectivity patterns in first-degree relatives. The innovation lies in the application of this algorithm to neuroimaging data, rather than in the algorithm itself. Our study demonstrates the effectiveness of SVM in this context, highlighting its potential as a tool for early detection and prognosis in schizophrenia research.",
  "optimization/meta": "The model employed in this study does not function as a meta-predictor. It relies solely on a Support Vector Machine (SVM) algorithm for classification. The SVM algorithm was chosen for its efficiency, particularly when dealing with relatively small sample sizes. A linear kernel SVM was implemented using the LIBSVM toolbox, with the parameter C set to its default value of 1. The Leave-One-Out Cross-Validation (LOOCV) strategy was used to evaluate the model's performance, ensuring that each subject's data was used once as test data while the classifier was trained on the remaining dataset. This approach helps in avoiding overfitting and allows for the direct extraction of feature weights.\n\nThe features used in the classification process were functional connectivity (FC) patterns derived from resting-state functional magnetic resonance imaging (RS-fMRI). These FC patterns were ranked according to their F-scores in descending order, and the top-ranked 644 features were selected for each iteration of the LOOCV. The discriminative score for each tested individual was acquired from the SVM classifier, and an ROC curve was created to assess the model's classification power.\n\nThe study did not combine outputs from multiple machine-learning algorithms to form a meta-predictor. Instead, it focused on the SVM algorithm's ability to distinguish between schizophrenia (SCZ) patients and healthy controls (HCs) based on their FC patterns. The training data for the SVM classifier was independent for each iteration of the LOOCV, ensuring that the model's performance was evaluated objectively.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps. Initially, resting-state functional magnetic resonance imaging (rs-fMRI) data were collected with specific parameters, including a repetition time of 2,530 ms, echo time of 3.5 ms, flip angle of 7 degrees, and a resolution of 1 × 1 × 1 mm. The first 10 images were discarded to ensure data stability, and the remaining 230 images were slice timing corrected and realigned to the first image. Motion parameters were assessed, and subjects with excessive motion were excluded. Frame-wise displacement was calculated, and no significant differences were found across the groups.\n\nTo reduce the effects of nuisance factors, signals from cerebrospinal fluid and white matter were regressed out, although the global signal was not removed. The data were then normalized into Montreal Neurological Institute space using the DARTEL toolbox and smoothed with a 6-mm full width at half maximum Gaussian kernel.\n\nThe brain was parcellated into 200 regions of interest using the Craddock atlas. The time series within each region were band-pass filtered between 0.01 and 0.08 Hz and then averaged. Functional connectivity (FC) was calculated between each pair of regions using Pearson’s correlation coefficients, resulting in 19,900-dimensional FC feature vectors for each subject.\n\nBefore building the classifier model, an initial feature selection step was performed using the F-score for feature ranking. Leave-one-out cross-validation (LOOCV) was employed to evaluate the classifier's performance. For each LOOCV iteration, features were ranked from highest to lowest according to their F-score, and the top 644 features were used to build the classifier. Consensus features, which were consistently selected across all iterations, were identified, and their weights were averaged. Additionally, a weight for each region of interest was calculated by summing half of the consensus feature weights associated with that region, representing its discriminatory ability between schizophrenia patients and healthy controls.",
  "optimization/parameters": "In our study, the Support Vector Machine (SVM) algorithm was employed for classification, utilizing a linear kernel to avoid overfitting and facilitate the direct extraction of feature weights. The parameter C, which controls the trade-off between achieving a low training error and a low testing error, was set to its default value of 1. This choice was made to maintain simplicity and to leverage the known efficiency of the SVM algorithm with relatively small sample sizes.\n\nThe number of features used in the model was determined through a feature selection process. Initially, functional connectivity (FC) measures were calculated between 200 regions of interest (ROIs) using Pearson’s correlation coefficients, resulting in 19,900 dimensional FC feature vectors for each subject. To reduce dimensionality, an F-score was used to rank these features. In each iteration of the leave-one-out cross-validation (LOOCV), the top-ranked 644 features were selected for building the classifier. This number was chosen based on the performance of the classifier, as it provided a good balance between dimensionality reduction and classification accuracy.\n\nConsensus features were identified as those that were consistently selected across all LOOCV iterations. This approach ensured that the selected features were robust and contributed significantly to the classification model. In total, 397 consensus features were identified, which were used to calculate the weights for each ROI. These weights represented the ability of each region to discriminate between patients with schizophrenia (SCZ) and healthy controls (HCs).",
  "optimization/features": "In the study, the input features used for classification were functional connectivity (FC) measures derived from resting-state functional magnetic resonance imaging (rs-fMRI) data. The whole brain was parcellated into 200 regions of interest (ROIs) using the Craddock atlas, resulting in 19,900 FC features for each subject.\n\nFeature selection was performed to reduce data dimensionality. An F-score was used for feature ranking, which is an effective method for this purpose. Leave-one-out cross-validation (LOOCV) was employed to evaluate the classifier's performance. For each LOOCV iteration, features were ranked from highest to lowest according to their F-score, and the top 644 features were selected to build the classifier.\n\nTo account for variations in feature selection across LOOCV iterations, consensus features were identified. These are the features that were consistently selected across all iterations. The weight for each consensus feature was defined as the average of the weights across the 76 LOOCV iterations. This approach ensures that the feature selection process is robust and not overly dependent on any single iteration.",
  "optimization/fitting": "In our study, we employed a linear kernel Support Vector Machine (SVM) for classification, which is particularly suitable when the number of features exceeds the number of samples. This scenario is common in neuroimaging studies, where the dimensionality of the data is typically much higher than the number of subjects.\n\nTo mitigate the risk of overfitting, we implemented a leave-one-out cross-validation (LOOCV) strategy. This method ensures that each subject is used once as a test sample while the model is trained on the remaining data. By doing so, we obtained a robust estimate of the model's performance and avoided overfitting to any single subject's data. Additionally, we used a linear kernel SVM, which is less prone to overfitting compared to non-linear kernels, especially when the number of features is large.\n\nTo further ensure the model's generalizability, we assessed the statistical significance of the accuracy using a permutation test. This test involves repeatedly shuffling the class labels and recalculating the accuracy to build a null distribution. The observed accuracy is then compared to this distribution to determine its significance. This approach helps to rule out the possibility of obtaining high accuracy by chance.\n\nRegarding underfitting, we selected the top-ranked 644 features based on their F-scores in each iteration of the LOOCV. This feature selection step helps to retain the most discriminative information while reducing the dimensionality of the data. The use of a linear kernel SVM also ensures that the model is capable of capturing linear relationships in the data, which is appropriate given the nature of our features.\n\nMoreover, the high classification accuracy (88.15%) and the area under the receiver operating characteristic curve (AUC) of 0.93 indicate that the model is well-fitted to the data and generalizes well to unseen samples. The consistency of these results across multiple iterations of the LOOCV further supports the robustness of our model.",
  "optimization/regularization": "To prevent overfitting in our study, we employed several techniques. Firstly, we used a linear kernel Support Vector Machine (SVM) for classification. This choice was made because linear SVMs are less prone to overfitting compared to non-linear kernels, especially when the sample size is relatively small. Additionally, the linear kernel allows for direct extraction of feature weights, which is beneficial for interpreting the model.\n\nWe also implemented leave-one-out cross-validation (LOOCV) to evaluate the performance of our classifier. In LOOCV, each subject is used once as the test data, and the classifier is trained on the remaining dataset. This method ensures that every data point is used for both training and testing, providing a robust estimate of the model's performance and helping to prevent overfitting.\n\nFurthermore, we performed feature selection using an F-score for feature ranking. This step helps in reducing the dimensionality of the data by selecting the most relevant features, which in turn helps to mitigate overfitting. We identified consensus features that were consistently selected across all iterations of the LOOCV, ensuring that the final model is based on the most reliable and informative features.\n\nLastly, we assessed the statistical significance of the accuracy using a permutation test. This test involves repeatedly shuffling the class labels and recalculating the accuracy to build a null distribution. The observed accuracy is then compared to this null distribution to determine its significance, providing an additional layer of validation against overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the publication. Specifically, we utilized a linear kernel Support Vector Machine (SVM) implemented using the LIBSVM toolbox, with the parameter C set to its default value of 1. The feature selection process involved ranking features based on their F-scores and selecting the top 644 features for each iteration of the Leave-One-Out Cross-Validation (LOOCV). The consensus features, which were consistently selected across all iterations, were identified and their weights were averaged to determine their contribution to the classification model.\n\nThe optimization schedule involved using LOOCV to evaluate the classifier's performance, where one subject was used as the test data and the classifier was trained on the remaining dataset. This process was repeated for each subject, ensuring that the model's performance was robust and not overfitted to any single subject's data.\n\nRegarding the availability of model files and optimization parameters, the specific model files used in our study are not publicly available. However, the methods and parameters described in the publication can be replicated using the LIBSVM toolbox, which is freely available under the BSD license. Researchers interested in replicating our study can follow the detailed methods section to implement the same hyper-parameter configurations and optimization schedule.\n\nThe publication itself is available under the terms of the Creative Commons Attribution License, which allows for the sharing and adaptation of the work, provided that appropriate credit is given. This ensures that the methods and findings reported in our study can be accessed and utilized by the scientific community for further research and validation.",
  "model/interpretability": "The model employed in this study is not a blackbox. The use of a linear kernel Support Vector Machine (SVM) allows for direct extraction of feature weights, making the model interpretable. This approach was chosen to reduce the risk of overfitting and to facilitate the understanding of which functional connectivity (FC) features and regions of interest (ROIs) contribute most to the classification.\n\nThe model's transparency is evident in several ways. Firstly, the linear SVM provides a weight vector that indicates the importance of each feature in the classification process. These weights can be examined to identify which FC features are most discriminative between patients with schizophrenia (SCZ) and healthy controls (HCs).\n\nSecondly, consensus features were identified as those that were consistently selected across all iterations of the leave-one-out cross-validation (LOOCV). This resulted in 397 consensus features, which are robust and reliable indicators of the differences between SCZ patients and HCs. The weights for these consensus features were averaged across all LOOCV iterations, providing a stable measure of their importance.\n\nAdditionally, the model's transparency extends to the ROIs. Weights for each ROI were calculated by summing half of the consensus feature weights associated with that region. This process highlighted 18 regions with weights significantly greater than the average, indicating their strong contribution to the model. These regions include key areas within the default mode network, frontal-parietal network, auditory network, and sensorimotor network.\n\nThe interpretability of the model is further supported by the correlation analysis between classification scores and cognitive function measures in first-degree relatives (FDRs). The significant negative correlation found between classification scores and semantic fluency scores suggests that the model captures meaningful patterns related to cognitive deficits associated with SCZ.\n\nIn summary, the use of a linear SVM with direct extraction of feature weights, the identification of consensus features, and the calculation of ROI weights all contribute to the model's transparency. This allows for a clear understanding of which FC patterns and brain regions are most relevant to the classification of SCZ patients and HCs, as well as the prediction of cognitive performance in FDRs.",
  "model/output": "The model employed in this study is a classification model. Specifically, a Support Vector Machine (SVM) algorithm with a linear kernel was used. This model was chosen for its efficiency with relatively small sample sizes. The SVM was trained to classify patients with schizophrenia (SCZ) and healthy controls (HCs), with SCZ patients labeled as 1 and HCs as -1. The model's performance was evaluated using leave-one-out cross-validation (LOOCV), and metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve (AUC) were obtained. The model achieved an accuracy of 88.15%, with an AUC of 0.93, indicating strong classification power. Additionally, the model was applied to classify first-degree relatives (FDRs) to determine if their functional connectivity patterns were more similar to those of SCZ patients or HCs. The classification scores derived from the model were also used to investigate correlations with cognitive function measures in FDRs.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in this study involved a leave-one-out cross-validation (LOOCV) strategy. In LOOCV, each subject's data is used once as the test set, while the remaining data is used for training the classifier. This process is repeated for each subject, ensuring that every subject's data is used for both training and testing.\n\nFor each iteration of the LOOCV, features were ranked based on their F-scores, and the top 644 features were selected to build the classifier. This approach was chosen to handle the high-dimensional nature of the functional connectivity (FC) data and to identify the most discriminative features.\n\nThe classifier used was a linear kernel support vector machine (SVM), implemented with the LIBSVM toolbox. The linear kernel was chosen to avoid overfitting and to allow for the direct extraction of feature weights. The parameter C was set to its default value of 1.\n\nThe performance of the classifier was evaluated using several metrics: accuracy, sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve (AUC). The statistical significance of the accuracy was assessed using a permutation test.\n\nAdditionally, the study investigated whether the final classification model could be used to determine if first-degree relatives (FDRs) of schizophrenia (SCZ) patients showed similar FC patterns to SCZ patients or healthy controls (HCs). The FC of each FDR was input as test data into each iteration of the LOOCV to obtain prediction labels. The classification score, which is the average of these prediction labels, was used as a measure to characterize the similarity of each FDR's FC pattern to an SCZ pattern.\n\nFurthermore, a general linear model was used to investigate the correlations between classification scores and measures of cognitive function in FDRs, with age, sex, and years of education as covariates. A semantic fluency test was administered to evaluate executive function and semantic memory, which are severely affected in SCZ.",
  "evaluation/measure": "In the evaluation of our classifier, several key performance metrics were reported to assess its effectiveness in distinguishing between schizophrenia (SCZ) patients and healthy controls (HCs). The primary metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve (AUC).\n\nThe accuracy of the linear Support Vector Machine (SVM) classifier reached up to 88.15%, indicating a high level of correctness in classifying individuals. Sensitivity, which measures the true positive rate, was reported at 84.06%, showing the classifier's ability to correctly identify SCZ patients. Specificity, which measures the true negative rate, was 92.18%, demonstrating the classifier's effectiveness in correctly identifying healthy controls.\n\nThe AUC, a comprehensive measure of the classifier's performance across all threshold levels, was 0.93. This high AUC value indicates strong discriminative power, suggesting that the classifier can effectively differentiate between SCZ patients and HCs.\n\nAdditionally, the statistical significance of the accuracy was assessed using a permutation test, which confirmed the robustness of the results with a p-value of less than 0.001. This set of metrics is representative of standard practices in the literature for evaluating classification models in neuroimaging studies, ensuring that our findings are comparable and reliable.",
  "evaluation/comparison": "Not applicable. The publication focuses on the use of a linear Support Vector Machine (SVM) for classifying individuals as patients with schizophrenia (SCZ) or healthy controls (HCs) based on functional connectivity (FC) patterns. The study does not mention any comparison to publicly available methods or simpler baselines on benchmark datasets. The evaluation primarily revolves around the performance of the SVM classifier itself, including metrics such as accuracy, sensitivity, specificity, and the area under the receiver operating characteristic (ROC) curve. The study also discusses the use of leave-one-out cross-validation (LOOCV) and the identification of consensus features and region weights. However, there is no indication of comparing the proposed method with other existing methods or baselines.",
  "evaluation/confidence": "The evaluation of the classifier's performance included several key metrics, each assessed with statistical rigor. The accuracy of the linear SVM classifier reached 88.15%, with a sensitivity of 84.06% and a specificity of 92.18%. The statistical significance of the accuracy was evaluated using a permutation test, which yielded a p-value of less than 0.001, indicating strong statistical significance.\n\nThe area under the receiver operating characteristic (ROC) curve (AUC) was 0.93, demonstrating the classifier's robust discriminative power. The ROC curve also included a 95% confidence interval, providing a visual representation of the classifier's performance variability.\n\nTo ensure the reliability of the results, a leave-one-out cross-validation (LOOCV) strategy was employed. This method involved using one subject as test data and training the classifier on the remaining dataset, repeating this process for each subject. This approach helped to mitigate overfitting and provided a more accurate assessment of the classifier's generalizability.\n\nAdditionally, the study identified consensus features that were consistently selected across all LOOCV iterations, further enhancing the confidence in the selected features' relevance. The weights for these consensus features were calculated as the average of the weights across all iterations, ensuring stability and reliability in the feature selection process.\n\nThe use of a linear kernel SVM, implemented with the LIBSVM toolbox, allowed for direct extraction of feature weights, which were then used to calculate region weights. This transparency in the feature weighting process added another layer of confidence in the results.\n\nOverall, the evaluation metrics, statistical tests, and cross-validation strategy collectively provide a high level of confidence in the classifier's performance and the robustness of the identified features.",
  "evaluation/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This indicates a commitment to transparency and reproducibility in research. However, the specific details on where and how the data will be made available, as well as the licensing terms, are not provided. Interested parties would need to contact the authors directly to obtain the raw data. This approach ensures that the data can be accessed by other researchers for verification or further analysis, but it may introduce some barriers due to the need for direct communication with the authors."
}