{
  "publication/title": "PmDNE: A Method Based on Network Embedding and Network Similarity Analysis for Predicting miRNA-Disease Association",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BioMed Research International",
  "publication/year": "2021",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- miRNA-disease association\n- Network embedding\n- Prediction\n- Machine learning\n- Random forest\n- DeepWalk\n- Similarity analysis\n- Bioinformatics\n- Biomedical networks\n- Classification",
  "dataset/provenance": "The dataset used in this study is sourced from the HMDD3.0 database, which is accessible at http://www.cuilab.cn/hmdde. This database was chosen for its up-to-date information on miRNA-disease associations, making it a reliable source for effective predictions. The dataset includes 894 disease nodes and 1208 miRNA nodes, with a total of 18733 documented associations between diseases and miRNAs.\n\nIn addition to the miRNA-disease association data, the study also utilizes disease similarity data. This data is constructed using a directed acyclic graph (DAG) to describe diseases, based on the medical subject headings (MeSH) descriptors available from the National Library of Medicine. The dataset contains 414003 related disease similarity relationships.\n\nFurthermore, miRNA similarity data is incorporated, calculated using the method proposed by Wang et al. This method provides functional similarity for 495 miRNA nodes, enhancing the comprehensiveness of the dataset.\n\nThe construction of the miRNA-disease bipartite network involves assigning a weight of 1 to edges where a correlation exists and a weight of 0 to nonexistent edges. This transformation allows the prediction method to be framed as a binary classification problem. For isomorphic networks, the weight of similarity data is directly used as the weight of the isomorphic network.",
  "dataset/splits": "The dataset was divided into a 4:1 ratio, resulting in two primary splits: a training set and a test set. This division was achieved through five-fold cross-validation, ensuring that the final features were robustly evaluated. The feature dimension for this process was set at 128 dimensions, leading to the extraction of 2102 node vectors. Given the unbalanced nature of the classification task, an equal number of unconnected edges were randomly selected to serve as negative examples. This approach helped in maintaining a balanced dataset for training and testing purposes. The random forest algorithm was employed to predict the parameters, with the weight of similarity between the two networks set at 0.5. The maximum number of steps for the random walk was 32, while the minimum was 1. The probability of stopping immediately during the random walk was set at 0.15. Additionally, three optimization objective functions were used with values of 0.0001, 0.01, and 0.1, respectively. The AUC values for the ROC and PR curves were 0.8954 ± 0.001 and 0.9002 ± 0.0015, respectively.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The optimization algorithm employed in this work leverages random gradient descent to update the embedding vectors. This approach is not entirely new, as it builds upon established techniques in the field of machine learning and network embedding. The use of random gradient descent is a well-known method for optimizing parameters in large-scale machine learning models, particularly in the context of neural networks and deep learning.\n\nThe decision to use this method in this specific context is driven by the need to handle the complexities of network embedding, where the goal is to learn meaningful representations of nodes in a graph. The algorithm maximizes an objective function that considers both the first and second similarity relations within the network. This is achieved through a combination of optimization objectives, each weighted by coefficients α, β, and γ, which balance the contributions of different similarity measures.\n\nThe choice of random gradient descent is practical for several reasons. Firstly, it allows for efficient updates of the embedding vectors, which is crucial given the high dimensionality of the feature space. Secondly, it enables the incorporation of negative sampling, a technique that simplifies the computation of the softmax function by transforming it into a binary classification problem. This makes the algorithm more scalable and suitable for large networks.\n\nWhile the algorithm itself is not novel, its application in this particular context—combining network similarity and random walks to improve prediction accuracy—is a significant contribution. The method has been implemented in Python, which, while not as efficient as C++, offers the advantage of extensive data processing libraries that facilitate code development and experimentation.\n\nThe results demonstrate that this approach yields superior performance compared to other network embedding methods, as evidenced by higher AUC values for both ROC and PR curves. This indicates that the algorithm effectively captures the underlying structure of the network, leading to more accurate predictions.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. Instead, it employs a random forest classifier for predictions. The data used for training and testing is divided into a 4:1 ratio, with five-fold cross-validation applied to obtain the final features. The dataset is unbalanced, and this issue is addressed by randomly selecting an equal number of unconnected edges as negative examples. The random forest classifier is then used to predict the parameters, with specific settings for the weight of similarity between networks, the maximum and minimum number of steps for random walks, the probability of stopping immediately, and the optimization objective functions. The AUC values for ROC and PR are reported as 0.8954 ± 0.001 and 0.9002 ± 0.0015, respectively.",
  "optimization/encoding": "The data encoding process involved several key steps to prepare the data for the machine-learning algorithm. Initially, the dataset was divided into a 4:1 ratio for training and testing purposes. This division was further refined using five-fold cross-validation to ensure robust feature extraction. The resulting features were represented in a 128-dimensional space, yielding 2102 node vectors.\n\nGiven the unbalanced nature of the classification task, negative examples were randomly selected to match the number of unconnected edges. This approach helped in balancing the dataset, which is crucial for training effective classifiers.\n\nThe random forest algorithm was employed to predict the parameters, with a specific focus on the similarity between two networks. The weight for this similarity was set to 0.5, indicating an equal importance between the two networks. The random walk process was configured with a maximum of 32 steps and a minimum of 1 step, with a stopping probability of 0.15. Additionally, three optimization objective functions were used, with values set to 0.0001, 0.01, and 0.1, respectively.\n\nThe evaluation metrics included AUC values for ROC and PR curves, which were found to be 0.8954 ± 0.001 and 0.9002 ± 0.0015, respectively. These metrics indicate the effectiveness of the prediction model. Furthermore, the impact of adding network similarity was assessed, showing that incorporating network similarity significantly improved the prediction accuracy. This was evident from the comparison of different embedding methods, where the approach that included network similarity yielded the best results.",
  "optimization/parameters": "In the optimization process of our model, several key parameters are utilized to fine-tune the performance and accuracy of the predictions. These parameters include the size of the context window (w_s), the number of negative samples (n_s), the dimension of the embedded vector (d), and the coefficients of the optimization function (α, β, and γ).\n\nThe context window size (w_s) is crucial as it determines the range of context considered during the random walk. A larger window size generally leads to more accurate predictions by capturing more contextual information. However, beyond a certain point, the increase in window size does not significantly improve the prediction accuracy, indicating that the context information becomes sufficient.\n\nThe number of negative samples (n_s) also plays a vital role. Increasing the number of negative samples tends to enhance the prediction accuracy, as it provides a more balanced dataset for training. This is particularly important in unbalanced classification tasks, where the model needs to learn from both positive and negative examples effectively.\n\nThe dimension of the embedded vector (d) is another critical parameter. Higher dimensions retain more original information, leading to more accurate predictions. However, similar to the context window size, there is a point beyond which increasing the dimension does not yield significant improvements.\n\nThe coefficients α, β, and γ are part of the optimization function and are used to balance the first and second similarity relationships. These coefficients have a substantial impact on the prediction results, as they regulate the proportion of explicit and implicit relationships in the embedded vector. An imbalance in these coefficients can lead to fluctuations in prediction accuracy.\n\nThe selection of these parameters was based on extensive experimentation and analysis. For instance, the maximum number of steps (max_t) for the random walk was set to 32, with a minimum of 1 step. The probability of stopping immediately during the random walk was set to 0.15. The coefficients for the optimization objective functions were chosen as 0.0001, 0.01, and 0.1, respectively. These values were determined through cross-validation and empirical testing to ensure optimal performance.",
  "optimization/features": "The input features used in our study consist of 128-dimensional vectors. These vectors are derived from node embeddings obtained through a five-fold cross-validation process on a 4:1 split dataset. The dataset is divided into training and test sets, ensuring that the final features are robust and generalizable.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, we addressed the challenge of unbalanced samples by randomly selecting an equal number of unconnected edges as negative examples. This approach helps in balancing the dataset and improving the classifier's performance.\n\nThe parameters for the random walk process, such as the maximum number of steps (32) and the probability of stopping immediately (0.15), were carefully chosen to optimize the embedding quality. Additionally, the weights for the similarity between the two networks were set to 0.5, indicating an equal importance given to both networks.\n\nThe optimization objective functions were tuned with values of 0.0001, 0.01, and 0.1, respectively, to balance the trade-offs between different similarity measures. These parameters were selected based on their impact on the area under the curve (AUC) for both the receiver operating characteristic (ROC) and precision-recall (PR) curves, ensuring that the model's predictions are both accurate and reliable.",
  "optimization/fitting": "The fitting method employed in this study involves a random forest classifier, which is known for its robustness and ability to handle high-dimensional data. The dataset used is unbalanced, and to address this, an equal number of unconnected edges were randomly selected as negative examples. This approach helps in balancing the dataset and ensuring that the classifier does not become biased towards the majority class.\n\nThe feature dimension is 128, and 2102 node vectors were obtained through five-fold cross-validation. This cross-validation technique helps in assessing the model's performance and generalizability by splitting the data into training and test sets multiple times. The use of cross-validation mitigates the risk of overfitting, as the model is trained and validated on different subsets of the data.\n\nThe random forest classifier was chosen for its ability to handle a large number of input variables without overfitting. The parameters for the random forest, such as the maximum number of steps (max_t) and the probability of stopping immediately, were carefully tuned to optimize performance. The AUC values of ROC and PR are 0.8954 ± 0.001 and 0.9002 ± 0.0015, respectively, indicating a strong predictive performance.\n\nTo further ensure that overfitting was not an issue, the model's performance was evaluated using multiple metrics, including precision, accuracy, F1 scores, and recall. These metrics provide a comprehensive view of the model's performance and help in identifying any potential issues with overfitting or underfitting.\n\nAdditionally, the influence of different networks on the results was analyzed. The best performance was observed when network similarity was added, indicating that the model effectively captures the underlying patterns in the data. This analysis also helps in ruling out underfitting, as the model's performance improves with the inclusion of relevant features.\n\nIn summary, the fitting method employed in this study involves a robust random forest classifier, cross-validation, and a comprehensive evaluation of model performance. These techniques help in mitigating the risks of overfitting and underfitting, ensuring that the model generalizes well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was cross-validation, specifically five-fold cross-validation. This technique helps to assess the model's performance and generalization ability by dividing the dataset into training and test sets multiple times, ensuring that the model is not merely memorizing the training data.\n\nAdditionally, we utilized random forests for prediction, which inherently provides a form of regularization. Random forests aggregate the results of multiple decision trees, each trained on a different subset of the data, reducing the risk of overfitting that can occur with a single decision tree.\n\nWe also carefully selected the parameters for our optimization functions. The coefficients α, β, and γ were tuned to balance the contributions of different similarity measures, ensuring that the model did not become overly reliant on any single feature. This balancing act helps in maintaining a generalizable model that performs well across various datasets.\n\nFurthermore, the dimension of the embedded vector, denoted as d, was chosen to retain sufficient original information while avoiding excessive complexity. By keeping the dimension high enough to capture essential patterns but not so high as to introduce noise, we aimed to strike a balance that minimizes overfitting.\n\nThe context window size, w_s, and the number of negative samples, n_s, were also optimized. A larger context window can provide more accurate predictions by including more relevant information, but it must be managed to avoid overfitting. Similarly, increasing the number of negative samples improves the model's ability to distinguish between positive and negative cases without leading to overfitting.\n\nIn summary, our approach to preventing overfitting involved a combination of cross-validation, the use of random forests, careful parameter tuning, and dimensionality management. These techniques collectively contribute to the development of a robust and generalizable model.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, details about the parameters such as the weight of the similarity between networks, the maximum and minimum number of steps for random walks, and the probabilities involved in the random walk process are provided. These parameters include the weight chosen for similarity (0.5), the maximum number of steps (32), the minimum number of steps (1), and the probability of stopping immediately (0.15). Additionally, the optimization objective functions and their respective values (0.0001, 0.01, and 0.1) are also mentioned.\n\nThe AUC values for ROC and PR curves, which are key indicators of the model's performance, are reported as 0.8954 ± 0.001 and 0.9002 ± 0.0015, respectively. These values demonstrate the effectiveness of the chosen parameters and the optimization process.\n\nRegarding the availability of model files and optimization schedules, the publication does not explicitly mention the provision of these resources. However, the methods and parameters described are sufficient for replication of the experiments. For access to any additional resources or code, readers may need to contact the authors directly, as the publication does not specify a public repository or license for the model files or optimization schedules.",
  "model/interpretability": "The model presented in this publication is not entirely a black box, as it incorporates several interpretable components and processes. The use of random forests for predictions is one such example, as random forests are generally considered interpretable models. The decision paths within the forest can be examined to understand which features are most important for making predictions.\n\nAdditionally, the model leverages similarity relationships between nodes, which are defined through explicit equations. These equations show how contextual and non-contextual word vectors are updated based on similarity measures. This process provides a clear mechanism for how the model learns node representations, making it more transparent than purely black-box models.\n\nThe use of random walks and skip-gram models to obtain similarity feature vectors also adds to the interpretability. The random walk strategy is designed to reflect the importance of nodes based on their connectivity, and the skip-gram model helps in capturing the contextual relationships between nodes. These steps can be traced and understood, providing insights into how the model processes the data.\n\nFurthermore, the evaluation criteria used, such as AUC, precision, accuracy, F1 scores, and recall, are standard and well-understood metrics. The results presented in tables and figures show the performance of different embedding methods, allowing for a clear comparison and interpretation of the model's effectiveness.\n\nIn summary, while the model does involve complex processes like node embedding and random walks, it includes several interpretable components and clear evaluation metrics. This makes it more transparent and understandable compared to purely black-box models.",
  "model/output": "The model is a classification model, specifically designed for binary classification tasks. It predicts miRNA-disease associations, which is a binary problem where the model determines whether a miRNA is associated with a particular disease or not.\n\nThe model uses random forests for making predictions, which is a classification technique. The performance of the model is evaluated using metrics typical for classification tasks, such as ROC_AUC, PR_AUC, precision, accuracy, F1 score, and recall. These metrics indicate that the model's primary goal is to classify instances into one of two categories accurately.\n\nThe model's output includes the area under the curve (AUC) for both the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR) curve. The AUC values for ROC and PR are 0.8954 and 0.9002, respectively, which demonstrate the model's ability to distinguish between the positive and negative classes.\n\nAdditionally, the model's performance is compared with other network embedding methods, further emphasizing its classification nature. The use of random forests, along with the evaluation metrics and comparison with other methods, confirms that the model is indeed a classification model.",
  "model/duration": "The model's execution time is at the minute level. This is due to the use of Python for implementation, which, while convenient for coding with its extensive data processing libraries, is generally less efficient than languages like C++ that are closer to the system level. Other methods implemented in C++ typically run in seconds.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps and metrics to ensure its robustness and accuracy. The dataset was divided into a 4:1 ratio for training and testing, respectively. To enhance the reliability of the results, five-fold cross-validation was employed, which helped in obtaining a more stable and generalizable performance metric.\n\nThe feature dimension was set to 128, resulting in 2102 node vectors. Given the unbalanced nature of the classification task, negative examples were randomly selected to match the number of unconnected edges. This approach helped in mitigating the bias that could arise from an imbalanced dataset.\n\nRandom forest was used as the prediction model, with the weight of similarity between the two networks set to 0.5. The maximum number of steps for the random walk was 32, with a minimum of 1 step. The probability of stopping immediately during the random walk was set to 0.15. Three different optimization objective functions were used, with values of 0.0001, 0.01, and 0.1, respectively.\n\nThe performance of the method was evaluated using several metrics, including the Area Under the Curve (AUC) for both the Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves. The AUC values obtained were 0.8954 ± 0.001 for ROC and 0.9002 ± 0.0015 for PR, indicating a strong predictive performance.\n\nAdditionally, the impact of adding network similarity was assessed. The results showed that incorporating network similarity significantly improved the prediction effect, as evidenced by the higher AUC values compared to methods that did not include network similarity.\n\nThe evaluation also considered computational efficiency, noting that the Python implementation, while convenient for coding due to its extensive data processing libraries, had a lower time efficiency compared to methods implemented in C++. The running time for this method was in the minute level, whereas other methods achieved second-level efficiency.\n\nIn summary, the method was rigorously evaluated using cross-validation, balanced sampling techniques, and multiple performance metrics. The results demonstrated the effectiveness of incorporating network similarity in improving prediction accuracy.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the effectiveness of our prediction model. The primary metrics reported include the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under the Precision-Recall Curve (AUC-PR). These metrics are crucial for evaluating the model's ability to distinguish between positive and negative classes, especially in the context of imbalanced datasets.\n\nAdditionally, we considered precision, accuracy, F1 score, and recall as secondary evaluation criteria. Precision measures the accuracy of positive predictions, while recall evaluates the model's ability to identify all relevant instances. The F1 score provides a harmonic mean of precision and recall, offering a single metric that balances both concerns. Accuracy, although less informative in imbalanced datasets, was also reported for completeness.\n\nThese metrics are widely used in the literature and provide a robust framework for comparing our model's performance with other state-of-the-art methods. The AUC-ROC and AUC-PR are particularly important as they offer a more nuanced view of the model's performance across different threshold settings, making them representative and comparable to other studies in the field.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our proposed method against several publicly available network embedding techniques to ensure a comprehensive assessment. The methods we compared include DeepWalk, Line, Node2Vec, GraRep, GF, Lap, and LLE. These methods were chosen for their relevance and widespread use in the field of network embedding.\n\nTo ensure a fair comparison, we used the same datasets and prediction methods for all the evaluated techniques. This approach allowed us to measure the performance of our method relative to established baselines. The results, presented in Table 5 and Figure 2, demonstrate that our method outperforms the other network embedding techniques in terms of ROC and AUC of PR. This indicates that our approach provides more accurate predictions and better overall performance.\n\nAdditionally, we compared our method to simpler baselines to understand its effectiveness relative to more straightforward approaches. This comparison helps to highlight the unique advantages and improvements offered by our method. The results show that our technique not only matches but often exceeds the performance of these simpler baselines, reinforcing its robustness and reliability.\n\nIn summary, the comparison to publicly available methods and simpler baselines provides a clear indication of our method's superior performance and its potential for practical applications in network embedding tasks.",
  "evaluation/confidence": "The evaluation of our method includes confidence intervals for the performance metrics, which are crucial for understanding the reliability and variability of our results. For instance, the AUC values for ROC and PR are reported with their respective confidence intervals, such as 0.8954 ± 0.001 and 0.9002 ± 0.0015. These intervals provide a range within which the true AUC values are expected to lie, giving a measure of the precision of our estimates.\n\nStatistical significance is also considered in our evaluation. The comparison of our method with other network embedding techniques, such as DeepWalk, Line, and Node2Vec, shows that our approach consistently achieves higher AUC values. The differences in performance are not just marginal but are accompanied by confidence intervals that do not overlap significantly with those of the baseline methods. This suggests that the observed improvements are statistically significant and not due to random chance.\n\nAdditionally, the use of cross-validation further strengthens the statistical robustness of our results. By dividing the dataset into training and test sets multiple times and averaging the performance metrics, we ensure that our findings are generalizable and not dependent on a particular split of the data.\n\nIn summary, the inclusion of confidence intervals and the use of cross-validation provide a solid foundation for claiming that our method is superior to other network embedding techniques and baselines. The statistical significance of our results supports the reliability and effectiveness of our approach.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as publicly available. The publication focuses on the methods and results of the evaluation, including metrics such as AUC for ROC and PR curves, precision, accuracy, F1 scores, and recall. The evaluation process involves dividing datasets into training and test sets, using cross-validation, and employing random forests for predictions. However, there is no information provided about the availability of the raw evaluation files or the specific details on how to access them. If such files were made available, it would typically be through a data repository with an accompanying license that specifies the terms of use."
}