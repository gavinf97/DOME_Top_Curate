{
  "publication/title": "APOD: accurate sequence-based predictor of disordered flexible linkers",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available",
  "publication/year": "2021",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Disordered flexible linkers\n- Protein prediction\n- Machine learning\n- Support vector machines\n- Logistic regression\n- Sequence analysis\n- Intrinsic disorder\n- Protein structure\n- Bioinformatics\n- Predictive modeling",
  "dataset/provenance": "The dataset used in this study is derived from functional annotations provided by DisProt. This dataset includes 5893 residues in 175 disordered flexible linker (DFL) regions, which are located in 117 intrinsically disordered proteins (IDPs). For convenience, proteins with at least one DFL are referred to as 'DFL proteins.' Additionally, a set of non-DFL proteins was extracted to test the predictor's ability to distinguish between DFL and non-DFL residues.\n\nTo reduce the number of potential false negatives, 131 proteins were collected, where the majority of residues are functionally annotated and not DFL. Only disordered residues with non-DFL functional annotations and structured residues were marked and used as negatives.\n\nThe corresponding 248 protein sequences were clustered using BLASTClust at 25% sequence identity. These clusters were then divided into training and test datasets. The training set, TR166, includes 166 sequences with 3661 DFL residues. The test set, TE82, consists of 82 chains with 2223 DFL residues. The placement of entire clusters into these datasets ensures that training and test proteins share below 25% sequence similarity.\n\nThe TR166 set was used to design and optimize the predictive model through 5-fold cross-validation. The independent TE82 set was utilized to assess and compare the APOD predictor with the current DFLpred predictor. The test dataset is similar in size to those used in CASP experiments and is slightly larger than the test set used by the authors of DFLpred. The TR166 and TE82 datasets are available for public access.",
  "dataset/splits": "Two data splits were created: a training set and a test set. The training set, referred to as TR166, consists of 166 sequences with 3661 DFL residues. The test set, labeled as TE82, is composed of 82 chains with 2223 DFL residues. The data was clustered using BLASTClust at 25% sequence identity, resulting in 194 clusters. These clusters were then randomly divided into the training and test datasets, ensuring that the proteins in each set share less than 25% sequence similarity. This approach helps in evaluating the model's performance on independent and dissimilar datasets. The training set was used for model design and optimization through 5-fold cross-validation, while the test set was utilized to assess and compare the performance of the predictor with existing methods.",
  "dataset/redundancy": "The datasets were split into training and test sets to ensure independence and to prevent data leakage. The training set, referred to as TR166, consists of 166 sequences with 3661 DFL residues. The test set, referred to as TE82, is composed of 82 chains with 2223 DFL residues. To ensure that the training and test proteins share below 25% sequence similarity, the corresponding 248 protein sequences were clustered using BLASTClust at the 25% sequence identity threshold. Entire clusters were then placed into either the training or test dataset.\n\nThis approach ensures that the training and test sets are independent, which is crucial for evaluating the generalizability of the predictive model. The test dataset is similar in size to those used in CASP experiments and is slightly larger than the test set used by the authors of DFLpred. This careful splitting helps in providing a robust evaluation of the model's performance on unseen data.",
  "dataset/availability": "The datasets used in our study, specifically the training set TR166 and the test set TE82, are publicly available. These datasets can be accessed at the following URL: [https://yanglab.nankai.edu.cn/APOD/benchmark/](https://yanglab.nankai.edu.cn/APOD/benchmark/).\n\nThe datasets were constructed by annotating residues using functional annotations from DisProt. We collected 131 proteins that have a majority of their residues functionally annotated and where this annotation is not DFL. The residues without functional annotations were excluded from the design and assessment. The proteins were clustered using BLASTClust at 25% sequence identity to ensure that the training and test sets share below 25% sequence similarity. This clustering was enforced to reduce the number of potential false negatives and to ensure that the training and test proteins are dissimilar.\n\nThe training set TR166 includes 166 sequences with 3661 DFL residues, while the test set TE82 is composed of 82 chains with 2223 DFL residues. The placement of entire clusters into the two datasets ensures that the training and test proteins share below 25% sequence similarity. This approach was taken to validate the performance of our predictive model on independent data.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes two popular machine learning algorithms: Logistic Regression (LR) and Support Vector Machine (SVM). These algorithms are well-established in the field of machine learning and have been extensively used in various predictive modeling tasks.\n\nThe LR algorithm has been successfully applied in previous studies to predict disordered flexible linkers (DFLs) and protein disorder. It is a simple yet effective model that has shown good performance in binary classification problems.\n\nThe SVM algorithm, on the other hand, is a powerful tool for classification tasks. It has been widely used in bioinformatics for predicting various structural and functional properties of proteins. The SVM model used in our study employs the Radial Basis Function (RBF) kernel, which is known for its ability to handle non-linear relationships in the data.\n\nBoth algorithms were carefully parametrized by searching over a suitable parameter space. For the LR algorithm, the ridge parameter was set to values ranging from 10^-6 to 10^6. For the SVM, the complexity constant (C) and the width of the kernel (c) were optimized by setting their values to powers of 2 ranging from 2^-5 to 2^5. Additionally, the sliding window size (ws) was optimized by considering values between 9 and 25, which correspond to the 10th percentile and the median length of DFLs in the training dataset, respectively.\n\nThe optimal values for the parameters were selected by maximizing the average Area Under the Receiver Operating Characteristic Curve (AUC) values calculated over the five test folds in the 5-fold cross-validation on the training set. The optimized model applies a sliding window size of 13, a complexity constant of 1, and a kernel width of 0.0625.\n\nThe results produced by the LR model were outperformed by the SVM model, which demonstrated superior predictive quality, especially in terms of consistency over the entire range of window sizes. The SVM model's AUC values ranged between 0.807 and 0.810, and its Matthews Correlation Coefficient (MCC) values varied between 0.355 and 0.380 across different window sizes. The highest AUC and MCC values for the SVM model were achieved with a window size of 13, which was selected for implementing the APOD predictor.",
  "optimization/meta": "The model described in this publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on a single machine learning algorithm, specifically the Support Vector Machine (SVM) model, which has been optimized for predicting disordered flexible linkers (DFLs). The SVM model is parametrized using a radial basis function (RBF) kernel, and its parameters, including the complexity constant (C), the width of the kernel (c), and the sliding window size (ws), are carefully optimized to maximize predictive performance.\n\nThe training process involves a 5-fold cross-validation on the training set TR166, ensuring that the model generalizes well to out-of-sample data. The optimization of the SVM model is geared towards ensuring that it performs accurately on independent test datasets, such as TE82, which shares low similarity with the training set. This approach helps in validating the model's predictive quality and its ability to generalize to new, unseen data.\n\nThe features used in the model include both protein-level features and local sliding window-based features. These features cover various aspects of the protein sequence, such as amino acid composition, putative secondary structure, relative solvent accessibility, sequence conservation, and disorder predictions. The model's performance is evaluated using metrics like the area under the receiver operating characteristic curve (AUC) and the Matthews correlation coefficient (MCC), which provide a comprehensive assessment of its predictive accuracy.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involved converting the input protein sequence into a rich feature vector. This vector includes both protein-level features and local sliding window-based features. The protein-level features quantify the disorder content (DisCon) of the entire protein. The local sliding window-based features capture various properties within a sliding window of residues. These properties include amino acid composition (AAC), conservation-based features (CONS) generated using PSI-BLAST, putative secondary structure (SS) and relative solvent accessibility (RSA) predicted with SPARKS-X, and putative intrinsic disorder derived from DISOPRED3 predictions. The disorder features include both sliding window-based disorder (SWdis) and protein-level disorder content (DisCon). The total number of features derived is 170, which provides a comprehensive representation of the protein sequence for the machine-learning model. This encoding ensures that the model can effectively utilize both global and local sequence information to make accurate predictions.",
  "optimization/parameters": "In our study, we optimized four key parameters for our predictive model. These parameters include the sliding window size (ws), the complexity constant (C) for the SVM, the width of the kernel (c), and the ridge parameter for the logistic regression (LR) model.\n\nThe sliding window size was optimized by considering values ranging between 9 and 25. These values correspond to the 10th percentile and the median length of disordered flexible linkers (DFL) in the training dataset, respectively. The optimal window size was selected by maximizing the average area under the receiver operating characteristic curve (AUC) values calculated over the five test folds in the 5-fold cross-validation on the training set.\n\nFor the SVM model, we used the radial basis function (RBF) kernel and optimized the complexity constant (C) and the width of the kernel (c). The values for C and c were set to 2^i, where i ranged from -5 to 5. The optimal values for C and c were determined by maximizing the average AUC values in the 5-fold cross-validation.\n\nThe ridge parameter for the LR model was set to 10^i, where i ranged from -6 to 6. The optimal ridge parameter was also selected by maximizing the average AUC values in the 5-fold cross-validation.\n\nThrough this optimization process, the final parameters selected for the APOD model were ws = 13, C = 1, and c = 0.0625. These parameters were chosen because they resulted in the highest average AUC values, indicating the best predictive performance on the training set.",
  "optimization/features": "The APOD predictor utilizes a total of 169 features as input. These features are derived from various sources, including amino acid composition, putative secondary structure, relative solvent accessibility, sequence conservation, and disorder predictions based on sliding windows and protein-level disorder.\n\nFeature selection was not explicitly mentioned as a separate process. Instead, the features were designed and grouped based on their biological relevance and potential predictive power. The predictive value of these features was quantified using the point biserial correlation coefficient (PBC) on the training set TR166. This process involved calculating the correlation between the feature values and the binary residue-level annotations of disordered flexible linkers (DFLs) across five folds of cross-validation.\n\nThe features were then evaluated based on their ability to differentiate between DFL and non-DFL residues. The most predictive features included local window-level conservation, window-level disorder, and protein-level disorder. These features were combined to form the final set of input features for the APOD predictor. The optimization of the model parameters, including the sliding window size, was performed using the training set to ensure that the model generalizes well to out-of-sample data.",
  "optimization/fitting": "The optimization process involved a careful selection of parameters to ensure the model's performance was neither overfitted nor underfitted. Two popular machine learning algorithms, Logistic Regression (LR) and Support Vector Machine (SVM), were comparatively tested. For the LR algorithm, the sole ridge parameter was set to values ranging from 10^-6 to 6, ensuring a broad search space to prevent underfitting. For the SVM, the RBF kernel was used, and two parameters, the complexity constant C and the width of the kernel c, were optimized. The values for C and c were set to range from 2^-5 to 2^5, providing a wide range of possibilities to avoid underfitting.\n\nTo address the potential issue of overfitting, especially given the number of parameters and the training dataset size, a 5-fold cross-validation was employed. This technique helps in assessing the model's performance on different subsets of the data, ensuring that the model generalizes well to unseen data. The optimal values for the sliding window size (ws), C, c, and ridge were selected by maximizing the average AUC values calculated over the five test folds. This approach ensures that the model's performance is consistent across different data splits, reducing the risk of overfitting.\n\nAdditionally, the predictive quality was assessed using the area under the receiver operating characteristic curve (AUC) and the Matthews correlation coefficient (MCC). The AUC quantifies the quality of the propensity scores, ranging from 0.5 for random-quality predictions to 1 for perfect predictions. The MCC measures the quality of the binary predictions, providing a balanced measure even when the classes are of very different sizes. The consistent improvements in AUC and MCC across different window sizes further validate the model's robustness and generalizability.\n\nThe optimized APOD model, which applies ws = 13, C = 1, and c = 0.0625, was found to outperform the LR model. The results produced by the LR model were consistently outperformed by the SVM model, especially given the consistency of the improvements over the entire range of the window sizes. This indicates that the SVM model is better equipped to handle the complexity of the data without overfitting.\n\nIn summary, the optimization process involved a thorough search of parameter space, the use of cross-validation to ensure generalizability, and the assessment of predictive quality using robust metrics. These steps collectively ensured that the model was neither overfitted nor underfitted, providing accurate and reliable predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our predictive model. One of the key methods used was regularization, specifically through the use of a ridge parameter in the Logistic Regression (LR) algorithm. The ridge parameter was set to values of 10^i, where i ranged from -6 to 6. This helped to penalize large coefficients, thereby reducing the model's complexity and preventing it from overfitting the training data.\n\nAdditionally, we utilized a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel. For the SVM, we optimized two parameters: the complexity constant C and the width of the kernel c. These parameters were set to values of 2^i, where i ranged from -5 to 5. Proper tuning of these parameters is crucial for balancing the model's ability to capture the underlying patterns in the data while avoiding overfitting.\n\nFurthermore, we performed 5-fold cross-validation on the training set TR166. This technique involves dividing the training data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. The average performance metrics, such as the area under the receiver operating characteristic curve (AUC) and the Matthews correlation coefficient (MCC), were calculated over the five folds. This approach helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nBy combining these regularization techniques and cross-validation, we aimed to develop a robust and reliable predictive model for identifying disordered flexible linkers (DFLs) in protein sequences.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are thoroughly detailed within the publication. Specifically, the sliding window sizes were optimized within a range of 9 to 25, corresponding to the 10th percentile and median length of disordered flexible linkers (DFL) in the training dataset. The optimal values for the window size (ws), complexity constant (C), and kernel width (c) were determined by maximizing the average area under the receiver operating characteristic curve (AUC) values across five test folds in a 5-fold cross-validation on the training set TR166. The optimized parameters for the APOD model are ws = 13, C = 1, and c = 0.0625.\n\nThe training and test datasets, TR166 and TE82, respectively, are available for public access at https://yanglab.nankai.edu.cn/APOD/benchmark/. These datasets include 166 sequences with 3661 DFL residues for training and 82 sequences with 2223 DFL residues for testing. The datasets were constructed to ensure that training and test proteins share below 25% sequence similarity, adhering to standard practices in predictive modeling to avoid overfitting.\n\nThe specific model files and optimization schedules are not explicitly provided in the publication, but the methodology and parameters used for optimization are clearly described. This allows for reproducibility of the results by following the detailed steps and using the provided datasets. The publication does not specify a particular license for the datasets or the methodology, but they are made available for academic use and further research in the field.",
  "model/interpretability": "The APOD model, which employs a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel, is not inherently transparent. SVMs, particularly those using non-linear kernels like RBF, are often considered black-box models because the decision boundaries they create are not easily interpretable. The model's predictions are based on complex, non-linear transformations of the input features, making it difficult to trace back how specific features influence the output.\n\nHowever, the interpretability of the APOD model can be enhanced through several aspects of its design and the features it uses. The model incorporates a variety of features derived from the input protein sequence, including amino acid composition, conservation-based features, putative structural characteristics, and disorder features. These features are designed to capture different aspects of the protein's structure and function, providing a more interpretable basis for the model's predictions.\n\nFor instance, the conservation-based features (CONS) and disorder features (DisCon and SWdis) have been shown to be highly predictive. The conservation features reflect the evolutionary pressure on specific residues, while the disorder features indicate regions of the protein that lack a fixed or ordered structure. By analyzing the importance of these features, one can gain insights into which aspects of the protein sequence are most influential in predicting disordered flexible linkers (DFLs).\n\nAdditionally, the model's performance is evaluated using metrics such as the area under the receiver operating characteristic curve (AUC) and the Matthews correlation coefficient (MCC), which provide a quantitative measure of the model's predictive quality. The consistent improvements in these metrics across different window sizes and the significant differences compared to other methods suggest that the model is effectively capturing relevant patterns in the data.\n\nIn summary, while the APOD model itself is not transparent due to the use of an SVM with an RBF kernel, the interpretability can be enhanced by examining the features that contribute most to the predictions. The inclusion of biologically meaningful features and the model's superior performance provide a basis for understanding how different aspects of the protein sequence influence the prediction of DFLs.",
  "model/output": "The model is a classification model. It predicts whether a residue is a disordered flexible linker (DFL) or not, which is a binary classification task. The model outputs two types of scores: a binary score indicating whether a residue is a DFL or non-DFL, and a numeric propensity score quantifying the likelihood that a given residue forms a DFL. The performance of the model is evaluated using metrics suitable for classification tasks on imbalanced datasets, such as the Matthews correlation coefficient (MCC), precision (Pre), and recall (Rec). The model uses a support vector machine (SVM) with a radial basis function (RBF) kernel, which is optimized for classification tasks. The area under the receiver operating characteristic (ROC) curve (AUC) is also used to assess the predictive quality of the propensity scores. The model's output is designed to provide accurate and reliable predictions of DFL residues in protein sequences.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The APOD predictor is freely accessible via a user-friendly web server. This web server is designed to handle all computations on the server side, requiring only a FASTA-formatted protein sequence as input. The results are displayed directly in the web browser window and can also be sent to the user's email address if provided. The web server can be accessed at https://yanglab.nankai.edu.cn/APOD. This accessibility ensures that researchers studying the dynamics and functions of disordered proteins and protein domains can easily utilize the tool.",
  "evaluation/method": "The evaluation of the APOD predictor involved several rigorous methods to ensure its predictive quality. Initially, a 5-fold cross-validation was performed on the training dataset to assess the model's performance. This process involved dividing the dataset into five subsets, training the model on four subsets, and testing it on the remaining one, repeating this procedure five times with different subsets as the test set. The predictive quality was quantified using the area under the receiver operating characteristic curve (AUC) and the Matthews correlation coefficient (MCC).\n\nTo further validate the model's generalizability, an independent test dataset was used. This dataset shared low similarity with the training set, ensuring that the model's performance was evaluated on unseen data. The APOD predictor achieved an AUC of 0.816 and an MCC of 0.418 on this independent test set, demonstrating its accuracy and robustness.\n\nStatistical significance of the differences in AUC and MCC between APOD and other methods was evaluated using the Anderson–Darling test and the Wilcoxon rank sum test. The test set was divided into 10 equally sized protein subsets, and the AUC and MCC values were compared across these subsets. Differences with a P-value less than 0.01 were considered statistically significant.\n\nAdditionally, an ablation analysis was conducted to assess the contribution of different feature groups to the model's predictive performance. This involved comparing the complete APOD model with versions that relied on single feature groups, such as amino acid composition, secondary structure, sequence conservation, and disorder features. The analysis helped identify the most predictive feature groups and their individual contributions to the model's accuracy.\n\nThe evaluation also included a comparison with indirect methods for predicting disordered flexible linkers (DFLs), such as domain predictors, flexibility predictors, and disorder predictors. The APOD predictor was found to outperform these indirect methods significantly, highlighting its superior predictive performance.",
  "evaluation/measure": "The performance metrics reported in this study include the area under the receiver operating characteristic curve (AUC), the Matthews correlation coefficient (MCC), precision (Pre), and recall (Rec). These metrics are chosen to evaluate the quality of both the binary predictions and the numeric propensity scores for disordered flexible linkers (DFLs).\n\nThe AUC quantifies the overall ability of the model to distinguish between DFL and non-DFL residues across all possible classification thresholds. It ranges from 0.5 (random-quality predictions) to 1 (perfect predictions). This metric is particularly useful for imbalanced datasets, where the majority of residues are non-DFLs.\n\nThe MCC is a balanced measure that takes into account true positives, true negatives, false positives, and false negatives. It provides a single value that summarizes the performance of the binary classifier, making it suitable for evaluating models on imbalanced datasets. The MCC ranges from -1 to 1, with higher values indicating better performance.\n\nPrecision and recall are also reported to provide additional insights into the model's performance. Precision measures the accuracy of the positive predictions (i.e., the proportion of true positives among all predicted positives), while recall measures the ability of the model to identify all relevant instances (i.e., the proportion of true positives among all actual positives). These metrics are particularly important for applications where the cost of false positives and false negatives differs.\n\nThe set of metrics reported in this study is representative of the literature on predictive modeling for protein features. AUC and MCC are commonly used to evaluate the performance of models on imbalanced datasets, while precision and recall provide additional context for understanding the trade-offs between different types of errors. By reporting these metrics, we aim to provide a comprehensive evaluation of the model's performance and its potential utility for predicting DFLs in protein sequences.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we conducted a thorough evaluation of our APOD predictor by comparing it against both publicly available methods and simpler baselines. For publicly available methods, we benchmarked APOD against DFLpred, a well-known tool for predicting disordered flexible linkers (DFLs). This comparison was performed on an independent test dataset, TE82, which shares low similarity with the training set, ensuring a fair and unbiased evaluation. APOD demonstrated significantly higher predictive performance, with improvements of 52% in precision, 186% in recall, 180% in Matthews correlation coefficient (MCC), and 28% in the area under the receiver operating characteristic curve (AUC) compared to DFLpred. These improvements were statistically significant, with P-values of 0.004 for MCC and 0.002 for AUC.\n\nIn addition to comparing with DFLpred, we also evaluated APOD against indirect approaches that predict DFLs based on disorder, domain boundaries, and flexibility. These methods included the domain predictor FUpred, the flexibility predictor PredyFlexy, and several disorder predictors such as DISOPRED3 and IUPred2A. We found that these indirect methods generally performed worse than APOD. For instance, FUpred achieved an AUC of 0.637 and an MCC of 0.162, which are significantly lower than APOD's performance. Similarly, the flexibility predictor PredyFlexy and the disorder predictors showed modest predictive performance, with AUC values ranging from 0.377 to 0.574 and MCC values close to zero or negative.\n\nFurthermore, we compared APOD with simpler baselines to assess the contribution of different feature groups and model parameters. We performed an ablation analysis on the training dataset, TR166, by comparing the complete APOD model with versions that relied on single feature groups. This analysis revealed that the inclusion of features such as amino acid composition, secondary structure, relative solvent accessibility, sequence conservation, and disorder-based features significantly enhanced the predictive performance. The optimized APOD model, which uses a support vector machine (SVM) with a radial basis function (RBF) kernel and specific parameter settings, outperformed a logistic regression (LR) model across different window sizes. The SVM model showed consistent improvements in both AUC and MCC, highlighting its superiority in capturing the complex patterns in the data.\n\nIn summary, our evaluation demonstrates that APOD provides accurate and reliable predictions of disordered flexible linkers, outperforming both publicly available methods and simpler baselines. The comprehensive comparison against various predictors and the detailed analysis of feature contributions underscore the robustness and effectiveness of the APOD model.",
  "evaluation/confidence": "The evaluation of the APOD predictor includes a thorough assessment of its predictive quality and statistical significance. The performance metrics, such as the area under the receiver operating characteristic curve (AUC) and the Matthews correlation coefficient (MCC), are used to quantify the model's accuracy. These metrics are computed over multiple test folds in a 5-fold cross-validation on the training set, ensuring robustness and reliability.\n\nTo evaluate the statistical significance of the differences in AUC and MCC values between APOD and other methods, a rigorous statistical test is employed. The test set is divided into 10 equally sized protein subsets, and the corresponding AUC and MCC values are calculated and compared. If the vectors of these values are normally distributed, as determined by the Anderson–Darling test at the 0.05 significance level, a t-test is used. Otherwise, the non-parametric Wilcoxon rank sum test is applied. Differences with a P-value less than 0.01 are considered statistically significant.\n\nThe results demonstrate that APOD outperforms existing methods, including DFLpred, with statistically significant improvements. For instance, APOD achieves an AUC of 0.816 and an MCC of 0.418 on the independent test dataset TE82, which are significantly higher than those of DFLpred. The improvements in predictive performance are consistent across various metrics and are supported by statistical tests, indicating that the observed differences are not due to random chance.\n\nAdditionally, the ROC curves for APOD are consistently above those of other methods, further validating its superior performance. The statistical tests confirm that the improvements in AUC and MCC are significant, with P-values of 0.002 and 0.004, respectively, when compared to DFLpred. This rigorous evaluation provides strong evidence that APOD is a reliable and accurate predictor of disordered flexible linkers.",
  "evaluation/availability": "The datasets used for training and testing our predictive model are publicly available. The training set, referred to as TR166, consists of 166 sequences with 3661 DFL residues. The test set, known as TE82, comprises 82 chains with 2223 DFL residues. These datasets can be accessed at https://yanglab.nankai.edu.cn/APOD/benchmark/. The datasets are designed to ensure that the training and test proteins share below 25% sequence similarity, which helps in evaluating the model's performance on independent data. The availability of these datasets allows for reproducibility and further validation of the APOD predictor's performance."
}