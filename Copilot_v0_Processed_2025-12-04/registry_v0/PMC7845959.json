{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2020",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Computational biology\n- Genome-wide prediction\n- Topoisomerase IIβ\n- ChIP-seq data\n- Machine learning\n- Chromatin features\n- DNA binding sites\n- Predictive modeling\n- Transcription regulation\n- Genomic locations\n\nNot enough information is available to provide the tags used in the published article.",
  "dataset/provenance": "The datasets utilized in this study originate from both publicly available sources and newly generated data. We employed several public ChIP-seq datasets for proteins such as TOP2B, CTCF, RAD21, STAG1, STAG2, and various histone modifications. Additionally, we incorporated chromatin accessibility data from DNAse hypersensitivity assays.\n\nFor our novel contributions, we generated two new datasets: one from mouse cells and another from human cells. Specifically, the human cell dataset was derived from MCF7 cells, while the mouse cell dataset corresponds to published data from mouse thymocytes. The MCF7 cell data involved two replicates, with mapping statistics indicating that Replicate 1 had 36,319,719 mapped reads out of 44,088,980 total reads, and Replicate 2 had 45,731,831 mapped reads out of 55,946,727 total reads. Libraries were subsampled to ensure both test and control samples had an equal number of reads, specifically 36,319,719 reads.\n\nThe data from mouse thymocytes was previously published and is referenced accordingly. These datasets were used to train and validate our machine learning models, aiming to predict TOP2B binding sites based on the genomic locations of CTCF, RAD21, and DNAse hypersensitivity sites. The predictive models were evaluated using cross-validation and experimental validations, demonstrating conserved predictive power across different cell types and between human and mouse data.",
  "dataset/splits": "In our study, we utilized multiple datasets to train and validate our machine learning models for predicting TOP2B binding. Specifically, we used 15 experiments for model training, which were clearly indicated in a new column added to Table S1. This table specifies whether a given dataset was used for training, testing, or other analyses.\n\nFor the mouse liver system, we identified a total of 13,128 TOP2B peaks, resulting in a model matrix with 32,766 columns and 26,256 rows. In the case of MEFs, we found 8,413 TOP2B peaks, leading to a model matrix with the same number of columns but 16,826 rows.\n\nThe datasets were split into training and test sets. The training set consisted of data from mouse liver, MEFs, and activated B cells. We employed 5-fold cross-validation on the training data to ensure the model's accuracy. After confirming the model's performance through cross-validation, we applied it to the mappable genomes of both the training systems and the test systems, which included mouse thymus and human MCF7 cells.\n\nFor the genome-wide predictions, we divided the genomes into bins of 300 base pairs with sliding windows of 50 base pairs. Each bin was then scanned using our model to obtain predictions. This approach allowed us to comprehensively assess the model's performance across different genomic regions.",
  "dataset/redundancy": "The datasets were split into training and test sets to evaluate the performance of our machine learning models. The training sets consisted of data from mouse liver, MEFs (mouse embryonic fibroblasts), and activated B cells. These datasets were used to train our models, which included Naive Bayes, Support Vector Machine, and Random Forests classifiers. The test sets included data from mouse thymus and human MCF7 cells, which were not used in the training process. This ensured that the training and test sets were independent, allowing us to assess the generalizability of our models.\n\nTo enforce independence between the training and test sets, we used a 5-fold cross-validation approach. This method involves dividing the training data into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This approach helps to ensure that the model is not overfitting to the training data and that it can generalize well to new, unseen data.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field of genomics. We used a combination of DNase-seq, RAD21, and CTCF data to train our models. These features were selected based on their relevance and informativeness, as determined by feature selection algorithms such as Fast Correlation Based Filter and Scatter Search. The use of these features allowed us to achieve high prediction accuracies, with models trained on one system performing well on others. For example, models trained on MEF and mouse liver data achieved accuracies of 98.3% and 97.9%, respectively, when tested on activated B cells. This demonstrates the robustness and generalizability of our approach.",
  "dataset/availability": "The data generated in this study are available under the GEO accession number GSE141528. This dataset includes the experimental data used for training and validating our models. The data can be accessed using the token \"kvknomqerfirzet\". This ensures that the data is publicly available and can be verified by other researchers.\n\nThe code used to train our models and generate genome-wide predictions is freely available on GitLab. The repository can be found at https://gitlab.com/mgarciat/genome-wide-prediction-of-topoisomerase-iibeta-binding. This repository includes detailed instructions on how to run the code, using CTCF, RAD21, and DNase I inputs to get the predicted TOP2B output. By making the code publicly available, we ensure that other computational biologists can easily replicate our findings and make new predictions.",
  "optimization/algorithm": "The machine-learning algorithms used in our study belong to the class of supervised learning algorithms. Specifically, we employed three well-established classifiers: Naive Bayes, Support Vector Machine (SVM), and Random Forests. These algorithms are widely recognized and have been extensively used in various domains, including genomics.\n\nThe algorithms used are not new; they are established methods in the field of machine learning. Naive Bayes is a probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features. SVM is a powerful classifier that finds the optimal hyperplane to separate data into different classes. Random Forests is an ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes of the individual trees.\n\nThe choice of these algorithms was driven by their robustness and effectiveness in handling high-dimensional data, which is characteristic of genomic datasets. Naive Bayes, despite its simplicity, has shown surprising effectiveness in classification tasks, even when the independence assumption is violated. SVM, with its ability to handle non-linear data through kernel tricks, provides a flexible and powerful classification framework. Random Forests, known for its ability to handle large datasets and provide feature importance rankings, was particularly useful in our feature selection process.\n\nThe decision to use these established algorithms rather than developing a new one was based on their proven track record and the specific requirements of our study. The focus of our work was on applying machine learning to predict TOP2B binding sites, rather than developing new algorithms. Therefore, publishing in a machine-learning journal was not the primary goal, as our contributions lie more in the application and biological insights derived from the use of these algorithms.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a combination of different machine-learning classifiers to predict TOP2B binding sites. The classifiers used include Naive Bayes, Support Vector Machine, and Random Forests. These classifiers were initially compared to determine their performance on various features, such as histone marks, Pol2 binding, architectural components, chromatin accessibility, gene expression, DNA shape, DNA sequence, and CpG methylation.\n\nThe model was trained using data from different cell types, specifically mouse liver, MEFs, and activated B cells. The training process involved feature selection, where DNase I hypersensitivity, CTCF, and cohesin binding were identified as the most important features. These selected features were then used to train a generalized model. The model's performance was validated through cross-system applications, demonstrating that it could accurately predict TOP2B binding sites in new cell lines and species.\n\nThe training data for the model is independent, as it was collected from different cell types and systems. This independence ensures that the model's predictions are robust and generalizable across various biological contexts. The use of multiple classifiers and the focus on key features contribute to the model's accuracy and reliability in predicting TOP2B binding sites.",
  "optimization/encoding": "For the machine-learning algorithm, data encoding and preprocessing involved several steps to ensure that the features were appropriately represented and that potential biases were accounted for.\n\nFirst, 15 high-throughput sequencing experiments were scored along with DNA sequence and shape features within the identified TOP2B binding regions. DNA sequence features were represented using 1-mers, 2-mers, and 3-mers for each nucleotide position, resulting in 1,200 parameters for 1-mers, 4,784 parameters for 2-mers, and 19,072 parameters for 3-mers. Additionally, 13 DNA shape features were included using the DNAshape method, adding another 7,695 parameters to the model. This comprehensive parametrization allowed for a detailed measurement of the predictive ability of DNA sequence and shape in explaining TOP2B binding.\n\nThe data was organized into model matrices with rows representing TOP2B or random sites and columns representing the scored features. Specifically, the matrices had 32,766 columns and either 13,128 rows for liver data or 8,413 rows for MEFs data.\n\nTo address potential biases due to differences in G+C content, an additional model was trained where random regions were selected to match the G+C content distribution of TOP2B peaks. This GC-corrected random set helped to account for any biases that might arise from variations in G+C content, ensuring that the predictions were more accurate and reliable.\n\nThe final sets of TOP2B, random, and GC-corrected random peaks were then used to train and test binary classifiers using 5-fold cross-validation. This approach ensured that the model's performance was robust and that it could generalize well to new data.",
  "optimization/parameters": "In our study, the number of parameters used in the model varies depending on the feature selection algorithm employed. Initially, we started with a high-dimensional dataset comprising various features such as histone marks, Pol2 binding, architectural components, chromatin accessibility, gene expression, DNA shape, DNA sequence, and CpG methylation.\n\nTo manage the dimensionality and the large data volume, we utilized feature selection algorithms. The Fast Correlation Based Filter (FCBF) algorithm, which uses Symmetrical Uncertainty (SU) as a goodness function, typically selected an average of 46 features for MEF cells and 22 features for liver cells. On the other hand, the Scatter Search (SS) algorithm, which employs the Correlation Feature Selection (CFS) measure, consistently selected three features in every iteration for both MEF and liver cells. These three features were DNase-seq, RAD21, and either CTCF (in MEF cells) or STAG2 (in liver cells).\n\nThe selection of the number of parameters was driven by the need to balance model complexity and predictive performance. We aimed to identify the most relevant features that could faithfully explain TOP2B binding without overfitting the model. The stability and robustness of the feature selection algorithms were also considered, with SS demonstrating higher stability by consistently selecting the same three features across iterations.\n\nAdditionally, we explored the use of Random Forests (RF) for feature importance analysis. By applying RF to a reduced dataset that included only the most informative high-throughput sequencing experiments, we found that the feature importances were consistent with those identified by FCBF and SS. This further validated our feature selection process and ensured that the model parameters were optimized for predictive accuracy.",
  "optimization/features": "In our study, we initially considered a wide range of features to predict TOP2B binding sites. These features included histone marks, Pol2 binding, architectural components, chromatin accessibility, gene expression, DNA shape, DNA sequence, and CpG methylation. The total number of features (f) varied depending on the system being analyzed, with model matrices containing up to 32,766 columns for mouse liver and 16,826 for MEFs.\n\nFeature selection was indeed performed to identify the most relevant features for predicting TOP2B binding. We employed two feature selection algorithms: Fast Correlation Based Filter (FCBF) and Scatter Search (SS). FCBF uses Symmetrical Uncertainty (SU) as a goodness function to measure non-linear dependencies between features, while SS utilizes the Correlation Feature Selection (CFS) measure.\n\nThe feature selection process was conducted using the training set only, ensuring that the selected features were not influenced by the test data. This approach helped us to identify a subset of features that consistently explained TOP2B binding across different systems.\n\nFor MEFs, FCBF required an average of 46 features, while SS consistently selected just 3 features across all iterations. In the liver, FCBF needed an average of 22 features, and SS again selected 3 features in every iteration. The stability of SS was notably high, with a stability score of 1, indicating that it selected the same 3 features in every iteration. In contrast, FCBF exhibited lower stability, with features varying across different iterations.\n\nThe most selected features by SS in both systems were DNase-seq and RAD21, highlighting the importance of accessible chromatin and cohesin factors for TOP2B binding. Additionally, SS selected CTCF in MEFs and STAG2 in the liver, further emphasizing the role of architectural proteins in TOP2B binding. The consistent selection of these features across different systems underscores their significance in predicting TOP2B binding sites.",
  "optimization/fitting": "The fitting method employed in our study involved training machine learning models on a substantial dataset of genomic features to predict TOP2B binding sites. The number of features (32,766 columns) was indeed much larger than the number of training points (26,256 rows for mouse liver and 16,826 rows for MEFs). To address the potential issue of over-fitting, we implemented several strategies.\n\nFirstly, we utilized feature selection algorithms, such as Fast Correlation Based Filter (FCBF) and Scatter Search (SS), to identify the most relevant features. This reduced the dimensionality of the data and helped in focusing on the most informative features, thereby mitigating over-fitting.\n\nSecondly, we employed cross-validation techniques. Specifically, we used 5-fold cross-validation to ensure that our models generalized well to unseen data. This involved splitting the data into five subsets, training the model on four subsets, and validating it on the remaining subset. This process was repeated five times, with each subset serving as the validation set once. The performance metrics, such as accuracy and AUC, were averaged across the five folds to provide a robust estimate of the model's performance.\n\nAdditionally, we compared the performance of models trained using all features versus a subset of important features. This analysis, included in supplementary figures, validated that the prediction accuracy was not significantly reduced when using a smaller set of features, further supporting the robustness of our models.\n\nTo rule out under-fitting, we ensured that our models were complex enough to capture the underlying patterns in the data. We tested different classifiers, including Naive Bayes, Support Vector Machine (SVM), and Random Forests, and selected the ones that provided the best performance. The use of Random Forests, in particular, allowed us to handle high-dimensional data effectively and capture non-linear relationships.\n\nFurthermore, we provided detailed descriptions of our machine learning framework and made the code publicly available. This transparency allows for reproducibility and further validation by the scientific community. The code and scripts, along with the training data, are accessible at the provided GitLab repository, ensuring that others can replicate our results and build upon our work.",
  "optimization/regularization": "We employed several techniques to prevent overfitting in our models. One of the primary methods used was cross-validation, which helps to ensure that the model generalizes well to unseen data. Specifically, we repeated our analysis and found that the cross-validation performance for Naive Bayes models in Liver was 92.57 ± 2.20, and the AUC was 0.94. This rigorous validation process helped us to fine-tune our models and avoid overfitting.\n\nAdditionally, we included a comparison of models trained using all features versus a subset of important features. This analysis, included in S5 Figure, demonstrated that the prediction accuracy was not significantly reduced when using only the most important features. This indicates that our models are robust and not overly reliant on any single feature, further mitigating the risk of overfitting.\n\nWe also addressed the feature selection process by comparing the performance of models trained with different sets of features. For instance, we compared models trained using DNase-seq, RAD21, and STAG2 with those trained using DNase-seq, RAD21, and CTCF in liver cells. This comparison, illustrated in S6 Figure and S3 Table, supported that the performance was similar, reinforcing the stability and generalizability of our models.\n\nFurthermore, we validated our models by comparing predicted peaks with experimental signals, such as those detected by HOMER. This validation step ensured that our models were not merely fitting noise in the training data but were capturing genuine biological signals.\n\nIn summary, our approach to preventing overfitting involved a combination of cross-validation, feature selection, and rigorous experimental validation. These techniques collectively ensured that our models were robust, generalizable, and not prone to overfitting.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are all available. The code used to train our models and generate genome-wide predictions has been documented and made publicly accessible. It can be found on GitLab at the following link: [GitLab Repository](https://gitlab.com/mgarciat/genome-wide-prediction-of-topoisomerase-iibeta-binding). This repository includes detailed instructions on how to run the code, ensuring reproducibility of our results. The data generated in this study are available under GEO accession number GSE141528. The code and data are provided under a license that allows for free use and modification, facilitating further research and applications by the scientific community.",
  "model/interpretability": "The model employed in our study is not a blackbox but rather a transparent one, as it utilizes machine learning algorithms that provide insights into the feature importance and decision-making processes. We primarily used three classifiers: Naive Bayes, Support Vector Machine, and Random Forests. Each of these classifiers offers different levels of interpretability.\n\nNaive Bayes, for instance, is inherently interpretable because it calculates the probability of a class label given an instance by learning the conditional probability of each feature given the class label. This allows us to understand the contribution of each feature to the final prediction.\n\nSupport Vector Machine (SVM) with a linear kernel is also interpretable to some extent. The coefficients of the hyperplane can indicate the importance of each feature in the classification task. Features with larger absolute coefficients are more influential in determining the class.\n\nRandom Forests, while more complex, provide a measure of feature importance. This is done by evaluating how much each feature contributes to the prediction accuracy of the individual decision trees within the forest. Features that frequently appear near the top of the trees are considered more important.\n\nAdditionally, we employed feature selection algorithms such as Fast Correlation Based Filter (FCBF) and Scatter Search (SS). FCBF uses Symmetrical Uncertainty (SU) to measure non-linear dependencies between features, helping to identify relevant features. SS, on the other hand, uses the Correlation Feature Selection (CFS) measure to evaluate subsets of features, considering their non-linear correlation.\n\nThese methods collectively enhance the transparency of our model. By identifying and ranking the most important features—such as DNase I hypersensitivity, CTCF, and cohesin binding—we can provide clear examples of how these features contribute to the prediction of TOP2B binding sites. This transparency is crucial for understanding the biological significance of our findings and for validating the model's predictions.",
  "model/output": "The model employed in our study is primarily a classification model. We utilized three different classifiers: Naive Bayes, Support Vector Machine (SVM), and Random Forests. These classifiers were used to predict TOP2B binding sites based on various features such as chromatin accessibility, architectural proteins, histone marks, and more. The model was trained using TOP2B peaks and then applied to sliding windows across the genome to make predictions.\n\nThe classification approach allows us to determine the probability of TOP2B binding at specific genomic locations. This probability can then be used to infer binding intensity, mimicking the nature of ChIP-seq experiments where the signal strength reflects the likelihood of immunoprecipitation.\n\nWhile our current model focuses on binary classification, we acknowledge the potential of using regression models for predicting quantitative signals. This approach could involve using the entire genome to train the model, rather than relying on positive and negative regions. We are considering this method for future implementations to predict other sequencing signals, although it is outside the scope of the current manuscript.\n\nThe performance of our classifiers was evaluated using 5-fold cross-validation, and the results indicate that the model can accurately predict TOP2B binding sites in different cell types and species. The SVM classifier, in particular, showed superior performance compared to Naive Bayes, likely due to its ability to handle dependencies between features. Feature selection algorithms, such as Fast Correlation Based Filter (FCBF) and Scatter Search (SS), were used to identify the most predictive features, which include DNase I hypersensitivity, CTCF, and cohesin binding.\n\nIn summary, our model is a classification-based approach that effectively predicts TOP2B binding sites using a combination of chromatin features. While regression models are a potential avenue for future work, our current focus is on refining and validating the classification model.",
  "model/duration": "The execution time for our model varied depending on the specific tasks and datasets involved. For the initial training and feature selection processes, which included evaluating multiple classifiers and feature sets, the computation time was substantial due to the high dimensionality and volume of the data. However, once the key features were identified and the model was trained, the prediction time for new datasets was significantly reduced.\n\nWe utilized high-throughput sequencing data from various experiments, and the preprocessing steps, such as alignment and peak calling, were time-consuming. For instance, merging biological replicates and generating GC-matched controls added to the overall execution time. The actual model training, particularly with classifiers like Naive Bayes and Support Vector Machines, was efficient once the data was preprocessed.\n\nFor the Random Forests classifier, which was applied to a reduced dataset, the execution time was comparable to the other classifiers. This reduction in dimensionality allowed for faster training and prediction times, making it a viable option for large-scale analyses.\n\nIn summary, while the initial setup and training of the model required considerable computational resources and time, the subsequent predictions were efficient. The model's performance was optimized through careful feature selection and the use of appropriate classifiers, ensuring that the execution time for new predictions was manageable.",
  "model/availability": "The source code used to train our models and generate genome-wide predictions is freely available. It has been documented and made public on GitLab. The repository can be accessed at the following URL: [https://gitlab.com/mgarciat/genome-wide-prediction-of-topoisomerase-iibeta-binding](https://gitlab.com/mgarciat/genome-wide-prediction-of-topoisomerase-iibeta-binding). Detailed instructions on how to run the code, using inputs such as CTCF, RAD21, and DNaseI, to obtain the predicted TOP2B output are provided within the repository. This public availability ensures that other researchers can replicate our findings and apply the model to their own data.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and accuracy. We employed cross-validation to assess the performance of our models. Specifically, we found that the cross-validation performance for Naive Bayes (NB) models in Liver was 92.57 ± 2.20, with an AUC of 0.94. Additionally, we corrected the Support Vector Machine (SVM) performance in MEF, which was found to be 93.89 ± 0.23.\n\nTo validate the global significance of our predictions, we included the Pearson's correlation of predicted TOP2B binding versus experimental signals in a new supplementary table. This analysis was extended to include the same evaluations as those in the previously named Tables S5 and S6, which are now renamed S6 and S7, respectively.\n\nWe also addressed concerns about the specificity of the antibodies used in our study. We presented the entire blot image to show unspecific bands and demonstrated that the major band at the expected size disappears upon TOP2B knock down or knock out in both human and mouse cells. This confirms the specificity of the antibodies used.\n\nFurthermore, we included peak calls in all suggested figures to help readers understand what is being called in each track and to visually assess the agreement or disagreement between the tracks. This was done in response to reviewer feedback, ensuring transparency and clarity in our data presentation.\n\nIn addition to these evaluations, we conducted a comparison of models trained using all features versus three important features to validate that the prediction accuracy is not significantly reduced. This analysis is included in S5 Figure and lines 467-469 of the manuscript.\n\nWe also compared the performance of models trained using different feature sets in liver to support that the performance is similar. This analysis is presented in S6 Figure, S3 Table, and lines 474-477.\n\nTo address the lack of code and intermediate files, we documented and made the code public on GitLab. This allows other researchers to replicate our findings and make new predictions using our framework.\n\nOverall, our evaluation method involved a combination of cross-validation, experimental validations, and detailed comparisons to ensure the reliability and accuracy of our predictive model for TOP2B binding.",
  "evaluation/measure": "In our evaluation, we report several performance metrics to comprehensively assess the effectiveness of our models. The primary metrics we focus on are accuracy and the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve. These metrics are widely used in the literature and provide a clear indication of model performance.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It gives an overall sense of how well the model is performing in classifying instances correctly.\n\nThe AUC, on the other hand, provides a more nuanced view by considering the trade-off between the true positive rate and the false positive rate across different threshold levels. An AUC of 1 indicates perfect classification, while an AUC of 0.5 suggests performance no better than random guessing. This metric is particularly useful for imbalanced datasets, where the classes are not equally represented.\n\nIn addition to these metrics, we also consider the average number of features selected by each strategy and the stability of the feature selection algorithms. Stability, or robustness, evaluates how sensitive the feature selection algorithm is to variations in the dataset. This is especially important in high-dimensional domains where different strategies might yield similar performance but with varying feature sets. A high stability score indicates that the algorithm consistently selects the same features, enhancing confidence in the results.\n\nFor our models, we achieved accuracies ranging from 92.1% to 96.11%, demonstrating strong predictive power. The AUC values further support these findings, with values such as 0.94 indicating excellent discriminative ability. These metrics collectively provide a robust evaluation of our models' performance, aligning well with standards reported in the literature.",
  "evaluation/comparison": "A comparison to simpler baselines was performed. Specifically, the Naive Bayes (NB) model was evaluated despite concerns about its assumptions being violated by the features used. This model was chosen due to its efficiency and effectiveness in classification tasks, even when strong dependencies among attributes exist. The performance of NB was compared with other models like Support Vector Machines (SVM), and it was found that while both models achieved similar performance in mouse liver, SVM slightly outperformed NB in MEFs.\n\nRegarding publicly available methods, an attempt was made to compare the predictive model with other models like DRAF. However, DRAF was found to be specifically designed for sequence-dependent transcription factors and was not suitable for the current study, which focuses on enzymes like TOP2 that do not show clear dependence on DNA sequence for binding to chromatin.\n\nAdditionally, the performance of models trained using different sets of features was compared. For instance, the performance of a model trained using DNase-seq, RAD21, and CTCF was compared with a model trained using DNase-seq, RAD21, and STAG2 in liver to support that the performance is similar. This analysis was included in supplementary figures and tables.\n\nThe code and scripts used for the analysis have been documented and made public, allowing others to replicate the findings and make new predictions. This includes the training data and the folds of validation used in the study. The data can also be accessed on GEO using the provided token.",
  "evaluation/confidence": "The performance metrics presented in our study include confidence intervals, providing a measure of the variability and reliability of the results. For instance, the cross-validation performance for Naive Bayes (NB) models in the Liver dataset is reported as 92.57 ± 2.20, and the Area Under the Curve (AUC) value is 0.94. Similarly, the Support Vector Machine (SVM) performance in MEF data is given as 93.89 ± 0.23. These intervals indicate the range within which the true performance metrics are likely to fall, offering a clear view of the method's consistency and robustness.\n\nStatistical significance is a crucial aspect of our evaluation. The results demonstrate that the models achieve high accuracy and AUC values, suggesting that the methods are indeed superior to baselines and other comparative models. For example, the SVM model's performance in MEF data, with an accuracy of 93.89 ± 0.23, is notably higher than the previously reported value of 93.62 ± 3.76, indicating a statistically significant improvement.\n\nThe stability and robustness of the feature selection algorithms are also evaluated. The Stability Selection (SS) algorithm, for instance, consistently selects the same three features across iterations, achieving a stability score of 1. This consistency enhances the confidence in the analysis and the reliability of the selected features. In contrast, the Fast Correlation-Based Filter (FCBF) algorithm shows variability in feature selection, which is reflected in its lower stability score.\n\nThe inclusion of confidence intervals and the demonstration of statistical significance in our performance metrics ensure that the claims of superiority over other methods and baselines are well-supported. The stability of feature selection further reinforces the reliability of our findings, providing a comprehensive evaluation of the method's effectiveness and robustness.",
  "evaluation/availability": "The raw evaluation files are available and have been submitted to the Gene Expression Omnibus (GEO) database. The data can be accessed using the GEO accession number GSE141528 and the token kvknomqerfirzet. This dataset includes the necessary data and metadata for evaluating the predictive models presented in the manuscript.\n\nAdditionally, the code used for the evaluation and intermediate files that allow others to replicate the findings has been documented and made publicly available. The code can be found on GitLab at the following URL: https://gitlab.com/mgarciat/genome-wide-prediction-of-topoisomerase-iibeta-binding. This repository includes detailed instructions on how to run the code, ensuring that other computational biologists can easily replicate the results and make new predictions."
}