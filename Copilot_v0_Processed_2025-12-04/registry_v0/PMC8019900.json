{
  "publication/title": "Prediction of Breast Masses",
  "publication/authors": "The authors who contributed to the article are:\n\n- JZ\n- YC\n- JD\n- YL\n- TB\n- DX\n\nJZ, YC, and JD conceived and designed the experiments. YC performed the experiments. JZ, YL, TB, and DX analyzed the data. YC, JZ, and YL participated in writing the manuscript. The final version of the manuscript has been reviewed and approved for publication by all authors.",
  "publication/journal": "Frontiers in Oncology",
  "publication/year": "2021",
  "publication/doi": "10.3389/fonc.2021.629321",
  "publication/tags": "- Mammography\n- Image feature\n- Deep learning\n- Clinical prediction\n- Radiomics\n- Breast cancer\n- Machine learning\n- Medical imaging\n- Breast masses\n- Transfer learning",
  "dataset/provenance": "The dataset used in this study consists of breast mass images, which were analyzed to predict the nature of the masses. The images were obtained from mammograms, specifically in craniocaudal (CC) and mediolateral oblique (MLO) views. The dataset was split into a training cohort and a test cohort. The training cohort comprised 744 regions of interest (ROIs), while the test cohort consisted of 244 ROIs. It is important to note that the ROIs of the test cohort were not included in the training cohort to ensure an unbiased evaluation.\n\nThe dataset underwent data augmentation to enhance the training process. For each ROI in the training cohort, transformations such as flipping and rotations (90, 180, and 270 degrees) were applied to generate seven new label-preserving samples. This augmentation aimed to increase the diversity of the training data and improve the robustness of the model.\n\nThe study utilized a combination of deep learning (DL) and handcrafted features for analysis. Handcrafted features included texture descriptors such as gray-level co-occurrence matrix (GLCM), gray-gradient co-occurrence matrix (GLGCM), gray-level difference statistics (GLDS), gray run-length matrix (GLRLM), local binary pattern (LBP), and Gaussian Markov random field (GMRF) features. Additionally, higher-order Gabor features and Hu’s moment invariants were extracted. In total, 455 handcrafted features were considered.\n\nClinical features were also incorporated into the analysis. These features included morphological descriptions of the masses, such as shape, margin type, breast composition, age, and mass size. These clinical parameters were encoded as numerical values to facilitate analysis.\n\nThe study employed a DL fusion network that combined VGG16 and Inception-V3 networks to extract deep features from the images. The VGG16 network was pre-trained on the ImageNet dataset, and its learned weights were transferred to the target network. The DL fusion network aimed to strengthen the ability of transfer learning by combining the strengths of both networks.\n\nThe performance of the DL fusion network was compared with that of the VGG16 fine-tuned network. The DL fusion network demonstrated improved accuracy, sensitivity, and F-score, indicating its superior performance in predicting breast masses. The study also highlighted the importance of combining image biomarkers with clinical parameters to enhance the classification performance in clinical practice.",
  "dataset/splits": "Two data splits were created: a training cohort and a test cohort. The training cohort consisted of 744 regions of interest (ROIs), while the test cohort had 244 ROIs. The ROIs of the test cohort were not included in the training cohort to ensure an unbiased evaluation. The distribution of data points in each split was random, aiming to maintain a representative sample for both training and testing phases.",
  "dataset/redundancy": "The dataset used in this study was split into a training cohort and a test cohort. This separation was done randomly, resulting in 744 regions of interest (ROIs) for the training cohort and 244 ROIs for the test cohort. It was ensured that the ROIs of the test cohort were not included in the training cohort, maintaining the independence of the two sets. This approach helps in evaluating the model's performance on unseen data, which is crucial for assessing its generalization capability.\n\nThe distribution of the dataset compares favorably with previously published machine learning datasets in the field of breast mass prediction. The random splitting method is a standard practice that helps in creating a balanced and representative dataset for training and testing purposes. By ensuring that the test cohort is independent of the training cohort, the study aims to provide a robust evaluation of the model's performance, reducing the risk of overfitting and ensuring that the results are reliable and reproducible.",
  "dataset/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This means that the data, including the data splits used, are not publicly released in a forum at this time. However, they can be obtained by requesting them from the authors. The authors have committed to providing the data without undue reservation, ensuring that other researchers can access and verify the findings. There is no specific mention of a license for the data, but the commitment to sharing the data without undue reservation implies that it will be made available for research purposes. The enforcement of this commitment is based on the authors' agreement to share the data upon request.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Stochastic Gradient Descent (SGD). This is a well-established optimization algorithm commonly used in training deep learning models. It is not a new algorithm, having been widely adopted in the machine learning community for many years.\n\nSGD is chosen for its efficiency in handling large datasets and its ability to converge quickly to a good solution. It works by iteratively updating the model parameters in the direction that reduces the loss function, using a subset of the data at each step. This makes it particularly suitable for training deep neural networks, which often involve a large number of parameters and require extensive computational resources.\n\nThe reason this algorithm is discussed in a medical journal rather than a machine-learning journal is that the focus of our work is on the application of machine learning techniques to medical imaging, specifically the prediction of breast masses. The optimization algorithm is a crucial component of our methodology, but it is not the primary innovation of our study. Our contributions lie in the integration of deep learning features with hand-crafted radiomics features and clinical parameters to improve the predictive performance of breast mass classification. The use of SGD is mentioned to provide a complete picture of our methodological approach, ensuring that the results are reproducible and understandable to readers in the medical field.",
  "optimization/meta": "The model developed in this study is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it combines different types of features directly extracted from the data.\n\nThe model integrates three types of features: hand-crafted radiomics features, deep learning-based features, and clinical parameters. The hand-crafted features are selected using the minimal-redundancy-maximal-relevance (mRMR) method, ensuring that the most significant features are retained. The deep learning features are extracted using a DL fusion network, which converts mass images into high-dimensional feature vectors. Clinical parameters, such as shape, margin type, breast composition, age, and mass size, are also included.\n\nThe combined model processes these features using Min-Max normalization and then applies a Support Vector Machine (SVM) with a linear kernel for classification. The SVM model is fine-tuned through an internal grid search with 10-fold cross-validation to optimize its hyperparameters.\n\nThe training data for the model is independent and consists of image biomarkers and clinical biomarkers from a training set of 744 regions of interest (ROIs). The prediction performance and model stability are evaluated in a test set of 244 ROIs and verified in an external validation set of 100 ROIs from 58 patients. This ensures that the model's performance is robust and reliable.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the mammography images were suitable for input into our machine-learning models. Initially, we obtained regions of interest (ROIs) by cropping the mammography images to remove unnecessary black backgrounds. This step helped focus the analysis on the relevant areas of the images.\n\nNext, we normalized the ROIs to a range of [0, 1] using a linear function. This normalization process scaled the original data proportionally, ensuring consistency across all images. The formula used for normalization was:\n\nX_norm = (X - X_min) / (X_max - X_min)\n\nwhere X_norm represents the normalized data, X is the original data of the mammography image, and X_min and X_max are the minimum and maximum values of the original data, respectively.\n\nFollowing normalization, we adjusted the ROI sizes to meet the standard input dimension requirements for most convolutional neural networks (CNNs). We achieved this by zero-filling the images under no deformation conditions, resizing them to 224 × 224 pixels. This step ensured that all images had the same dimensions, which is essential for batch processing in neural networks.\n\nAdditionally, we encoded clinical features as numerical values to obtain true feature values. For example, the shape of the mass was encoded as 1 for round, 2 for oval, and 3 for irregular. Similarly, margin types and breast composition were encoded numerically to facilitate integration with the image data in our models.\n\nThese preprocessing steps were essential for preparing the data for feature extraction and subsequent model training. By standardizing the image dimensions and normalizing the pixel values, we ensured that the input data was consistent and suitable for the machine-learning algorithms used in our study.",
  "optimization/parameters": "In our study, we utilized a combination of different types of features to predict benign or malignant breast masses. Specifically, we employed 30 hand-crafted features, 27 deep learning (DL) features, and 5 clinical features. The hand-crafted features were selected using the minimal-redundancy-maximal-relevance (mRMR) method, which helps in reducing the dimensionality and improving the robustness of the model. The DL features were extracted using a DL fusion network, which converts the mass images into a 1024-dimensional feature vector, subsequently reduced to 27 dimensions through the mRMR method. The clinical features included shape, margin type, breast composition, age, and mass size.\n\nThe selection of these features was driven by the need to capture both the intrinsic characteristics of the mass images and the descriptive clinical information. The hand-crafted features provided texture characterization, while the DL features encoded deeper, more abstract information from the images. The clinical parameters offered additional context that is crucial for clinical decision-making. By combining these different types of features, we aimed to enhance the model's predictive performance and stability. The final model, which integrated all these features, demonstrated superior accuracy, specificity, and AUC compared to models using only clinical parameters or other individual feature sets.",
  "optimization/features": "In our study, we utilized a combination of clinical, hand-crafted, and deep learning (DL) features as input for our models. Initially, we extracted 455 hand-crafted features, which included first-order histogram features, second-order texture features, Hu’s moment invariants features, and high-order Gabor features. Additionally, we obtained 1024-dimensional DL features from our DL fusion network, which combined VGG16 and Inception-V3 networks.\n\nTo enhance the robustness and reliability of our models, we performed feature selection using the minimal-redundancy-maximal-relevance (mRMR) method. This process was conducted exclusively on the training set to prevent data leakage and ensure the generalizability of our results. Through feature selection, we retained 30 hand-crafted features and 27 DL features, resulting in a total of 57 features used as input for our classifier. This reduction in dimensionality helped to improve the training efficiency and the overall performance of our models.",
  "optimization/fitting": "In our study, we employed a deep learning fusion network for feature extraction, which inherently involves a large number of parameters. To address the potential issue of overfitting, given the relatively small medical image datasets, we utilized transfer learning. Specifically, we fine-tuned a pre-trained VGG16 network and combined it with an Inception-V3 network. This approach leverages the learned weights from extensive pre-training on the ImageNet dataset, providing a robust starting point for our specific task. Additionally, we implemented data augmentation techniques, such as flipping and rotating the images, to generate more training samples and enhance the model's generalization ability.\n\nTo further mitigate overfitting, we employed dropout regularization in the fully connected layers of our network. Dropout randomly sets a fraction of the input units to zero during training, which helps prevent the model from becoming too reliant on any single feature. We also used early stopping based on the validation loss, halting the training process when performance on the validation set ceased to improve.\n\nRegarding underfitting, we ensured that our model had sufficient capacity to learn the complex patterns in the data by using a deep architecture with multiple convolutional and fully connected layers. The use of transfer learning also provided a strong foundation, allowing the model to focus on learning task-specific features rather than general image features from scratch. Moreover, we performed extensive hyperparameter tuning, including adjusting the learning rate, batch size, and number of epochs, to optimize the model's performance.\n\nIn summary, our approach combined transfer learning, data augmentation, dropout regularization, and early stopping to address overfitting, while the deep architecture and hyperparameter tuning helped to prevent underfitting. These strategies collectively contributed to the development of a robust and generalizable model for predicting breast masses.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and enhance the robustness of our models. One key method used was dropout, which was applied to the fully connected layers of our neural networks. Dropout works by randomly setting a fraction of the input units to zero at each update during training time, which helps prevent overfitting by ensuring that the network does not become too reliant on any single neuron.\n\nAdditionally, we utilized weight decay, also known as L2 regularization, during the training of our networks. Weight decay adds a penalty equal to the squared magnitude of the coefficients to the loss function, encouraging the model to keep the weights small and thus reducing the risk of overfitting.\n\nWe also implemented data augmentation techniques. For each region of interest (ROI) in the training cohort, we generated seven new label-preserving samples using a combination of flipping and rotation transformations (90, 180, and 270 degrees). This augmentation helped to increase the diversity of the training data, making the model more generalizable to unseen data.\n\nFurthermore, we employed transfer learning, which involves using a pre-trained network (VGG16) and fine-tuning it on our specific dataset. This approach leverages the knowledge gained from a large dataset (ImageNet) and adapts it to our smaller, domain-specific dataset, thereby improving the model's performance and reducing the risk of overfitting.\n\nLastly, we used the minimal-redundancy-maximal-relevance (mRMR) feature selection method to reduce the dimensionality of our feature set. By selecting the most relevant and non-redundant features, we aimed to improve the model's performance and generalization ability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the architecture of the Vgg16 fine-tuned network, including the number of layers used for training, the epoch and learning rate settings, and the optimization algorithm (Stochastic Gradient Descent with momentum and weight decay), are all described. Additionally, the parameter settings for the DL fusion network, which combines Vgg16 and Inception-V3, are referenced similarly to the Vgg16 network.\n\nThe model files themselves are not directly provided in the publication, but the methods and configurations for training the models are thoroughly documented. This includes the use of transfer learning with pre-trained weights from the ImageNet dataset and the specific layers that were fine-tuned.\n\nRegarding the availability of optimization parameters, these are implicitly included in the description of the training process. For instance, the use of dropout for regularization in the fully connected layer and the settings for the soft-max classifier are mentioned.\n\nAs for the license, the publication itself is available under the terms of the Frontiers in Oncology journal, which typically allows for open access and reuse with proper citation. However, specific model files or code implementations would need to be requested directly from the authors, as indicated in the data availability statement. The raw data supporting the conclusions of the article will be made available by the authors without undue reservation.",
  "model/interpretability": "The model we developed combines deep learning (DL) features, hand-crafted features, and clinical parameters to predict benign or malignant breast masses. While the integration of these diverse features enhances predictive performance, it also introduces complexity, making the model somewhat of a black box.\n\nThe deep learning component, specifically the DL fusion network, automatically extracts high-dimensional feature vectors from mass images. These features are abstract and complex, reflecting intricate patterns within the images that are not easily interpretable by humans. The network's layers transform the input images through convolutional and fully connected layers, capturing spatial hierarchies and intricate details, but the exact nature of these transformations is not straightforward to decipher.\n\nHand-crafted features, on the other hand, are more interpretable. These features include texture descriptors like gray-level co-occurrence matrix features, gray run-length matrix features, and higher-order Gabor features. Each of these features has a clear mathematical definition and can be linked to specific characteristics of the mass images, such as texture and pattern. For example, gray-level co-occurrence matrix features quantify the texture by considering the spatial relationship of pixels, while Gabor features capture frequency and orientation information.\n\nClinical parameters, such as shape, margin type, breast composition, age, and mass size, are also interpretable. These parameters are descriptive and can be directly related to clinical observations and BIRADS classification. For instance, the shape of a mass (round, oval, or irregular) and its margin type (clear, shadow, differential leaf, fuzzy, or glitch) are visual characteristics that radiologists use to assess the likelihood of malignancy.\n\nHowever, the combined model integrates these diverse features using a support vector machine (SVM) with a linear kernel. While SVM is a powerful classification tool, the relationships it learns between the features are not easily interpretable. The model's decisions are based on complex interactions between the features, making it challenging to trace back to specific input characteristics.\n\nIn summary, while the hand-crafted features and clinical parameters provide some level of interpretability, the deep learning component and the overall combined model remain largely a black box. The model's strength lies in its ability to capture and integrate complex patterns from diverse data sources, but this comes at the cost of transparency. Future work could focus on developing techniques to enhance the interpretability of the deep learning features and the combined model's decisions.",
  "model/output": "The model developed is a classification model designed to predict whether breast masses are benign or malignant. It combines deep learning (DL) features, hand-crafted features, and clinical parameters to achieve this classification. The model uses a support vector machine (SVM) with a linear kernel for the classification task. The SVM aims to separate hyperplanes in a high-dimensional feature space, providing an efficient method for learning and classification.\n\nThe output of the model is a prediction of the mass type, either benign or malignant. The performance of the model was evaluated using metrics such as accuracy, specificity, and the area under the curve (AUC). In the test cohort, the combined model achieved an accuracy of 0.910, a specificity of 0.934, and an AUC of 0.962. These results demonstrate the model's effectiveness in distinguishing between benign and malignant breast masses.\n\nThe model's robustness and reliability were further validated in an external validation cohort, where it showed improved accuracy and AUC compared to the clinical model. The combined model's ability to integrate different types of information—image biomarkers and clinical parameters—enhances its predictive performance, making it a valuable tool for clinical practice.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method involved several key steps to ensure the robustness and reliability of our models. We began by randomly selecting a total of 744 regions of interest (ROIs) for the training cohort and 244 ROIs for the test cohort. This division allowed us to train our models on a substantial dataset while reserving a separate set for unbiased evaluation.\n\nWe utilized a confusion matrix to assess the performance of our approach. From this matrix, we calculated several critical metrics, including the area under the curve (AUC), accuracy, sensitivity, specificity, precision, and F-score. These metrics provided a comprehensive view of the models' discriminative performance and stability.\n\nTo further validate our results, we employed Delong's test, which evaluated the statistical significance of the AUC. A P-value of less than 0.05 was considered significant, ensuring that our findings were not due to random chance.\n\nIn addition to these standard evaluation metrics, we compared the performance of different feature combination schemes using support vector machines (SVM). This comparison was illustrated in a table and corresponding ROC curves, which visually represented the predictive performance of various models in the test cohort.\n\nOur evaluation also included an external validation set, which allowed us to assess the generalizability of our models to different institutions and environments. This step was crucial for providing credibility to our inferences and demonstrating the robustness of our combined model.\n\nOverall, our evaluation method was rigorous and multifaceted, ensuring that our models were thoroughly tested and validated.",
  "evaluation/measure": "In our study, we employed a comprehensive set of performance metrics to evaluate the predictive performance of our models. These metrics include accuracy, sensitivity, specificity, precision, F-score, and the area under the receiver operating characteristic curve (AUC). These metrics were chosen to provide a thorough assessment of the models' discriminative performance and stability.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. Sensitivity, also known as recall, indicates the proportion of actual positives that are correctly identified by the model. Specificity measures the proportion of actual negatives that are correctly identified. Precision reflects the proportion of positive identifications that are actually correct. The F-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. The AUC provides an aggregate measure of performance across all classification thresholds.\n\nThese metrics are widely used in the literature for evaluating machine learning models, particularly in medical imaging and diagnostic tasks. They offer a comprehensive view of the model's performance, ensuring that we capture both the effectiveness and reliability of our predictions. By reporting these metrics, we aim to provide a clear and representative evaluation of our models' capabilities in predicting benign or malignant breast masses.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets such as MIAS or DDSM. However, we did compare our approach to simpler baselines to evaluate its effectiveness.\n\nWe evaluated the performance of different feature combination schemes using a Support Vector Machine (SVM) classifier. These schemes included hand-crafted radiomics features, clinical parameters, and deep learning features, both individually and in combination. The hand-crafted features were selected using the minimum Redundancy Maximum Relevance (mRMR) method to ensure they reflected the essential characteristics of the masses.\n\nOur deep learning approach involved a fusion network designed to automatically learn intrinsic characteristics from mass images. This network did not rely on hand-coded feature extraction, which is a significant advantage over traditional methods. We also combined image biomarkers with clinical parameters to assess the classification effects in clinical practice.\n\nThe results showed that the combined model, which integrated deep learning features, hand-crafted radiomics features, and clinical parameters, outperformed models that relied solely on clinical parameters or image features. This indicates that our approach provides a more robust and reliable method for predicting benign or malignant breast masses.\n\nIn summary, while we did not compare our method directly to publicly available benchmarks, we did perform extensive comparisons to simpler baselines, demonstrating the superior performance of our combined model.",
  "evaluation/confidence": "The evaluation of our model's performance included several key metrics such as accuracy, sensitivity, specificity, precision, F-score, and AUC. To ensure the robustness and reliability of these metrics, we employed Delong's test to evaluate the statistical significance of the AUC results. This test helped us determine whether the differences in AUC values between our models and baselines were statistically significant. A P-value of less than 0.05 was considered significant, indicating that our model's performance improvements were not due to random chance.\n\nIn addition to Delong's test, we used a confusion matrix to calculate the aforementioned metrics, providing a comprehensive view of our model's performance. The confusion matrix allowed us to assess the true positive, true negative, false positive, and false negative rates, which are crucial for understanding the model's strengths and weaknesses.\n\nFurthermore, we validated our model using an external validation cohort, which demonstrated the model's generalizability and robustness. The combined model showed improved accuracy, specificity, and AUC compared to the clinical model in this external validation set, with statistically significant P-values.\n\nWhile we did not explicitly mention confidence intervals for all performance metrics, the use of Delong's test and external validation provides strong evidence of our model's superior performance. The statistical significance of our results, along with the improved metrics in the external validation cohort, gives us high confidence in the reliability and generalizability of our findings.",
  "evaluation/availability": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. This ensures that other researchers can access and verify the findings presented in the study. The availability of the raw data is crucial for transparency and reproducibility in scientific research. By providing access to the raw data, we aim to facilitate further investigation and potential building upon our work. This approach aligns with best practices in scientific publishing, promoting openness and collaboration within the research community."
}