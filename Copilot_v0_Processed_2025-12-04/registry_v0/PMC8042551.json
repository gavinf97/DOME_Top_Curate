{
  "publication/title": "Computational Analysis of Diet-Derived Catechins on Vasculogenic Mimicry in Ovarian Cancer Cells",
  "publication/authors": "The authors who contributed to the article are:\n\n- Uthamacumaran, who is the first author of the paper.\n- Borhane Annabi, who is the corresponding author and holds an Institutional Research Chair in Cancer Prevention and Treatment. He is affiliated with the Laboratoire d’Oncologie Moléculaire, Département de Chimie, Université du Québec à Montréal. He is also the principal investigator and received funding from the Natural Sciences and Engineering Research Council of Canada (NSERC) for this study.\n\nThe specific contributions of the other authors are not detailed in the provided information.",
  "publication/journal": "Cancer Informatics",
  "publication/year": "2021",
  "publication/doi": "10.1177/11769351211009229",
  "publication/tags": "- Computational analysis\n- Machine learning\n- Vasculogenic mimicry\n- Ovarian cancer\n- Epithelial-mesenchymal transition\n- Green tea catechins\n- Image processing\n- Drug-induced perturbation\n- Complex processes\n- Cancer cell behavior",
  "dataset/provenance": "The dataset utilized in this study originates from in vitro experiments involving ovarian cancer cell models, specifically ES2 and SKOV3 cells. These cells were cultured on Matrigel to trigger vasculogenic mimicry (VM), and images of the resulting capillary-like structures were captured at different time points.\n\nThe dataset consists of images that were divided into quadrants to increase the sample size for training. For instance, in some analyses, each image in duplicate sets was divided into 4 quadrants, resulting in a total of 48 images for certain analyses. Additionally, other analyses involved 32 images, where each of the 8 catechin groups was split into 4 quadrants.\n\nThe data used in this study includes wavelet coefficients and percolation scores derived from the images of the treated cells. These features were extracted to train and test various machine learning models, including support vector machines (SVMs) with different kernels. The dataset was also used to compute fractal dimensions and perform statistical analyses to assess the predictive power of the extracted features.\n\nThe dataset has not been previously used in other published papers or by the community, as it is specific to this study's experimental setup and analyses. The focus was on evaluating the chemopreventive properties of diet-derived catechins and their ability to inhibit VM processes in ovarian cancer cells. The dataset's design and the computational techniques applied are novel contributions to the field.",
  "dataset/splits": "In our study, we utilized multiple data splits to ensure robust training and validation of our models. For the regression analyses, we divided the dataset into an 80% training set and a 20% testing set. Additionally, we employed 5-fold cross-validation to further validate the performance of our models.\n\nFor the training of the linear SVM regression model with wavelet coefficients from ungallated catechins-treated cells, each image in duplicate sets was divided into 4 quadrants. This approach increased the training set sample size to 48 images.\n\nIn the classification tasks, when assessing the predictive power of the extracted features, we used a new set of 24 images for the linear SVM and a set of 32 images for the Gaussian SVM. The 32 images were derived from 8 catechin groups, each split into 4 quadrants.\n\nThe distribution of data points in each split was designed to maximize the diversity and representativeness of the training and testing sets, ensuring that our models could generalize well to new, unseen data. This careful splitting strategy helped us achieve high predictive performance and reliability in our analyses.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is Support Vector Machines (SVMs). SVMs are a set of supervised learning methods used for classification, regression, and outliers detection.\n\nThe specific SVM algorithms employed in our research are not new; they are well-established techniques in the field of machine learning. We utilized both linear and Gaussian (also known as Radial Basis Function, RBF) SVM kernels. The choice of kernel is crucial as it determines the type of decision boundary that can be used to separate the classes. The linear SVM kernel is used when the data is linearly separable, while the Gaussian SVM kernel is more flexible and can handle cases where the data is not linearly separable.\n\nThe reason these algorithms were not published in a machine-learning journal is that our primary focus was on applying these established methods to a specific biological problem rather than developing new machine-learning algorithms. Our study aimed to demonstrate the effectiveness of SVM classifiers in analyzing complex biological data, specifically in the context of vasculogenic mimicry (VM) in ovarian cancer cells. The novelty of our work lies in the application of these machine-learning techniques to a novel biological dataset and the insights gained from this application.\n\nIn summary, we used standard SVM algorithms with linear and Gaussian kernels to classify and regress data related to VM structures in ovarian cancer cells. The choice of these algorithms was driven by their proven effectiveness in handling complex data, and our work contributes to the broader application of machine learning in computational biology.",
  "optimization/meta": "In our study, we did not employ a meta-predictor. Instead, we utilized support vector machines (SVMs) with different kernels to analyze and predict the effects of various catechins on vasculogenic mimicry (VM) in ovarian cancer cell models. Specifically, we used linear SVM and Gaussian SVM kernels to evaluate the predictive power of wavelet coefficients and percolation scores derived from our image analysis.\n\nThe linear SVM was initially trained using wavelet coefficients from ungallated catechin-treated cells, but it showed poor performance when applied to gallated molecules, particularly the EGC1/2 groups. However, when trained with coefficients from gallated catechins, the linear SVM demonstrated high prediction performance with an R-squared value of 0.99 and a low RMSE.\n\nFor a more robust analysis, we also employed a Gaussian SVM, which outperformed the linear SVM. The Gaussian SVM was trained using mean branching cluster statistics from 2D wavelet analysis of all 8 catechins, resulting in a high prediction performance with an RMSE of 2.469 ± 0.0078 and an R-squared value of 0.98.\n\nAdditionally, we assessed the predictive power of percolation scores using a fine Gaussian SVM classifier. This classifier achieved perfect prediction with a speed of 200 observations per second when presented with a new set of images. In contrast, the linear SVM showed poor classification accuracy with the same data.\n\nThroughout our analyses, we ensured that the training data sets were independent to maintain the integrity of our predictions. We performed 5-fold cross-validation and used separate data sets for training and testing to validate the performance of our models.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We began by extracting features from images of vasculogenic mimicry (VM) structures formed by ovarian cancer cells. These images were subjected to 2D wavelet analysis, which decomposed the images into wavelet coefficients. This process helped in capturing the intricate details and patterns within the VM structures.\n\nEach image was divided into four quadrants to increase the sample size for training our models. This division allowed us to generate a more robust dataset, enhancing the reliability of our predictions. The wavelet coefficients obtained from these images were then used to train our support vector machine (SVM) models. We employed both linear and Gaussian SVM kernels to assess their performance in different scenarios.\n\nFor the regression tasks, we computed the R-squared value and the root mean square error (RMSE) to evaluate the accuracy of our models. A smaller RMSE indicated higher prediction accuracy. Additionally, we performed statistical significance testing using tools like Origin Pro, Graphpad Prism, XLSTAT, and MATLAB to ensure the validity of our results.\n\nIn summary, our data encoding involved wavelet analysis to extract meaningful features from VM images, followed by preprocessing steps such as image division and statistical analysis to prepare the data for machine-learning algorithms. This approach enabled us to achieve high predictive power in identifying the chemopreventive properties of diet-derived catechins.",
  "optimization/parameters": "In our study, the input parameters for the models were derived from the wavelet coefficients obtained from the analysis of images of vasculogenic mimicry (VM) structures. Specifically, each image in duplicate sets was divided into 4 quadrants to increase the training set sample size, resulting in a total of 48 images for the training set.\n\nThe selection of these parameters was guided by the need to capture the essential features of the VM structures. The wavelet coefficients were chosen because they effectively represent the frequency components of the images, which are crucial for understanding the branching patterns and connectivity of the VM structures.\n\nTwo types of support vector machine (SVM) kernels were tested: a linear SVM kernel and a Gaussian SVM kernel with different scales. The linear SVM was initially used to draw a hyperplane of maximum-margin to divide the data into two separate groups: ungallated and gallated catechins-treated cells. However, the linear SVM showed poor performance when predicting the ungallated molecules coefficients, indicating that the data were not linearly separable.\n\nTo address this, a Gaussian SVM kernel was employed, which provided better performance. The Gaussian SVM kernel is more flexible and can capture non-linear relationships in the data, making it suitable for the complex patterns observed in the VM structures.\n\nThe choice of parameters was further validated through statistical analyses, including the computation of R-square and root mean square error (RMSE) between the true and regressed values. These metrics helped assess the accuracy of the regression models and ensured that the selected parameters effectively captured the relevant features of the VM structures.\n\nIn summary, the input parameters for the models were carefully selected based on the wavelet coefficients derived from the VM images. The use of both linear and Gaussian SVM kernels allowed for a comprehensive analysis of the data, with the Gaussian SVM kernel ultimately providing the best performance.",
  "optimization/features": "The study utilized several image processing techniques to extract features from the data. Specifically, three main feature extractors were employed: fractal dimension analysis, wavelet analysis, and percolation clustering.\n\nFractal dimension analysis was performed using a 2D box-counting algorithm, which computed the fractal dimension (Df) of the vasculogenic mimicry (VM) structures. This analysis involved dividing images into quadrants to increase the sample size for training.\n\nWavelet analysis was conducted using the Haar wavelet basis at level 3. This method decomposed the images into wavelet coefficients, which provided a measure of the level of convolution at which branching clusters could be reconstructed. The horizontal wavelet coefficients were particularly noted for their ability to indicate cluster connectivity.\n\nPercolation clustering statistics were also computed, focusing on the critical exponents for phase transition from random cell distributions to branching clusters. This analysis helped in understanding the differences between ungallated and gallated catechin-treated groups.\n\nFeature selection was implicitly performed by choosing the most relevant features from each of these analyses. For instance, the wavelet coefficients and percolation scores were used to train support vector machine (SVM) models, indicating that these features were deemed important for the classification and regression tasks.\n\nThe number of features (f) used as input varied depending on the specific analysis. For example, the wavelet coefficients and percolation scores were used in combination with SVM models, suggesting that multiple features were considered. However, the exact number of features is not explicitly stated.\n\nIt is important to note that the feature selection process was likely conducted using the training set only, as this is a standard practice to avoid data leakage and ensure the model's generalizability. This approach helps in maintaining the integrity of the validation and testing phases, ensuring that the model's performance is accurately assessed on unseen data.",
  "optimization/fitting": "The fitting method employed in this study involved training support vector machine (SVM) models using various feature extraction techniques. The number of parameters in the models was indeed larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several strategies were implemented.\n\nFirstly, the images were divided into quadrants to increase the training set sample size, providing more data points for the models to learn from. Additionally, an 80% training set and a 20% testing set were used, along with 5-fold cross-validation. This approach ensured that the models were evaluated on multiple subsets of the data, reducing the likelihood of overfitting to any single subset.\n\nTwo types of SVM kernels were tested: a linear SVM kernel and a Gaussian SVM kernel. The Gaussian SVM, in particular, is known for its ability to handle non-linear relationships and high-dimensional spaces, which helped in capturing the complex patterns in the data without overfitting.\n\nTo assess the accuracy of the regression models, R-square and the root mean square error (RMSE) were computed between the true and the regressed values. A small RMSE indicated high prediction accuracy, and the models were evaluated on their ability to generalize to new, unseen data.\n\nFurthermore, the performance of the models was compared using different feature extractors: fractal dimension analysis, wavelet analysis, and percolation clustering. The wavelet analysis and percolation clustering outperformed the fractal dimension analysis, demonstrating their effectiveness in capturing the relevant features of the vasculogenic mimicry structures.\n\nThe problem of overfitting was also challenged by the lack of performance of the fractal dimension in classifying the treatment groups compared to the other two feature extractors. This indicated that the models were not merely memorizing the training data but were indeed learning meaningful patterns.\n\nIn summary, the combination of data augmentation, cross-validation, and the use of appropriate kernel functions helped in ruling out overfitting. The models' performance on unseen data and the comparison with different feature extractors further validated their robustness and generalizability.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was cross-validation. Specifically, we utilized 5-fold cross-validation during the training of our support vector machine (SVM) models. This technique involves dividing the dataset into five subsets, training the model on four of these subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. By doing so, we ensured that our model's performance was evaluated on multiple subsets of the data, reducing the risk of overfitting to any single subset.\n\nAdditionally, we increased the training set sample size by dividing each image in duplicate sets into four quadrants. This approach effectively augmented our dataset, providing more diverse training examples and helping the model generalize better to unseen data.\n\nFurthermore, we compared the performance of different SVM kernels, including linear and Gaussian kernels. The choice of kernel can significantly impact the model's ability to generalize. By testing multiple kernels, we selected the one that best balanced model complexity and performance, further mitigating the risk of overfitting.\n\nIn summary, our use of cross-validation, data augmentation through image division, and kernel selection contributed to the prevention of overfitting, ensuring that our models were robust and generalizable.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the publication. Specifically, we employed a support vector machine (SVM) regression model with both linear and Gaussian kernels. The linear SVM was trained using wavelet coefficients from ungallated catechins-treated cells, with images divided into quadrants to increase the sample size. The training set consisted of 80% of the data, while 20% was reserved for testing, and a 5-fold cross-validation was implemented.\n\nFor the Gaussian SVM, we utilized a kernel with different scales, and the performance was assessed using metrics such as R-squared and root mean square error (RMSE). The specific configurations and parameters, including the kernel scales and cross-validation details, are presented in the results section.\n\nThe model files and optimization parameters are not explicitly provided as downloadable resources, but the methods and configurations are thoroughly described in the text. This allows for reproducibility of the experiments by following the detailed procedures outlined.\n\nRegarding the availability and licensing, the methods and configurations described in the publication are intended for academic and research purposes. While specific model files are not shared, the detailed descriptions and parameters provided enable other researchers to implement similar models and reproduce the results. The publication itself is available under standard academic publishing licenses, which typically permit use for non-commercial research and educational purposes.",
  "model/interpretability": "The models employed in our study, specifically the Support Vector Machine (SVM) regression and classification models, can be considered somewhat interpretable, although they are not entirely transparent. SVMs are known for their ability to provide a clear decision boundary, which in the case of linear SVMs, is a hyperplane that maximizes the margin between different classes. This hyperplane can be interpreted as a linear combination of the input features, making it possible to understand which features contribute most to the classification or regression task.\n\nFor instance, in our analysis, the linear SVM regression model was trained using wavelet coefficients from ungallated catechins-treated cells. The model's performance was evaluated using metrics such as R-squared and Root Mean Square Error (RMSE), which provided insights into the model's predictive accuracy. The linear SVM's decision boundary helped in understanding how different wavelet coefficients influenced the prediction of vasculogenic mimicry (VM) structures.\n\nHowever, when using a Gaussian SVM kernel, the decision boundary becomes more complex and non-linear, making it less interpretable. The Gaussian kernel transforms the data into a higher-dimensional space where a linear separator can be found, but this transformation is not straightforward to interpret. Despite this, the Gaussian SVM showed superior performance in predicting VM patterns, highlighting the trade-off between interpretability and predictive power.\n\nIn summary, while the linear SVM offers some level of interpretability through its decision boundary, the Gaussian SVM, although more accurate, is less transparent. The choice between these models depends on the specific requirements of the analysis, balancing the need for interpretability with the desire for high predictive accuracy.",
  "model/output": "The model employed in our study encompasses both regression and classification analyses. For regression tasks, we utilized Support Vector Machine (SVM) models, specifically linear and Gaussian SVMs, to predict response curves based on wavelet coefficients extracted from images of cells treated with various catechins. The performance of these regression models was evaluated using metrics such as R-squared and Root Mean Square Error (RMSE). For classification tasks, we also employed SVM classifiers, including linear and fine Gaussian SVMs, to distinguish between different catechin-treated groups. The classification performance was assessed using accuracy and prediction speed. The choice of kernel—linear or Gaussian—was crucial and depended on whether the data were linearly separable. Overall, the Gaussian SVM generally demonstrated superior performance compared to the linear SVM in both regression and classification tasks.",
  "model/duration": "The execution time of the models varied depending on the type of support vector machine (SVM) used and the specific task. For the linear SVM regression predictor on vasculogenic mimicry (VM) images from ungallated catechin-treated cells, the prediction speed was approximately 79 observations per second. However, when the linear SVM was trained on gallated catechins-treated group coefficients from a new set of images, the prediction speed increased significantly to about 920 observations per second. This improvement in speed is notable and indicates that the model performed more efficiently with the gallated catechins data.\n\nFor the fine Gaussian SVM classifier, which was used to assess percolation scores, the prediction speed was around 200 observations per second. This classifier demonstrated perfect prediction accuracy with the given dataset, highlighting its effectiveness in distinguishing between different catechin-treated groups.\n\nIn summary, the execution times for the models ranged from 79 to 920 observations per second, with the Gaussian SVM showing particularly high efficiency and accuracy in classification tasks.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the methods employed in this study involved several rigorous steps to ensure the robustness and accuracy of the results. For the regression and classification analyses, a support vector machine (SVM) was utilized. The SVM regression model was trained using wavelet coefficients derived from images of ungallated catechins-treated cells. To enhance the training set sample size, each image in duplicate sets was divided into four quadrants. The training process involved an 80% training set, a 20% testing set, and a 5-fold cross-validation to assess the model's performance.\n\nTwo types of SVM kernels were tested: a linear SVM kernel and a Gaussian SVM kernel with varying scales. The accuracy of the regression models was evaluated using the R-square and root mean square error (RMSE) metrics, which compare the true values to the regressed values. A lower RMSE indicates higher prediction accuracy.\n\nFor classification tasks, the percolation scores of ungallated and gallated catechin-treated groups were subjected to a fine Gaussian SVM classifier. This classifier demonstrated perfect prediction performance when presented with a new set of images, achieving a prediction speed of 200 observations per second. In contrast, a linear SVM showed poorer classification accuracy, highlighting the importance of choosing the appropriate kernel based on the data's separability.\n\nAdditionally, the SVM classifier was assessed on fractal dimension analyses. However, the classifier's performance was poor due to the ambiguity of the fractal dimension scores for the gallated groups. This indicates that the box-counting algorithm used for fractal dimension analysis may not be sufficient for capturing the predictive power of vasculogenic mimicry (VM) structures. Alternative methods, such as multifractal analysis or different fractal dimension estimators, are suggested for future studies.\n\nThe evaluation also included statistical significance testing and graphical analyses performed using various software tools, including Origin Pro, Graphpad Prism, XLSTAT, and MATLAB. These analyses provided further insights into the data and supported the findings of the study. Overall, the evaluation methods ensured that the computational techniques used could effectively quantify the chemopreventive activities of galloylated catechins and their impact on VM processes.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our image processing and classification techniques. For the regression models, we computed the R-square (r²) and the root mean square error (RMSE) to assess the accuracy of the response curve. A smaller RMSE indicates higher prediction accuracy. For classification tasks, we primarily used support vector machines (SVMs) and reported the positive predictive values, accuracy, and prediction speed. The positive predictive value indicates the proportion of true positives among all positive results. Accuracy measures the overall correctness of the classifier, while prediction speed is crucial for real-time applications.\n\nWe also performed 5-fold cross-validation to ensure the robustness of our models. This technique involves dividing the data into five parts, training the model on four parts, and testing it on the remaining part, repeating this process five times. This approach helps to validate the model's performance and generalizability.\n\nAdditionally, we compared the performance of different SVM kernels, specifically the linear SVM and the Gaussian SVM. The Gaussian SVM consistently showed superior performance, particularly in capturing the predictive power of vasculogenic mimicry (VM) structures. This is evident in the perfect prediction achieved with the fine Gaussian SVM classifier on percolation scores, demonstrating its high accuracy and speed.\n\nThe use of these metrics aligns with standard practices in the literature, ensuring that our evaluation is representative and comprehensive. The combination of regression and classification metrics provides a thorough assessment of our models' performance, highlighting their strengths and areas for potential improvement.",
  "evaluation/comparison": "In our study, we compared the performance of different machine learning algorithms and feature extraction methods to evaluate their effectiveness in analyzing vasculogenic mimicry (VM) structures. Specifically, we assessed the predictive power of wavelet clustering scores, percolation scores, and fractal dimension analysis using support vector machines (SVMs) with both linear and Gaussian kernels.\n\nWe found that the Gaussian SVM kernel outperformed the linear SVM kernel, indicating that the data were not linearly separable. The wavelet clustering scores and percolation scores demonstrated high predictive performance, with the Gaussian SVM achieving a root mean square error (RMSE) of 2.469 ± 0.0078 and an r-squared value of 0.98. In contrast, the linear SVM showed poorer performance.\n\nFor the percolation scores, the fine Gaussian SVM classifier achieved perfect prediction with a speed of 200 observations per second when presented with a new set of images. However, the linear SVM had only 56.9% accuracy. This highlights the importance of choosing the appropriate kernel based on the separability of the data.\n\nWhen assessing the fractal dimension analyses, both the Gaussian and linear SVMs performed poorly. The fractal dimension scores were too close to each other among the different groups, leading to ambiguous results and poor classification accuracy. The Gaussian SVM had 0% classification accuracy, while the linear SVM had 6.3% accuracy. This demonstrates that the box-counting algorithm used for fractal dimension analysis lacks predictive power in assessing drug-mediated changes in VM structures.\n\nOur findings suggest that more advanced techniques, such as multifractal analysis or alternative fractal dimension estimators like the sandbox method, may be required to improve the classification performance. Additionally, we propose that neural networks and random forests could offer greater predictive outcomes compared to SVM-based approaches, especially when dealing with larger and more heterogeneous datasets.\n\nIn summary, our comparison of methods revealed that wavelet analysis and percolation clustering outperformed fractal dimension analysis as feature extractors for VM structures. The choice of kernel in SVM classification is crucial and depends on the nature of the data. Future studies should explore more sophisticated image classification approaches and pattern recognition techniques, including deep learning architectures, to enhance predictive power and handle the complexity of VM processes.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of our methods involved several statistical analyses to ensure the robustness and significance of our findings. We employed a 5-fold cross-validation technique to assess the performance of our classifiers, which helps in understanding the variability and reliability of the results. For the regression models, we computed the R-squared value and the root mean square error (RMSE) to evaluate the prediction accuracy. A small RMSE indicates high prediction accuracy, and we observed this in our models, particularly with the Gaussian SVM.\n\nStatistical significance was tested using the Mann-Whitney 2-tailed test, which showed a P value of .012 for the percolation scores between ungallated and gallated molecules-treated groups. This P value, being less than .05, indicates a significant difference between the two groups, reinforcing the reliability of our results.\n\nAdditionally, we used multiple software tools for statistical significance testing and graphical analyses, including Origin Pro, Graphpad Prism, XLSTAT, and MATLAB. These tools helped in validating the consistency and accuracy of our data.\n\nThe performance metrics, such as the R-squared value and RMSE, were computed with confidence intervals to provide a clearer picture of the model's performance. For instance, the RMSE of 2.469 ± 0.0078 and an R-squared value of 0.98 for the Gaussian SVM highlight the model's high predictive power and reliability.\n\nIn summary, our evaluation methods included rigorous statistical testing and cross-validation techniques, ensuring that our results are statistically significant and reliable. The use of multiple software tools further validated our findings, providing confidence in the superiority of our methods over others and baselines.",
  "evaluation/availability": "Not enough information is available."
}