{
  "publication/title": "Supervised machine learning outperforms taxonomy-based environmental DNA metabarcoding applied to biomonitoring.",
  "publication/authors": "The authors who contributed to the article are Verena Dully, Thomas A. Wilding, Timo Mühlhaus, and Thorsten Stoeck. Verena Dully, Thomas A. Wilding, and Timo Mühlhaus contributed to the writing and editing of the paper. Thorsten Stoeck supervised the project, conceptualized the study, acquired funding, and wrote the original draft.",
  "publication/journal": "Computational and Structural Biotechnology Journal",
  "publication/year": "2021",
  "publication/doi": "https://doi.org/10.1016/j.csbj.2021.04.005",
  "publication/tags": "- Machine Learning\n- Environmental DNA\n- Metabarcoding\n- Random Forest Algorithm\n- Marine Ecology\n- Aquaculture\n- Bacterial Sequencing\n- Ecological Quality Assessment\n- Supervised Learning\n- Biotic Index Prediction",
  "dataset/provenance": "The datasets used in this study originate from various environmental samples, each focusing on different ecological predictions. The ScoSa dataset consists of 76 samples, with an average of 37,642 reads per sample, and is used to predict the distance from salmon farms and the salmon production phase. The BasCo dataset comprises 39 samples, averaging 76,259 reads per sample, and is employed to predict the microgAMBI index, which infers ecological quality. The BallWa dataset includes 59 samples, with an average of 91,598 reads per sample, and is utilized to predict the country of origin for ballast water samples. Lastly, the NorSa dataset contains 95 samples, averaging 31,057 reads per sample, and is used to predict the aquafarm location and the AMBI ecological quality index.\n\nEach dataset underwent a rarefaction process to create downsampled versions, with the number of sequences per sample ranging from the full dataset size down to as few as 50 sequences. This process was conducted to determine the minimum number of sequences required to achieve a targeted level of random forest (RF) prediction performance, which matches the prediction accuracy of the full dataset. The datasets were chosen based on their relevance to specific ecological and environmental predictions, and they have not been extensively used in previous studies or by the broader community in the exact manner presented here. The focus was on understanding how the number of sequences per sample affects the predictive performance of RF models in different ecological contexts.",
  "dataset/splits": "The dataset was split into four distinct datasets for analysis: Basque coastal (BasCo), ballast water (BallWa), Norwegian salmon farm (NorSa), and Scottish salmon farm (ScoSa). The BasCo dataset included 39 samples, the BallWa dataset consisted of 59 samples, the NorSa dataset had 95 samples, and the ScoSa dataset contained 76 samples.\n\nThe ScoSa dataset was further divided into three production phases based on sampling times: pre-production phase (25 samples, collected between March and May 2018), early salmon production phase (24 samples, collected between June and August 2018), and late salmon production phase (27 samples, collected between September 2018 and March 2019). These phases were defined to represent different stages of salmon production and their potential impacts on the environment. The pre-production phase had no salmon-related impact on the seafloor, while the early and late production phases had increasing fish biomass, with averages of 107 tons and 680 tons, respectively.",
  "dataset/redundancy": "The datasets used in this study were split into full and downsampled datasets to evaluate the impact of sequence depth on random forest (RF) classification performance. From each of the four full datasets, 13 rarefied datasets were created, resulting in a total of 52 datasets. The full datasets were used to define the \"targeted RF prediction performance,\" serving as a benchmark for comparison.\n\nThe training and test sets were independent, with the full dataset used to train the initial RF model. Downsampling was then performed to create subsets with varying sequence depths, ranging from 50 to 15,000 sequences per sample. This process ensured that the training data adequately characterized the classes being mapped, as the full dataset was used to establish the baseline performance.\n\nThe distribution of classes across the datasets was relatively even, with a minimum of 3 and a maximum of 5 classes. This balance helped to minimize the impact of class imbalance on the classification performance. The datasets included V3-V4 16S rDNA gene amplicon surveys of marine bacterial communities from different environments, such as Scottish salmon farm sediment samples, marine sediment samples from the Basque coast, ballast water samples, and Norwegian salmon farm sediment samples.\n\nThe number of sequences per sample in the first rarefied dataset was set to match the sample with the lowest sequence coverage in each dataset, except for the BallWa dataset, where it was set to 15,000 sequences to facilitate comparisons. Subsequent downsampling steps reduced the sequence depth incrementally, allowing for a thorough evaluation of the minimum number of sequences required for targeted classification performance.\n\nThe rarefaction curves and sampling saturation profiles for each dataset were analyzed to ensure that the downsampled datasets represented a meaningful subset of the full dataset. This approach provided a comprehensive assessment of how sequence depth affects RF classification accuracy, offering insights into optimal sampling designs for environmental sequencing studies.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the Random Forest (RF) algorithm. This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe RF algorithm is not new; it has been extensively used and validated in various fields, including ecological studies. The choice of RF for our study was driven by its robustness, ability to handle large datasets, and effectiveness in dealing with feature distributions that are skewed and class distributions that are unbalanced.\n\nThe RF algorithm was implemented using the randomForest package in R, which is a widely recognized and reliable tool for RF analysis. The decision to use this specific implementation was based on its efficiency and the comprehensive support it offers for parameter tuning and model evaluation.\n\nThe RF algorithm's parameters were carefully tuned to optimize performance. For instance, we constructed 6000 trees, which is significantly more than what is typically used in comparable analyses. This approach helps to minimize the effect of tuning parameters on the identification of the smallest number of sequences needed for targeted classification performance.\n\nThe use of out-of-bag (OOB) samples is an essential feature of the RF algorithm. For each observation, a random forest predictor is constructed by averaging only those decision trees in which the observation did not appear. This provides an almost identical error estimate to that obtained by N-fold cross-validation, ensuring the reliability of our models.\n\nIn summary, the RF algorithm was chosen for its proven effectiveness and robustness in handling complex datasets, making it well-suited for our ecological studies. The implementation and tuning of the algorithm were conducted with a focus on achieving high accuracy and reliability in classification performance.",
  "optimization/meta": "The model described in the publication does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The Random Forest (RF) algorithm is used directly on the training datasets, which consist of bacterial Amplicon Sequence Variants (ASVs) as features and defined reference label values or categories for classification or regression.\n\nThe RF algorithm is trained to relate specific combinations of ASVs to these reference labels. The essential feature of the RF algorithm used is the out-of-bag (OOB) samples, which provide an error estimate similar to N-fold cross-validation. The mtry value, a parameter for classification approaches, is set to the square root number of features as recommended.\n\nThe training data for each of the four bacterial ASV datasets (BasCo, BallWa, NorSa, and ScoSa) are independent and consist of two sets of data obtained from the same sample: the bacterial ASVs as features and the reference labels specific to each dataset. These reference labels include microgAMBI class, ballast water origin, AMBI class/farm, and distance/salmon production phase, respectively. The RF algorithm is then trained to relate specific combinations of ASVs to these defined reference label values or categories.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for the effective application of the random forest (RF) machine learning algorithm. The datasets used consisted of bacterial amplicon sequences, specifically the V3-V4 region of the 16S rRNA gene. These sequences were obtained from various environmental samples, including coastal waters, ballast water, and salmon farming sites.\n\nThe initial step involved quality filtering and trimming of the raw sequence data to remove low-quality reads and potential contaminants. This ensured that only high-quality sequences were used for further analysis. The sequences were then clustered into amplicon sequence variants (ASVs) using a de novo clustering approach, which groups similar sequences together based on a defined similarity threshold. This step helped in reducing the dimensionality of the data while retaining the biological diversity present in the samples.\n\nAfter obtaining the ASVs, the data were rarefied to normalize the sequencing depth across samples. Rarefaction involves randomly subsampling the sequences to a uniform depth, which helps in mitigating the effects of varying sequencing efforts. This step is essential for avoiding sequencing-depth artifacts and ensuring that the RF algorithm can accurately classify the samples based on their biological content rather than technical variations.\n\nThe rarefied data were then used to create feature matrices, where each row represented a sample and each column represented an ASV. The values in the matrix indicated the presence or absence (or abundance) of each ASV in the corresponding sample. These feature matrices served as the input for the RF algorithm.\n\nAdditionally, reference labels were assigned to each sample based on the specific classification task. For example, in the Basque coastal dataset, the reference labels corresponded to the microgAMBI class, which indicates the ecological quality of the samples. In the ballast water dataset, the reference labels indicated the geographic origin of the samples. These reference labels were used to train the RF models, enabling them to learn the relationships between the ASV features and the classification categories.\n\nIn summary, the data encoding and preprocessing involved quality filtering, ASV clustering, rarefaction, and the creation of feature matrices with corresponding reference labels. These steps ensured that the data were in a suitable format for the RF algorithm, allowing for accurate and reliable classification of the environmental samples.",
  "optimization/parameters": "In our study, we utilized the random forest (RF) algorithm, which involves several key parameters. One of the crucial parameters is the number of features (mtry) considered for splitting at each tree node. For classification tasks, we followed the recommendation to set the mtry value to the square root of the number of features. This approach helps in balancing the bias-variance trade-off and improving the model's generalization performance.\n\nAdditionally, we constructed RF models using 6000 trees, which is a parameter that determines the number of decision trees in the forest. This number was chosen based on empirical observations that increasing the number of trees beyond this point did not significantly improve the out-of-bag error estimate (OOB-E) for randomly chosen datasets.\n\nTo ensure the robustness of our models, we repeated the calculations for each dataset multiple times. For each dataset, we initially calculated a \"full model\" using the default mtry value. Subsequently, we computed six additional models, each with the default mtry value adjusted by plus/minus one, two, and three. This process was repeated twice using different base trees to secure the prediction capacity of the full model.\n\nThe R package caret was employed to infer kappa statistics for each RF model, providing a measure of agreement between the predicted and actual categories. This comprehensive approach to parameter selection and model validation ensures that our findings are reliable and reproducible.",
  "optimization/features": "In our study, the input features used for the Random Forest (RF) algorithm were Amplicon Sequence Variants (ASVs). These ASVs were derived from V3-V4 16S rRNA gene amplicon surveys of marine bacterial communities. The number of features (f) varied depending on the dataset and the specific analysis being conducted.\n\nInitially, full models were calculated using all available sequences from each dataset. This involved transforming the ASV-to-sample matrix into a relative abundance matrix to account for differences in sequencing depth between samples. For each dataset, multiple models were constructed with varying numbers of features to determine the optimal number required for targeted RF prediction performance.\n\nFeature selection was implicitly performed through the process of downsampling the datasets. This involved constructing RF models with rarified datasets to identify the minimum number of sequences within a sample needed to achieve the desired prediction performance. By systematically reducing the number of features, we could assess the impact on model performance and identify the most informative features.\n\nThe feature selection process was conducted using the training set only, ensuring that the models were not biased by information from the test set. This approach helped to minimize overfitting and ensure that the models generalized well to new, unseen data. The tuning parameters, such as the number of trees and the mtry value, were also varied to further optimize the models and ensure robust performance.",
  "optimization/fitting": "The fitting method employed in this study utilized random forest (RF) algorithms, which are known for their robustness and ability to handle high-dimensional data. The number of parameters, specifically the number of features (ASVs), was indeed much larger than the number of training points (samples) in each dataset. To address potential overfitting, several strategies were implemented.\n\nFirstly, the use of out-of-bag (OOB) samples provided an internal estimate of the model's performance, similar to cross-validation. This helped in assessing the generalization capability of the model. Additionally, the number of decision trees in the forest was set to 6000, which is significantly higher than what is commonly used in comparable analyses. This large number of trees helps in decorrelating the individual trees, thereby reducing the risk of overfitting.\n\nFurthermore, the mtry parameter, which determines the number of features considered for splitting at each tree node, was tuned around its default value (square root of the number of features) to ensure optimal performance. This tuning was repeated multiple times with different base trees to secure the prediction capacity of the full model.\n\nTo rule out underfitting, the models were constructed using all available sequences in the initial step, ensuring that the models had enough information to capture the underlying patterns. The transformation of the ASV-to-sample matrix into a relative abundance matrix also helped in compensating for differences in sequencing depth between samples, providing a more stable input for the RF models.\n\nThe use of the caret package to infer kappa statistics for each RF model provided a measure of agreement between the predicted and actual categories, further validating the model's performance. Additionally, the construction of models with rarefied datasets helped in determining the minimum number of sequences required to maintain the targeted prediction performance, ensuring that the models were not underfitted.\n\nIn summary, the fitting method employed in this study carefully balanced the risk of overfitting and underfitting through the use of OOB samples, a large number of decision trees, tuning of the mtry parameter, and the construction of models with rarefied datasets. These strategies ensured that the RF models were robust and capable of generalizing well to new data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our random forest (RF) models. One key method involved the use of out-of-bag (OOB) samples. For each observation, we constructed a random forest predictor by averaging only those decision trees in which the observation did not appear. This approach provided an OOB error estimate that is almost identical to that obtained by N-fold cross-validation, helping to assess the model's performance on unseen data.\n\nAdditionally, we varied the number of decision trees in our models. We ran 6000 trees for each model, which is significantly higher than the number of trees used in comparable analyses. This large number of trees helped to reduce the variance and improve the generalization of our models.\n\nWe also experimented with different values of the mtry parameter, which determines the number of features considered for splitting at each tree node. We calculated models with the default mtry value and then adjusted it by plus/minus one, two, and three. This process was repeated multiple times using different base trees to secure the prediction capacity of the full model.\n\nFurthermore, we transformed the ASV-to-sample matrix into a relative abundance matrix for each ASV. This transformation helped to compensate for differences in sequencing depth between samples, ensuring that our models were not biased by variations in sequencing effort.\n\nBy implementing these regularization techniques, we aimed to minimize overfitting and enhance the predictive performance of our RF models.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported within the publication. Specifically, we detail the use of out-of-bag (OOB) samples for error estimation, the number of trees in the random forest, and the mtry value settings. We also mention the use of the R package caret to infer kappa statistics for each random forest model.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly provided in the publication. The focus is on the methodology and results rather than the distribution of specific model files. However, the steps and parameters used to train the models are described, allowing for reproducibility.\n\nThe publication does not specify the license under which the reported configurations and parameters are made available. Typically, academic publications allow for the use of methodologies and parameters for research purposes, but specific licensing details would need to be clarified with the authors or the publishing journal.",
  "model/interpretability": "The Random Forest (RF) algorithm used in our study is not a black-box model. It is inherently interpretable due to its structure and the way it operates. The RF algorithm constructs multiple decision trees during the training process, and each tree is built using a random subset of the data and a random subset of the features. This process introduces randomness and helps to reduce overfitting, but it also makes the model more interpretable because the decisions made by each tree can be traced back to specific features and their combinations.\n\nOne of the key features of the RF algorithm is the use of out-of-bag (OOB) samples. For each observation, a predictor is constructed by averaging only those decision trees in which the observation did not appear. This provides an OOB error estimate that is almost identical to that obtained by N-fold cross-validation, giving us a reliable measure of the model's performance without the need for a separate validation set.\n\nThe RF algorithm also allows for the calculation of feature importance, which indicates the contribution of each feature to the model's predictions. This can be done by measuring the increase in the model's error when the values of a feature are permuted. Features that result in a large increase in error are considered more important. In our study, we used this feature importance measure to identify the most relevant ASVs (Amplicon Sequence Variants) for predicting the target variables.\n\nAdditionally, the RF algorithm provides the ability to visualize the decision trees, which can give insights into how the model makes predictions. Each decision tree consists of a series of splits based on the values of the features, leading to a final prediction. By examining the decision trees, one can understand the logical structure of the model and the criteria used to make predictions.\n\nIn summary, the RF algorithm used in our study is transparent and interpretable. It provides measures of feature importance, uses OOB samples for error estimation, and allows for the visualization of decision trees, making it a valuable tool for understanding the relationships between the features and the target variables.",
  "model/output": "The model employed in our study is primarily used for classification tasks. We utilized the Random Forest (RF) algorithm to predict categorical outcomes based on the bacterial V3-V4 16S rDNA metabarcodes obtained from various samples. For instance, in the ScoSa dataset, we predicted the distance of samples from the salmon farm and the salmon production phase, categorizing them into predefined classes. Similarly, for the BasCo dataset, we predicted the biotic index microgAMBI to infer the ecological quality of a sample, classifying it into categories such as high, good, moderate, or poor ecological quality. In the BallWa dataset, we predicted the origin of ballast water samples, categorizing them into China, Singapore, or the USA. For the NorSa dataset, we predicted the geographic location of the salmon farming site and the biotic index AMBI, classifying the ecological quality into good, moderate, poor, or bad. The RF algorithm was trained to relate specific combinations of amplicon sequence variants (ASVs) to these defined reference label values or categories, making it a classification model.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation method employed in our study involved a comprehensive approach to assess the performance of Random Forest (RF) models using both full and downsampled datasets. Initially, we calculated so-called \"full models\" (FM) using all available sequences from each of the four datasets. These models were trained to relate specific combinations of Amplicon Sequence Variants (ASVs) to defined reference label values, either for regression or classification tasks.\n\nTo ensure robust evaluation, we utilized out-of-bag (OOB) samples, which are observations not included in the construction of a particular decision tree. This method provides an almost identical error estimate to N-fold cross-validation. For each model, we ran 6000 trees, and randomly chosen datasets did not show any OOB error improvements when increasing the number of decision trees further.\n\nWe transformed the ASV-to-sample matrix into a relative abundance matrix to compensate for differences in sequencing depth between samples. This matrix was used to calculate RF models with features and reference labels as mentioned. For each dataset, we repeated the calculation of the FM several times, adjusting the mtry value (the number of features considered for splitting at each tree node) to the default value plus/minus one, two, and three. Each of these analyses was repeated twice using different base trees to secure the prediction capacity of the full model.\n\nThe R package caret was used to infer kappa statistics for each RF model, providing a measure of agreement between the predicted and actual values. In the second step, we constructed RF models with the rarefied (downsampled) datasets to determine the minimum number of sequences required within a sample to achieve the targeted RF prediction performance. We created 13 rarefied datasets from each of the four full datasets, with sequence numbers ranging from the lowest sequence coverage to as few as 50 sequences per sample. For each of these 52 downsampled datasets, RF analyses were conducted as described for the full model.\n\nThis evaluation method allowed us to assess the performance of RF models under varying conditions and to identify the minimum sequencing depth required for reliable predictions. The results indicated that datasets with well-defined boundaries of classes required fewer sequences within each sample to achieve the targeted classification performance, while more subtle differences among classes required more sequences.",
  "evaluation/measure": "In the evaluation of our models, we primarily focused on two key performance metrics: prediction accuracy and kappa statistics. Prediction accuracy provides a straightforward measure of how often the model's predictions match the actual values. This metric is crucial for understanding the overall effectiveness of our random forest (RF) models.\n\nKappa statistics, on the other hand, offer a more nuanced evaluation by accounting for the agreement between predicted and actual values while adjusting for the possibility of chance agreement. This metric is particularly useful in scenarios where the classes are imbalanced, as it provides a more reliable indication of the model's performance beyond mere accuracy. We used kappa values to categorize the agreement between observed and predicted classifications, with values greater than 0.8 indicating \"almost perfect agreement\" and values below 0.6 indicating poor agreement.\n\nAdditionally, we utilized out-of-bag (OOB) error estimates, which are almost identical to those obtained by N-fold cross-validation. The OOB error provides an internal estimate of the model's generalization performance, helping us to assess how well the model is likely to perform on unseen data.\n\nThese metrics are representative of standard practices in the literature for evaluating classification models, particularly in ecological studies involving RF algorithms. By reporting both prediction accuracy and kappa statistics, we ensure a comprehensive assessment of our models' performance, taking into account both the overall correctness of predictions and the strength of agreement between predicted and actual values.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on evaluating the performance of random forest (RF) predictions using different dataset sizes within our own datasets. We created 13 rarefied datasets from each of the four full datasets, with sequence counts ranging from the lowest sequence coverage in each dataset down to as few as 50 sequences. This approach allowed us to assess how the number of sequences per sample affects the prediction accuracy and kappa values for different ecological variables.\n\nFor simpler baselines, we did not explicitly compare our RF models to other machine learning algorithms or simpler statistical methods. However, we did analyze the impact of dataset size on the performance of RF models, which can be seen as a form of baseline comparison. By downsampling our datasets, we were able to determine the minimum number of sequences required to achieve targeted RF prediction performance. For example, we found that for the ScoSa dataset, as few as 50 sequences per sample were sufficient to maintain high prediction accuracy for salmon production phase, while at least 5000 sequences were needed for accurate distance predictions from the salmon farm.\n\nOur study design involved creating multiple downsampled datasets and conducting RF analyses on each, which allowed us to evaluate the robustness of our models under different conditions. This approach provided insights into the minimum data requirements for achieving reliable RF predictions in ecological studies using taxonomic metabarcodes of marine microbial communities.",
  "evaluation/confidence": "The evaluation of our method's performance involved rigorous statistical analysis to ensure the reliability and significance of our results. We employed out-of-bag (OOB) error estimates, which are nearly identical to those obtained through N-fold cross-validation. This approach provided a robust measure of our model's performance without the need for separate validation datasets.\n\nFor each dataset, we calculated full models using all available sequences and then downsampled these datasets to determine the minimum number of sequences required to maintain the targeted prediction performance. The performance metrics, including prediction accuracy and kappa statistics, were assessed for each downsampled model. Kappa values, which measure the agreement between predicted and actual classifications, were categorized as perfect (>0.8), moderate (0.6-0.8), or poor (<0.6). This categorization helped us to clearly interpret the level of agreement achieved by our models.\n\nStatistical significance was evaluated by repeating the calculations multiple times with different base trees, ensuring that the prediction capacity of the full model was secure. The use of the R package caret allowed us to infer kappa statistics for each random forest model, providing a standardized measure of agreement.\n\nIn summary, our evaluation included confidence intervals through OOB error estimates and repeated calculations to ensure statistical significance. The performance metrics were thoroughly analyzed to claim the superiority of our method over baselines, with clear categorization of agreement levels through kappa statistics.",
  "evaluation/availability": "Not enough information is available."
}