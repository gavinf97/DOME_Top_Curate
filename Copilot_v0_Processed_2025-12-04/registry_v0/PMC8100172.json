{
  "publication/title": "Regression plane concept: analysing continuous cellular processes with machine learning",
  "publication/authors": "The authors who contributed to the article are as follows:\n\nP.H. conceived and led the project, supervised the project, and wrote the manuscript.\n\nA.Sz. developed the Regression Plane tool, the trajectory tool, designed and performed the experiments, tested the software tool, wrote the manuscript, and prepared the figures included in the paper.\n\nA.B. developed the trajectory tool, tested the software tool, and wrote the manuscript.\n\nT.B. debugged and released the software, designed and performed the experiments, and tested the software tool.\n\nI.G.V., E.M., Cs.M., S.T., and I.B. designed and performed the experiments.\n\nF.P. designed and performed the experiments, tested the software tool, wrote the manuscript, prepared the figures included in the paper, and prepared the documentation and website.\n\nE.I., Y.Y., I.A., J.P., L.P., V.P., and V.H. supervised the project.\n\nL.P. and J.P. designed and performed the experiments.\n\nV.P. and V.H. wrote the manuscript.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "Nature Communications",
  "publication/year": "2021",
  "publication/doi": "10.1038/s41467-021-22866-x",
  "publication/tags": "- Machine Learning\n- Cellular Processes\n- Regression Analysis\n- Image Analysis\n- Continuous Data\n- Mitosis\n- GFP Tagging\n- Unsupervised Learning\n- Live-Cell Imaging\n- Classification",
  "dataset/provenance": "The datasets utilized in this study originate from various sources, each with distinct characteristics and sample sizes.\n\nFor the synthetic dataset, microscopy images were organized into a 24-well plate format. This dataset comprises 216 images, with approximately 40 cells per image, totaling 8,117 cells. The sample size was chosen to reflect a moderately-sized high-content screening experiment, ensuring a reasonable number of cells per well to clearly separate distributions.\n\nThe lipid droplet dataset involved cells displaced into a 384-well plate, with 9 images acquired per well for 2 identical plates. This resulted in a total of 3,956 images, encompassing 232,084 cells, with over 2,200 cells per siRNA. The sample sizes align with the accepted standards in the field, as reported in similar high-throughput siRNA studies.\n\nThe Mitocheck dataset, previously described in a 2018 study, includes images of 29 endogenously tagged proteins distributed across 92 folders. The cell count ranged from 1 to 11 per folder, totaling 498 analyzed cells. Each cell was imaged over 40 frames, resulting in 19,920 images in total. The sample number was determined by the original data from the 2018 study.\n\nThe blood cell differentiation dataset involved blood samples from 10 larvae. Image sequences were acquired on three channels: brightfield, mCherry, and EGFP, yielding a total of 4,230 images across 2 plates. The sample sizes in this experiment correspond to the accepted standards in the field.\n\nAdditionally, active learning experiments were repeated 50 times to assess the performance of active learning strategies. This repetition number matches the current standards in the field of active learning.\n\nThe datasets used in this study have been made publicly available. The Mitocheck dataset can be accessed at http://www.mitocheck.org/mitotic_cell_atlas/downloads/v1.0.1/. The Drosophila dataset is available at https://doi.org/10.6084/m9.figshare.c.5075093.v1. The synthetic dataset can be downloaded from https://data.broadinstitute.org/bbbc/image_sets.html (dataset ID: BBBC031). The training set generated in this study is available as Supplementary Data 2.",
  "dataset/splits": "In our study, we utilized several datasets, each with its own specific splits and data points.\n\nFor the synthetic dataset, we organized microscopy images into a 24-well plate format. Each well contained 9 images, with approximately 40 cells per image, resulting in a total of 216 images and 8117 cells. This dataset was designed to resemble a moderately-sized high-content screening experiment.\n\nThe lipid droplet dataset involved cells displaced into a 384-well plate. We acquired 9 images per well per channel for 2 identical plates, yielding a total of 3,956 images of 232,084 cells. This sample size aligns with the accepted standards in the field for high-throughput siRNA studies.\n\nThe MitoCheck dataset, originally described in a previous study, included images of 29 endogenously tagged proteins distributed across 92 folders. The cell count ranged from 1 to 11 per folder, totaling 498 analyzed cells. Each cell was imaged over 40 frames, resulting in 19,920 images.\n\nFor the blood cell differentiation dataset, blood samples from 10 larvae were collected. We acquired 15-frame image sequences per field (141 fields) on three channels: brightfield, mCherry, and EGFP. This resulted in a total of 4,230 images across 2 plates, with 2,115 images per plate.\n\nIn the active learning experiments, each experiment was repeated 50 times to assess the performance of the active learning strategies. This repetition number is consistent with current standards in the field.\n\nIn summary, our datasets were split and organized according to established protocols and standards in their respective fields, ensuring robustness and comparability in our analyses.",
  "dataset/redundancy": "In our study, we employed several datasets, each with specific characteristics and handling methods to ensure robustness and independence between training and test sets.\n\nFor the synthetic dataset, we organized microscopy images into a 24-well plate format, with 9 images per well and approximately 40 cells per image. This resulted in a total of 216 images and 8,117 cells. The dataset was designed to resemble a moderately-sized high-content screening experiment, with considerations for plate size and cell distribution to ensure clear separation of cell populations.\n\nThe lipid droplet dataset involved cells displaced into a 384-well plate, with 9 images per well per channel for 2 identical plates, yielding a total of 3,956 images of 232,084 cells. The sample sizes align with the accepted standards in the field, ensuring comparability with previous high-throughput siRNA studies.\n\nThe Mitocheck dataset, previously described, included images of 29 endogenously tagged proteins distributed across 92 folders, with cell numbers ranging from 1 to 11 per folder, totaling 498 analyzed cells. Each cell was imaged over 40 frames, resulting in 19,920 images. The sample number was determined by the original data.\n\nFor the blood cell differentiation dataset, blood samples from 10 larvae were collected, and 15-frame image sequences per field (141 fields) were acquired on three channels: brightfield, mCherry, and EGFP. This yielded a total of 4,230 images (2 plates, 2,115 images each). The sample sizes in this experiment correspond to the accepted standard in the field.\n\nIn the active learning experiments, each experiment was repeated 50 times to assess the performance of the active learning strategies. The sample size matches the current standard in the field, with similar repetition numbers, initial training set sizes, and active queries reported in previous studies.\n\nTo ensure the independence of training and test sets, we employed randomization and blinding where applicable. For instance, in the synthetic dataset, users were randomly assigned to different groups (regression vs. classification), and testers were blinded from any information regarding the ground truth of the experiment. In the lipid droplet dataset, siRNA treatments to the wells were allocated randomly. For the active learning experiments, the initial test sets were randomized.\n\nThe distribution of our datasets compares favorably with previously published machine learning datasets in the field. For example, the sample sizes and repetition numbers in our active learning experiments align with those reported in studies by Yang et al., Settles et al., and Schein et al. Similarly, the lipid droplet dataset's sample sizes are consistent with those reported in studies by Bartz et al. and Chapuis et al. This ensures that our findings are comparable and reproducible within the existing literature.",
  "dataset/availability": "The datasets used in this study are publicly available. The synthetic dataset can be accessed at the Broad Bioimage Benchmark Collection (BBBC) with the dataset ID BBBC031. The lipid droplet dataset is available on Figshare with the DOI 10.6084/m9.figshare.c.5067638.v1. The Mitocheck dataset, previously described in Cai et al. 2018, is accessible at http://www.mitocheck.org/mitotic_cell_atlas/downloads/v1.0.1/. The Drosophila dataset is also available on Figshare with the DOI 10.6084/m9.figshare.c.5075093.v1. The training set generated in this study is provided as Supplementary Data 2.\n\nThe data splits used in the study are clearly defined. For the synthetic dataset, the images were organized into a 24-well plate format, with 9 images per well and approximately 40 cells per image. The lipid droplet dataset was organized into a 384-well plate format, with 9 images per well per channel for 2 identical plates. The Mitocheck dataset included images of 29 endogenously tagged proteins distributed across 92 folders, with each cell imaged over 40 frames. The blood cell differentiation dataset involved blood samples from 10 larvae, with 15-frame image sequences acquired per field on 3 channels.\n\nThe availability of these datasets ensures reproducibility and transparency in our research. The datasets are released under licenses that allow for public access and use, adhering to the standards set by the respective repositories. The enforcement of data availability was managed through the submission and review processes of these repositories, ensuring that the data is accessible and properly documented.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is regression, specifically multi-parametric active regression. This approach is not entirely new, but our implementation and application to biological processes are innovative. We introduce the Regression Plane (RP), a user-friendly discovery tool that enables class-free phenotypic supervised machine learning to describe and explore biological data in a continuous manner.\n\nThe reason this work was published in a scientific journal focused on biology rather than a machine-learning journal is that the primary innovation lies in the application of these techniques to biological processes. Our method addresses a significant gap in biological research by providing a way to analyze continuous cellular processes without the need for discretization. This is particularly valuable in high-content screening scenarios where the multitude of different phenotypes makes traditional classification methods challenging.\n\nOur approach compares traditional classification with regression in a simulated experimental setup, demonstrating the advantages of regression for continuous data. We also apply our framework to identify genes involved in regulating triglyceride levels in human cells, analyze a time-lapse dataset on mitosis, and show that hemocyte differentiation in Drosophila melanogaster has continuous characteristics. These applications highlight the practical benefits of our method in biological research.",
  "optimization/meta": "The meta-predictor approach used in our work involves the Committee Members method, which is inspired by the QueryByCommittee active classification technique. This method leverages a set of models, referred to as a committee, to build a measure of disagreement among the models. In the context of regression, this approach does not rely on discrete outputs or probabilistic models directly. Instead, it uses the quadratic mean of the Euclidean distance between the committee consensus and the individual predictions from each committee member.\n\nThe Committee Members method does not solely depend on the outputs of other machine-learning algorithms as input; rather, it utilizes a collection of models trained on the same dataset to assess disagreement. The models constituting the committee can include various regression algorithms such as Random Forest, Gaussian Process, Neural Network, and Support Vector Machine. These models are trained independently on the available training samples, and their predictions are compared to identify areas of disagreement.\n\nIt is crucial to ensure that the training data for each model in the committee is independent to maintain the integrity of the disagreement measure. The Committee Members method explicitly aims to reduce the number of training samples needed to achieve a representative training set by automatically proposing cells for annotation. This active learning strategy helps in efficiently selecting the most informative samples, thereby improving the overall performance of the regression models.\n\nThe performance of the Committee Members method was evaluated using four regression models on two datasets: Lipid droplets and MitoCheck. The results demonstrated that this approach outperformed random sampling in most scenarios, highlighting its effectiveness in active regression tasks. The method's ability to identify and reduce uncertainty in predictions makes it a valuable tool for enhancing the performance of regression models in various applications.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. For the MitoCheck dataset, we began with image segmentation, which was not necessary as the public dataset already included cell masks. This allowed us to proceed directly to feature extraction.\n\nWe utilized 3D images, which were maximum projected to extract standard features in 2D. Morphological features were derived from both the nuclei and cytoplasm objects. Additionally, texture features were extracted from the nuclear staining using scales of 3 and 5. To enhance the feature set, we included the first-order derivatives of all features with respect to cell trajectories. These derivatives were calculated using a 1st order central difference on two scales, h = [1,3]. The complete list of extracted features is detailed in the supplementary material.\n\nFor the regression models, we employed Random Forest as the primary regression model for the MitoCheck dataset. This choice was driven by its robustness and ability to handle complex, high-dimensional data.\n\nIn our synthetic dataset, we utilized a regression plane approach to analyze continuous cellular processes. The ground truth processes were overlaid on a regression plane, corresponding to various perturbation designs and annotations created by microscopy experts. The identified processes were represented by grey lines, computed using a Kernel Density Estimation function and an energy minimization algorithm. This method ensured that the six non-latent continuous processes were distinctly represented, except in one case.\n\nOverall, our data encoding and preprocessing steps were designed to maximize the informational content and ensure compatibility with our machine-learning algorithms, enabling accurate and insightful analysis of biological data.",
  "optimization/parameters": "In our model, the number of parameters used varies depending on the specific mean and covariance functions employed. For mean functions, the number of parameters ranges from 0 to the product of the input dimension and the output dimension. Specifically, the nearest neighbor mean function has 0 parameters, the constant mean function has parameters equal to the output dimension, and the linear mean function has parameters equal to the product of the input dimension and the output dimension.\n\nFor covariance functions, the number of parameters also varies. The squared exponential with isotropic distance and the neural network covariance functions each have 2 parameters. The squared exponential with automatic relevance determination has parameters equal to the input dimension plus 1. It is important to note that the actual number of parameters for covariance functions is multiplied by the output dimension.\n\nThe selection of these parameters was guided by heuristic initialization methods designed to enhance the performance of the iterative optimization processes, such as gradient descent. These initial settings significantly influence the quality of the ultimate hyperparameter set, ensuring that the model can effectively capture the underlying patterns in the data.",
  "optimization/features": "In the optimization process, the number of features used as input varies depending on the specific dataset and experiment. For the synthetic dataset, standard intensity and morphological features were extracted from the detected cells. The full list of these features is provided elsewhere in the supplementary materials. For the MitoCheck dataset, features were extracted from both the nuclei and cytoplasm objects, including morphological features, texture features, and first-order derivatives with respect to cell trajectories. The Drosophila blood cell differentiation experiment involved extracting morphological features from identified cells and intensity features from all channels. The complete list of extracted features for this dataset is available online.\n\nRegarding feature selection, the provided information does not explicitly mention whether feature selection was performed. However, it is implied that the features used were standard and extracted based on the specific characteristics of the cells in each experiment. If feature selection was performed, it would have been done using the training set only, adhering to best practices in machine learning to avoid data leakage and ensure the model's generalizability.",
  "optimization/fitting": "The fitting methods employed in our study involve several regression models, each with its own set of hyperparameters and optimization techniques. To address the potential issues of over-fitting and under-fitting, we implemented various strategies tailored to each model.\n\nFor Gaussian Processes, we used a non-parametric approach that inherently provides a probabilistic output, including an estimate of uncertainty. This helps in assessing the model's performance and reliability. The hyperparameters, such as the mean and covariance functions, were optimized using iterative methods like gradient descent. To ensure robust initialization, we designed heuristic methods for setting initial hyperparameters, which significantly affect the final optimization outcome. This approach helps in mitigating over-fitting by providing a balanced model that generalizes well to unseen data.\n\nNeural Networks were trained with a single layer containing 30 nodes and a log-sigmoid activation function. The relatively small number of nodes and the use of regularization techniques helped in preventing over-fitting. Additionally, the performance was evaluated using cross-validation, ensuring that the model did not under-fit the training data.\n\nRandom Forest and Support Vector Machine were trained with default parameters from Weka, a widely-used machine learning library. These models are less prone to over-fitting due to their ensemble nature (Random Forest) and the use of kernel tricks (Support Vector Machine). Cross-validation was also employed to assess their performance and ensure they were neither over-fitting nor under-fitting the data.\n\nIn the context of active learning, we implemented several strategies to iteratively improve the training set. These strategies, such as uncertainty sampling and committee members, help in selecting the most informative samples for annotation, thereby reducing the risk of over-fitting and under-fitting. The performance of these active learning methods was evaluated using Relative Root Mean Squared Error (RRMSE) on a test set, ensuring that the models generalized well to new data.\n\nOverall, the combination of probabilistic outputs, heuristic hyperparameter initialization, regularization techniques, and active learning strategies ensured that our models were neither over-fitting nor under-fitting the data. The use of cross-validation and independent test sets further validated the robustness and generalizability of our regression models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was active learning, which aims to reduce the number of training samples needed to achieve a representative training set. This approach helps in selecting the most informative samples for annotation, thereby improving the model's performance and generalization.\n\nAdditionally, we utilized Gaussian Processes (GPs) as one of our regression models. GPs are inherently probabilistic, providing not only the predictive mean but also an estimate of the uncertainty. This probabilistic output is crucial for assessing the model's performance and reliability. The variance returned by the GP can be used to identify regions where the model is uncertain, guiding further data collection or model refinement.\n\nFor the Gaussian Processes, we optimized hyperparameters using iterative methods like gradient descent. Proper initialization of these hyperparameters is critical for achieving good performance. We designed heuristic methods for initializing hyperparameters for various mean and covariance functions, ensuring that the models were well-tuned and less prone to overfitting.\n\nFurthermore, we implemented a committee-based approach inspired by the QueryByCommittee active classification method. This method involves building a set of models (a committee) from the available training samples and defining a measure of disagreement among them. For regression tasks, we used the quadratic mean of the Euclidean distance between the committee consensus and individual predictions. This approach helps in identifying samples where the models disagree, indicating areas where the model might be uncertain or overfitting.\n\nIn summary, our study incorporated active learning, probabilistic modeling with Gaussian Processes, and committee-based methods to prevent overfitting and enhance the generalization of our regression models. These techniques collectively contribute to the robustness and reliability of our findings.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are reported in the supplementary materials. Specifically, the hyperparameter initialization for the Gaussian Processes (GP) framework is detailed in Supplementary Note 3. This note includes the mean and covariance functions used, along with their initializations. The exact configurations and settings for these functions are provided in tables within the supplementary note.\n\nThe software tools and custom algorithms developed for this research are available under specific licenses. For instance, the customized SIMCEP version is available as Supplementary Software 2. Additionally, several other software tools used in the data analysis are freely available. CellProfiler v1 can be accessed at its official website, and the CIDRE framework is available on GitHub. The nucleAIzer pipeline source code is also hosted on GitHub, and ACC's source code and standalone versions are available at www.cellclassifier.org.\n\nFor the experiments involving Matlab, version 9.5.0.1298439 (R2018b) was used, and for ImageJ, version 1.49b was utilized. The data analysis involving these tools is thoroughly documented, ensuring reproducibility.\n\nRegarding the availability of model files and optimization parameters, these are not explicitly detailed in the provided information. However, the supplementary materials and the software repositories linked above should contain the necessary information for reproducing the optimization processes and configurations used in the study.\n\nIn summary, the hyper-parameter configurations and optimization parameters are reported in the supplementary materials, and the software tools are available under specified licenses. Model files and detailed optimization parameters may be inferred from the supplementary materials and associated software repositories.",
  "model/interpretability": "The model presented in our work is not a blackbox. It is designed to be interpretable and transparent, allowing users to understand the underlying processes and decisions made by the model. One of the key innovations of our approach is the use of regression planes, which provide a continuous representation of cellular processes. This continuous approach allows for a more nuanced and interpretable analysis compared to traditional discrete classification methods.\n\nIn our experiments, we used various regression and classification models, such as Gaussian Processes, Linear Regression, Multilayer Perceptron, Random Forest, and Support Vector Machines. Each of these models has its own strengths and can be interpreted in different ways. For instance, Gaussian Processes provide a probabilistic framework that includes uncertainty estimates, making it easier to understand the confidence of the model's predictions. Random Forest models, on the other hand, can provide feature importance scores, indicating which features are most influential in the model's decisions.\n\nOur supplementary materials include detailed descriptions of the image analysis pipelines and feature extraction processes. For example, in the synthetic dataset experiments, we detected nuclei and cytoplasm using ground truth masks and extracted standard intensity and morphological features from the detected cells. These features were then used to train the regression and classification models. The full list of features is provided in the supplementary materials, allowing users to understand exactly what data the models are based on.\n\nAdditionally, our model includes unsupervised exploratory tools such as NeRV, PCA, t-SNE, and hierarchical clustering. These tools help in revealing perturbations that significantly alter the distribution of the cell population, providing further insights into the model's behavior and the underlying biological processes.\n\nIn summary, our model is designed to be transparent and interpretable, with various tools and techniques available to understand the model's decisions and the underlying data. This transparency is crucial for building trust in the model and for facilitating further scientific discovery.",
  "model/output": "The model in question is a regression model, specifically designed for multi-target regression. It aims to classify cells in a continuous manner, predicting a two-dimensional output format. This involves determining an (x, y) position for each cell on the Regression Plane. The model provides cell-by-cell predictions, associating each cell with its coordinates on the plane. Various regression models are available, including Gaussian Processes, K-Nearest Neighbours, Linear Regression, Multi-Layer Perceptron, Random Forest, Fast Decision Tree, Support Vector Machine for Regression, Neural Network, and others. These models can be integrated into the software using Object-Oriented Programming, allowing for the extension of implemented regression methods. The output of the model includes the coordinates of each cell classified in a regression class, which can be visualized by selecting a cell and enabling the \"Cell classes\" visualization. The model also supports active regression methods, which aim to reduce the number of training samples needed by automatically proposing cells for annotation. Additionally, the model can provide probabilistic outputs, offering uncertainty estimates for the predictions. This information is valuable for assessing the modelâ€™s performance and implementing active learning strategies.",
  "model/duration": "The execution time for the model was limited to a maximum of two hours for all microscopy experts participating in the experiment. This time constraint was applied uniformly to ensure a standardized evaluation period. The experts were tasked with exploring a synthetic dataset and utilizing specific tools and scripts provided for their respective approaches, either standard classification or regression methods. The time frame was designed to allow for thorough exploration within a reasonable and consistent time limit.",
  "model/availability": "The source code for the software used in this research is publicly available. The Advanced Cell Classifier (ACC) software, which includes the Regression Plane tool, can be accessed from the official website. The source code is licensed under the GNU General Public License version 3, allowing users to redistribute and modify it freely, provided they adhere to the license terms.\n\nFor those who prefer not to compile the software from source, standalone versions of ACC are also available for download from the same website. These versions are compatible with Windows, Linux, and OS X operating systems and require the MATLAB Runtime to execute. Detailed installation instructions are provided on the website to guide users through the process.\n\nAdditionally, the software incorporates several third-party tools, including \"dredviz,\" \"track.pro,\" \"Mulan,\" and \"Weka,\" each with their respective licenses. These tools are integrated into the ACC codebase to enhance its functionality.\n\nThe website also offers supplementary materials, such as video tutorials and literature references, to assist users in understanding and utilizing the software effectively. For the most up-to-date version of the source code, users can clone the repository using the provided git command. This ensures access to the latest developments and improvements made to the software.",
  "evaluation/method": "The evaluation of our methods involved several approaches to ensure robustness and reliability. For the synthetic dataset, we conducted experiments with 10 independent users, each receiving a tutorial on the tools they would use. These users were divided into two groups: one using standard classification approaches and the other using regression methods. Each group was provided with specific software and scripts to analyze the dataset, and they were given a limited time to explore and classify the data. This setup allowed us to assess the performance of different classification and regression strategies under controlled conditions.\n\nFor the lipid droplet dataset, we utilized a 384-well plate format, acquiring 9 images per well per channel across two identical plates. This resulted in a total of 3,956 images, encompassing over 232,000 cells. The sample sizes were chosen to align with accepted standards in the field, ensuring that our findings are comparable to previous high-throughput siRNA studies.\n\nIn the case of the MitoCheck dataset, we worked with a publicly available dataset that included images of 29 endogenously tagged proteins. Each cell was imaged over 40 frames, leading to a total of 19,920 images. The sample number was determined by the original data, and we ensured that our analysis methods were consistent with the dataset's structure.\n\nThe blood cell differentiation dataset involved collecting blood samples from 10 larvae and acquiring 15-frame image sequences per field across three channels. This yielded a total of 4,230 images, with sample sizes that matched the accepted standards in the field. The experiments were replicated successfully three times, providing a robust evaluation of our methods.\n\nFor the active learning experiments, we repeated each experiment 50 times to assess the performance of various active learning strategies. This repetition ensured that our results were reliable and that the strategies were thoroughly evaluated. Additionally, we used fixed pseudo-random generator seed values to ensure repeatability, and the experiment was replicated two times, yielding consistent results.\n\nIn summary, our evaluation methods included independent user experiments, adherence to field standards for sample sizes, replication of experiments, and the use of fixed seed values for repeatability. These approaches collectively ensured that our methods were rigorously evaluated and that our findings are reliable and comparable to existing research in the field.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the effectiveness of our methods. For active regression algorithms, we primarily used Relative Root Mean Squared Error (RRMSE) to measure performance. This metric provides a normalized measure of the differences between predicted and actual values, allowing for a clear comparison across different datasets and methods. Additionally, we reported the average Area Under the RRMSE Curve, where a lower value indicates better performance. This metric helps in understanding the cumulative performance of the algorithms over multiple iterations.\n\nWe also highlighted methods that showed superior performance compared to random sampling, marking them in green, with the best-performing method highlighted in pink. This visual distinction aids in quickly identifying the most effective strategies.\n\nFor the active learning experiments, we repeated each experiment 50 times to ensure robust performance assessment. This repetition aligns with the current standards in the field, as reported in various studies. The consistency of our results across these repetitions underscores the reliability of our findings.\n\nIn summary, our choice of performance metrics is representative of the standards in the field, ensuring that our evaluation is both rigorous and comparable to existing literature. The use of RRMSE and the Area Under the RRMSE Curve provides a clear and comprehensive assessment of our methods' performance.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our proposed active learning methods with several publicly available regression models. Specifically, we assessed the performance of four regression models: Random Forest, Gaussian Process, Neural Network, and Support Vector Machine. These models were chosen to provide a comprehensive benchmark, as they represent a range of approaches commonly used in the field.\n\nThe comparison was performed on two datasets: Lipid droplets and MitoCheck, which contained 457 and 585 annotated cells, respectively. For each dataset, we started by randomly isolating one-third of the available samples to form a test set, leaving the remaining two-thirds in a pool. From this pool, 10 cells were randomly selected to initialize the training set, which was then iteratively extended with 290 cells according to the active query strategy. In each iteration, a regression model was trained, and the relative root mean square error (RRMSE) was calculated on the test set.\n\nThe results from 50 independent runs demonstrated that, in all but one scenario (Gaussian Process in the MitoCheck dataset), at least one active learning technique outperformed random sampling. This indicates that our methods are effective in improving model performance. The Random Forest and Gaussian Process models achieved smaller RRMSE values than the other two methods, which limited the active strategies' ability to significantly improve performance in these cases. However, the CommitteeMembers strategy resulted in the lowest average area under the curve value in 5 out of the 8 cases, highlighting its superior performance.\n\nAdditionally, we compared our methods to simpler baselines, such as random sampling, to ensure that the improvements observed were not merely due to the complexity of the models but rather to the effectiveness of the active learning strategies. The consistent outperformance of our methods against random sampling across multiple datasets and models underscores their robustness and reliability.",
  "evaluation/confidence": "The evaluation of our methods includes a thorough assessment of performance metrics, ensuring that the results are statistically significant and that our claims of superiority over other methods and baselines are well-founded.\n\nPerformance metrics such as Relative Root Mean Squared Error (RRMSE) are presented with confidence intervals. For instance, in our active regression experiments, the performance of various methods is measured and reported as the mean RRMSE from 50 independent runs, with error bars indicating the standard deviation. This approach provides a clear indication of the variability and reliability of our results.\n\nStatistical significance is a crucial aspect of our evaluation. We have employed a range of statistical tests to ensure that our findings are robust. For example, in our active learning experiments, we repeated each experiment 50 times to assess the performance of different active learning strategies. This repetition allows us to calculate effect sizes and perform null hypothesis testing, providing P values that indicate the statistical significance of our results.\n\nMoreover, we have described all covariates tested, assumptions made, and corrections applied in our statistical analyses. This transparency ensures that our evaluation is rigorous and that our claims are supported by sound statistical evidence.\n\nIn summary, our evaluation includes confidence intervals for performance metrics and employs statistical tests to demonstrate the significance of our results. This comprehensive approach ensures that our methods are superior to others and baselines, providing a strong foundation for our claims.",
  "evaluation/availability": "The raw evaluation files for specific figures are available with this paper. These include data for Figure 1f, Supplementary Figure 1c, and Supplementary Figure 3d. Additionally, several datasets used in the study are publicly accessible. The synthetic dataset can be found at the Broad Bioimage Benchmark Collection. The lipid droplet dataset is available via Figshare. The Mitocheck dataset, previously described in a 2018 study, is accessible through the Mitocheck website. The Drosophila dataset is also hosted on Figshare. The training set generated in this study is provided as Supplementary Data 2. All these datasets are intended to support the reproducibility and transparency of the research findings."
}