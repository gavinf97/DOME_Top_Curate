{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2021",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Protein Structure Prediction\n- Machine Learning\n- Deep Learning\n- Contact Prediction\n- Bioinformatics\n- Computational Biology\n- Neural Networks\n- 2DResNet\n- Baseline Experiments\n- Literature Comparison",
  "dataset/provenance": "The dataset used in this study is derived from the CATH database, specifically utilizing the 35% sequence similarity cluster representatives as of March 16, 2018. This dataset contains a total of 31,247 non-redundant domains. These domains were further partitioned into training and validation sets, with 29,247 proteins allocated to the training set and 1,820 proteins to the validation set. To ensure no overlap between the partitions, all domains from the same homologous superfamily were kept within the same partition.\n\nThe dataset construction process involved using sequence databases such as Uniclust30 (as of October 2017), UniRef90 (as of March 2018), and Metaclust50 (as of January 2018). These databases were selected to ensure that there is no overlap between the sequence databases and the test set, maintaining the integrity of the evaluation process.\n\nThis dataset is identical to the one used by AlphaFold, ensuring a fair comparison between the methods. The use of these specific sequence databases and the partitioning strategy helps in avoiding potential biases and ensures that the results are robust and comparable to other studies in the field.",
  "dataset/splits": "The dataset used in this study consists of non-redundant domains, which were partitioned into two main sets: a training set and a validation set. The training set contains 29,247 proteins, while the validation set comprises 1,820 proteins. To ensure there was no overlap between these sets, all domains from the same homologous superfamily were kept within the same partition during the data partitioning process. This approach helps to avoid potential overlap and ensures that the training and validation sets are independent of each other.",
  "dataset/redundancy": "The datasets were split into training and validation sets, containing 29,247 and 1,820 proteins, respectively. To ensure independence between the training and test sets, all domains from the same homologous superfamily were kept in the same partition during the data partitioning process. This approach helps to avoid potential overlap between the sets.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in the field of protein structure prediction. Specifically, the benchmark dataset was constructed using 35% sequence similarity cluster representatives from CATH, which is a common practice to ensure diversity and reduce redundancy. This dataset is identical to the one used by AlphaFold, ensuring a fair comparison.\n\nTo further ensure the independence of the sequence databases used for constructing multiple sequence alignments (MSAs), the sequence databases were released before the independent test sets. This temporal separation guarantees that there is no overlap between the sequence databases and the test set, reinforcing the independence of the datasets.",
  "dataset/availability": "The data used in this study, including the data splits, has been made publicly available. All material, including benchmark datasets, evaluation scripts, and evaluation results, can be accessed at http://protein.ict.ac.cn/ProFOLD/ProFOLD.benchmark.tar.gz. This includes the training and validation sets, which were partitioned to avoid potential overlap, ensuring that all domains from the same homologous superfamily were kept in the same partition.\n\nThe web server has been updated to include the method described in the manuscript. Additionally, an automated prediction server using ProFOLD is currently under development, which will provide free access to the academic community.\n\nThe sequence databases used for constructing MSAs, such as Uniclust30, UniRef90, and Metaclust50, were released before the independent test sets, ensuring no overlap between the sequence databases and the test set. The release dates of these databases have been clearly indicated in the manuscript to provide transparency and reproducibility.\n\nThe data is available for public use, and the efforts to reduce overlap between training and test sets have been clearly documented. The release of the data and the updates to the web server ensure that the methodology and results can be verified and utilized by other researchers in the field.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages deep learning techniques, specifically convolutional neural networks (CNNs), to model sequences within multiple sequence alignments (MSAs). This approach deviates from traditional methods that rely on the Potts model or Markov Random Fields. Instead, we utilize CNNs to capture intricate patterns within the MSAs, which are then used to predict protein structures.\n\nThe machine-learning algorithm class used is that of convolutional neural networks, which are a subset of deep learning algorithms. These networks are well-established in the field of machine learning and have been extensively used for various tasks, including image recognition and, more recently, protein structure prediction.\n\nThe algorithm itself is not entirely new; however, its application in the context of protein structure prediction, particularly in modeling MSAs, is innovative. The use of CNNs for this specific purpose represents a novel approach that has shown promising results in our experiments. The decision to publish this work in a protein structure prediction journal rather than a machine-learning journal is driven by the focus on the biological application and the significant improvements observed in protein structure prediction tasks. The primary goal of this research is to advance the field of protein structure prediction, and thus, it is more appropriately presented in a journal that specializes in this area.\n\nThe algorithm's effectiveness has been demonstrated through rigorous testing on the CASP13 targets, where it outperformed existing methods in both contact prediction and 3D modeling accuracy. This success underscores the potential of using CNNs for protein structure prediction and highlights the importance of exploring new applications for established machine-learning techniques.",
  "optimization/meta": "The model described in this publication is not a traditional meta-predictor that combines predictions from multiple machine-learning algorithms as its primary input. Instead, it introduces a novel approach to protein structure prediction by leveraging deep learning techniques to model multiple sequence alignments (MSAs) directly.\n\nThe core of the method involves a multiple sequence encoder and a coevolutionary aggregator, which are integrated into the structure prediction network. These components are learned end-to-end, allowing the model to capture complex coevolutionary patterns without relying on predefined, hand-crafted features. This approach differs from conventional methods that use pairwise statistics, such as covariance matrices or precision matrices, to extract coevolutionary information.\n\nTo address concerns about the fairness of comparisons with other methods, baseline models were created by replacing the MSA encoder and coevolutionary aggregator with more traditional coevolutionary extractors. Specifically, two baseline models were implemented:\n\n1. **Covariance matrix+2DResNet**: This model uses the covariance matrix as input to train a 2D ResNet.\n2. **CCMPred+2DResNet**: This model uses the output from CCMpred (a full L*L*21*21 matrix) as input to train a 2D ResNet.\n\nThese baseline models help to isolate the contributions of the learnable \"encoder and aggregator\" framework, demonstrating that the superior performance of the proposed method comes from this innovative approach.\n\nRegarding the independence of training data, the benchmark dataset used for training and testing was constructed to be identical to that used by AlphaFold. This dataset was derived from the CATH 35% sequence similarity cluster representatives, ensuring a fair comparison and minimizing potential overlaps between training and test sets. The dataset contains 31,247 non-redundant domains, providing a robust foundation for evaluating the model's performance.",
  "optimization/encoding": "In our study, the data encoding process involved several key steps to prepare the multiple sequence alignments (MSAs) for the machine-learning algorithm. The target protein's MSA was first processed to extract mutations of specific residues, which were then embedded into vectors using a convolutional neural network (CNN). This CNN focuses on neighboring residues rather than distant pairs, capturing local contextual information. The output of the CNN for the i-th target residue is represented as a vector that includes the effects of its neighboring residues.\n\nTo handle the large size of MSAs, we implemented MSA sampling, where we randomly extracted up to 1000 homologous proteins as representatives. This approach helped in constraining memory usage and avoiding potential overfitting. Additionally, we performed distance matrix cropping, splitting the distance matrix of the target protein into 128x128 crops. Each crop contained pairwise distances between groups of 128 consecutive residues, further reducing memory requirements and improving prediction robustness.\n\nThe aggregated embedding features and outer products were calculated using these vectors. The outer product operation effectively captured the correlation between two variables, providing information on the conditional joint-residue distribution. This process was repeated for all homologous proteins to approximate the joint distribution of residue pairs.\n\nThe output channel size of the MSA encoder was set to 64, generating a total of 4224 aggregated co-evolution features for any two residues in the target protein. These features were then used by the distance estimator, which employed a 2D-ResNet with 72 residual blocks to estimate inter-residue distances. Each residual block consisted of two batch-normalization layers, two 2D dilated convolution layers, and exponential linear unit (ELU) nonlinearities.\n\nIn summary, the data encoding process involved extracting and embedding residue mutations, sampling MSAs, cropping distance matrices, and calculating aggregated features and outer products. These steps ensured that the machine-learning algorithm could handle large MSAs efficiently while capturing relevant co-evolutionary information.",
  "optimization/parameters": "In our study, we employed a 2DResNet architecture with 72 residual blocks and 96 channels, which resulted in a model with approximately 12.44 million parameters. This configuration was chosen to balance performance and model size.\n\nTo determine the optimal number of parameters, we conducted experiments with three variants of the 2DResNet:\n\n* A \"shallow\" version with 36 residual blocks and 96 channels, totaling about 6.46 million parameters.\n* A \"deeper\" version with 96 residual blocks and 96 channels, totaling about 16.43 million parameters.\n* A \"wide\" version with 36 residual blocks and 128 channels, totaling about 11.19 million parameters.\n\nThe performance of these variants was evaluated on the CASP13 FM targets and a validation set. The results indicated that the model's precision was more sensitive to the number of residual blocks than the number of channels. Specifically, the \"shallow\" model showed lower precision compared to the standard model, while increasing the number of channels in the \"shallow\" model did not significantly improve precision. Further increasing the number of residual blocks in the \"deeper\" model resulted in a slight improvement in precision but a sharp increase in the number of parameters.\n\nBased on these findings, we selected the 2DResNet with 72 residual blocks and 96 channels as it provided a good balance between performance and model size.",
  "optimization/features": "The input features for the baseline models vary depending on the specific model. For baseline-CCM and baseline-Cov, the input features consist of a covariance matrix calculated from the target multiple sequence alignment (MSA), which is a full L x L x 21 x 21 matrix, along with sequence profile features. These models do not use additional features such as predicted secondary structure, accessible surface area, or mutual information.\n\nIn contrast, baseline-CF utilizes a more comprehensive set of features. These include amino acid types, sequence profile, predicted secondary structure, mutual information, covariance matrix (a full L x L x 21 x 21 matrix), and CCMpred output (also a full L x L x 21 x 21 matrix).\n\nFeature selection was not explicitly mentioned as a process performed. The features used were determined based on the design of each baseline model, with baseline-CF incorporating a broader range of features to potentially enhance performance. The partitioning of the dataset into training and validation sets ensured that there was no overlap between partitions, which would have implications for any feature selection process if it were performed.",
  "optimization/fitting": "The fitting method employed in our study involved a careful balance between model complexity and data availability to ensure robust performance without overfitting or underfitting.\n\nThe model utilized a 2D ResNet with 72 residual blocks and 96 channels, which resulted in a total of approximately 12.44 million parameters. Given the training set consisting of 29,247 proteins, the number of parameters is indeed substantial relative to the number of training points. However, several strategies were implemented to mitigate the risk of overfitting.\n\nFirstly, data augmentation techniques were employed, such as randomly sampling 1000 homologous proteins from the largest MSAs to construct appropriately sized MSAs. This approach not only reduced the memory requirements but also introduced variability in the training data, helping the model generalize better.\n\nAdditionally, distance matrix cropping was performed to further reduce memory requirements and improve prediction robustness. This technique ensured that the model focused on relevant information while avoiding the noise that could arise from processing excessively large MSAs.\n\nTo rule out overfitting, we utilized a validation set containing 1,820 proteins, which was kept separate from the training set. This validation set was used to monitor the model's performance during training, ensuring that improvements in the training set translated to better generalization on unseen data.\n\nTo address underfitting, we experimented with different architectures, including shallower and deeper versions of the 2D ResNet. The performance of these variants was evaluated on both the validation set and the CASP13 targets. The standard ProFOLD model, with 72 residual blocks and 96 channels, was found to provide the best balance between performance and model size, indicating that it was neither too simple nor too complex for the task at hand.\n\nFurthermore, the use of cross-entropy loss and the Adam optimizer with a learning rate of 1e-3 ensured efficient training and convergence. The training process involved about 15 hours for 60,000 steps, which was sufficient to allow the model to learn the underlying patterns in the data without becoming overly specialized to the training set.\n\nIn summary, the fitting method was designed to handle the complexity of the data while preventing overfitting through data augmentation, validation monitoring, and architectural experimentation. The chosen model configuration demonstrated robust performance, indicating that both overfitting and underfitting were effectively managed.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One key method involved the use of data augmentation. Specifically, for very large multiple sequence alignments (MSAs), we randomly extracted a subset of homologous proteins to construct an MSA of appropriate size. This approach helped in managing the memory constraints of the GPU while also introducing variability in the training data, which can aid in preventing overfitting.\n\nAdditionally, we implemented distance matrix cropping. This technique reduces the memory requirement and improves the prediction robustness by focusing on the most relevant parts of the data. By cropping the distance matrices, we ensure that the model generalizes better to unseen data, thereby mitigating the risk of overfitting.\n\nFurthermore, we utilized a comprehensive benchmark dataset that was constructed using 35% sequence similarity cluster representatives from the CATH database. This dataset was partitioned into training and validation sets, ensuring that there was no overlap between partitions. This careful partitioning helps in evaluating the model's performance on independent data, providing a more reliable assessment of its generalization capabilities.\n\nThe use of a 2D ResNet with 72 residual blocks and 96 channels was also crucial in our optimization process. This architecture was chosen after testing various configurations, including shallower and deeper networks, as well as networks with different channel sizes. The selected configuration balanced performance and model size, ensuring that the model was neither too complex (which could lead to overfitting) nor too simple (which could underfit the data).\n\nOverall, these techniques collectively contributed to the prevention of overfitting, ensuring that our model performed well on both training and validation datasets.",
  "optimization/config": "In our study, we have made a concerted effort to ensure that all relevant details regarding our methods and configurations are readily accessible to the scientific community. The hyperparameter configurations and optimization schedules for our models, including ProFOLD and the baseline models, are thoroughly described in the supplementary materials. Specifically, the implementation details of the baseline models, including features, datasets, and neural network architecture, are provided in a new section added to the supplementary text.\n\nFor ProFOLD, the hyperparameters were carefully selected to balance prediction performance and model size. We tested various configurations of the 2DResNet, including different numbers of residual blocks and channels, and reported the results in the supplementary tables. The standard configuration used in our study is a 2DResNet with 72 residual blocks and 96 channels, which was found to offer an optimal trade-off between performance and computational efficiency.\n\nAll material related to our study, including benchmark datasets, evaluation scripts, and evaluation results, has been uploaded and is available for download. This includes the code and data that were initially uploaded to the Nature Communications manuscript center and have now been made publicly accessible on our updated web server. The web server, located at http://protein.ict.ac.cn/ProFOLD/, includes the method described in this manuscript and provides all necessary resources for replication and further research.\n\nAdditionally, we are in the process of developing an automated prediction server using ProFOLD, which will be freely accessible to the academic community. This server will facilitate the use of our methods for protein structure prediction and related applications.\n\nRegarding the availability of model files and optimization parameters, these are included in the supplementary materials and can be accessed through the provided links. The data and code are released under a license that allows for academic use, ensuring that researchers can utilize these resources for their own studies while adhering to ethical and legal standards.",
  "model/interpretability": "The model ProFOLD is designed with an \"end-to-end\" framework, which allows it to learn inter-residue distances directly from multiple sequence alignments (MSA). This approach avoids the use of hand-crafted features, which are often less interpretable and can introduce biases. By learning directly from the data, ProFOLD can provide more transparent insights into the relationships between residues.\n\nOne of the key advantages of ProFOLD is its ability to leverage deep learning techniques to extract meaningful patterns from the data. The model uses a 2DResNet architecture with 72 residual blocks and 96 channels, which has been shown to balance performance and model size effectively. This architecture allows for a more interpretable understanding of how different features contribute to the final predictions.\n\nFor example, the model's performance is more sensitive to the number of residual blocks than the number of channels. This indicates that the depth of the network plays a crucial role in capturing complex patterns in the data. Additionally, the model's precision is roughly fixed when increasing the number of residual blocks beyond a certain point, suggesting that there is an optimal depth for the network.\n\nFurthermore, the comparison of ProFOLD with baseline models demonstrates its advantages in learning from raw data. The baseline models, which rely on hand-crafted features, show lower accuracy compared to ProFOLD. This highlights the importance of using data-driven approaches for feature learning, which can provide more transparent and interpretable results.\n\nIn summary, ProFOLD is a transparent model that leverages deep learning techniques to learn directly from the data. Its \"end-to-end\" framework and 2DResNet architecture allow for a more interpretable understanding of the relationships between residues, making it a powerful tool for protein structure prediction.",
  "model/output": "The model described in this publication is primarily a regression model. It focuses on predicting inter-residue distances and contacts within protein structures, which are continuous values rather than discrete classes. The output of the model is used to generate 3D protein structures, a task that involves estimating the spatial relationships between amino acid residues. This process is inherently a regression problem, as it deals with predicting continuous variables.\n\nThe model employs a convolutional neural network (CNN) to process input features derived from multiple sequence alignments (MSAs). These features include sequence profiles, covariance matrices, and outputs from tools like CCMpred. The CNN architecture, specifically a 2D ResNet, is designed to capture complex patterns in the input data and produce accurate predictions of inter-residue distances.\n\nThe performance of the model is evaluated using metrics such as contact precision and 3D modeling accuracy. Contact precision measures how well the model predicts contacts between residues, while 3D modeling accuracy assesses the overall quality of the predicted protein structures. The model has been tested on various datasets, including CASP13 targets, and has shown superior performance compared to existing methods.\n\nIn response to reviewer feedback, additional baseline models were implemented to ensure fair comparisons. These include models that use CCMpred outputs and covariance matrices as inputs to the 2D ResNet. The results demonstrate that the proposed model's \"encoder and aggregator\" framework significantly contributes to the accurate estimation of inter-residue distances.\n\nOverall, the model's output is a set of predicted inter-residue distances, which are then used to construct 3D protein structures. The regression nature of the model allows it to provide continuous predictions, which are crucial for accurate protein structure prediction.",
  "model/duration": "The execution time for training the model was approximately 15 hours, spanning 60,000 steps. This duration was achieved using a mini-batch of 3 crops on each of 4 GPU workers. The specific architecture and hyperparameters, such as the use of an Adam optimizer with a learning rate of 1e-3 and ELU nonlinearities, contributed to this training time. The model's design, including the use of 1D and 2D ResNet structures with specified channels and blocks, also played a role in determining the overall training duration.",
  "model/availability": "The source code and data for the method described in this manuscript have been made available. Initially, the code and data were uploaded to the Nature Communications manuscript center during the first submission to avoid potential conflicts with their policy. Subsequently, the web server has been updated to include the method described in this manuscript. The web server can be accessed at http://protein.ict.ac.cn/ProFOLD/.\n\nAll relevant materials, including benchmark datasets, evaluation scripts, and evaluation results, have been uploaded to http://protein.ict.ac.cn/ProFOLD/ProFOLD.benchmark.tar.gz for convenience. This ensures that the community has access to the necessary resources to replicate and build upon the work presented in this manuscript.\n\nAdditionally, an automated prediction server using ProFOLD is currently under development. This server will provide free access to the academic community, facilitating broader use and further research.",
  "evaluation/method": "Our method was evaluated through a series of experiments designed to demonstrate its superiority over existing approaches in protein structure prediction. We conducted extensive tests using the CASP13 targets, which are widely recognized benchmarks in the field. Our evaluations focused on both contact prediction and 3D modeling accuracy.\n\nTo ensure a fair comparison, we implemented two baseline models. The first baseline, \"Covariance matrix+2DResNet,\" involved replacing the MSA encoder and coevolutionary aggregator with a covariance matrix and training a 2D ResNet from this matrix. The second baseline, \"CCMPred+2DResNet,\" replaced the MSA encoder and coevolutionary aggregator with CCMpred, training the 2D ResNet from the CCMpred output.\n\nWe also addressed concerns about the effectiveness of our baseline experiments by performing an elaborate literature search and summarizing existing methods. This allowed us to provide a more comprehensive comparison and better judge the advantages of our proposed method.\n\nAdditionally, we conducted experiments to understand the improvement brought by the new component of CopulaNet, specifically the 1D convolutional network for MSA processing. We ensured that the baseline methods did not ignore sequence profile, mutual information, and the full CCMpred co-evolution matrix, allowing us to assess how much extra information the 1D conv network learns from MSAs over manually-engineered features.\n\nOur evaluations included comparing our method with other deep learning-based approaches such as RaptorX, AlphaFold, and trRosetta. We acknowledged the potential ambiguities caused by differences in training setups and addressed them by training variants of our network with traditional coevolutionary extractors.\n\nOverall, our method demonstrated superior performance in both contact prediction and 3D modeling accuracy, validating its effectiveness and novelty in protein structure prediction.",
  "evaluation/measure": "In the evaluation of our proposed method, we report several key performance metrics to assess the accuracy and effectiveness of our approach. The primary metric we focus on is the precision of long-range contact prediction, which is crucial for understanding protein structures. We evaluate this precision at different thresholds: the most probable L, L/2, and L/5 contacts. These metrics provide a comprehensive view of how well our method performs in predicting contacts at various levels of confidence.\n\nTo ensure that our set of metrics is representative and comparable to existing literature, we have conducted extensive comparisons with baseline methods and state-of-the-art techniques. For instance, we compare our method against \"CCMpred+2DResNet\" and \"Covariance matrix+2DResNet,\" showing that our approach achieves higher precision in long-range contact prediction. We also evaluate against other methods like A7D, RaptorX, and trRosetta, demonstrating that our method either matches or surpasses their performance on the same datasets.\n\nAdditionally, we address concerns raised about the effectiveness of our baseline experiments by performing elaborate literature searches and summarizing existing methods. This ensures that our comparisons are fair and that our reported metrics are in line with what is expected in the field. We also provide detailed evaluations using identical multiple sequence alignments (MSAs) to ensure consistency and reliability in our results.\n\nIn summary, the performance metrics reported in our evaluation are precision for long-range contact prediction at different thresholds. These metrics are representative and comparable to those reported in the literature, providing a clear and comprehensive assessment of our method's advantages over existing techniques.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, the performance of the proposed method, ProFOLD, was compared with other deep learning-based approaches such as RaptorX, AlphaFold, and trRosetta. These methods were chosen because they are well-established in the field of protein structure prediction. The benchmark dataset used for this comparison was constructed using the CATH 35% sequence similarity cluster representatives, containing a total of 31,247 non-redundant domains. This dataset is identical to the one used by AlphaFold, ensuring a fair comparison.\n\nIn addition to comparing with these advanced methods, simpler baselines were also implemented and evaluated. Two baseline models were created by replacing the MSA encoder and coevolutionary aggregator in the proposed method with more traditional coevolutionary extractors. The first baseline model, \"Covariance matrix+2DResNet,\" used the covariance matrix as input for training a 2D ResNet. The second baseline model, \"CCMPred+2DResNet,\" used the output of CCMPred, a full L*L*21*21 matrix, for the same purpose. The performance of these baseline models was compared with ProFOLD on the 31 CASP13 FM targets and a validation set containing 1820 domains. ProFOLD demonstrated superior precision for long-range contact prediction compared to both baseline models.\n\nFurthermore, an additional baseline model, denoted as \"baseline-CF,\" was implemented to better understand the contribution of the new component in CopulaNet, the 1D convolutional network for MSA processing. This baseline model used all manually-engineered features derived from MSAs, including 1D features like one-hot amino acid type, sequence profile, and secondary structure, as well as 2D features like mutual information, covariance matrix, and the full CCMpred co-evolution matrix. The performance of this baseline model was also included in the comparison to provide a more comprehensive evaluation.",
  "evaluation/confidence": "The evaluation of our method includes a thorough assessment of its performance metrics, ensuring that the results are statistically significant and reliable. We have conducted extensive experiments to compare our approach with existing methods and baselines, focusing on both contact prediction and 3D modeling accuracy.\n\nTo address the statistical significance of our results, we have performed multiple tests and comparisons. For instance, we evaluated our method against baseline models such as \"CCMPred+2DResNet\" and \"Covariance matrix+2DResNet,\" and we found that our method outperforms these baselines significantly. The precision for long-range contact prediction was higher for our method compared to these baselines, demonstrating its superiority.\n\nWe also compared our method with other state-of-the-art approaches like RaptorX and trRosetta. The results showed that our method achieved better performance in both contact prediction and 3D modeling accuracy. For example, on the 31 CASP13 FM targets, our method achieved higher precision for long-range contact prediction than the baseline model baseline-CF. These results were consistent across different datasets, including the validation set, indicating the robustness of our approach.\n\nIn addition to these comparisons, we have included confidence intervals for our performance metrics to provide a more comprehensive understanding of the results. This ensures that the observed differences in performance are not due to random variation but are statistically significant. We have also addressed concerns about the fairness of comparisons by using identical multiple sequence alignments (MSAs) for evaluating different methods, which helps in making a more accurate assessment of their performance.\n\nOverall, the evaluation of our method is rigorous and thorough, with a focus on statistical significance and reliability. The results demonstrate that our approach is superior to existing methods and baselines, providing a strong foundation for its application in protein structure prediction.",
  "evaluation/availability": "All material necessary for evaluation, including benchmark datasets, evaluation scripts, and evaluation results, has been made publicly available. This can be accessed at http://protein.ict.ac.cn/ProFOLD/ProFOLD.benchmark.tar.gz. This includes the raw evaluation files needed to reproduce the results presented in the manuscript.\n\nThe web server has also been updated to include the method described in the manuscript, and an automated prediction server using ProFOLD is currently under development. This server will provide free access to the academic community, ensuring that the evaluation process is transparent and reproducible.\n\nThe code and data were initially uploaded to the Nature Communication manuscript center during the first submission to comply with their policies. However, they are now available on the updated web server, ensuring that all necessary resources are accessible to the public."
}