{
  "publication/title": "Predicting anticancer hyperfoods with graph convolutional networks",
  "publication/authors": "The authors who contributed to the article are:\n\n- Guadalupe Gonzalez\n- Shunwang Gong\n- Ivan Laponogov\n- Michael Bronstein\n- Kirill Veselkov\n\nNot sure about the specific contributions of each author to the paper.",
  "publication/journal": "Human Genomics",
  "publication/year": "2021",
  "publication/doi": "https://doi.org/10.1186/s40246-021-00333-4",
  "publication/tags": "- Systems biology\n- Graph deep learning\n- Genomics\n- Hyperfoods\n- Cancer research\n- Drug discovery\n- Machine learning\n- Bioinformatics\n- Network propagation\n- Computational biology\n- Personalized nutrition\n- Anticancer therapeutics\n- Graph convolutional networks\n- Drug-protein interactions\n- Human interactome",
  "dataset/provenance": "The dataset used in our study is a human protein-encoding gene-gene network, which was compiled from several sources. We utilized data from STRING v10.5, UniProt (January 2019), COSMIC (January 2019), and NCBI Gene (January 2019). Specifically, we selected the subset of experimentally validated protein-protein interactions from STRING and removed isolated nodes, retaining the largest connected component. This resulted in a network consisting of 15,135 nodes and 177,848 edges.\n\nInformation on clinically approved drugs was extracted from DrugBank v5 and DrugCentral (February 2019). Additionally, food molecules were obtained from FooDB (November 2018), which included 7,793 entries. Drug- and food molecule-gene encoded protein interactions were sourced from STITCH (January 2019).\n\nPathway data was extracted from the Kyoto Encyclopedia of Genes and Genomes (KEGG), specifically version 7.1 from MSigDB. This data was used to create a pathway assignation matrix, where each gene in the PPI network was mapped to its corresponding pathways. Out of the 15,135 genes, 4,590 had at least one pathway assigned.\n\nEach drug or food molecule is represented by a graph of protein-protein interactions, with 15,135 nodes and 177,848 edges. The feature vector for each molecule includes binary features indicating whether a gene is a target of the drug or food molecule. Our dataset contains 2,048 drugs and 7,793 food molecules. The classification labels for the cancer task were obtained following the procedure outlined in a previous study, resulting in 209 positive and 1,839 negative drug labels.",
  "dataset/splits": "In our study, we employed a 5-fold cross-validation strategy to assess the performance of our models. This approach involves dividing the dataset into five distinct splits. In each split, 20% of the data is allocated to the test set, while the remaining 80% is used for training. From this 80%, an additional 10% is reserved as a validation set to facilitate early stopping during the training process. This ensures that the model's performance is evaluated on unseen data, providing a robust assessment of its generalization capabilities. The splits are generated by stratifying samples with respect to their labels, which helps maintain the class distribution across different splits. This is particularly important given the highly imbalanced nature of our dataset, where only 10.2% of the drugs are classified as anticancer.",
  "dataset/redundancy": "The dataset used in this study was compiled from various sources, including STRING, UniProt, COSMIC, NCBI Gene, DrugBank, DrugCentral, FooDB, and STITCH. The protein-protein interaction (PPI) network consisted of 15,135 nodes and 177,848 edges, with isolated nodes removed to keep the largest connected component. The dataset included 2,048 drugs and 7,793 food molecules, with classification labels indicating whether a drug was approved for cancer treatment.\n\nTo assess model performance, 5-fold cross-validation was employed. In each split, 20% of the data was reserved for testing, while the remaining 80% was used for training and validation. From this 80%, 10% was further set aside for validation to perform early stopping. All splits were generated by stratifying samples with respect to their labels to ensure that each fold had a similar distribution of positive and negative samples. This stratification was crucial given the highly imbalanced nature of the dataset, where only 10.2% of the drugs were labeled as anticancer.\n\nThe training and test sets were independent due to the cross-validation process, which ensures that each sample is used only once for testing and not included in the training set during the same fold. This independence was enforced by the stratified splitting method, which maintains the class distribution across all folds.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the context of drug discovery and anticancer research. The use of a large, well-curated PPI network and the inclusion of both drugs and food molecules provide a comprehensive basis for predicting anticancer properties. The imbalanced nature of the dataset, with a minority of anticancer drugs, is a common challenge in this field and was addressed through stratified splitting and class re-scaling during training. This approach ensures that the model is robust and generalizable, even in the presence of class imbalances.",
  "dataset/availability": "The data used in our study can be collected from several publicly available sources. Genome data can be obtained from STRING, UniProt, COSMIC, and NCBI Gene. These databases provide comprehensive information on protein-protein interactions, protein sequences, somatic mutations in cancer, and gene-centered information, respectively. The pathway assignation matrix data can be downloaded from the Gene Set Enrichment Analysis (GSEA) website.\n\nDrug data can be extracted from DrugBank, DrugCentral, and STITCH. These resources offer detailed information on drugs, their targets, and interactions. Additionally, food data can be extracted from FooDB and STITCH.\n\nTo ensure reproducibility, the code and data required to reproduce our results are available on GitHub. This includes the specific data splits used in our experiments, allowing other researchers to validate our findings and build upon our work. The availability of these resources ensures transparency and facilitates further research in the field.\n\nThe data from these sources is freely accessible, and the use of the code and data from GitHub is governed by the licenses associated with each respective platform. This ensures that users can access and utilize the data and code in accordance with the terms set by the providers.",
  "optimization/algorithm": "The machine-learning algorithm class used in our work is graph neural networks, specifically graph convolutional networks. These networks operate directly on graph-structured data, making them well-suited for our task of predicting anticancer therapeutics based on drug-protein interactions represented as graphs.\n\nThe algorithm is not entirely new; it builds upon existing graph neural network architectures. However, the application of these networks to the specific problem of predicting anticancer therapeutics from food-derived molecules is novel. Our approach integrates the graph convolutional network with a multi-layer perceptron (MLP) for classification, forming an end-to-end model that learns drug representations optimized for the prediction task.\n\nThe reason this work was published in a genomics journal rather than a machine-learning journal is that the primary focus and contribution of our study are in the domain of nutritional science and cancer research. We demonstrate the practical application of graph neural networks to a specific biological problem, showcasing their potential in the field of personalized nutrition and cancer therapeutics. The technical innovations in the machine-learning algorithm are secondary to the biological insights and practical applications presented in the paper.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it operates directly on graphs representing drugs and their associated node features. The approach involves an end-to-end graph neural network model that learns drug representations to optimize performance in predicting anticancer therapeutics.\n\nThe model is not a meta-predictor in the traditional sense, as it does not combine the outputs of multiple machine-learning methods. However, the evaluation process does compare the performance of the proposed model against a baseline model. The baseline model uses a different approach, representing drug-protein interactions as binary signals on the human PPI network and applying Random Walk with Restart (RWR) to learn the systemic genome-wide response to drug intervention. The learned representations from this baseline method are then used as input to a Support Vector Machine (SVM) for binary classification of anticancer/non-anticancer drugs.\n\nThe training data for the proposed model and the baseline model are independent. The proposed model is trained on graphs representing drugs and their targets, while the baseline model uses drug-protein interactions on the human PPI network. The comparison between the two models is done to evaluate the effectiveness of the proposed graph neural network approach against the baseline method.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the dataset for the machine-learning algorithm. We began by compiling a human protein-encoding gene-gene network using data from several sources, including STRING, UniProt, COSMIC, and NCBI Gene. From this network, we selected a subset of experimentally validated protein-protein interactions, removing isolated nodes and retaining the largest connected component, which consisted of 15,135 nodes and 177,848 edges.\n\nInformation on clinically approved drugs was extracted from DrugBank and DrugCentral, while food molecules were obtained from FooDB. Drug- and food molecule-gene encoded protein interactions were extracted from STITCH. Pathways were extracted from the Kyoto Encyclopedia of Genes and Genomes (KEGG) and used to create a pathway assignation matrix. This matrix indicated whether each gene was involved in a specific pathway.\n\nEach drug and food molecule was represented as a graph of protein-protein interactions, with a feature vector indicating whether each gene was a target of the drug or food molecule. This binary feature vector was used to identify drug molecules based on their protein-coding gene targets, focusing on their systemic-wide effects on the protein-protein interaction network.\n\nThe dataset contained 2,048 drugs and 7,793 food molecules. Classification labels for the cancer task were obtained following a procedure from a previous study, resulting in 209 positive and 1,839 negative drug labels. This preprocessing ensured that the data was in a suitable format for the graph neural network model, which could then learn representations of drugs and predict their anticancer properties.",
  "optimization/parameters": "In our study, the number of parameters in the model varies depending on the specific architecture and hyperparameters chosen. The convolutional layers in our model have 8 hidden units each. The first prediction layer has 32 hidden units, while the final prediction layer has 2 output units. The number of convolutional layers can be 1, 2, or 3, and the number of dropout layers can be 1 or 2. These layers contribute to the overall complexity and capacity of the model.\n\nThe selection of these parameters was determined through a systematic hyperparameter search. We explored different configurations for the learning rate, L2 regularization, number of convolutional layers, number of dropout layers, batch normalization, feature normalization, and the n-hops for the ChebNet layer. The specific values for these hyperparameters were chosen based on a grid search over candidate values, aiming to optimize the model's performance on a validation set. This process ensures that the selected parameters are well-suited for the task of predicting anticancer drugs.",
  "optimization/features": "The input features for the model are represented as a graph, where each drug is associated with node features. The specific number of features (f) used as input is not explicitly stated, but it is clear that the features are related to genes, as the model aims to recover gene targets present in cancer-related pathways.\n\nFeature selection was implicitly performed through the use of attribution scores and pathway pooling. Attribution scores were used to assess the contribution of each input feature to the model's prediction, focusing on genes relevant to the anticancer properties of drugs. Pathway pooling was considered as an alternative architecture to aggregate node features according to biological pathways, which could be seen as a form of feature selection based on biological knowledge.\n\nThe feature selection process, if performed, was likely done using the training set only, as this is a common practice to prevent data leakage and ensure that the model generalizes well to unseen data. However, the exact details of the feature selection process are not provided.",
  "optimization/fitting": "The model employed in this study is a graph neural network designed to predict anticancer therapeutics. The number of parameters in the model is indeed larger than the number of training points, which is a common scenario in deep learning, especially when dealing with complex data structures like graphs.\n\nTo address the risk of overfitting, several strategies were implemented. First, the model was trained using cross-entropy loss with class re-scaling to handle the class imbalance in the dataset. This ensures that the model does not become biased towards the majority class. Second, early stopping was used during training, which halts the training process if the validation loss does not decrease for a specified number of epochs (20 epochs in this case). This helps in preventing the model from memorizing the training data. Additionally, dropout layers were included in the model architecture to randomly set a fraction of input units to zero during training, which helps in regularizing the model and reducing overfitting. L2 regularization was also applied to the weights of the neural network to penalize large weights and encourage simpler models.\n\nTo rule out underfitting, the model's performance was evaluated using multiple metrics, including balanced accuracy, F1 score, and AUPR. The use of these metrics provides a comprehensive evaluation of the model's performance, especially in the context of imbalanced datasets. Furthermore, hyperparameter tuning was performed using a validation set and a grid search over candidate hyperparameter values. This ensures that the model is not too simple to capture the underlying patterns in the data. The model's performance was also compared against a baseline model, demonstrating its superiority in predicting anticancer therapeutics.\n\nThe training process was conducted using powerful GPUs, specifically NVIDIA Tesla V100 and GeForce RTX 2080, which allowed for efficient computation and the ability to train complex models. The model's architecture includes convolutional layers, dropout layers, and fully connected layers, all of which contribute to its capacity to learn from the data without underfitting. The use of graph convolutional layers allows the model to leverage the structural information in the data, further enhancing its ability to generalize to unseen data.",
  "optimization/regularization": "In our study, several regularization techniques were employed to prevent overfitting and improve the generalization of our models. One key technique used was dropout, which involves randomly setting a fraction of the input units to zero at each update during training time. This helps to prevent units from co-adapting too much, thereby improving the model's ability to generalize to unseen data. We performed a hyperparameter search to determine the optimal number of dropout layers, considering configurations with one or two dropout layers.\n\nAdditionally, L2 regularization was applied to the weights of the neural network. This technique adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights, encouraging the model to keep the weights small and reducing the risk of overfitting. The strength of this regularization was also tuned as part of our hyperparameter search, with candidate values including 1e-5, 1e-4, and 5e-4.\n\nBatch normalization was another regularization method used after the convolutional layers. This technique normalizes the inputs of each layer to have a mean of zero and a variance of one, which helps to stabilize and accelerate the training process. We explored both the inclusion and exclusion of batch normalization as part of our hyperparameter search.\n\nFurthermore, feature normalization was applied to the input data to ensure that the features had a consistent scale, which can help improve the convergence and performance of the neural network. This normalization was also considered as a hyperparameter during our search.\n\nThese regularization techniques, combined with early stopping based on validation loss, helped to mitigate overfitting and ensure that our models generalized well to the test data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The hyper-parameters were determined using a validation set with a grid search over candidate values. For the baseline approach, the grid search for the restart probability included values such as 0.001, 0.01, 0.1, 0.2, and so on up to 0.9. For neural models, specific hyper-parameter candidates can be found in a table within the publication, including learning rates, L2-regularization values, the number of convolutional layers, dropout layers, batch normalization, feature normalization, and n-hops for ChebNet.\n\nThe model training involved optimizing hyper-parameters using cross-entropy loss and was conducted in an end-to-end fashion with back-propagation. The training was performed for a maximum of 100 epochs with the Adam optimizer and early stopping with a window size of 20. Training stopped if the validation loss did not decrease by at least 1e-4 for 20 consecutive epochs.\n\nThe implementation of the model was done using PyTorch and the TorchGeometric Library. The final prediction layer had 2 output units, and a mini-batch size of 16 was used. The models were trained on NVIDIA Tesla V100 and GeForce RTX 2080 GPUs.\n\nRegarding the availability of model files and optimization parameters, the publication does not explicitly mention where these can be accessed. However, the article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This license allows for the sharing of the methodology and results, but specific model files and optimization parameters may need to be requested directly from the authors or obtained through additional means not detailed in the publication.",
  "model/interpretability": "The model employed in this study is not a black box; it incorporates mechanisms to enhance interpretability. To assess whether the trained model has learned the top biological pathways responsible for the anticancer properties of drugs, an attribution method is used. This method assigns scores to each input feature, reflecting the contribution of that feature to the model's prediction. By inspecting these attribution scores, it is possible to identify which genes were most relevant to the model’s decision.\n\nThe Integrated Gradients method is utilized to compute attributions to individual genes. This method satisfies fundamental axioms for attribution methods: sensitivity and implementation invariance. It provides attributions relative to a baseline input, where all drug targets are set to zero. The integrated gradients are defined as the path integral of the gradients along the straight-line path from the baseline to the input.\n\nThe attribution recall score is computed for the best-performing model to evaluate how well the model predicts drugs as anticancer based on the feature values in cancer-related genes. The attribution recall score for the most positively attributed genes is 85.29%, indicating that these genes are preferentially found in cancer-related pathways. This means the model classifies drugs as anticancer based on the value of the input features in cancer-related genes, adding to the biological plausibility of the model.\n\nTo further validate the model’s attributions, six use cases are investigated: the top three correctly and incorrectly classified drugs as anticancer with experimentally validated pathways in the literature. For each drug, the drug representation computed by the best model is obtained, and over-represented pathways are identified using the PreRanked module in Gene Set Enrichment Analysis (GSEA). The over-represented pathways obtained from the drug representations are compared to the knowledge available in the literature on these drugs. For all six drugs studied, the over-represented pathways successfully recovered pathways described in the literature along with cancer-related pathways. This indicates that the representations learned capture the mechanisms of action of drugs.",
  "model/output": "The model is designed for a classification task. Specifically, it predicts whether a given drug or food molecule has anticancer properties. The output of the model is a probability distribution indicating the likelihood of a drug or molecule belonging to the anticancer or non-anticancer category. This distribution is generated using a multi-layer perceptron (MLP) that processes the transformed representation of the drugs. The final output is a probability score for each class, which is then used to make a binary classification decision. The model's performance is evaluated using metrics such as balanced accuracy, F1 score, and area under the precision-recall curve (AUPR), which are standard for classification tasks.",
  "model/duration": "The execution time of the models was measured in milliseconds per sample per epoch. This metric allows for an estimation of the total training time required for different datasets. The time complexity of the graph neural layers and models used is detailed in a table, with the three proposed variants of graph convolutional layers showing comparable time complexity. The ChebNet layer's complexity additionally depends on the n-hop used for neighborhood aggregation. The models were trained on NVIDIA Tesla V100 and GeForce RTX 2080 GPUs. The training process involved optimizing hyperparameters using cross-entropy loss and the Adam optimizer, with early stopping implemented to prevent overfitting. The models were trained for a maximum of 100 epochs, with training stopping if the validation loss did not decrease by at least 1e-4 for 20 consecutive epochs. The specific running times for different layers and models are provided in a table, giving a clear picture of the computational efficiency of each component.",
  "model/availability": "The source code for the models discussed in this publication is not explicitly mentioned as being released. However, the models were implemented using PyTorch and the TorchGeometric Library, which are open-source frameworks. This means that while the specific code used for this research may not be publicly available, the tools and libraries used to develop the models are accessible to the public.\n\nThe publication does not provide details about an executable, web server, virtual machine, or container instance that would allow users to run the algorithm directly. Therefore, it is not possible to provide information on how to access or use such a method.\n\nThe publication does not mention any specific license under which the source code or methods might be released. Therefore, it is not possible to provide information on the licensing terms.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach to assess the performance of the models in predicting anticancer drugs. The primary evaluation metric was the comparison against a baseline model introduced in a previous study. This baseline model represented drug-protein interactions as binary signals on the human protein-protein interaction (PPI) network and applied random walk with restart (RWR) to learn the systemic genome-wide response to drug intervention. The learned representations were then used as input to a support vector machine (SVM) for the binary classification task of anticancer versus non-anticancer drugs.\n\nTo evaluate the performance, various metrics were used, including balanced accuracy, F1 score, and area under the precision-recall curve (AUPR). Balanced accuracy is the average of recall obtained on each class, F1 is the weighted average of precision and recall for the positive (anticancer) class, and AUPR represents the average precision across all recall values.\n\nThe models were evaluated using 5-fold cross-validation, where in each split, 20% of the data was kept as the test set, and from the remaining 80%, 10% was used as a validation set to perform early stopping. All splits were generated by stratifying samples with respect to labels. Given the highly unbalanced dataset (only 10.2% of drugs are anticancer), the contribution of each class to the loss function was re-scaled to be inversely proportional to class frequencies during training.\n\nAdditionally, to motivate the use of network propagation, versions of the baseline and proposed methods without network propagation were also evaluated. For these versions, an SVM classifier was used as the counterpart to the baseline method, and a multi-layer perceptron (MLP) was used as the counterpart to the proposed neural models.\n\nHyperparameter settings for every method were determined using a validation set with a grid search over candidate hyperparameter values. For the baseline approach, the grid search for the restart probability ranged from 0.001 to 0.9. For neural models, hyperparameter candidates included learning rate, L2-regularization, number of convolutional layers, number of dropout layers, batch normalization, feature normalization, and n-hops for ChebNet.\n\nThe models were trained on NVIDIA Tesla V100 and GeForce RTX 2080 GPUs. The training process involved optimizing hyperparameters using cross-entropy loss and the Adam optimizer, with early stopping based on validation loss. The models were implemented using PyTorch and the Torch Geometric Library.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the models' capabilities in predicting anticancer drugs. These metrics include balanced accuracy, F1 score, and the area under the precision-recall curve (AUPR). Balanced accuracy is particularly useful in our context due to the highly imbalanced nature of the dataset, where only a small percentage of drugs are classified as anticancer. It provides an average of recall obtained on each class, ensuring that both classes are equally considered. The F1 score, which is the weighted average of precision and recall for the positive (anticancer) class, is crucial for evaluating the performance on the minority class. AUPR represents the average precision across all recall values and is especially informative for imbalanced datasets, as it focuses on the performance of the positive class.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, particularly in scenarios involving imbalanced datasets. By reporting these metrics, we aim to provide a clear and representative evaluation of our models' performance, allowing for comparisons with other studies in the field. Additionally, we compare our models against a baseline approach introduced in previous work, further validating the robustness and effectiveness of our methods.",
  "evaluation/comparison": "In our evaluation, we compared the performance of our proposed models against a baseline approach introduced in a previous study. This baseline method represented drug-protein interactions as binary signals on the human protein-protein interaction (PPI) network and applied random walk with restart (RWR) to learn the systemic genome-wide response to drug intervention. The learned representations were then used as input to a support vector machine (SVM) for the binary classification task of anticancer versus non-anticancer drugs.\n\nTo further motivate the use of network propagation, we also evaluated versions of both the baseline and our proposed methods without incorporating network propagation. For these evaluations, we used an SVM classifier as a counterpart to the baseline method and a multi-layer perceptron (MLP) as a counterpart to our proposed neural models.\n\nWe employed various metrics for the comparative analysis of performance, including balanced accuracy, F1 score, and area under the precision-recall curve (AUPR). Balanced accuracy is the average of recall obtained on each class, F1 is the weighted average of precision and recall for the positive (anticancer) class, and AUPR represents the average precision across all recall values.\n\nHyperparameter settings for every method were determined using a validation set with a grid search over candidate hyperparameter values. For the baseline approach, the grid search for the restart probability ranged from 0.001 to 0.9. For our neural models, hyperparameter candidates included learning rates, L2-regularization values, the number of convolutional layers, the number of dropout layers, batch normalization, feature normalization, and the number of hops for the ChebNet layer.\n\nOur models were trained on NVIDIA Tesla V100 and GeForce RTX 2080 GPUs. The results demonstrated that our proposed models, particularly the ChebNet variant, outperformed the baseline approach by a significant margin. This indicates that using a learnable network propagation framework enhances the performance in predicting anticancer drugs.",
  "evaluation/confidence": "The evaluation of the models includes several performance metrics, each accompanied by confidence intervals. These metrics are presented as mean values with standard deviations, indicating the variability and reliability of the results across different splits. For instance, the balanced accuracy, F1 score, and AUPR are reported with their respective standard deviations, providing a clear picture of the model's performance consistency.\n\nStatistical significance is assessed through the comparison of these metrics across different models and baselines. The results demonstrate that the proposed models, particularly the ChebNet variant, outperform the baseline approaches by a significant margin. The ChebNet model shows a notable improvement in the F1 score and AUPR, with increases of 16.15% and 6.48% respectively, compared to the baseline. This performance gain is statistically significant, as evidenced by the consistent and substantial differences in the reported metrics.\n\nThe use of cross-validation further strengthens the confidence in the results. The models are evaluated using 5-fold cross-validation, ensuring that the performance metrics are robust and not dependent on a specific data split. This approach helps in mitigating the risk of overfitting and provides a more reliable estimate of the model's generalizability.\n\nAdditionally, the attribution recall score for the best-performing model is 85.29%, indicating that the model's decisions are preferentially based on cancer-related genes. This high recall score suggests that the model is biologically plausible and that its predictions are grounded in relevant biological pathways.\n\nIn summary, the evaluation confidence is high due to the inclusion of confidence intervals, statistically significant performance improvements, and the use of cross-validation. These factors collectively support the claim that the proposed models, particularly the ChebNet variant, are superior to the baseline approaches in predicting anticancer drugs.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available for public release. The code and data to reproduce the results can be downloaded from GitHub, which implies that the evaluation process can be replicated using the provided resources. However, specific raw evaluation files, such as individual model outputs or detailed performance metrics for each fold of cross-validation, are not explicitly stated to be publicly available. For those interested in the evaluation process, the GitHub repository is the primary resource for reproducing the results and understanding the evaluation methodology."
}