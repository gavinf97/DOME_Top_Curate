{
  "publication/title": "A new molecular classification to drive precision treatment strategies in primary Sjögren’s syndrome",
  "publication/authors": "The authors who contributed to the article are as follows:\n\nChristophe Jamin, Concepción Marañón, Lucas Le Lann, Quentin Simon, Bénédicte Rouvière, Nieves Varela, Brian Muchmore, Aleksandra Dufour, Montserrat Alvarez, Carlo Chizzolini, Jonathan Cremer, Ellen De Langhe, Nuria Barbarroja, Chary Lopez-Pedrera, Velia Gerl, Laleh Khodadadi, Qingyu Cheng, Anne Buttgereit, Zuzanna Makowska, Aurélie De Groof, Julie Ducreux, Elena Trombetta, Tianlu Li, Damiana Alvarez-Errico, Torsten Witte, Katja Kniesch, Nancy Azevedo, Esmeralda Neves, Sambasiva Rao, Pierre-Emmanuel Jouve, and Jacques-Olivier Pers.\n\nJacques-Olivier Pers supervised the work and wrote the manuscript. All authors approved the content of the paper and its related supplementary files and agreed to the submission policies.\n\nSeveral authors had specific affiliations during the research project:\n\nR.L., F.M., and Z.M. were regular employees of Bayer A.G. At present, R.L. and Z.M. are regular employees of Nuvisan ICB GmbH, a company providing contract research services.\n\nP.S., S.H., S.C.G., L.X., M.G., P.M., and L. L. were regular employees of Institut de Recherches Internationales Servier at the time of the research project.\n\nB.C., C.B., and E.D. were PhD students financed by Institut de Recherches Internationales Servier when they contributed to the research project.\n\nAll other authors confirmed signing the ICMJE form for Disclosure of Potential Conflicts of Interest and none of them have any conflict of interest related to this work.\n\nPierre-Emmanuel Jouve is affiliated with AltraBio SAS, Lyon, France.",
  "publication/journal": "Nature Communications",
  "publication/year": "2021",
  "publication/doi": "10.1038/s41467-021-23472-7",
  "publication/tags": "- Primary Sjögren's Syndrome\n- Cluster Analysis\n- Machine Learning\n- Predictive Modeling\n- Flow Cytometry\n- DNA Methylation\n- Cytokine Analysis\n- Bioinformatics\n- Observational Study\n- Patient Stratification",
  "dataset/provenance": "The dataset utilized in this study originates from the PRECISESADS consortium, a European multi-center cross-sectional study involving patients from seven systemic autoimmune diseases. Specifically, our study focused on patients with primary Sjögren's syndrome (pSS) and healthy volunteers (HV). The recruitment for the cross-sectional study took place between December 2014 and October 2017, involving 19 institutions across 9 countries: Austria, Belgium, France, Germany, Hungary, Italy, Portugal, Spain, and Switzerland.\n\nThe final study cohort, after quality control and validation, comprises 304 patients with pSS and 330 healthy volunteers. The diagnosis of pSS was made according to the 2002 American-European Consensus Group classification criteria, requiring the presence of anti-SSA antibodies and/or a positive focus on a minor salivary gland biopsy.\n\nThe dataset includes RNA-sequencing data obtained from whole blood samples, along with biological and clinical information collected from each individual. The composite model developed in this study was validated using the transcriptome of 37 newly diagnosed pSS patients recruited in an inception study, also obtained from the PRECISESADS consortium. These inception patients were recruited by 10 institutions in Spain, Belgium, France, Italy, Germany, and Switzerland, and were diagnosed within less than a year since pSS diagnosis.\n\nThe data has been deposited in ELIXIR and is available upon request, with the access procedure described on the ELIXIR data landing page. The permalink for the dataset is doi:10.17881/th9v-xt85. The PRECISESADS Consortium has committed to secure patient data access through the ELIXIR platform, ensuring that the use of patient data is limited to scientific research in autoimmune diseases.\n\nThe cross-sectional and inception studies adhered to the standards set by the International Conference on Harmonization and Good Clinical Practice (ICH-GCP) and the ethical principles of the Declaration of Helsinki (2013). Each patient signed an informed consent prior to study inclusion, and the protocols were approved by the Ethical Review Boards of the participating institutions. The studies are registered in ClinicalTrials.com with numbers NCT02890121 and NCT02890134, respectively.",
  "dataset/splits": "The dataset was split into several parts to facilitate the development and validation of the predictive model. Initially, a cross-sectional cohort was divided into discovery and validation subgroups. The discovery subgroup consisted of 227 patients, while the validation subgroup had 77 patients. This split was used to determine and confirm the different clusters.\n\nThe predictive model was first trained on the validation subgroup. Subsequently, it was evaluated on the discovery dataset to ensure its precision. After achieving satisfactory performance on this evaluation dataset, the model was further tested on a new inception cohort comprising 37 patients. This step was crucial to demonstrate that the predictive model did not overfit during the training phase and could generalize well to an external dataset.\n\nAdditionally, the dataset underwent dimensional reduction using patients from both the discovery and validation cohorts. This process involved identifying differentially expressed genes, selecting genes that contributed most to differentiating the clusters, and finally training the composite model with the selected genes. This approach helped in reducing the number of variables and making the model more robust to overfitting.",
  "dataset/redundancy": "The datasets were split into discovery and validation subgroups to determine and confirm the different clusters. The process for the development of the composite model began with a training phase on the validation subgroup, which consisted of 77 patients. Subsequently, the model was evaluated on the discovery dataset, comprising 227 patients who were not included in the training phase. This approach ensured that the training and test sets were independent, thereby providing a robust evaluation of the model's performance.\n\nTo further validate the model, it was applied to a new inception cohort, which included patients who were not part of the cross-sectional cohort. This step was crucial in demonstrating that the predictive model did not overfit during the training phase and could be generalized to an external dataset. The model achieved high precision on the evaluation dataset, exceeding 95%, which indicated its reliability and effectiveness.\n\nThe distribution of the datasets used in this study is comparable to previously published machine learning datasets in terms of the separation of training and test sets. The use of independent cohorts for training and testing is a standard practice in machine learning to ensure that the model's performance is not biased by the training data. This approach helps in assessing the model's ability to generalize to new, unseen data, which is essential for its practical application in clinical settings.",
  "dataset/availability": "The data supporting the findings of this study have been deposited in ELIXIR and are hosted by ELIXIR Luxembourg. The data is available upon request, with the access procedure described on the ELIXIR data landing page. The permanent link for data access is doi:10.17881/th9v-xt85.\n\nThe PRECISESADS Consortium has committed to secure patient data access through the ELIXIR platform. This commitment was formally given in writing to all patients at the end of the project and to the involved Ethical Committees. The future use of the project database is framed according to the scope of the patient information and consent forms, where the use of patient data is limited to scientific research in autoimmune diseases.\n\nELIXIR reviews applicants' requests and prepares Data Access Committee’s decisions on access to the data. These decisions are communicated to the data providers, who have 10 days to exercise their right to veto; otherwise, access is granted to the user. This process ensures that the data is released in a controlled manner, respecting patient confidentiality and ethical guidelines.",
  "optimization/algorithm": "The optimization algorithm employed in our study utilizes machine learning techniques to develop a predictive model for primary Sjögren’s syndrome. Specifically, we used the Boruta algorithm for feature selection, which is a wrapper built around the random forest classification algorithm. This algorithm helps in identifying the most relevant genes contributing to the differentiation of clusters within our dataset. The Boruta algorithm works by creating shadow features, which are random permutations of the original features, and then comparing the importance of these shadow features to the real features. Features that are significantly more important than the shadow features are retained, ensuring that the selected features are robust and not merely noise.\n\nFollowing feature selection, we employed the XGBoost algorithm, which is an implementation of gradient boosting machines designed for speed and performance. XGBoost is known for its efficiency and effectiveness in handling large datasets and is widely used in various machine learning competitions. We used XGBoost with both binary logistic and softmax objective functions to train our models. The binary logistic function was used to predict the C4 cluster versus all other clusters, while the softmax function was used for multi-class classification to discriminate between the C1, C2, and C3 clusters.\n\nThe algorithms used in this study are not new but are well-established in the machine learning community. The choice of these algorithms was driven by their proven track record in handling complex datasets and their ability to provide high predictive accuracy. The focus of our publication is on the application of these algorithms to the specific problem of molecular classification in primary Sjögren’s syndrome, rather than the development of new machine learning techniques. Therefore, publishing in a machine-learning journal was not the primary objective, as our work is more aligned with the clinical and biological aspects of the disease.",
  "optimization/meta": "The model employs a meta-predictor approach, utilizing data from multiple machine-learning algorithms as input. Specifically, it consists of two main models: C4 and C1-C2-C3. The C4 model uses three predictors to discriminate between C4 and all other clusters, while the C1-C2-C3 model uses four predictors to predict each of the respective clusters. Each predictor is a decision tree with a maximum depth of three, composed of nodes evaluating specific parameters of an observation. The leaves of these trees contain scores corresponding to the probability of the observation belonging to the evaluated class.\n\nThe meta-predictor integrates these individual predictors to form a composite model. The final probability for a given class is determined by summing the leaf values from the predictors and applying a sigmoid function. This approach ensures that the model can handle complex relationships within the data.\n\nRegarding the independence of the training data, the model was initially trained on a validation subgroup of 77 patients from a cross-sectional cohort. This cohort was divided into discovery and validation subgroups to determine and confirm the different clusters. The model was then evaluated on a discovery dataset consisting of 227 patients who were not part of the training phase. This evaluation achieved an accuracy of over 95%, indicating that the model generalizes well to unseen data. Additionally, the model was tested on an independent inception cohort of 37 patients, further validating its performance and ensuring that the training data was independent.\n\nThe use of multiple predictors and the clear separation of training and evaluation datasets strengthen the robustness of the model, reducing the risk of overfitting and ensuring that it can be applied to new, external datasets.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several steps to ensure robust feature selection and model training. Initially, we identified 1154 differentially expressed genes across four clusters. To reduce dimensionality and focus on the most relevant features, we employed the Boruta algorithm. This algorithm helped us select genes that significantly contributed to differentiating the clusters by comparing real features with shadow features, which were permutations of the original data. Features with z-scores significantly lower than the maximum shadow feature z-score were marked as unimportant and removed.\n\nFor the C4 prediction model, we identified 255 candidate genes, while for the multi-class prediction model (C1, C2, and C3), we identified 597 candidate genes. These selected genes were then used to train our models. The preprocessing also included interpolation for genes with constant expression across all clusters and healthy volunteers, ensuring that the model could process other cohorts of patients. This interpolation involved selecting six genes with specific criteria and computing interpolated values based on their expression levels.\n\nThe final sets of selected features were further refined to 10 genes for the C4 prediction model and 31 genes for the multi-class prediction model. These refined sets were used to train our models using the xgboost-tree approach with appropriate objective functions for binary and multi-class classification. The models were trained on a validation subset of the data and evaluated on a discovery dataset to ensure they did not overfit and could generalize to new data. This comprehensive preprocessing and encoding strategy ensured that our machine-learning models were robust and reliable for predicting patient clusters.",
  "optimization/parameters": "The composite model for cluster prediction in primary Sjögren’s syndrome involves a multi-step feature selection process. Initially, differentially expressed genes (DEGs) were identified using a fold change (FC) threshold of ≥1.5 and a false discovery rate (FDR) of ≤0.05 across various cluster comparisons. This resulted in 1154 DEGs.\n\nTo further refine these genes, the Boruta algorithm was employed. This algorithm extends the dataset by adding \"shadow features,\" which are random permutations of the original features, to remove any correlation with the target variable. A random forest classifier is then run on the entire dataset, and z-scores are computed for all features. Features with z-scores significantly higher than the maximum z-score of the shadow features are considered important, while those with lower z-scores are marked as unimportant and removed. This process ensures that only robust features contributing significantly to the classification problem are retained.\n\nThe feature selection was performed twice: once to predict the C4 cluster versus all others and once to discriminate between the C1, C2, and C3 clusters. This resulted in two sets of selected features: 255 genes for the C4 prediction dataset and 597 genes for the C1, C2, and C3 prediction dataset.\n\nSubsequently, the xgboost-tree approach was used to train models on these datasets. For the C4 prediction, a binary logistic objective function was applied using the 255 genes identified by Boruta. For the multi-classification of C1, C2, and C3, a softmax objective function was used with the 597 genes. The final sets of selected features were further refined to 10 genes for the C4 prediction model and 31 genes for the multi-classification model.\n\nThe composite model integrates the results of these two prediction models to classify all four clusters. The model's performance was evaluated on both the discovery and validation sets, achieving high accuracy. This approach ensures that the model is robust and generalizable to new datasets.",
  "optimization/features": "The input features for our model were initially selected through a rigorous feature selection process. We began by identifying differentially expressed genes (DEGs) across four clusters, focusing on those with a fold change (FC) greater than 1.5 and a false discovery rate (FDR) less than 0.05. This process involved comparing various cluster combinations, resulting in a subset of 1154 DEGs.\n\nTo further refine these features, we employed the Boruta algorithm, which is designed to identify the most significant features contributing to classification problems. This algorithm works by creating \"shadow features,\" which are random permutations of the original features, and then comparing their importance scores. Features that consistently show higher importance than the shadow features are retained, while others are marked as unimportant and removed.\n\nThe feature selection process was conducted using both the discovery and validation sets, ensuring that the selected features were robust and not overfitted to a single dataset. This step was crucial for reducing the dimensionality of our dataset and focusing on the most relevant genes for classification.\n\nFollowing this, we used the selected features to train our models. For the C4 prediction model, 255 genes were identified, while for the multi-classification model (C1, C2, and C3), 597 genes were selected. These genes were then used to train our models using the xgboost-tree approach with a binary logistic objective function for C4 prediction and a softmax objective function for multi-classification.\n\nIn summary, feature selection was performed using both the discovery and validation sets, and the final models were trained using a subset of the most significant genes identified through this process. This approach ensured that our models were robust and capable of generalizing to new datasets.",
  "optimization/fitting": "The fitting method employed in our study involved a comprehensive approach to ensure robustness and generalizability of our predictive model. Initially, we utilized a dataset comprising RNA-Seq data from all subjects to perform dimensional reduction. This process involved identifying differentially expressed genes (DEGs) and using the Boruta algorithm to select genes that significantly contributed to differentiating the clusters. This step was crucial in reducing the number of variables, thereby mitigating the risk of overfitting.\n\nTo address the concern of overfitting, we employed a rigorous cross-validation strategy. The dataset was divided into discovery and validation subgroups. The model was first trained on the validation subgroup, which consisted of 77 patients. Subsequently, we evaluated the model on the discovery dataset, comprising 227 patients who were not part of the training phase. This evaluation yielded an accuracy of over 95%, indicating that the model performed well on unseen data.\n\nFurthermore, to provide additional evidence that the predictive model did not overfit, we tested it on an independent inception cohort of 37 patients. The model's performance on this external dataset confirmed its robustness and generalizability. The accuracy of the model was 95.15%, with high precision in both the first and second steps of the prediction process.\n\nIn terms of underfitting, the model's performance metrics, such as accuracy and the area under the Receiver Operating Characteristic curve, were thoroughly evaluated. The model achieved high accuracy on both the validation and discovery datasets, suggesting that it captured the underlying patterns in the data effectively. Additionally, the use of a composite model, which combined the results of two separate classification models, ensured that the model was not overly simplistic.\n\nThe relatively small size and heterogeneity of one of the clusters (C4) were addressed by solving two classification problems: identifying C4 versus all clusters and discriminating between the other clusters (C1, C2, and C3). This approach allowed us to handle the complexity of the data and ensure that the model was not underfitting.\n\nOverall, the fitting method involved a careful balance between model complexity and performance, with extensive validation steps to rule out both overfitting and underfitting. The use of cross-validation, independent cohort testing, and a composite model approach ensured that the final model was robust and generalizable.",
  "optimization/regularization": "In our study, we implemented several techniques to prevent overfitting and ensure the robustness of our predictive model. One of the key methods used was dimensionality reduction. We began by identifying 1154 differentially expressed genes across the four clusters. To refine this set, we employed the Boruta algorithm, which helped us select genes that contributed most to differentiating these clusters. This process resulted in 255 candidate genes for training the C4 model and 597 candidate genes for the C1-C2-C3 multiclass prediction model. This reduction in the number of variables helped mitigate overfitting by focusing on the most relevant features.\n\nAdditionally, we used a validation set to train our model, which was crucial given the heterogeneity of C4 patients in this set. The model was then evaluated on a discovery set, achieving high accuracy. This approach ensured that our model was not merely memorizing the training data but could generalize to new, unseen data.\n\nWe also implemented an interpolation function based on six genes with constant expression across all clusters and healthy volunteers. This function allowed our model to process other cohorts of patients, further enhancing its generalizability and robustness.\n\nMoreover, we utilized machine learning techniques such as random forests and xgboost-tree approaches, which are known for their ability to handle complex datasets and reduce overfitting. The models were trained with a balanced class initialization and a maximum depth of five, ensuring that they did not become too complex and overfit the training data.\n\nOverall, these techniques collectively helped us build a predictive model that is both accurate and generalizable, minimizing the risk of overfitting.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are detailed within the supplementary materials. Specifically, the Boruta algorithm was employed for feature selection, and machine learning approaches were carried out using Python programs based on modules such as numpy, scikit-learn, and xgboost. The composite model, which integrates these configurations, is available on the laboratory's GitHub repository. This repository provides access to the analysis tool and the necessary code for replication and further testing. The data supporting our findings have been deposited in ELIXIR, hosted by ELIXIR Luxembourg, and are accessible upon request with the identifier provided. The access procedure is described on the ELIXIR data landing page, ensuring that the configurations and parameters are transparent and reproducible. The repository and data access comply with open-source licenses, facilitating community use and validation.",
  "model/interpretability": "The model employed in our study is designed to be interpretable, rather than a black-box system. This transparency is achieved through the use of decision trees and a composite model structure that allows for clear insights into the classification process.\n\nThe composite model consists of two main parts: a binary classifier for distinguishing the C4 cluster from all others, and a multi-class classifier for differentiating between the C1, C2, and C3 clusters. Each of these classifiers is built using decision trees, which are inherently interpretable. Decision trees break down the classification process into a series of if-then statements based on specific gene expressions, making it straightforward to trace how a particular prediction is made.\n\nFor instance, in the C4 model, three predictors are used, each evaluating specific parameters of an observation. These predictors have a maximum depth of three, meaning they make decisions based on a series of up to three gene expression thresholds. The leaves of these trees contain scores that correspond to the probability of an observation belonging to the C4 class. These scores are then summed and transformed by a sigmoid function to produce the final probability.\n\nSimilarly, the multi-class classifier for C1, C2, and C3 uses four predictors, each with a maximum depth of three. The process involves evaluating specific gene expressions at each node, with the final probability of belonging to a particular cluster derived from the leaf scores.\n\nThis structure ensures that the model's decisions are transparent and can be easily understood. For example, if a patient's gene expression levels fall below certain thresholds at various nodes, the model can be traced to see how these levels contribute to the final classification. This interpretability is crucial for medical applications, where understanding the basis of a prediction is as important as the prediction itself.",
  "model/output": "The model developed is a classification model designed to predict the membership of patients with primary Sjögren’s syndrome to one of four distinct clusters. The model employs a composite approach, utilizing two main predictors: one for discriminating between cluster C4 and all other clusters, and another for differentiating among clusters C1, C2, and C3.\n\nThe C4 model uses three predictors, each with a maximum depth of three, to identify patients belonging to cluster C4. Similarly, the C1-C2-C3 model employs four predictors with the same maximum depth to classify patients into clusters C1, C2, or C3. Altogether, the model consists of 15 predictors.\n\nEach predictor is composed of nodes that evaluate specific parameters of an observation, and the leaves contain scores corresponding to the probability of the observation belonging to the evaluated class. The final probability is derived from the sum of the leaf values, transformed by a sigmoid function. Leaf values can be negative, with a score of 0 representing a 50% probability.\n\nThe model's performance was evaluated on both a discovery set and an independent inception cohort. On the discovery set, the model achieved an accuracy of 95.15%, with 99.12% and 95.57% accuracy for the first and second steps, respectively. When applied to the independent inception cohort, the model successfully classified patients into the four clusters, with distributions consistent with those observed in the discovery and validation sets. This consistency strengthens the validation of the composite model.\n\nAdditionally, the model includes an interpolation function based on six genes with constant expression across all clusters, allowing it to process other cohorts of patients. This function ensures the model's generalizability and robustness. The composite model is integrated into an analysis tool available on the laboratory's GitHub repository, facilitating its use by other researchers.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the composite model has been made publicly available. The model is integrated into an analysis tool that can be accessed through the laboratory's GitHub repository. This repository provides the necessary resources for others to run the algorithm and test it on their own datasets. The specific address for the GitHub repository is https://lbai-infolab.github.io/SjTree/. The code is released under a license that allows for community use and further development, facilitating transparency and reproducibility in research. Additionally, the repository includes detailed documentation and pseudocode to guide users through the implementation process. This approach ensures that the model can be validated and applied by other researchers, promoting collaborative efforts in the scientific community.",
  "evaluation/method": "The evaluation of the composite predictive model involved several rigorous steps to ensure its robustness and generalizability. Initially, the model was trained using a validation subgroup consisting of 77 patients from the cross-sectional cohort. This training phase was followed by an evaluation on a discovery dataset comprising 227 patients who were not part of the training set. The model achieved over 95% precision on this evaluation dataset, indicating strong performance.\n\nTo further validate the model, it was tested on an independent inception cohort of 37 newly diagnosed patients. This cohort was recruited from different institutions and countries, ensuring that the patients were not part of the original training or evaluation datasets. The model's predictions on this independent cohort showed a distribution of patients across the four clusters that was consistent with the profiles identified in the discovery and validation sets. This step was crucial in demonstrating that the model did not overfit the training data and could generalize well to new, unseen data.\n\nAdditionally, the model's performance was evaluated using accuracy as the primary metric. The accuracy of the model on the discovery set was 95.15%, with high accuracy in the individual steps of the composite model. The confusion matrix, discriminant function analysis, and probabilities for cluster membership were also analyzed to provide a comprehensive evaluation of the model's performance.\n\nTo facilitate further validation by other researchers, the model and its associated code were made available on a public repository. This allows other teams to test the model on their own datasets, providing an additional layer of validation and ensuring the reproducibility of the results. The model's performance was also evaluated using a minimal list of 257 discriminative genes, which were used to generate a heat map and confirm the clustering profiles.\n\nIn summary, the evaluation method involved a combination of cross-validation within the cross-sectional cohort, testing on an independent inception cohort, and making the model available for external validation. These steps ensured that the composite predictive model is robust, generalizable, and reliable for predicting patient clusters in primary Sjögren’s syndrome.",
  "evaluation/measure": "The performance of our predictive model is primarily evaluated using accuracy, which is a common and appropriate metric for this type of analysis. We achieved an accuracy of 95% on the discovery dataset and 92% on the validation dataset, indicating strong model performance.\n\nIn response to reviewer suggestions, we also considered reporting the area under the Receiver Operating Characteristic (ROC) curve. However, due to the nature of our composite model, we could only accurately report the ROC curve for one of the clusters (C4). The model first assesses the probability that a patient belongs to C4, and if this probability is low, it then calculates the probabilities for the other clusters (C1, C2, and C3). Reporting ROC curves for C1, C2, and C3 would involve approximations that could potentially mislead readers about the model's performance. Therefore, we opted to use a confusion matrix to represent the model's performance, as it provides a clear and unbiased view of the model's predictive accuracy across all clusters.\n\nAdditionally, we used the Boruta algorithm to identify features that significantly contribute to predicting a patient's cluster membership. This algorithm helps ensure that the features used in our model are robust and not merely noise or artifacts. The final sets of selected features consisted of 10 genes for the C4 prediction model and 31 genes for the multi-classification model (C1, C2, or C3).\n\nThe accuracies of the models during the training phase were 94.81% for the C4 prediction model and 96.72% for the multi-classification model. These high accuracies suggest that our model is effective in predicting cluster membership based on the selected features.\n\nIn summary, our model's performance is evaluated using accuracy and a confusion matrix, which provide a comprehensive view of its predictive capabilities. While we considered reporting ROC curves, the nature of our model made this approach less suitable. The use of the Boruta algorithm ensures that our model is based on robust and significant features, further validating its performance.",
  "evaluation/comparison": "Not enough information is available.",
  "evaluation/confidence": "The evaluation of our predictive model was conducted using a 75/25 derivation/validation split, which is a standard approach to ensure the robustness of the model. The model's performance was primarily assessed using accuracy, achieving 95% accuracy on the discovery dataset and 92% on the validation dataset. These results indicate strong predictive power.\n\nTo further validate the model's performance, we considered reporting the area under the Receiver Operating Characteristic (ROC) curve. This metric provides a comprehensive evaluation of the model's ability to distinguish between different clusters. However, specific details on the ROC curves and their confidence intervals were not provided in the current evaluation.\n\nStatistical significance was ensured through various tests. For instance, differential cytokine concentration comparisons between clusters and healthy volunteers (HV) were performed using two-tailed pairwise Wilcoxon-rank sum tests. This non-parametric test is robust and appropriate for comparing two related samples, ensuring that any observed differences are statistically significant.\n\nThe model's development and evaluation were described in adequate detail, and the experimental protocol was appropriate. The conclusions drawn from the modeling analyses and data interpretation are valid and reliable. The use of established statistical methods and the achievement of high accuracy on both the discovery and validation datasets provide confidence in the model's performance.\n\nHowever, it is important to note that while the model shows promising results, further validation in longitudinal cohorts is necessary to confirm the stability of the clusters over time and under different treatment conditions. This will help in understanding whether patients remain in their initial clusters regardless of disease activity and treatments received.\n\nIn summary, the evaluation of the predictive model is robust, with high accuracy and statistically significant results. The use of appropriate statistical tests and the detailed description of the methodology provide confidence in the model's performance. Future work will focus on validating these findings in additional cohorts and longitudinal studies.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the data supporting the findings of this study have been deposited and are available upon request. The data is hosted by ELIXIR Luxembourg, with a permanent link provided for access. The access procedure is described on the ELIXIR data landing page. The PRECISESADS Consortium has committed to secure patient data access through the ELIXIR platform, ensuring that all requests are reviewed and approved by a Data Access Committee. This process includes communication with data providers, who have the right to veto access within a specified timeframe. The future use of the project database is framed according to the scope of patient information and consent forms, limiting the use of patient data to scientific research in autoimmune diseases."
}