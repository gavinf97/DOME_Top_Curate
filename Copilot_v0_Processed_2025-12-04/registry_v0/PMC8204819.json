{
  "publication/title": "A ML PROGRAM to identify COVID-19 & other diseases from hematology data",
  "publication/authors": "The authors who contributed to this article are as follows:\n\nP. A. Gladding was the main author, contributing to ideation, ethics, data collation, and BigML modeling.\n\nZ. Ayar contributed to data extraction and linkage to ICD10 coding.\n\nK. Smith, J. Atkinson, and E. McChlery contributed to RT-PCR data extraction and proofing.\n\nP. Patel, J. Pearce, S. Puwakdandawa, and D. Tarrant contributed to Sysmex IPU and Cellavision data extraction and proofing.\n\nM. Hanna, N. Gow, K. Read, and H. Bhally contributed to infectious and hematology disease input and proofing.\n\nP. Jayathissa and J. Wallace contributed to ethics, machine learning input, and proofing.\n\nS. Norton contributed to flow cytometry data analysis and proofing.\n\nN. Kasabov and C. S. Calude contributed to machine learning modeling, ethics, and proofing.\n\nD. Steel and C. McKenzie contributed to Sysmex support and proofing.",
  "publication/journal": "Future Science OA",
  "publication/year": "2021",
  "publication/doi": "10.2144/fsoa-2020-0207",
  "publication/tags": "- Biological age\n- COVID-19\n- Full blood count\n- Heart failure\n- Hematology\n- Machine learning\n- Pneumonia\n- Communicable diseases\n- Noncommunicable diseases\n- Data analysis",
  "dataset/provenance": "The dataset used in this study was collected from the Information Process Unit (IPU) connected to Sysmex XN-1000 and XN-3000 hematology analyzers at Waitakere Hospital and North Shore Hospital, respectively. These analyzers are part of the Waitematā District Health Board’s (WDHB) network and provided both basic and advanced hematology parameters.\n\nThe data collection period spanned from July 1, 2019, to June 8, 2020, encompassing hospital data from both inpatients and outpatients within the WDHB catchment area. This retrospective observational study included a nested cohort and case-control design, focusing on patients who tested positive for SARS-CoV-2, along with a matched number of controls.\n\nIn addition to hematology data, flow cytometry standard (FCS) files were downloaded for SARS-CoV-2 positive patients and their matched controls. Standard biochemical laboratory data were extracted using a Python script from an SQL database. International Classification of Diseases (ICD-10) primary diagnoses, age, ethnicity, sex, and mortality data were also obtained using specific hospital encounter numbers through SQL queries.\n\nThe dataset is rich in information about health states and includes predictions for biological age, gender, individuality, and both communicable and noncommunicable diseases, such as COVID-19 and heart failure. However, the sample size for COVID-19 patients was relatively small, which may have led to overfitting in the machine learning models. This limitation was addressed by using transparent, explainable artificial intelligence (AI) with BigML to interrogate the features used in predictive models.\n\nThe data used in this project were obtained with ethical approval but without informed consent, due to the impracticality of obtaining it at scale. This approach highlights the importance of maintaining security and privacy, especially for indigenous populations, and emphasizes the need for governance and kaitiakitanga in handling such sensitive information.",
  "dataset/splits": "In our study, we employed multiple data splits to ensure robust model training, testing, and validation. For most of our models, we divided the data into an 80:20 split, allocating 80% for training and 20% for testing. However, for specific conditions like pneumonia, urinary tract infection, and heart failure, we included an additional independent validation set.\n\nFor pneumonia, we used 250 patients with pneumonia and an equal number of controls for training and testing. Additionally, 122 pneumonia cases and 14,453 controls were used for independent validation. In the case of urinary tract infection, 168 patients and an equal number of controls were used for training and testing, with 61 urinary tract infection cases and 14,453 controls for validation. For heart failure, the specific numbers are not detailed, but the same general approach of training, testing, and validation sets was followed.\n\nFor COVID-19 prediction, we used 102 instances (serial full blood counts in nine patients) and 204 control full blood counts for training and testing. The validation set consisted of 11 full blood counts from three COVID-19 patients and 6,770 controls.\n\nIn summary, our dataset splits typically involved a training set comprising 80% of the data, a testing set with 20%, and an additional independent validation set for specific conditions. This approach helped us to rigorously evaluate the performance and generalizability of our machine learning models.",
  "dataset/redundancy": "The datasets used in this study were split into training and test sets with an 80:20 ratio. For certain conditions like pneumonia, urinary tract infection, and heart failure, an additional independent validation set was included. This splitting ensured that the training and test sets were independent, preventing data leakage and overfitting. The independence was enforced by using unique patient data for each set, ensuring that the models were evaluated on unseen data.\n\nThe distribution of the datasets aimed to cover a broad spectrum of health states, including both communicable and noncommunicable diseases. However, it is important to note that the COVID-19 data was skewed toward more severely ill patients who underwent multiple blood tests, which may not represent the entire spectrum of the disease. For COVID-19 predictions, serial results from PCR-positive cases were used to capture various stages of the disease, assuming this would provide a more comprehensive view.\n\nThe models were developed using various machine learning techniques, including decision trees, ensembles, logistic regression, and deep neural networks. The OptiML process was employed for automated model selection and parameter optimization, utilizing Bayesian parameter optimization and Monte Carlo cross-validation. This approach helped in finding the best supervised models for tasks such as sex classification and age prediction.\n\nIn summary, the datasets were carefully split and managed to ensure independence and robustness, with a focus on leveraging existing hematology data to predict various health conditions. The methods used aimed to provide reliable and generalizable results, although the limitations of the data, such as the skew toward more severe COVID-19 cases, were acknowledged.",
  "dataset/availability": "The materials, data, code, and associated protocols are available to readers upon application to the corresponding author and the Waitematā Privacy, Security and Governance group. A limited data sharing agreement is required for access. The BigML models, however, will be shared without limitations. This work is licensed under the Creative Commons Attribution 4.0 License, which allows for open access and sharing with proper attribution. The data sharing process is enforced through the application and approval procedure managed by the corresponding author and the governance group.",
  "optimization/algorithm": "The machine-learning algorithm class used is supervised learning, specifically involving decision trees, ensembles, logistic regression, and deep neural networks (DNN). The models were developed using the BigML platform, which provides tools for applying these machine-learning techniques.\n\nThe optimization process employed is called OptiML, an automated BigML optimization process for model selection and parametrization. OptiML uses Bayesian parameter optimization and Monte Carlo cross-validation to find the best supervised model for tasks such as sex classification and predicting age using regression.\n\nThe algorithms used are not new; they are well-established methods in the field of machine learning. The reason these algorithms were not published in a machine-learning journal is that the focus of the publication is on their application to specific medical problems, such as identifying COVID-19 and other diseases from hematology data, rather than the development of new machine-learning algorithms. The emphasis is on the practical implementation and evaluation of these models in a clinical context, demonstrating their utility in predicting biological age, gender, individuality, and various diseases.",
  "optimization/meta": "The meta-predictor leverages data from various machine-learning algorithms as input. It integrates multiple models, including decision trees, ensembles, logistic regression, and deep neural networks (DNNs). These models were developed using data split into training and test sets, with an additional independent validation set for specific conditions like pneumonia, urinary tract infection, and heart failure. The use of an independent validation set ensures that the training data is distinct from the validation data, maintaining the integrity of the model evaluation process. The OptiML process, which employs Bayesian parameter optimization and Monte Carlo cross-validation, was used to select and optimize the best supervised models for tasks such as sex classification and age prediction. This approach ensures that the meta-predictor benefits from the strengths of multiple machine-learning methods, providing robust and reliable predictions.",
  "optimization/encoding": "The data used for the machine-learning models were derived from hematology analyzer raw data, which included up to 38 clinical parameters and 50 research parameters. These parameters were generated using fluorescence flow cytometry, impedance, hydrodynamic focusing, and cyanide-free sodium lauryl sulphate (SLS) for hemoglobin measurement. The instrument was capable of processing up to 100 samples per hour using an 88µl sample volume.\n\nFor the machine-learning models, the data were split into training and test sets with an 80:20 ratio. Additionally, for certain diseases like pneumonia, urinary tract infection, and heart failure, an independent validation set was included. This approach ensured that the models were trained on a substantial portion of the data while also having a separate set for testing and validation.\n\nThe data preprocessing involved handling missing values and ensuring that only unique patient data were used for training to avoid overfitting. For COVID-19 prediction, serial results for each positive case were used to capture the various stages of the disease and convalescence. This was particularly important given the low number of COVID-19 PCR-positive cases.\n\nUnivariate analysis was performed using the student t-test for continuous parametric variables, and receiver-operating characteristic curve analysis was used to assess the performance of diagnostic biomarkers by c-statistic. All tests were two-tailed, and a p-value of less than 0.05 was deemed statistically significant, except where Bonferroni correction for multiplicity was applied.\n\nThe machine-learning models utilized decision trees, ensembles, logistic regression, and deep neural networks (DNNs) with transparency. The OptiML process, an automated optimization tool, was used for model selection and parametrization. This process employed Bayesian parameter optimization and Monte Carlo cross-validation to find the best supervised models for sex classification and predicting age using regression.\n\nFor unsupervised machine learning, t-stochastic neighbor embedding (t-SNE) was used to visualize temporal changes and clustering of numeric hematology data. This technique helped in comparing survivors with non-survivors and understanding the shifts in cellular population characteristics over the course of the disease.",
  "optimization/parameters": "In our study, the selection of input parameters for the machine learning models was a critical step in ensuring the robustness and accuracy of our predictions. We utilized a comprehensive set of clinical and research parameters derived from hematology data. Specifically, up to 38 clinical parameters and 50 research parameters, or derivatives thereof, were considered. These parameters were generated using advanced techniques such as fluorescence flow cytometry, impedance, hydrodynamic focusing, and cyanide-free sodium lauryl sulphate (SLS) for hemoglobin measurement.\n\nThe selection of these parameters was guided by their potential to provide meaningful insights into the health states of patients. The goal was to identify features that could discriminate and predict objectives such as biological age, gender, individuality, and both communicable and noncommunicable diseases. This involved a thorough examination of the data to ensure that the selected parameters were relevant and informative.\n\nTo optimize the model, we employed OptiML, an automated process that uses Bayesian parameter optimization and Monte Carlo cross-validation. This approach helped in identifying the best supervised model for sex classification and predicting age using regression. The OptiML process ensured that the selected parameters were not only statistically significant but also practically useful in the context of the diseases being studied.\n\nIn summary, the input parameters were carefully chosen based on their ability to provide rich information about health states. The use of advanced techniques and optimization processes ensured that the selected parameters were both relevant and effective in achieving the objectives of the study.",
  "optimization/features": "The machine learning models utilized a variety of features derived from hematology data. The specific number of features (f) used as input varied depending on the model and the condition being predicted. For instance, in the prediction of pneumonia, the highest-ranked features included NE-FSC(ch), MO-X(ch), LY-Z(ch), NE-SSC(ch), and LY-WY. Similarly, for urinary tract infection, the top features were MO-X(ch), MO-WZ, PCT (%), RBC (10^12/L), and LY-Z(ch). In the case of COVID-19 prediction, features like Q-Flag (Abn Lympho?) and HFLC% were notably important.\n\nFeature selection was indeed performed to identify the most relevant features for each model. This process involved ranking features based on their importance in predicting the target outcomes. The selection was conducted using the training set only, ensuring that the test and validation sets remained unbiased. This approach helped in focusing on the most informative features, thereby improving the model's performance and interpretability. The features used varied according to the machine learning method and the specific prediction task, with some features like MicroR (%) consistently ranking high in certain models.",
  "optimization/fitting": "The fitting method employed in this study involved a careful balance to avoid both overfitting and underfitting. The dataset was split into training and test sets, with an 80:20 ratio, ensuring a robust evaluation of model performance. Additionally, for certain diseases like pneumonia, urinary tract infection, and heart failure, an independent validation set was included to further assess the model's generalizability.\n\nTo address the potential issue of overfitting, especially given the complexity of the models used (such as decision trees, ensembles, logistic regression, and deep neural networks), several strategies were implemented. OptiML, an automated optimization process, was utilized to find the best supervised model for classification and regression tasks. This process employs Bayesian parameter optimization and Monte Carlo cross-validation, which helps in tuning the model parameters effectively and reducing the risk of overfitting.\n\nMoreover, the use of an independent validation set provided an additional layer of verification, ensuring that the models performed well on unseen data. This approach helped in ruling out overfitting by demonstrating that the models could generalize beyond the training data.\n\nOn the other hand, underfitting was mitigated by using advanced machine learning techniques and ensuring that the models were complex enough to capture the underlying patterns in the data. The use of ensembles and deep neural networks, which are capable of learning intricate relationships, helped in building models that were not too simplistic.\n\nIn summary, the fitting method involved a combination of data splitting, advanced optimization techniques, and independent validation to ensure that the models were neither overfitted nor underfitted. This comprehensive approach helped in achieving reliable and generalizable predictions.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our machine learning models. One key method used was the splitting of data into training, test, and independent validation sets. For most models, we used an 80:20 split for training and testing, and for specific conditions like pneumonia, urinary tract infection, and heart failure, we included an additional independent validation set. This approach helped to assess the model's performance on unseen data, reducing the risk of overfitting.\n\nAdditionally, we utilized Bayesian parameter optimization and Monte Carlo cross-validation through the OptiML process. This automated optimization helped in selecting the best supervised models for tasks such as sex classification and age prediction using regression. These techniques ensured that our models were not only accurate but also generalizable to new data.\n\nFor the COVID-19 model, we noted that the high performance in the training and test sets was likely due to overfitting, given the limited data available. However, even in the independent validation set, the model still performed better than the highest univariate predictor, indicating its potential utility despite the data limitations.\n\nIn summary, our approach included data splitting, Bayesian optimization, and cross-validation to mitigate overfitting and enhance the reliability of our machine learning models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters used in our study are available to interested parties. These materials, along with the associated protocols and data, can be accessed by applying to the corresponding author and the Waitematā Privacy, Security and Governance (PSGG) group. A limited data sharing agreement will be required for access. Additionally, the BigML models developed during this research will be shared without any limitations, ensuring that other researchers can replicate and build upon our work. The materials are licensed under the Creative Commons Attribution 4.0 License, which allows for open access and reuse with proper attribution. This approach ensures transparency and facilitates further advancements in the field.",
  "model/interpretability": "The models employed in this study are not entirely black-box, as efforts were made to ensure transparency and interpretability. For instance, decision trees and ensembles were used, which are inherently interpretable due to their structure. Additionally, logistic regression models were utilized, which provide clear coefficients for each feature, indicating their contribution to the prediction.\n\nIn the context of predicting heart failure, the highest ranked features included RDW-SD(fL), PCT/M, BA-D#(10∧9/L), LY-WY, and LY-Z(ch). These features were identified as significant contributors to the model's predictions, offering insights into the biological mechanisms underlying the disease.\n\nFor the prediction of COVID-19, the highest ranked features included Q-Flag (Abn Lympho?) and HFLC%. These features were crucial in distinguishing between COVID-19 and non-COVID-19 patients, and their importance was visually demonstrated through UMAP visualizations. These visualizations helped in interpreting the differences in cellular responses over time and outcomes, providing a clearer understanding of the model's decision-making process.\n\nMoreover, t-SNE was used to visualize temporal changes and clustering of numeric hematology data, comparing survivors with non-survivors. This technique helped in understanding the progression of the disease and the model's ability to capture these changes.\n\nIn summary, while some models like deep neural networks can be complex, the use of interpretable models and visualization techniques ensured that the predictions were transparent and the underlying patterns were understandable. This approach allowed for a deeper understanding of the data and the biological significance of the features used in the models.",
  "model/output": "The model encompasses both classification and regression tasks. For classification, various machine learning models were employed to predict biological sex, pneumonia, urinary tract infection, heart failure, and COVID-19. For instance, a bootstrap decision forest was used to predict biological sex, achieving an Area Under the Receiver Operating Curve (AUROC) of 0.84 in the training set and 0.83 in the test set. Similarly, boosted decision trees and random decision forests were utilized for predicting infectious diseases like pneumonia and urinary tract infections, with AUROCs ranging from 0.68 to 0.84. For COVID-19 prediction, a boosted decision tree model showed exceptional performance with an AUROC of 0.98 in the training set and 0.99 in the test set, although this was likely due to overfitting.\n\nIn terms of regression, a deep neural network (DNN) was used to predict age based on full blood count (FBC) data. The DNN achieved an R-squared value of 0.55 in the training set and 0.59 in the test set, with a mean absolute error of 12 years. This indicates a moderate level of accuracy in predicting chronological age from hematology data.\n\nAdditionally, the model included an unsupervised learning component using t-stochastic neighbor embedding (t-SNE) to visualize the personalized nature of FBC metadata. This technique helped in clustering and identifying individual patterns in the data, which is crucial for understanding the variability and individuality in FBC results over time.\n\nOverall, the model demonstrates versatility in handling both classification and regression tasks, leveraging various machine learning techniques to extract valuable insights from hematology data.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning models developed in this project is not explicitly released. However, the models themselves are shared without limitations. The models were created using BigML, a machine learning platform that provides various tools for model development and deployment. BigML offers a web-based interface for applying machine learning models, including decision trees, ensembles, logistic regression, and deep neural networks. The models can be accessed and utilized through BigML's platform, which allows for transparency and ease of use.\n\nFor those interested in replicating or building upon the work, the materials, data, code, and associated protocols are available upon application to the corresponding author and the Waitematā Privacy, Security and Governance (PSGG) group. A limited data sharing agreement may be required to access these resources. This approach ensures that the research can be verified and built upon by other researchers while maintaining necessary data privacy and security measures.",
  "evaluation/method": "The evaluation of the machine learning models involved several rigorous steps to ensure their robustness and generalizability. Data was split into training and test sets using an 80:20 ratio. For specific conditions like pneumonia, urinary tract infection, and heart failure, an additional independent validation set was included to assess the models' performance on unseen data.\n\nFor pneumonia, a boosted decision tree model was evaluated. It achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.82 in the training set and 0.83 in the test set. The model was further validated on an independent dataset, yielding an AUROC of 0.74, with a sensitivity of 58% and specificity of 79%.\n\nIn the case of urinary tract infections, a random decision forest model was used. It showed an AUROC of 0.81 in the training set and 0.84 in the test set. Independent validation resulted in an AUROC of 0.68, with a sensitivity of 52% and specificity of 79%.\n\nFor COVID-19 prediction, a boosted decision tree model was employed. It demonstrated an exceptionally high AUROC of 0.98 in the training set and 0.99 in the test set. However, this performance was likely due to overfitting. In the independent validation set, the model still performed well, exceeding the highest univariate predictor of COVID-19.\n\nStatistical significance was determined using two-tailed tests with a p-value threshold of less than 0.05, except where Bonferroni correction for multiplicity was applied. Medcalc software version 16.8.4 was used for data analysis, and BigML was utilized for applying machine learning models, including decision trees, ensembles, logistic regression, and deep neural networks.\n\nThe OptiML process, which involves Bayesian parameter optimization and Monte Carlo cross-validation, was used to select and parameterize the best supervised models for sex classification and age prediction using regression. Unsupervised machine learning techniques, such as t-stochastic neighbor embedding (t-SNE), were also employed for data visualization and clustering.",
  "evaluation/measure": "In our evaluation, we reported several performance metrics to comprehensively assess the effectiveness of our machine learning models. For classification tasks, we primarily used the Area Under the Receiver Operating Curve (AUROC), which provides a single scalar value that represents the ability of the model to distinguish between classes. This metric is widely used in the literature due to its robustness, especially when dealing with imbalanced datasets.\n\nIn addition to AUROC, we reported sensitivity and specificity, which measure the true positive rate and true negative rate, respectively. These metrics are crucial for understanding the model's performance in clinical settings, where the consequences of false positives and false negatives can be significant. We also provided the phi coefficient, which is a measure of association between the predicted and actual classes, ranging from -1 to 1.\n\nFor regression tasks, such as predicting age, we used the coefficient of determination (R²), which indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. We also reported the mean absolute error (MAE) to provide an intuitive measure of the model's prediction accuracy.\n\nFor the task of predicting individuality in full blood count (FBC) patterns, we reported accuracy, precision, recall, and the F-measure. These metrics are essential for evaluating the model's performance in identifying unique patients based on their FBC data.\n\nThe set of metrics we reported is representative of the standards in the literature, ensuring that our results are comparable with other studies in the field. By providing a comprehensive set of performance metrics, we aim to give a clear and thorough evaluation of our models' capabilities and limitations.",
  "evaluation/comparison": "Not applicable. The publication focuses on the development and evaluation of machine learning models using specific datasets related to infectious diseases and COVID-19. It does not mention any comparison to publicly available methods on benchmark datasets or to simpler baselines. The evaluation primarily involves the performance of models such as boosted decision trees, random decision forests, and logistic regression on the datasets used for pneumonia, urinary tract infection, and COVID-19 prediction. The models were assessed using metrics like AUROC, sensitivity, and specificity, but there is no indication of a comparison with other publicly available methods or simpler baselines.",
  "evaluation/confidence": "The evaluation of our machine learning models included statistical significance testing, with a threshold of p< 0.05 deemed statistically significant, except where Bonferroni correction for multiplicity was applied. This ensures that the results are robust and not due to random chance.\n\nConfidence intervals were provided for some of the performance metrics. For instance, in the pneumonia model, the Area Under the Receiver Operating Curve (AUROC) in the validation set was reported with a 95% Confidence Interval (CI) of 0.73–0.75. This provides a range within which the true AUROC is likely to fall, giving an indication of the precision of the estimate.\n\nThe statistical significance of the results was assessed, and p-values were reported. For example, the pneumonia model had a p-value of < 0.0001, indicating that the results are highly statistically significant. This suggests that the model's performance is unlikely to be due to random variation and that the method is superior to random guessing.\n\nIn summary, the performance metrics included confidence intervals where applicable, and the results were statistically significant, providing strong evidence that the methods used are effective and superior to baselines.",
  "evaluation/availability": "The raw evaluation files are not publicly available. However, the materials, data, code, and associated protocols can be accessed by readers upon application to the corresponding author and the Waitematā Privacy, Security and Governance group. A limited data sharing agreement will be required for access. The machine learning models developed using BigML will be shared without any limitations. This work is licensed under the Creative Commons Attribution 4.0 License, which allows for open access and sharing with proper attribution."
}