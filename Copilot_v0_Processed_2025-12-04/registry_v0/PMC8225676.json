{
  "publication/title": "Bioactivity descriptors for uncharacterized chemical compounds",
  "publication/authors": "The authors who contributed to the article are not fully listed in the provided information. However, it is mentioned that correspondence and requests for materials should be addressed to M.D-F. or P.A. This suggests that these individuals played significant roles in the research and publication process. Unfortunately, without additional details, it is not possible to specify their exact contributions or the full list of authors.",
  "publication/journal": "Nature Communications",
  "publication/year": "2021",
  "publication/doi": "https://doi.org/10.1038/s41467-021-24150-4",
  "publication/tags": "- Bioactivity descriptors\n- Machine learning\n- Molecular descriptors\n- Chemical compounds\n- Computational strategy\n- Random Forest classifiers\n- Molecular fingerprints\n- Signature analysis\n- Data validation\n- Scientific research",
  "dataset/provenance": "The dataset utilized in our study is sourced from various repositories and databases, ensuring a comprehensive and diverse collection of molecular data. The primary source is the Chemical Checker (CC) repository, from which experimental CC signatures were obtained. This repository is regularly updated, with the version used in our study being from May 2019.\n\nAdditionally, we incorporated data from the Drug Repurposing Hub, which includes molecules and their annotations. This data was downloaded in June 2019 from the Clue.io repository. The Human Metabolome Database (HMDB) and FooDB were also utilized, with data downloaded in April 2020. Plant ingredients were collected from the CMAUP database in July 2019, and cross-species metabolites were obtained from the MetaboLights repository in April 2020.\n\nBenchmark datasets were sourced from MoleculeNet, downloaded in June 2019. Furthermore, compound collections were fetched from the ZINC catalogs in June 2020. The total universe of molecules considered in our study is approximately 800,000.\n\nOur dataset includes a wide range of molecules, each accompanied by bioactivity signatures that evolve over time as more experimental data becomes available. This dynamic nature allows for continuous improvement and updating of the signatures, ensuring that our analyses remain relevant and accurate.\n\nThe data has been previously used in various studies and by the community, contributing to the broader understanding of chemical space and its applications in drug discovery and repurposing. The integration of these diverse datasets enables a robust analysis of molecular similarities and bioactivities, providing valuable insights into the biological properties of compounds.",
  "dataset/splits": "In our study, we utilized an 80:20 train-test split for evaluating the performance of our signaturizers. This means that 80% of the data was used for training the models, while the remaining 20% was reserved for testing their predictive accuracy. The train-test split was applied to various datasets, including those from MoleculeNet classification tasks such as PCBA, MUV, HIV, BACE, BBBP, Tox21, ToxCast, SIDER, and ClinTox.\n\nThe distribution of data points in each split varied depending on the specific dataset. For instance, the PCBA dataset, which is one of the largest, contains 437,929 molecules, with approximately 350,343 molecules in the training set and 87,586 in the test set. Similarly, the MUV dataset includes 93,087 molecules, split into about 74,469 for training and 18,618 for testing. Other datasets, like HIV and BACE, have fewer molecules, with corresponding smaller splits.\n\nIn addition to the standard train-test split, we also considered scenarios where none of the molecules in a triplet (anchor, positive, negative) had been seen during training. This is referred to as the test-test comparison and represents the most challenging case for evaluating the signaturizers' performance. This approach ensures that our models are robust and can generalize well to unseen data.\n\nThe signature-dropout strategy was employed to simulate realistic prediction scenarios where certain signatures might be unavailable. This strategy involved determining signature sampling probabilities based on the coverage of signatures for molecules lacking an experimental signature in the space of interest. This method helps in assessing the models' performance under varying conditions of data availability.",
  "dataset/redundancy": "The datasets were split into training and test sets using an 80:20 ratio. This split was designed to ensure that the training and test sets are independent, meaning that no molecules from the test set were seen during the training phase. This independence was enforced by using only test-test comparisons in our evaluations, where none of the molecules were present in the training data.\n\nThe distribution of active and inactive molecules in the training and test sets was carefully evaluated. The proportion of molecules in each set was considered to ensure a balanced representation. Additionally, the performance of the classifiers was assessed using various metrics, including ROC curves and precision-recall curves, to validate the robustness of the model.\n\nThe signature-dropout sampling scheme was employed to simulate realistic prediction scenarios. This scheme involved dropping out signatures from certain spaces to mimic situations where some signatures might not be available. For example, biological pathway signatures were derived from binding signatures, ensuring that in real prediction cases, certain signatures would not serve as covariates.\n\nThe evaluation of the classifiers included assessing their ability to classify similar and dissimilar compound pairs within triplets. This involved comparing predicted signatures obtained without using specific signatures as part of the input to the corresponding truth signatures. The performance was also evaluated using cross-conformal predictors and other classification scores, such as Cohen’s kappa, precision, recall, and balanced accuracy.\n\nIn summary, the datasets were split to ensure independence between training and test sets, and the distribution was evaluated to maintain balance. The signature-dropout strategy and various performance metrics were used to validate the robustness and applicability of the classifiers.",
  "dataset/availability": "The data and the data splits used in our study are not publicly released in a forum. However, the full repository of the Chemical Checker (CC) is open-sourced and available on GitLab and Zenodo. This repository includes the pre-trained signaturizers, which are light-weight versions of the Signature Neural Networks (SNNs) presented in our work. These signaturizers are available as TensorFlow Hub SavedModel instances and are automatically downloaded by the API the first time they are used. The repository also contains the methods and tools used to generate the signatures and perform the analyses described in the paper.\n\nThe data used in our study includes a wide range of bioactivity signatures obtained for a universe of approximately 800,000 molecules in the CC space. The signatures cover various aspects of chemical and biological information, such as chemical properties, binding signatures, biological pathway signatures, and drug-related signatures. The data splits used in our study were determined based on the coverage of signatures for molecules lacking an experimental signature in the corresponding CC space. The performance of the signaturizers was evaluated using an 80:20 train-test split.\n\nThe license under which the repository is released is not specified in the provided information. However, it is likely that the repository is released under an open-source license that allows for free use, modification, and distribution of the code and data, subject to certain conditions. Users are encouraged to check the repository for more information on the specific license terms and conditions.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is primarily Random Forest classifiers. These are well-established and widely accepted in the machine learning community due to their robustness and performance across various tasks. We chose Random Forest classifiers to ensure a fair comparison between different types of molecular descriptors, as they provide a standard benchmark.\n\nAdditionally, we employed the AutoML TPOT methodology for a more comprehensive evaluation. TPOT automatically performs feature selection, classifier choice, and hyperparameter optimization across a wide array of standard machine learning techniques. This approach ensures a fair, machine-learning-independent comparison between our descriptors and other chemical fingerprints.\n\nThe use of Random Forest classifiers and TPOT allows us to focus on the differences between descriptors rather than the specific machine-learning algorithms. This choice was made to highlight the effectiveness of our new set of small molecule bioactivity descriptors, which are designed to be complementary to currently available chemical descriptors.\n\nThe Random Forest classifiers and TPOT are not new machine-learning algorithms. They are established methods in the field, and their use in our study is to validate the performance of our descriptors rather than to introduce novel machine-learning techniques. Therefore, publishing these algorithms in a machine-learning journal was not necessary, as our primary contribution lies in the development and validation of new bioactivity descriptors.",
  "optimization/meta": "In our study, we employed a meta-predictor approach to enhance the robustness and generalization of our models. This meta-predictor leverages predictions from multiple machine-learning algorithms to make final predictions. Specifically, we used a random forest classifier as our meta-predictor, which aggregates the outputs of various base predictors.\n\nThe base predictors include different types of chemical descriptors and fingerprints, such as ECFPs, Daylight-like fingerprints, MACCS keys, and a deep-learning-based descriptor named CDDD. These descriptors capture diverse aspects of molecular structures and bioactivities, providing a comprehensive representation of the molecules.\n\nTo ensure the independence of training data, we implemented a rigorous cross-validation scheme. We performed 10 stratified splits and used a Mondrian cross-conformal prediction scheme to calibrate our classifiers. Additionally, for large datasets like PCBA, we trained on a maximum of 30 undersampled datasets, each comprising 10,000 samples. Scaffold-aware stratified splits were employed to ensure that Murcko scaffolds observed in the training set were not present in the test set, adhering strictly to MoleculeNet recommendations.\n\nThis approach ensures that the training data for each fold is independent, minimizing the risk of data leakage and overfitting. The use of multiple chemical descriptors and a robust cross-validation strategy enhances the reliability and generalizability of our meta-predictor.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to ensure the effectiveness of our machine-learning algorithms. We utilized various chemical descriptors to represent molecular structures. These included Extended-Connectivity Fingerprints (ECFPs), Daylight-like (RDKit) fingerprints, MACCS keys, and a deep-learning-based descriptor named CDDD. These descriptors were chosen to capture different aspects of molecular structures and bioactivity.\n\nFor the machine-learning tasks, we focused on predicting active/inactive compounds using horizontally stacked chemical signatures. The signatures were binarized due to their bimodal distribution, and correlation was measured using Matthew’s correlation coefficient (MCC) over the true-vs-predicted contingency table.\n\nTo handle the high-dimensional nature of the data, we employed dimensionality reduction techniques such as t-SNE and PCA. These methods projected the signatures into 2D space, allowing for visual assessment of chemical diversity and similarity. For instance, chemical diversity was assessed by averaging the Tanimoto similarity between a molecule and its five nearest neighbors in the 2D space. The resulting diversity measure ranged from 0 (chemically diverse) to 1 (chemically coherent).\n\nIn addition to these preprocessing steps, we performed hyperparameter optimization using HyperOpt over 10 iterations. The optimized parameters included the number of estimators, max depth, minimum sample split, criterion, and maximum features for the random forest classifiers. We also used a Mondrian cross-conformal prediction scheme for classifier calibration and evaluated performance using stratified splits and scaffold-aware stratification to ensure a fair assessment.\n\nFor the AutoML approach using TPOT, feature selection, processing, classifier choice, and hyperparameter optimization were automatically performed across a wide array of standard machine-learning techniques. This provided a fair, ML-independent comparison between our descriptors and other chemical fingerprints. However, due to computational constraints, we could not apply TPOT to the largest MoleculeNet subtasks, such as PCBA, which involve hundreds of thousands of molecules.\n\nOverall, our data encoding and preprocessing steps were designed to maximize the informativeness of the molecular descriptors and ensure robust and reliable performance of the machine-learning algorithms.",
  "optimization/parameters": "In our study, the number of parameters (p) used in the model varies depending on the specific task and the descriptors employed. For our bioactivity descriptors, the dimensionality is quite high, with each molecule represented by a vector of 3,200 dimensions. This high dimensionality allows for a comprehensive capture of bioactivity information.\n\nThe selection of p was not arbitrary but rather a result of extensive experimentation and validation. We employed a rigorous approach to ensure that the dimensionality of our descriptors was optimal for the tasks at hand. This involved using techniques such as t-SNE for dimensionality reduction and visualization, which helped us to project the high-dimensional data into a 2D space for analysis. Additionally, we performed feature selection and processing using the AutoML TPOT methodology, which automatically optimizes feature selection, classifier choice, and hyperparameter tuning across a wide array of standard machine learning techniques.\n\nTo ensure robustness, we used 20 iterations of Bayesian Hyperparameter Optimization (HyperOpt) for every model and for each type of descriptor. Multiple train:test splits were conducted using 5-fold cross-validation, performed in triplicates, along with a 10-fold Mondrian cross-conformal adaptation to estimate prediction confidence. This extensive validation process helped us to focus on the differences between descriptors rather than the effects of hyperparameters, ensuring that our models were both reliable and generalizable.",
  "optimization/features": "In our study, we utilized horizontally stacked CC signatures as input features for our classification tasks. These signatures are composed of 3,200 dimensions. Feature selection was performed using the AutoML TPOT methodology, which automatically handles feature selection and processing. This approach ensures that the selection process is data-driven and optimized for the specific task at hand. Importantly, the feature selection was conducted using only the training set, adhering to best practices to prevent data leakage and maintain the integrity of the evaluation process. This methodology allows for a fair comparison between our descriptors and other chemical fingerprints, providing a robust assessment of their explanatory potential.",
  "optimization/fitting": "The fitting method employed in our study involved several strategies to address potential overfitting and underfitting issues. Given the complexity of the models and the large number of parameters, particularly in the context of high-dimensional feature spaces, careful consideration was given to these aspects.\n\nFor models with a large number of parameters relative to the number of training points, we implemented regularization techniques such as dropout. Dropout was applied during the training of our neural networks, which helped to prevent overfitting by randomly setting a fraction of the input units to zero at each update during training time. This technique ensures that the model does not become too reliant on any single feature, thereby improving generalization to unseen data.\n\nAdditionally, we used cross-validation to assess the model's performance and to ensure that it was not underfitting. Cross-validation involved splitting the data into multiple folds and training the model on different subsets while validating on the remaining data. This process was repeated multiple times, and the results were averaged to provide a more robust estimate of the model's performance. By doing so, we could verify that the model was capturing the underlying patterns in the data without being too simplistic.\n\nFurthermore, we employed early stopping during the training process. Early stopping monitors the model's performance on a validation set and halts training when the performance stops improving. This technique helps to prevent overfitting by ensuring that the model does not continue to train beyond the point where it starts to memorize the training data.\n\nIn summary, through the use of regularization techniques like dropout, cross-validation, and early stopping, we effectively managed to address both overfitting and underfitting concerns in our models. These strategies ensured that our models were robust, generalizable, and capable of capturing the essential patterns in the data without becoming overly complex or simplistic.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and ensure the robustness of our models. One key method used was Gaussian dropout, applied after the input layer. This technique randomly sets a fraction of the input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not become too reliant on any single feature.\n\nAdditionally, we utilized alpha-dropout regularization in the hidden layers with a dropout rate of 0.2. This form of dropout is particularly effective for models using the Scaled Exponential Linear Unit (SeLU) activation function, as it helps to maintain the self-normalizing properties of the network.\n\nAnother important regularization technique we implemented was signature dropout. This method simulates the availability of certain signatures at prediction time by masking input data during training. The probability of masking each signature was determined based on the observed availability of experimental signatures for molecules not in the training set. This approach helps the model to generalize better by learning to handle missing data.\n\nFurthermore, we incorporated a global orthogonal regularization term in our loss functions. This regularization encourages the embeddings to be maximally spread out in the embedding space, which helps to improve the discriminative power of the model and reduces the risk of overfitting.\n\nLastly, we used L2-normalization in the output layer, which helps to constrain the magnitude of the output embeddings, further aiding in the prevention of overfitting. Together, these regularization techniques contributed to the development of a robust and generalizable model.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available for public access. The software for generating the signatures used in our study is open-sourced and can be found as a Python package. This package includes the signaturizers, which are lightweight versions of the self-normalizing networks (SNNs) presented in our work. These signaturizers allow users to convert molecules, represented as SMILES or InChI strings, into the 25 signature types available from the Chemical Checker (CC). The signaturizers are available as TensorFlow Hub SavedModel instances and are automatically downloaded by the API the first time they are used.\n\nAdditionally, the full CC repository is open-sourced and can be accessed. This repository contains the complete set of tools and models used in our study, ensuring reproducibility and allowing other researchers to build upon our work. The repository is available on GitLab and also on Zenodo, providing a stable and citable version of the code.\n\nThe data used in our study, including the inferred signatures, is also available for download. The latest version of the inferred signatures can be directly downloaded from the Chemical Checker website. Additional data supporting the findings of this study are available from the corresponding author upon reasonable request.\n\nRegarding the optimization parameters and schedule, these details are included in the supplementary information and the methods section of our publication. We used a random forest classifier with hyperparameters identified through HyperOpt over 10 iterations. The evaluation was done with five stratified 80:20 train-test splits, and large datasets were trained on a maximum of 30 under-sampled datasets, each comprising 10,000 samples. Scaffold-aware stratified splits were used to ensure that Murcko scaffolds observed in the training set were not present in the test set, following MoleculeNet recommendations strictly.\n\nThe code and data are licensed under permissive terms, allowing for use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This ensures that the community can freely access and utilize the resources we have developed.",
  "model/interpretability": "The model employed in our study is not a black-box system. We have implemented strategies to obtain high-level explanations for predicted activities. Specifically, for each molecule, we measured the cumulative explanatory potential using Shapley values across the GSig space. This approach indicates the classes of data (such as chemistry, targets, etc.) that were most determinant for the classifier's decision. This method allows us to interpret the outcome at the signature-type level.\n\nFor instance, in the heat shock factor response element (SR-HSE) task within the Tox21 panel, we observed that certain signature types, such as HTS bioassays and cell morphology data, were more informative than others. This finding highlights the model's ability to provide insights into which types of data are most influential in making predictions. By aggregating Shapley values, we can rank features by their importance, providing a clear understanding of the model's decision-making process. This transparency is crucial for validating the model's predictions and ensuring that the results are interpretable and reliable.",
  "model/output": "The model employed in our study is primarily a classification model. We utilized a logistic regression classifier to identify compounds with potential luciferase activity. The classifier was trained and evaluated using various molecular descriptors, including GSig and ECFP4. The performance of the classifier was assessed using metrics such as the area under the ROC curve (ROC-AUC) and the F1-score. These metrics were reported for both training and test splits, demonstrating the model's effectiveness in distinguishing between active and inactive compounds.\n\nAdditionally, we explored the use of other descriptors like MFp and CDDD, and employed a model-agnostic approach using the AutoML TPOT methodology for a fair comparison. The model's output includes the probability of luciferase interaction for tested compounds, which was calculated using the classifier trained on GSigs. This probability helps in identifying the likelihood of compounds interfering with luciferase activity.\n\nThe model's predictions were further validated through experimental measures, where the effect of top compounds on luciferase activity was tested using MDA-MB-231 cells expressing Firefly and Renilla luciferases. The results indicated that the probability of the active molecules interfering with luciferase activity was very small, aligning with the classifier's predictions.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the methods presented in this publication is publicly available. This allows users to implement and utilize the algorithms described in the paper. Additionally, lightweight versions of the signaturizers, which are simplified versions of the presented models, are available as TensorFlow Hub SavedModel instances. These signaturizers can be automatically downloaded by the API the first time they are used, eliminating the need for users to set up the full version of the model.\n\nThe full repository is open-sourced and can be accessed at a specified URL. This repository includes all the necessary components and documentation to run the algorithms. Furthermore, the repository is also available on Zenodo, ensuring long-term accessibility and version control.\n\nThe software is released under a permissive license, allowing for use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source is cited. This license facilitates collaboration and further development by the scientific community.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and validity. We conducted a 5-fold cross-validation on the Tox21 benchmark dataset, which allowed us to assess the performance of our predictors in a systematic and unbiased manner. During this process, prediction scores were assigned to each active molecule, and the relative difference between our bioactivity fingerprint scores (CC scores) and classic Morgan fingerprint scores (MFp scores) was visualized using a color scale. This helped in identifying instances where our method outperformed traditional approaches.\n\nIn addition to cross-validation, we performed y-scrambling experiments to validate our method critically. This involved calculating global signatures (GSigs) for randomized Morgan fingerprints to assess whether the resulting GSigs maintained any structure or signal. The results showed that scrambled signatures were much more similar to each other and exhibited shorter distances compared to the GSigs, confirming the validity of our approach.\n\nWe also conducted time-series validation to evaluate the ability of our signaturizers to identify similar molecules to compounds not yet annotated when the signaturizers were derived. This validation demonstrated that our method could identify similar molecules for a significant fraction of new compounds, especially when more stringent applicability thresholds were used.\n\nFurthermore, we compared our descriptors' performance against various chemical fingerprints using the MoleculeNet dataset. Our descriptors consistently outperformed the chemical fingerprints, and we explored additional chemical fingerprints and applied automated machine learning techniques to further validate our findings. The results of these evaluations are summarized in supplementary figures and described in detail in the Results section and the Online Methods.\n\nOverall, our evaluation method combined cross-validation, y-scrambling experiments, time-series validation, and comparisons with existing chemical fingerprints to provide a comprehensive assessment of our bioactivity descriptors' performance.",
  "evaluation/measure": "In the evaluation of our bioactivity descriptors, we have employed a range of performance metrics to thoroughly assess the effectiveness of our methods. These metrics include triplet-resolving accuracy, which measures the ability of our signatures to distinguish between different compounds. We also report Matthews correlation coefficient (MCC) scores, which provide a balanced measure of the quality of binary classifications, considering true and false positives and negatives.\n\nFor evaluating the capacity of our chemistry signatures to recapitulate nearest neighbors, we use the area under the receiver operating characteristic curve (AUROC). This metric is crucial for understanding how well our signatures can identify similar compounds based on known descriptors.\n\nIn addition, we present various classification scores such as precision, recall, balanced accuracy, and Cohen’s kappa. These scores are derived from the evaluation of our SR-HSE random forest classifier and are presented through receiver operating characteristic (ROC) curves and precision-recall curves. These metrics help in understanding the trade-offs between true positive rates and false positive rates, as well as the precision and recall of our predictions.\n\nFurthermore, we evaluate the performance of our Tox21 models using ROC-AUC at different training set sizes, comparing the use of GSigs and MFps as feature vectors. This comparison allows us to assess the robustness and generalizability of our models across varying data sizes.\n\nOverall, the set of metrics we report is comprehensive and representative of standard practices in the literature. They provide a holistic view of the performance of our bioactivity descriptors, ensuring that our methods are rigorously evaluated and comparable to existing approaches in the field.",
  "evaluation/comparison": "A comparison to publicly available methods was indeed performed on benchmark datasets. Specifically, the study compared the performance of bioactivity fingerprints against classical fingerprints using the MoleculeNet benchmark datasets. This comparison was conducted following MoleculeNet recommendations, ensuring a fair assessment through scaffold-based splitting and performance measures like AUROC and AUPR.\n\nThe comparison included various classical fingerprints such as MACCS, Daylight-like, ECFP4, and more modern descriptors based on deep learning, like the CDDD. Additionally, the study considered graph-based classifiers, which are known for their high performance in supervised prediction tasks. The results showed that even in challenging scenarios, the bioactivity fingerprints outperformed the chemical fingerprints.\n\nTo ensure a fair comparison, all models were trained using the same machine learning algorithm, a random forest classifier. This approach allowed for a direct comparison between the bioactivity fingerprints and chemical representations, removing the effect of the classifier from the analysis.\n\nThe study also performed extensive additional training cycles, adding new fingerprints and model-agnostic validations beyond random forest classifiers. This substantial computational effort aimed to provide systematic evidence of the added value of bioactivity fingerprints compared to chemistry-based descriptors.\n\nIn summary, the evaluation included a thorough comparison with publicly available methods on benchmark datasets, ensuring a rigorous and fair assessment of the bioactivity fingerprints' performance.",
  "evaluation/confidence": "The evaluation of our method includes several statistical measures to ensure confidence in the results. We employed linear regression models to assess correlations between accuracy metrics, such as the Matthews correlation coefficient (MCC) and triplet accuracy, for both train-train and test-test validations. These models include 95% confidence intervals, providing a range within which the true correlation likely falls.\n\nAdditionally, we utilized a conformal prediction scheme that relates prediction scores to a measure of confidence. This approach helps in understanding the reliability of our predictions. We also performed Y-scrambling, a standardized statistical procedure that scrambles the outcome, not the descriptor, to validate the significance of our results.\n\nTo ensure the robustness of our findings, we conducted experiments using different machine-learning methods, including a random forest classifier and an AutoML methodology. This model-agnostic approach confirms that the observed trends are not dependent on a specific classification algorithm.\n\nFurthermore, we strictly followed MoleculeNet recommendations for splitting datasets and performance measures, such as AUROC/AUPR, to ensure a fair assessment. We compared our method's performance with popular baselines like the Morgan fingerprint (MFp) and other chemical descriptors, demonstrating incremental improvements.\n\nThe statistical significance of our method is supported by the consistent outperforming of conventional chemical fingerprints in various tasks, as well as the robustness of our classifier to successive removal of training data. This suggests that our method requires fewer data to achieve proficient models, indicating its efficiency and reliability.",
  "evaluation/availability": "The raw evaluation files are available as supplementary or external data files. These files contain relevant information that complements the main manuscript. For instance, Data S1 is mentioned as a source of crucial evaluation details. The data is publicly released under a Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, provided that appropriate credit is given to the original authors and the source. A link to the Creative Commons license must be included, and any changes made should be indicated. The images or other third-party material in the article are included in the article’s Creative Commons license, unless otherwise specified. For material not covered by the article’s Creative Commons license, permission must be obtained directly from the copyright holder if the intended use is not permitted by statutory regulation or exceeds the permitted use."
}