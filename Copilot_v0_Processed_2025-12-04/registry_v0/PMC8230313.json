{
  "publication/title": "Identification of Circulating Diagnostic Biomarkers for Coronary Microvascular Disease in Postmenopausal Women Using Machine-Learning Techniques",
  "publication/authors": "The authors who contributed to this article are:\n\n- Z.M.E. contributed to conceptualization, resources, writing—original draft preparation, writing—review and editing, visualization, supervision, project administration, and funding acquisition.\n- E.T. contributed to conceptualization, methodology, resources, writing—original draft preparation, writing—review and editing.\n- A.A.E. contributed to methodology, formal analysis, writing—original draft preparation, visualization.\n- H.E.Y. contributed to methodology.\n- S.V.E. contributed to methodology.\n- F.A.A. contributed to methodology.\n- Y.-J.L. contributed to software, formal analysis, visualization.\n- L.M. contributed to software.\n- J.Ž. contributed to software.\n- S.A. contributed to formal analysis.\n\nAll authors have read and agreed to the published version of the manuscript.",
  "publication/journal": "Metabolites",
  "publication/year": "2021",
  "publication/doi": "10.3390/metabo11060339",
  "publication/tags": "- Coronary Microvascular Disease\n- Postmenopausal Women\n- Diagnostic Biomarkers\n- Machine Learning\n- Metabolomics\n- Gas Chromatography-Mass Spectrometry\n- Biomarker Identification\n- Cardiovascular Disease\n- Metabolic Profiling\n- Clinical Studies",
  "dataset/provenance": "The dataset used in this study was sourced from a prospective observational cohort study involving 70 patients. These patients were categorized into three groups: 23 diagnosed with coronary microvascular dysfunction (CMD), 21 diagnosed with coronary artery disease (CAD), and 26 defined as the control group. The study was approved by the Izmir Katip Celebi University Interventional Clinical Studies Institutional Review Board and conducted in compliance with the Helsinki Declaration. Participants provided consent for the use of their specimens and medical data to determine biomarkers of CMD in postmenopausal women.\n\nThe data collection process included detailed anamnesis, physical examinations, body mass index (BMI) measurements, medications, transthoracic echocardiograms, blood pressure measurements, and routine biochemical examinations. These examinations included measurements of total cholesterol (TC), high-density lipoprotein (HDL) cholesterol, low-density lipoprotein (LDL) cholesterol, triglyceride, glucose, urea, creatinine, aspartate transaminase (AST), alanine transaminase (ALT), hemoglobin (Hb), white blood cell (WBC), mean corpuscular volume (MCV), and platelet (PLT) levels.\n\nBlood samples were obtained from patients who underwent coronary angiography, which was performed using the Judkins technique through the femoral or radial artery. Plasma samples prepared from these blood samples were stored at −80°C for further processing. The study focused on detecting and quantifying circulating metabolites in plasma using gas chromatography-mass spectrometry (GC/MS) analysis. This analysis was performed at the UIUC Metabolomics Center, following established protocols.\n\nThe raw data consisted of 175 metabolites measured across the 70 patients. This dataset was normalized using min/max scaling and underwent data imputation to handle missing values. Metabolic feature columns with more than 40% missing data were eliminated. An iterative imputer was trained to infill missing values, and the performance of the imputer was measured using the R2 metric. Metabolite columns with an R2 score less than 0.3 and more than 5% missing values were excluded. The final dataset, after preprocessing, included 75 metabolites, which were used for further analysis.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "Not enough information is available.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is the random forest classifier. This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees.\n\nThe random forest algorithm is not new; it has been widely used and studied in the machine-learning community for many years. It is a robust and versatile method that is known for its ability to handle high-dimensional data and provide good generalization performance.\n\nThe reason it was not published in a machine-learning journal is that our primary focus was on applying this algorithm to identify biomarkers associated with coronary microvascular dysfunction (CMD) and coronary artery disease (CAD). The novelty of our work lies in the application of this established machine-learning technique to a specific biomedical problem, rather than in the development of a new algorithm. Our study contributes to the field of metabolomics and cardiovascular research by demonstrating the potential of random forest classifiers in identifying metabolic biomarkers for CMD and CAD. The algorithm was used as part of a broader analytical pipeline that included data preprocessing, feature selection, and classification. The recursive feature elimination with cross-validation (RFECV) method was employed to select the most relevant metabolites, and the random forest classifier was then used to build a predictive model based on these selected features. The performance of the classifier was evaluated using metrics such as the area under the curve (AUC) and the F1 score, and the results were visualized using receiver operating characteristic (ROC) and precision-recall (PR) curves.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. Instead, it employs a single machine-learning approach, specifically the random forest algorithm, for classification tasks. The random forest algorithm was used to test the classification performance of various metabolite sets, including stearic acid only, ornithine only, a combination of stearic acid and ornithine, and a set of 15 metabolites selected through recursive feature elimination with cross-validation (RFECV).\n\nThe RFECV algorithm was utilized to iteratively compute cross-validation scores by eliminating metabolite feature columns, aiming to identify the optimal set of features for classification. This process helped in selecting the 15 metabolites that provided the highest classification performance.\n\nThe training and testing of the model were conducted using 5-fold cross-validation with a fixed random seed, ensuring that the data was split independently for each fold. This method helps in assessing the model's performance and generalizability by using different subsets of the data for training and validation.\n\nNot applicable.",
  "optimization/encoding": "The raw data consisted of 175 metabolites measured across 70 patients. Initially, the data was normalized using min/max scaling to ensure all features were on the same scale. To handle missing data, an iterative imputer was employed, which iteratively estimated and filled in missing values. Metabolite features with more than 40% missing data were eliminated. The performance of the imputer was evaluated using the R2 measure, and features with an R2 score less than 0.3 and more than 5% missing values were removed. This preprocessing step reduced the feature set to 75 metabolites.\n\nFollowing this, data standardization was performed by removing the mean and scaling the variance to achieve zero mean and unit variance for each metabolite feature. This standardized data was then used for further analysis. Feature selection was conducted using the recursive feature elimination with cross-validation (RFECV) method, with a random forest classifier serving as the estimator and 5-fold cross-validation for validation. This process identified the most relevant metabolites for classification, ultimately selecting 15 metabolites that provided the highest classification performance.",
  "optimization/parameters": "The model utilized 15 metabolites as input parameters. This number was selected through the use of the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm. The RFECV algorithm iteratively computed the cross-validation score each time it eliminated a metabolite feature column. The classification achieved significantly better scores when more than 10 features were selected, with the highest score obtained when 15 metabolites were chosen. This process ensured that the selected features provided the best classification performance for the given dataset.",
  "optimization/features": "The input features used in our study were metabolites measured across patients. Initially, the raw data consisted of 175 metabolites. However, through a series of preprocessing steps, including normalization, data imputation, and standardization, the number of features was reduced. Metabolite columns with more than 40% missing data were eliminated, and an iterative imputer was used to handle the remaining missing values. Features with an R2 measure less than 0.3 and more than 5% missing values were also removed. This preprocessing step ultimately reduced the feature columns to 75 metabolites.\n\nFeature selection was performed on the preprocessed dataset using the recursive feature elimination with cross-validation (RFECV) algorithm. This process involved iteratively computing the cross-validation score each time a metabolite feature column was eliminated. The classification achieved significantly better scores when more than 10 features were selected, with the highest score obtained when 15 metabolites were chosen. Therefore, the final set of input features consisted of 15 metabolites, which were used for further analysis and classification tasks. The feature selection process was conducted using the training set only, ensuring that the selected features were not influenced by the test data.",
  "optimization/fitting": "The study involved a dataset consisting of 175 metabolites measured across 70 patients. Initially, the number of features (metabolites) was indeed much larger than the number of training points (patients). To address potential overfitting, several preprocessing and feature selection steps were undertaken.\n\nFirst, data normalization using min/max scaling was performed, followed by data imputation to handle missing values. Metabolite features with more than 40% missing data were eliminated. An iterative imputer was then trained to infill missing values, and features with an R2 measure less than 0.3 and more than 5% missing values were removed. This preprocessing step reduced the feature columns to 75 metabolites.\n\nNext, feature selection was performed using the recursive feature elimination with cross-validation (RFECV) algorithm. This iterative process computed the cross-validation score each time a metabolite feature column was eliminated, ensuring that only the most relevant features were retained. The classification performance significantly improved when more than 10 features were selected, with the highest score achieved using 15 metabolites.\n\nTo further validate the model and rule out underfitting, a binary classification was implemented using the random forest algorithm. The binary labels were \"CMD\" and \"non-CMD,\" combining both the CAD and control group samples. The training and testing were conducted using 5-fold cross-validation with a fixed random seed. This approach helped in assessing the model's generalizability and ensuring that it was not too simplistic to capture the underlying patterns in the data.\n\nThe performance was measured and visualized using receiver operating curves (ROC) and precision-recall (PR) curves. The mean area under the curve (AUC) and mean F1 score were computed for different metabolite sets, including stearic acid only, ornithine only, stearic acid and ornithine together, and the 15 metabolites selected by RFECV. The results indicated that the model was able to achieve a balanced performance, neither overfitting nor underfitting the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting during the machine learning analysis. One of the key methods used was recursive feature elimination with cross-validation (RFECV). This technique iteratively removes the least important features and builds a model on those remaining features, using cross-validation to score the model. By doing so, it helps in selecting the most relevant features and reduces the risk of overfitting.\n\nAdditionally, we utilized 5-fold cross-validation during the training and testing of our models. This approach ensures that the model is evaluated on different subsets of the data, providing a more robust estimate of its performance and helping to prevent overfitting.\n\nWe also performed data normalization and standardization as part of our preprocessing steps. Normalization using min/max scaling and standardization involving mean removal and variance scaling helped in ensuring that all features contributed equally to the model, further mitigating the risk of overfitting.\n\nMoreover, we handled missing data through imputation techniques, which involved using an iterative imputer to fill in missing values. This step is crucial as it ensures that the model is trained on a complete dataset, reducing the likelihood of overfitting due to incomplete data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are not explicitly detailed in the main text. However, the methodology section provides insights into the machine learning processes employed. For instance, we utilized the recursive feature elimination with cross-validation (RFECV) function within the sklearn feature selection module, with a random forest classifier serving as the estimator and 5-fold cross-validation for validation. The preprocessing steps, including normalization, data imputation, and standardization, are also described, which are crucial for understanding the optimization process.\n\nRegarding model files and optimization parameters, these specific details are not provided in the main text. The supplementary materials, available online, include additional figures that might offer further insights into the optimization process, such as the results of the RFECV algorithm and the relative abundance of selected metabolites. However, explicit model files or detailed optimization parameters are not made available in the main text or supplementary materials.\n\nFor access to the supplementary materials, they can be found online at the provided link. The license under which these materials are available is not specified in the provided information.",
  "model/interpretability": "The model employed in this study is not a blackbox. It utilizes a random forest algorithm, which is inherently interpretable. Random forests are ensembles of decision trees, and each tree can be visualized to understand the decision-making process. This transparency allows for the identification of the most important features contributing to the classification.\n\nFor instance, the recursive feature elimination with cross-validation (RFECV) algorithm was used to select the most relevant metabolites. This process iteratively computed cross-validation scores while eliminating less important features, ultimately identifying a set of 15 metabolites that provided the highest classification performance. This subset of metabolites can be examined to understand their biological significance and how they contribute to the classification of CMD versus non-CMD.\n\nAdditionally, the study identified specific metabolites such as stearic acid and ornithine as significant indicators of CMD. The performance of these metabolites, both individually and in combination, was evaluated using receiver operating curves (ROC) and precision-recall (PR) curves. This detailed analysis provides clear examples of how the model's decisions can be interpreted and validated.\n\nFurthermore, the use of an unpaired t-test to identify significantly different metabolites between CMD vs. control and CAD vs. control groups adds another layer of interpretability. This statistical method highlights specific metabolites that differ significantly between groups, providing a clear understanding of the biological differences being captured by the model.\n\nIn summary, the model's transparency is evident through the use of interpretable algorithms like random forests and statistical methods like t-tests. The identification of key metabolites and their contributions to the classification process allows for a clear understanding of the model's decision-making, making it a valuable tool for biological interpretation.",
  "model/output": "The model employed in this study is a classification model. Specifically, a random forest algorithm was used for binary classification to distinguish between \"CMD\" and \"non-CMD\" groups. The classification performance was evaluated using metrics such as the area under the curve (AUC) and the F_1 score. The model's performance was assessed using 5-fold cross-validation, ensuring robustness in the classification results. Various sets of metabolites, including stearic acid, ornithine, a combination of both, and a signature set of 15 metabolites, were tested to identify the most effective biomarkers for CMD classification. The highest classification performance was achieved with the 15-metabolite signature, indicating its potential as a biomarker set for CMD diagnosis.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure robust and reliable results. Initially, the raw data, consisting of 175 metabolites measured across 70 patients, underwent normalization using min/max scaling. Data imputation was then employed to handle missing values, with feature columns having more than 40% missing data being eliminated. An iterative imputer was trained to infill missing values, and its performance was measured using the R2 metric. Metabolite columns with an R2 score less than 0.3 and more than 5% missing values were removed. The final preprocessing step involved data standardization, achieving zero mean and unit variance for each metabolite feature column, reducing the dataset to 75 metabolites.\n\nFor feature selection, the recursive feature elimination with cross-validation (RFECV) algorithm was utilized. This algorithm iteratively computed the cross-validation score each time it eliminated a metabolite feature column. The classification performance significantly improved when more than 10 features were selected, with the highest score achieved using 15 metabolites.\n\nTo test the classification performance, a binary classification was implemented using the random forest algorithm. The binary labels were \"CMD\" and \"non-CMD,\" combining both the CAD and control group samples. The classification performance was evaluated using 5-fold cross-validation with a fixed random seed. The results were visualized using receiver operating curves (ROC) and precision-recall (PR) curves. The mean area under the curve (AUC) and mean F1 score were computed for different metabolite sets: stearic acid only, ornithine only, stearic acid and ornithine together, and the 15 metabolites selected by RFECV. The highest mean AUC of 0.63 and mean F1 score of 0.40 were obtained using the 15 metabolites set.",
  "evaluation/measure": "In the evaluation of our biomarker signature set, we focused on several key performance metrics to assess the classification effectiveness. The primary metrics reported are the mean area under the curve (AUC) and the mean F_1 score, both of which are crucial for evaluating the performance of binary classifiers.\n\nThe AUC provides a comprehensive measure of the classifier's ability to distinguish between the two classes, \"CMD\" and \"non-CMD\". It ranges from 0 to 1, where a higher value indicates better performance. We computed the mean AUC for different sets of metabolites, including stearic acid only, ornithine only, a combination of stearic acid and ornithine, and a set of 15 metabolites selected by the RFECV algorithm. The highest mean AUC was observed for the 15-metabolite set, indicating its superior discriminative power.\n\nThe mean F_1 score, which is the harmonic mean of precision and recall, offers a balanced measure of the classifier's accuracy, especially when dealing with imbalanced datasets. We reported the mean F_1 scores for the same metabolite sets, providing insights into the classifier's precision and recall trade-offs. The mean F_1 scores were relatively consistent across different metabolite sets, with slight variations.\n\nThese metrics are widely used in the literature for evaluating classification models, particularly in biomedical research. They provide a robust framework for comparing the performance of different biomarker sets and ensuring that the selected features are effective in distinguishing between the target classes. The use of 5-fold cross-validation with a fixed random seed further enhances the reliability of our performance measures, ensuring that the results are not dependent on a particular data split.",
  "evaluation/comparison": "In our study, we did not perform a direct comparison to publicly available methods on benchmark datasets. Instead, our focus was on developing and evaluating a biomarker signature set specific to our dataset, which consisted of 175 metabolites measured across 70 patients. We employed the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm to iteratively select the most relevant features, ultimately identifying a set of 15 metabolites that provided the highest classification performance.\n\nRegarding simpler baselines, we did compare the classification performance of individual metabolites and combinations thereof. Specifically, we evaluated the performance of stearic acid alone, ornithine alone, and the combination of stearic acid and ornithine. These comparisons served as simpler baselines against which we could measure the improvement provided by our selected set of 15 metabolites. The results indicated that the set of 15 metabolites achieved the highest mean area under the curve (AUC) of 0.63 and a mean F_1 score of 0.40, outperforming the simpler baselines.\n\nOur approach involved using a random forest algorithm for binary classification, with the binary labels being \"CMD\" and \"non-CMD\" (which combined both the CAD and control group samples). The training and testing were conducted using 5-fold cross-validation with a fixed random seed to ensure the robustness of our results. The performance was measured and visualized using receiver operating curves (ROC) and precision-recall (PR) curves.\n\nIn summary, while we did not compare our method to publicly available methods on benchmark datasets, we did perform comparisons to simpler baselines using individual and combined metabolites. This allowed us to demonstrate the effectiveness of our selected biomarker signature set in classifying CMD versus non-CMD samples.",
  "evaluation/confidence": "The evaluation of our method includes several performance metrics, each accompanied by confidence intervals to provide a clear understanding of their reliability. For instance, the mean area under the curve (AUC) and mean F_1 score for different metabolite sets were computed with their respective standard deviations. This approach allows us to assess the variability and consistency of our results.\n\nStatistical significance was a key consideration in our analysis. We used an unpaired t-test to identify metabolites that differed significantly between groups, with p-values indicating the level of significance. For example, stearic acid and ornithine were found to be significantly different indicators of CMD, while valine was statistically different in the CAD group compared to the control group.\n\nIn our classification tasks, we employed 5-fold cross-validation to ensure that our results were robust and not due to overfitting. The random forest algorithm was used to test the classification performance of various metabolite sets, and the results were visualized using receiver operating curves (ROC) and precision–recall (PR) curves. The mean AUC and F_1 scores, along with their standard deviations, provide a comprehensive view of the model's performance and its statistical significance.\n\nAdditionally, we used the recursive feature elimination with cross-validation (RFECV) algorithm to select the most informative metabolites. This iterative process helped us identify that classification scores improved significantly when more than 10 features were selected, with the highest score achieved using 15 metabolites. This further supports the statistical significance of our method and its superiority over using fewer metabolites.\n\nOverall, the inclusion of confidence intervals and the use of statistical tests ensure that our performance metrics are reliable and that our method's superiority can be confidently claimed.",
  "evaluation/availability": "Not applicable"
}