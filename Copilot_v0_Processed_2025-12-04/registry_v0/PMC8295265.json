{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to this article are:\n\n- L.K. who conceptualized and administered the study, drafted the manuscript, and revised and approved the final draft.\n- G.H. who designed and implemented the deep network predictor, collected and annotated datasets, carried out experiments, and analyzed results.\n- K.W. who designed and implemented the deep network predictor, collected and annotated datasets, carried out experiments, and analyzed results.\n- Z.W. who designed and implemented the deep network predictor, collected and annotated datasets, carried out experiments, and analyzed results.\n- J.G. who designed and implemented the deep network predictor, carried out experiments, and analyzed results.\n- A.K. who designed and implemented the random forest predictors, collected and annotated datasets, carried out experiments, analyzed results, implemented and deployed the webserver, and revised and approved the final draft of the manuscript.\n- S.G. who implemented and deployed the webserver.",
  "publication/journal": "Nature Communications",
  "publication/year": "2021",
  "publication/doi": "10.1038/s41467-021-24773-7",
  "publication/tags": "- Intrinsic disorder prediction\n- Deep learning\n- Machine learning\n- Protein sequence analysis\n- Bioinformatics\n- Random forest\n- DisProt database\n- Statistical significance\n- Cross-validation\n- Computational biology\n\nNot sure if the above tags are the ones provided in the published article.",
  "dataset/provenance": "The dataset used in this study is primarily sourced from the DisProt database, specifically version 7.0. This database is a well-known resource in the community for experimentally annotated disordered proteins. The test set was initially a random sampling from DisProt 7.0, but it was later filtered to remove proteins that share over 25% sequence similarity with the training proteins. This filtering resulted in a test dataset of 176 proteins, down from the original 200.\n\nThe training, validation, and test datasets are available for public access at http://biomine.cs.vcu.edu/servers/flDPnn/. The CAID dataset, which was also utilized, is available on the DisProt website at https://www.disprot.org/. The CAID dataset includes proteins where amino acids are labeled as structured unless they are experimentally annotated as disordered. Disorder function annotations were extracted from DisProt.\n\nThe datasets used in this study are consistent with those used in previous comparative assessments, such as CAID and other similar experiments. The training datasets of the participating disorder predictors were also used, following the same labeling conventions where amino acids in test proteins are labeled as structured unless they are experimentally annotated as disordered. This approach ensures that the datasets are comparable and that the results can be reliably assessed against other methods in the field.",
  "dataset/splits": "The dataset used in this study includes three main splits: training, validation, and test datasets. Initially, the test set consisted of 200 proteins randomly sampled from the DisProt 7.0 database. However, to address concerns about sequence similarity and potential overfitting, the test set was refined. Proteins in the test set that shared over 25% sequence similarity with the training proteins were removed. This filtering process reduced the test set to 176 proteins, as 24 of the original test proteins were found to be similar to the training set.\n\nThe training and validation datasets are not explicitly detailed in terms of the number of data points, but they are designed to cover a broad range of proteins to ensure comprehensive training and validation of the predictive models. The datasets are available for public access, allowing for reproducibility and further analysis.\n\nThe distribution of data points in each split is designed to ensure that the models are trained and validated on diverse and representative samples, while the test set is used to evaluate the final performance of the models. The refinement of the test set to reduce sequence similarity helps in providing a more robust assessment of the models' predictive performance.",
  "dataset/redundancy": "The datasets used in our study were derived from the DisProt 7.0 database. Initially, the test set was a random sampling from this database. However, to address concerns about potential overfitting, we implemented a filtering process to ensure that the test proteins did not share more than 25% sequence similarity with the training proteins. This step was crucial to maintain the independence of the training and test sets, thereby avoiding any bias that could disadvantage other methods.\n\nThe original test set consisted of 200 proteins, but after applying the similarity filter, it was reduced to 176 proteins. This adjustment was made to ensure that the test dataset was representative and independent of the training data. We did not perform 10-fold cross-validation due to the significant computational cost associated with training deep neural networks. Instead, we relied on dropout techniques to mitigate overfitting and ensured that the test data was never used to optimize the network.\n\nThe distribution of our datasets aligns with the principles of previously published machine learning datasets, emphasizing the importance of independent and representative splits. This approach helps in achieving robust and generalizable results, which are essential for the reliability of our predictions.",
  "dataset/availability": "The data used in this study, including the data splits, are not publicly released in a forum. The test set was originally a random sampling from the DisProt 7.0 database, but it was later filtered to remove proteins that share over 25% sequence similarity with the training proteins. This similarity-reduced test dataset, which includes 176 proteins, is not made publicly available.\n\nThe procedure to collect the similarity-reduced test dataset is explained in the \"Datasets\" section of the manuscript. However, the actual dataset itself is not provided in a public forum or under a specific license. This decision was likely made to maintain the integrity of the test set and prevent overfitting by other researchers.\n\nThe enforcement of this data policy is not explicitly stated, but it is implied that the authors have kept the test dataset private to ensure fair comparison and to prevent other methods from being disadvantageously overfitted to the same data. The focus is on providing the methodology and results, rather than the raw data itself.",
  "optimization/algorithm": "The machine-learning algorithm class used for predicting disorder is deep feedforward neural networks. This choice was motivated by the popularity and success of deep learning in the field of disorder prediction. The architecture consists of an input layer with 318 nodes, corresponding to the number of features, followed by a series of dropout layers and hidden layers. The dropout layers, with a dropout rate of 0.2, are included to prevent overfitting. The hidden layers progressively reduce in size, with 64 nodes in the first hidden layer and 8 nodes in the second, culminating in an output layer with one node that produces the disorder propensities. ReLU activation functions are used for all nodes except the output node, which employs a sigmoid function to scale the propensities appropriately.\n\nThe hyper-parameters, including the number of layers and nodes in the hidden layers, were empirically selected using a grid search. This optimization process aimed to maximize the area under the curve (AUC) on the validation dataset. Additionally, a shallow machine-learning algorithm, logistic regression, was experimented with but yielded lower quality predictions compared to the deep neural network.\n\nFor predicting disorder functions, the random forest (RF) algorithm was utilized. This choice was driven by the success of RF in related disorder function predictors and the limited amount of training data available. The hyper-parameters for RF, such as the number of trees and tree depth, were optimized using grid search and 3-fold cross-validation on the training dataset. This approach was chosen for its robustness against overfitting and the efficiency of RF training.\n\nThe implementation of these predictive models was carried out using Python 3, with key packages including scikit-learn, keras, tensorflow, and pandas. The models were trained, validated, and deployed using these tools, ensuring a streamlined and efficient workflow.",
  "optimization/meta": "The model described in this publication does indeed use data from other machine-learning algorithms as input, making it a type of meta-predictor. Specifically, it incorporates outputs from IUPred, a fast intrinsic disorder (ID) predictor based on biophysical principles. This choice is justified by the common practice in the field of using one ID predictor as input for another to refine predictions, rather than combining multiple predictors in a meta-model.\n\nThe meta-predictor consists of a deep feedforward neural network for predicting disorder and a random forest algorithm for predicting disorder functions. The neural network is designed with multiple layers, including dropout layers to prevent overfitting, and uses ReLU activation functions. The random forest algorithm was chosen due to its success in related disorder function predictors and its robustness to overfitting, as demonstrated through cross-validation.\n\nRegarding the independence of the training data, it is clear that provisions were made to avoid overfitting. For instance, the neural network's hyper-parameters were optimized using a grid search that maximized AUC on a validation dataset. Similarly, the random forest's hyper-parameters were optimized through cross-validation on the training dataset. These methods ensure that the training data is used independently and that the models are robust to overfitting. Additionally, the use of a sliding window approach for feature extraction at the residue and window levels further supports the independence and generalizability of the training data.",
  "optimization/encoding": "The data encoding process for the machine-learning algorithm involved transforming the sequence profile into three distinct feature sets, each focusing on different levels of information: residue-level, window-level, and protein-level.\n\nAt the residue-level, the features were derived from individual residues within a small window of five residues for disorder prediction and one residue for disorder function prediction. This approach allowed the model to capture local sequence information.\n\nFor the window-level features, the profile information was aggregated by computing averages over larger windows. Specifically, a window of 15 residues was used for disorder prediction, while a window of 11 residues was employed for disorder function prediction. This aggregation helped in capturing more global sequence patterns.\n\nAdditionally, a novel protein-level encoding was introduced to quantify the overall bias of a given sequence to be disordered or to have functional regions. This encoding included the sequence-average of the profile values, sequence length, and the distance to each sequence terminus. These features provided a broader context about the entire protein sequence.\n\nThe sliding window approach was utilized to extract both residue-level and window-level information, where the residue in the middle of a small sequence segment was predicted based on the information from all residues in that segment. This method ensured that the model could consider both local and global sequence context during prediction.\n\nOverall, the encoding process aimed to provide a comprehensive representation of the sequence data, enabling the machine-learning algorithm to make accurate predictions about disorder and disorder functions.",
  "optimization/parameters": "In our study, we employed two main machine learning algorithms: a deep feedforward neural network for predicting disorder and a random forest algorithm for predicting disorder functions.\n\nFor the neural network, the input layer consists of 318 nodes, which corresponds to the number of features used. These features are derived from residue-level, window-level, and protein-level information. The architecture includes a dropout layer with a 0.2 dropout rate, followed by a hidden layer with 64 nodes, another dropout layer with a 0.2 dropout rate, a second hidden layer with 8 nodes, and finally, an output layer with one node that produces the disorder propensities. The hyper-parameters, including the number of layers and nodes in the hidden layers, were selected through a grid search process. This process involved training the networks on the training dataset and optimizing them to maximize the Area Under the Curve (AUC) on the validation dataset.\n\nFor the random forest algorithm, we considered two key hyper-parameters: the number of trees (ranging from 10 to 100) and the maximal tree depth (ranging from 2 to 20). We implemented a grid search based on 3-fold cross-validation on the training dataset to empirically optimize these hyper-parameters, aiming to maximize the AUC.\n\nIn summary, the selection of parameters was driven by a systematic grid search approach, ensuring that the models were optimized for predictive performance.",
  "optimization/features": "The input features for our predictor are derived from three main levels of information: residue-level, window-level, and protein-level. The residue-level features cover profile values for individual residues within a small window, while the window-level features aggregate profile information over a larger window. Additionally, protein-level features quantify the overall bias of a given sequence to be disordered or to have functional regions.\n\nThe total number of input features is 318. These features are carefully designed to capture various aspects of the protein sequence, including conservation scores derived from PSI-BLAST profiles and disorder predictions from IUPred. The use of these features allows our model to make accurate predictions about intrinsic disorder and disorder functions in protein sequences.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the features were carefully chosen based on their relevance to the problem of predicting intrinsic disorder and disorder functions. The selection of these features was guided by domain knowledge and previous successful applications in the field. The features were validated through ablation studies, which demonstrated the importance of each feature set in improving the predictive performance of our model.\n\nThe training process involved using a deep feedforward neural network for disorder prediction and a random forest algorithm for disorder function prediction. The hyperparameters for these models were optimized using grid search and cross-validation on the training dataset. This approach ensured that the models were trained to maximize performance on the validation dataset, without overfitting to the training data. The use of cross-validation further ensured that the models generalize well to unseen data.",
  "optimization/fitting": "In our study, we employed deep feedforward neural networks for predicting disorder, which indeed have a large number of parameters compared to the number of training points. To address potential overfitting, we incorporated dropout layers with a 0.2 dropout rate in our network architecture. These layers randomly set a fraction of input units to zero at each update during training time, which helps prevent overfitting by ensuring that the network does not become too reliant on any single feature.\n\nAdditionally, we used a grid search to empirically select the hyper-parameters, including the number of layers and nodes in the hidden layers. This process involved training the networks on the training dataset and optimizing them to maximize the Area Under the Curve (AUC) on the validation dataset. This approach helps in finding the optimal network architecture that generalizes well to unseen data, further mitigating overfitting.\n\nFor the prediction of disorder functions, we used the random forest (RF) algorithm. Given the limited amount of training data, deep networks were not feasible. The RF algorithm is robust to overfitting due to its ensemble nature, where multiple decision trees are combined to make predictions. We performed a grid search to optimize the RF hyper-parameters, including the number of trees and tree depth, using 3-fold cross-validation on the training dataset. This method is more robust to overfitting compared to using a single validation dataset.\n\nTo rule out underfitting, we ensured that our models were complex enough to capture the underlying patterns in the data. For the neural network, we used multiple hidden layers and a sufficient number of nodes to allow the model to learn intricate relationships. For the random forest, the grid search helped in selecting an appropriate number of trees and tree depth, ensuring that the model was not too simple to capture the data's complexity.\n\nIn summary, we employed dropout layers, grid search for hyper-parameter optimization, and cross-validation to address overfitting and underfitting concerns in our models. These techniques helped us build robust predictors for intrinsic disorder and disorder functions.",
  "optimization/regularization": "To prevent overfitting, several techniques were employed during the optimization of our models. For the deep feedforward neural network used for disorder prediction, dropout layers were included. These layers randomly set a fraction of input units to zero at each update during training time, which helps to prevent overfitting. A dropout rate of 0.2 was used, meaning 20% of the units were dropped out. Additionally, we used a validation dataset to monitor the model's performance and stop training when the performance on this dataset started to degrade, a technique known as early stopping.\n\nFor the random forest models used to predict disorder functions, we employed cross-validation instead of a separate validation dataset. Specifically, we used 3-fold cross-validation on the training dataset to optimize the hyperparameters, such as the number of trees and tree depth. This approach is more robust to overfitting compared to using a single validation set. Furthermore, the relatively small size of the training data for disorder functions made cross-validation a practical choice, as it allowed us to make better use of the available data.",
  "optimization/config": "The hyper-parameter configurations used in our study are available and have been reported in detail. Specifically, we conducted a grid search-based parametrization for our random forest models to predict four disorder functions. We explored two key hyperparameters: the number of trees, which we varied from 10 to 100 in increments of 10, and the maximal tree depth, which we varied from 2 to 20 in increments of 2. This grid search was implemented using 3-fold cross-validation on the training dataset to ensure robust performance evaluation.\n\nThe optimization parameters and the results of this grid search are included in the supplementary materials of our publication. These materials are accessible online and are licensed under a Creative Commons Attribution 4.0 International License. This license allows for the use, sharing, adaptation, distribution, and reproduction of the material, provided that appropriate credit is given to the original authors and the source is cited. The supplementary information can be found at the provided DOI link.\n\nAdditionally, the webserver and code associated with our study are available, and we have addressed the availability of both options in the \"Discussion\" and \"Webserver and Code\" sections of the revised article. Direct links to these resources are also provided on the webserver site, ensuring that users can easily access the necessary tools and information.",
  "model/interpretability": "The model presented in this publication, flDPnn, primarily relies on deep learning techniques, which are often considered black-box models due to their complexity and the difficulty in interpreting the internal workings of neural networks. The deep feedforward neural network used for disorder prediction consists of multiple layers, including dropout layers to prevent overfitting, and uses ReLU activation functions. While this architecture is effective for capturing complex patterns in the data, it does not inherently provide transparency into how specific predictions are made.\n\nHowever, efforts have been made to ensure that the contributions of different input features are understood. Ablation studies were conducted to assess the impact of removing specific features, such as those generated by PSI-BLAST and IUPred. These studies help to identify which features are crucial for the model's performance, providing some level of interpretability. For instance, the use of a PSI-BLAST profile and IUPred outputs as inputs was justified through these ablation experiments, demonstrating their significant contributions to the model's accuracy.\n\nAdditionally, the random forest algorithm used for disorder function prediction is generally more interpretable than deep neural networks. Random forests can provide insights into feature importance, allowing researchers to understand which features are most influential in the predictions. This aspect of the model offers a degree of transparency, complementing the more opaque deep learning components.\n\nIn summary, while the deep learning components of flDPnn are largely black-box, the inclusion of random forests and the conduct of ablation studies provide some interpretability. These methods help to elucidate the importance of various input features, offering a clearer understanding of the model's decision-making process.",
  "model/output": "The model is primarily a classification model, but it also provides regression-like outputs. It predicts disorder propensities, which are continuous values indicating the likelihood of a residue being disordered. These propensities can be seen as regression outputs. However, the model also produces binary predictions for each residue, classifying them as either disordered or not, which is a classification task. Additionally, the model predicts disorder functions, which are also binary classifications. The outputs include numeric propensities for disorder and disorder functions, along with corresponding binary predictions for each residue in the input protein chain(s). These results are delivered via a unique URL after the prediction process is completed on the server side. The webserver supports batch predictions of up to 20 FASTA-formatted protein sequences per request.",
  "model/duration": "The execution time of our model, flDPnn, is notably efficient. It typically takes between 5 to 10 seconds per protein to produce disorder and disorder function predictions. This speed is achieved through the use of runtime-efficient tools for deriving inputs, such as PSI-BLAST search on the small SwissProt database and fast IUPred, along with the single-sequence version of PSIPRED. This efficiency makes flDPnn one of the fastest disorder predictors available. In fact, it is at least an order of magnitude faster than many of its close competitors, as highlighted in recent articles summarizing the CAID results. This rapid execution time does not compromise the accuracy of the predictions, making flDPnn a highly effective tool for quick and reliable disorder assessments.",
  "model/availability": "The source code for flDPnn is publicly available. It can be accessed via a GitLab repository. Additionally, to simplify local installation, a Docker container is provided. This container includes all necessary dependencies, making it easier for users to run the software in their local environment without dealing with complex installation processes. The Docker container is also available on GitLab. Furthermore, flDPnn is accessible as a web server, allowing users to perform batch predictions of up to 20 FASTA-formatted protein sequences per request. The web server handles requests through a queue system, ensuring load balancing and automated processing on the server side. Results are delivered in multiple formats, including a parsable CSV file, an interactive graphical format, and a PNG image file. The web server is hosted and can be accessed online. The software is released under a permissive license, allowing for use, sharing, adaptation, distribution, and reproduction, provided that appropriate credit is given to the original authors.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure robustness and reliability. Initially, the method was assessed using the CAID dataset, which provided a broad benchmark for various predictors, including slow tools. However, the statistical significance of the differences among top-performing methods, including fIDPnn, was not reliably distinguishable in the CAID assessment.\n\nTo address this, a new similarity-reduced test dataset was created by filtering out test proteins that shared over 25% sequence similarity with the training proteins. This step was crucial to avoid overfitting and to ensure a fair comparison with other methods. The original test set of 200 proteins was reduced to 176 proteins, with 24 proteins removed due to similarity.\n\nThe evaluation also included a detailed ablation analysis, which was defined in a new Supplementary Table S2. This analysis helped to establish the precise contribution of various input features, such as PSI-BLAST profiles and IUPred predictions. The ablation study involved creating six different setups, each excluding specific features, to understand their impact on the predictive performance.\n\nStatistical significance was assessed using resampling techniques inspired by CASP. Half of the test dataset was resampled 10 times, and the results were compared between fIDPnn and each of the six ablation variants. This approach evaluated whether the improvements offered by the full version of fIDPnn were robust across different datasets. Two-sided paired t-tests or Wilcoxon tests were used, depending on the normality of the measured values, as assessed by the Anderson-Darling test.\n\nAdditionally, a full list of p-values for each pair of predictors was provided to support the claims of statistical significance. This list included p-values for the prediction of disordered residues and fully disordered proteins, ensuring transparency and rigor in the evaluation process.\n\nThe results from the similarity-reduced test dataset were re-run for several figures, including Figures 2B, 2C, 2E, 3, and 4. While the performance metrics decreased by about 1%, this change affected all tested methods equally, leaving the overall observations and conclusions unaffected. The procedure for collecting the similarity-reduced test dataset was explained in the \"Datasets\" section, and the use of this new dataset was mentioned at the beginning of the \"Prediction of intrinsic disorder\" section. The discussion of the results was also updated to reflect the new findings.",
  "evaluation/measure": "In our evaluation, we report several key performance metrics to assess the quality of our predictions. These metrics include the Matthews correlation coefficient (MCC), the F1-measure, the true positive rate (TPR), and the false positive rate (FPR). The MCC provides a balanced measure of the quality of binary classifications, considering true and false positives and negatives. The F1-measure is the harmonic mean of precision and sensitivity, offering a single metric that balances both concerns. The TPR, also known as sensitivity, indicates the proportion of actual positives correctly identified, while the FPR shows the proportion of actual negatives incorrectly identified as positives. These metrics are widely used in the literature and provide a comprehensive view of our model's performance.\n\nAdditionally, we evaluate the area under the receiver operating characteristic curve (AUC), which summarizes the trade-off between the TPR and FPR across different threshold settings. This metric is crucial for understanding the overall performance of our disorder function predictions. We also provide p-values to assess the statistical significance of the differences in predictive performance between our method and others, ensuring that our reported improvements are robust and not due to random chance. This set of metrics is representative of the standards in the field and allows for a thorough comparison with other methods.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Our method, fIDPnn, was evaluated in the Critical Assessment of protein Intrinsic Disorder (CAID) competition, where it demonstrated state-of-the-art performance. This independent evaluation provides a strong claim that fIDPnn is currently the leading method in the field.\n\nIn addition to comparing with other publicly available methods, we also performed an ablation study to assess the contribution of individual features and components of our model. This study involved creating several variants of fIDPnn with different features excluded and comparing their performance to the full model. The results, presented in supplementary tables, show that each component significantly contributes to the overall performance of fIDPnn.\n\nFurthermore, we compared fIDPnn to a simpler baseline method, fIDPlr, which uses linear regression instead of a deep neural network. While fIDPnn showed better performance, the difference was not very large, suggesting that the input features are well-selected and play a crucial role in the predictive power of our model.\n\nWe also addressed the use of a fast intrinsic disorder predictor, IUPred, as an input feature for fIDPnn. This choice was justified by the fact that IUPred provides valuable biophysical insights, and its integration into fIDPnn allows for a meta-predictor approach that combines different sources of information.\n\nTo ensure the robustness of our findings, we evaluated statistical significance using resampling techniques and appropriate statistical tests. This process involved resampling half of the test dataset multiple times and comparing the results between fIDPnn and other methods or its ablation variants. The p-values obtained from these tests support the claim that fIDPnn provides statistically significant improvements over other tools in most cases.\n\nOverall, our evaluation demonstrates that fIDPnn outperforms both publicly available methods and simpler baselines, highlighting the importance of carefully designed input features and the use of advanced machine learning techniques.",
  "evaluation/confidence": "The evaluation of our method, fIDPnn, includes a thorough assessment of statistical significance to ensure the robustness of our claims. We have provided detailed p-value matrices for various comparisons to support our assertions.\n\nFor the prediction of disordered residues and fully disordered proteins, we evaluated statistical significance by resampling half the test dataset 10 times and comparing results for each pair of predictors. This approach, inspired by CASP, helps to determine whether the improvements offered by fIDPnn are consistent across different datasets. We used two-sided paired t-tests for normally distributed data, assessed with the Anderson-Darling test, and the Wilcoxon test for non-normal data. The resulting p-values are presented in Supplementary Table 1, where bold font identifies p-values greater than 0.05.\n\nIn addition to the overall comparisons, we conducted ablation studies to assess the contribution of different features in fIDPnn. The p-values for these comparisons are provided in Supplementary Table 3, showing the statistical significance of improvements when various features are excluded. This detailed analysis helps to validate the robustness of fIDPnn's performance.\n\nFurthermore, we addressed concerns about statistical significance on the CAID dataset. While the differences among top-performing methods in CAID were not statistically significant, our evaluations on the test dataset demonstrate that fIDPnn's performance is statistically superior to other fast predictors. We have clarified this distinction in the manuscript by separating the discussion of CAID results and test set results into different paragraphs.\n\nOverall, our evaluation includes comprehensive statistical analyses to ensure that the performance metrics are reliable and that the claims of fIDPnn's superiority are well-supported.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as being available for download. However, the article is licensed under a Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. This means that while the raw evaluation files may not be directly available, the content of the article, including the results and methods, can be used and shared under the terms of this license.\n\nFor specific datasets or supplementary materials, such as the similarity-reduced test dataset and supplementary tables, these are mentioned as being available. For instance, a new Supplementary Table S2 is provided in the \"Source Data.xlsx\" file, and there are references to other supplementary materials that can be accessed online. The webserver site also provides direct links to relevant tools and datasets, ensuring that the necessary resources for replication and further study are accessible.\n\nAdditionally, the peer review information and reviewer comments are available, which can provide further insights into the evaluation process and the robustness of the methods used. This transparency supports the reproducibility of the study and allows other researchers to build upon the work presented."
}