{
  "publication/title": "Machine learning reveals mesenchymal breast carcinoma cell adaptation in response to matrix stiffness.",
  "publication/authors": "The authors who contributed to the article are:\n\nVlada S. Rozova, who contributed to investigation, methodology, project administration, resources, software, supervision, validation, visualization, and writing – original draft and review & editing.\n\nAyad G. Anwer, who contributed to investigation, methodology, and resources.\n\nAnna E. Guller, who contributed to investigation, methodology, project administration, resources, supervision, validation, and writing – review & editing.\n\nHamidreza Aboulkheyr Es, who contributed to investigation, validation, and writing – original draft and review & editing.\n\nZahra Khabir, who contributed to methodology.\n\nAnastasiya I. Sokolova, who contributed to formal analysis and writing – review & editing.\n\nMaxim U. Gavrilov, who contributed to validation.\n\nJean Paul Thiery, who contributed to supervision, validation, and writing – review & editing.\n\nAndrei V. Zvyagin, who contributed to conceptualization, project administration, supervision, and writing – original draft and review & editing.\n\nEwa M. Goldys, who contributed to supervision and writing – review & editing.\n\nMajid Ebrahimi Warkiani, who contributed to supervision and writing – review & editing.",
  "publication/journal": "PLOS Computational Biology",
  "publication/year": "2021",
  "publication/doi": "https://doi.org/10.1371/journal.pcbi.1009193",
  "publication/tags": "- Breast carcinoma\n- Matrix stiffness\n- Cell adaptation\n- Machine learning\n- Hierarchical clustering\n- Morphological analysis\n- Single-cell profiling\n- Genomic data\n- EMT signature\n- Computational biology\n- Random Forest classification\n- Feature selection\n- Statistical analysis\n- Tumor microenvironment\n- Cancer cell heterogeneity",
  "dataset/provenance": "The dataset used in this study was obtained from the Genomic Data Commons, specifically for TCGA-BRCA samples. This dataset includes updated clinical data and sample genomic information. The analysis involved 150 measurements describing individual cell morphology and its relationship with surrounding cells. Additionally, depending on the staining group, 140 measurements were obtained describing the intensity and distribution of fluorescent signal from cytokeratins and vimentin, and 69 measurements describing the expression of E-cadherin. The total number of cells analyzed was 826, and the features were used to identify three distinct cell morphotypes through hierarchical clustering. The dataset was split into training and test sets, with 10% of the data used for model evaluation. The features were standardized using z-score normalization to ensure equal weighting regardless of scale and nature. The study also utilized dimensionality reduction techniques such as Principal Component Analysis (PCA) to explore the data. The dataset has not been used in previous papers by the community, but it builds upon established methods and signatures, such as the ECM-signature (18 genes) and classical EMT-signature (14 genes), which were analyzed and selected according to previous studies.",
  "dataset/splits": "The dataset was split into two main parts: a training set and a test set. The test set comprised 10% of the data, while the remaining 90% was used for training. This split was used to evaluate the performance of the Random Forest Classifier model, which was employed to predict cell morphology based on selected features. Hyperparameter tuning was performed using randomized search with K-fold cross-validation to optimize the model parameters and achieve the best results. The performance of the model was evaluated using the F1 score, which is a weighted average of precision and recall, due to the unequal sizes of the classes.",
  "dataset/redundancy": "The dataset was split into training and test sets, with 10% of the data reserved for model evaluation. This split ensures that the training and test sets are independent, allowing for an unbiased evaluation of the model's performance.\n\nTo enforce independence between the training and test sets, a random split was performed. This process helps to prevent data leakage, where information from the test set might inadvertently influence the training process. By maintaining independent sets, the model's ability to generalize to unseen data can be more accurately assessed.\n\nThe distribution of the dataset compares favorably to previously published machine learning datasets in the field. The use of hierarchical clustering and feature selection techniques helped to identify and mitigate redundancy within the dataset. Features with high Pearson’s correlation coefficients were excluded, reducing the dataset from 150 to 100 features. This step ensures that the model is trained on a diverse set of features, enhancing its robustness and generalizability. The dataset's structure and preprocessing steps align with best practices in machine learning, ensuring that the results are reliable and reproducible.",
  "dataset/availability": "The data used in this study, including the data splits, are not publicly released in a forum. The clinical data and sample genomic information for TCGA-BRCA samples were obtained from the Genomic Data Commons. However, the specific datasets generated and analyzed during the current study are not available in a public repository. The data analysis was performed using TCGAbiolinks and Bioconductor packages under R program. The study involved various measurements and features extracted from cell images, which were analyzed using hierarchical clustering and a Random Forest model. The results of these analyses are presented in the publication, but the raw data and specific data splits are not made publicly available. The study acknowledges the contributions of individuals who assisted in data analysis and provides details on the experimental procedures and data analysis workflow in the supplementary materials. However, the data itself is not released in a public forum, and there is no information provided on how the availability of the data was enforced.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is the Random Forest Classifier. This is a well-established ensemble learning method that operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nThe Random Forest Classifier is not a new algorithm. It has been widely used and studied in various fields, including bioinformatics and computational biology. The reason it was not published in a machine-learning journal is that the focus of this study is on the biological insights gained from applying this algorithm to understand breast carcinoma cell adaptation to matrix stiffness, rather than on the development or improvement of the algorithm itself. The study leverages the established capabilities of Random Forest to analyze complex biological data and derive meaningful conclusions about cell behavior in response to different substrate stiffnesses.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "The data encoding and preprocessing steps were crucial for preparing the dataset for the machine-learning algorithm. Initially, 150 measurements were calculated for each cell, describing its morphology and context. Additionally, depending on the staining group, 140 measurements were obtained for the intensity and distribution of fluorescent signals from cytokeratins and vimentin, and 69 measurements for the expression of E-cadherin.\n\nBefore further analysis, all features were standardized using z-score normalization. This involved subtracting the mean from each column and dividing by the standard deviation, ensuring that all features were considered with equal weights regardless of their scale or nature.\n\nFeature selection was performed iteratively by removing features with a Pearson’s correlation coefficient greater than 0.8, resulting in the exclusion of 50 features. The remaining 100 features were used to assess cluster separability and train a Random Forest model.\n\nFor the hierarchical clustering, agglomerative hierarchical clustering was performed on the entire cell population using the 150 morphological and contextual features. This process involved recursively merging clusters based on the smallest variance, and the results were visualized using a dendrogram.\n\nThe dataset was split into training and test sets, with 10% of the data used for model evaluation. Hyperparameter tuning was conducted using randomized search with K-fold cross-validation to optimize the model parameters and achieve the best performance. The performance of the model was evaluated using the F1 score, which is a weighted average of precision and recall, due to the unequal sizes of the classes.",
  "optimization/parameters": "The model utilized 100 features for training and assessment of cluster separability. These features were selected through an iterative feature selection process, where features with a Pearson’s correlation coefficient greater than 0.8 were removed. This process ensured that the remaining features were not highly correlated, providing a more robust set of parameters for the model. The selection of these features was crucial for the performance of the Random Forest Classifier, which was used to predict cell morphotypes based on the selected morphological and contextual features.",
  "optimization/features": "In the optimization process, we initially considered 150 morphological and contextual features for each cell. To enhance the model's performance and reduce redundancy, feature selection was performed. This involved removing features with a Pearson’s correlation coefficient greater than 0.8, resulting in the exclusion of 50 features. Consequently, 100 features were retained and used for assessing cluster separability and training the Random Forest model. The feature selection process was conducted using the entire dataset, not just the training set. This approach ensured that the selected features were robust and generalizable across the entire population of cells.",
  "optimization/fitting": "The fitting method employed in this study involved a Random Forest Classifier to predict cell morphotypes based on selected morphological and contextual features. The dataset consisted of 150 measurements per cell, describing morphology and context, along with additional measurements for fluorescent signal intensity and distribution, and biomarker expression.\n\nThe number of features was initially large, but iterative feature selection was performed to exclude those with a Pearson’s correlation coefficient greater than 0.8, reducing the feature set to 100. This step helped mitigate the risk of overfitting by ensuring that the features used were independent and relevant.\n\nTo further address overfitting, hyperparameter tuning was conducted using randomized search with K-fold cross-validation. This technique optimizes model parameters and helps in achieving the best results by ensuring that the model generalizes well to unseen data. The performance of the model was evaluated using the F1 score, which is a weighted average of precision and recall, making it suitable for handling class imbalances.\n\nUnderfitting was addressed by ensuring that the model was complex enough to capture the underlying patterns in the data. The use of a Random Forest Classifier, which is an ensemble method, helped in building a robust model that could capture non-linear relationships and interactions between features. Additionally, the dataset was split into training and test sets, with 10% of the data reserved for testing, ensuring that the model was evaluated on unseen data.\n\nThe hierarchical clustering and feature selection processes also contributed to building a well-fitted model. Features were standardized using z-score normalization, allowing all features to be considered equally regardless of their scale. Cluster validity indices, such as the silhouette score and the Davies-Bouldin index, were used to determine the optimal number of clusters, ensuring that the model captured the inherent structure of the data.\n\nIn summary, the fitting method involved careful feature selection, hyperparameter tuning, and the use of ensemble methods to build a model that neither overfits nor underfits the data. The evaluation metrics and cross-validation techniques ensured that the model's performance was reliable and generalizable.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model. One of the key methods used was feature selection. We performed iterative feature selection, removing features with a Pearson’s correlation coefficient greater than 0.8. This process helped in reducing the dimensionality of our data and eliminating redundant features, which can lead to overfitting. As a result, we retained 100 features that were used to assess cluster separability and train our Random Forest model.\n\nAdditionally, we utilized hierarchical clustering with cluster validity indices to determine the optimal number of clusters in our dataset. This approach helped in identifying the natural grouping of cells based on their morphological and contextual features, rather than imposing an arbitrary number of clusters. The silhouette score and the Davies-Bouldin index supported the presence of 2 to 3 clusters, and we concluded that cells exhibit 3 general morphotypes.\n\nFor model evaluation, we used K-fold cross-validation during hyperparameter tuning. This technique involves partitioning the data into k subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process is repeated k times, with each subset used as the validation set once. This method helps in ensuring that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nFurthermore, we evaluated the performance of our model using the F1 score, which is a weighted average of precision and recall. This metric is particularly useful when dealing with imbalanced classes, as it provides a balanced measure of the model's performance.\n\nIn summary, we implemented feature selection, hierarchical clustering with cluster validity indices, K-fold cross-validation, and the F1 score as metrics to prevent overfitting and ensure the reliability of our model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available. We performed hyperparameter tuning using randomized search with K-fold cross-validation to optimize the model parameters and achieve the best results. The performance of the model was evaluated using the F1 score, which is a weighted average of precision and recall, due to the unequal sizes of the classes. However, the specific model files and optimization parameters are not explicitly detailed in the provided information. The data used for model evaluation is available, but the exact details on where to access the hyper-parameter configurations, optimization schedule, and any associated licenses are not specified.",
  "model/interpretability": "The model employed in this study is not a black box; it incorporates several transparent and interpretable components. One of the key methods used is hierarchical clustering (HC), which groups cells into distinct morphotypes based on their morphological and contextual features. This clustering process is visualizable through dendrograms, which show the hierarchical relationships between clusters. Additionally, the medoid of each cluster, representing the most typical example of each cell morph, is identified and displayed, providing a clear and interpretable representation of the different cell types.\n\nFeature selection is another crucial aspect of the model's transparency. Initially, 150 features were considered, but iterative feature selection reduced this to 100 features by removing those with high Pearson’s correlation coefficients. This process ensures that the most relevant features are used for cluster separability and model training. The top six features contributing to cluster separation, such as compactness, eccentricity, and circularity, are explicitly identified and their distributions are visualized using box plots. This allows for an understanding of how these features differ between the identified cell morphs.\n\nThe Random Forest Classifier, used to predict cell morphs, also contributes to the model's interpretability. The importance of each feature in the classification process can be assessed, providing insights into which morphological and contextual characteristics are most influential in determining cell type. Furthermore, the model's performance is evaluated using the F1 score, which considers both precision and recall, offering a comprehensive view of the model's effectiveness.\n\nIn summary, the model's transparency is achieved through visualizable clustering results, clear feature selection processes, and interpretable feature importance assessments. These elements collectively ensure that the model's decisions and predictions are understandable and verifiable.",
  "model/output": "The model employed in this study is a classification model. Specifically, a Random Forest Classifier was utilized to predict cell morphotypes based on selected morphological and contextual features. The dataset was divided into training and test sets, with 10% of the data reserved for testing. The model's performance was evaluated using the F1 score, which is a weighted average of precision and recall, particularly suitable for handling class imbalances. Hyperparameter tuning was conducted using randomized search with K-fold cross-validation to optimize the model parameters and achieve the best possible results. The features used for classification were carefully selected through iterative feature selection, removing those with high Pearson’s correlation coefficients to ensure the most relevant and independent features were considered. The model's output was visualized using a dendrogram from hierarchical clustering, which helped in identifying three distinct cell morphotypes. Additionally, the top features contributing to cluster separation were ranked and visualized using box plots to provide insights into the key morphological and contextual characteristics that differentiate the cell types.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of the method involved several key steps to ensure robustness and accuracy. Hyperparameter tuning was conducted using randomized search with K-fold cross-validation. This approach helped optimize the model parameters and achieve the best possible results. The performance of the model was assessed using the F1 score, which is a weighted average of precision and recall. This metric was chosen due to the unequal sizes of the classes in the dataset, providing a balanced measure of the model's effectiveness.\n\nIn addition to the model evaluation, various data analyses were performed. These included gene expression, mutation, copy number variation (CNV), and Pearson's correlation coefficient analyses. The ECM-signature (18 genes) and classical EMT-signature (14 genes) were analyzed and selected based on previous studies. The data used for these analyses were obtained from the Genomic Data Commons, specifically for TCGA-BRCA samples.\n\nStatistical analyses were also conducted to determine associations between different variables. For instance, Spearman’s rank correlation coefficient was used to determine associations between protein concentrations and the number of attached cells. To identify cell properties associated with substrate stiffness, Spearman’s and Kendall’s rank correlation coefficients, as well as distance correlation, were calculated and compared. Distance correlation was particularly useful as it can capture both monotonic and non-monotonic relationships.\n\nFeature selection was an important part of the evaluation process. Iterative feature selection was performed, removing features with a Pearson’s correlation coefficient greater than 0.8. This resulted in 50 features being excluded, leaving 100 features that were used to assess cluster separability and train a Random Forest model.\n\nThe experiments were replicated three times, with nine to twelve sections randomly selected and captured by a confocal microscope for each sample. This ensured the reliability and reproducibility of the results. The statistical significance of the findings was reported with p-values, indicating the level of confidence in the observed effects.",
  "evaluation/measure": "The performance of the model was evaluated using the F1 score, which is a weighted average of precision and recall. This metric was chosen due to the unequal sizes of the classes in the dataset. The F1 score provides a single metric that balances both the precision and the recall, making it suitable for evaluating models on imbalanced datasets.\n\nThe use of the F1 score is representative of common practices in the literature, especially when dealing with classification problems involving imbalanced classes. This metric is widely recognized for its ability to provide a comprehensive evaluation of a model's performance by considering both false positives and false negatives.\n\nAdditionally, hyperparameter tuning was performed using randomized search with K-fold cross-validation. This approach helps in optimizing the model parameters to achieve the best possible results. The use of cross-validation ensures that the model's performance is evaluated on multiple subsets of the data, providing a more robust estimate of its generalization capability.\n\nIn summary, the F1 score was used as the primary performance metric, supported by rigorous hyperparameter tuning and cross-validation techniques. This set of metrics and methods is consistent with established practices in the field, ensuring a reliable and representative evaluation of the model's performance.",
  "evaluation/comparison": "Not applicable. The publication focuses on the analysis of breast carcinoma cell adaptation to matrix stiffness using various statistical and machine learning techniques. It does not mention any comparison to publicly available methods or simpler baselines on benchmark datasets. The evaluation primarily involves the use of hierarchical clustering, Random Forest classification, and correlation analyses to understand cell morphology and its relationship with substrate stiffness. The performance of the model was evaluated using the F1 score due to unequal class sizes, and hyperparameter tuning was performed using randomized search with K-fold cross-validation. However, there is no indication of comparing the proposed methods with existing or simpler baseline methods on benchmark datasets.",
  "evaluation/confidence": "Confidence intervals were calculated at the 95% level for both mean values and proportions. Statistical significance was reported with p-values, where p-value < 0.05, p-value < 0.01, and p-value < 0.001 were considered significant. Statistical discernibility was assessed using unpaired two-tailed Student’s t-test with Welch’s correction for unequal variances. This approach ensures that the results are robust and that the claims about the method's superiority are statistically sound. Additionally, the experiments were replicated three times, with nine to twelve sections randomly selected and captured by confocal microscope for each sample, further enhancing the reliability of the findings.",
  "evaluation/availability": "Not applicable."
}