{
  "publication/title": "Mortality Prediction Utilizing Blood Biomarkers to Predict the Severity of COVID-19 Using Machine Learning Technique",
  "publication/authors": "The authors who contributed to the article are:\n\n- Tawsifur Rahman\n- Fajer A. Al-Ishaq\n- Fatima S. Al-Mohannadi\n- Reem S. Mubarak\n- Maryam H. Al-Hitmi\n- Khandaker Reajul Islam\n- Amith Khandakar\n- Ali Ait Hssain\n- Somaya Al-Madeed\n- Susu M. Zughaier\n- Muhammad E. H. Chowdhury\n\nAll authors have worked in the investigation and drafting of the paper. All authors have read and agreed to the published version of the manuscript.\n\nThe specific contributions are as follows:\n\n- Conceptualization: Muhammad E. H. Chowdhury and Susu M. Zughaier\n- Methodology: Tawsifur Rahman, Khandaker Reajul Islam, and Amith Khandakar\n- Validation: Fajer A. Al-Ishaq, Fatima S. Al-Mohannadi, Reem S. Mubarak, and Maryam H. Al-Hitmi\n- Formal analysis: Ali Ait Hssain, Somaya Al-Madeed, Muhammad E. H. Chowdhury, and Susu M. Zughaier",
  "publication/journal": "Diagnostics",
  "publication/year": "2021",
  "publication/doi": "10.3390/diagnostics11091582",
  "publication/tags": "- COVID-19\n- Mortality Prediction\n- Machine Learning\n- Blood Biomarkers\n- Severity Prediction\n- Healthcare\n- Clinical Evaluation\n- Nomogram-Based Scoring\n- Data Analysis\n- Pandemic Management",
  "dataset/provenance": "In our study, we utilized two distinct clinical biomarker datasets sourced from different countries. The first dataset, referred to as Dataset-1, was compiled from the Emergency Department of a metropolitan and academic hospital in Boston. This data collection occurred during the initial wave of the COVID-19 pandemic, spanning from March 24, 2020, to April 30, 2020. The dataset comprises 384 patients, all of whom were 18 years or older and exhibited clinical concerns at the time of hospital admission with acute respiratory illness. These patients met at least one of the following criteria: tachypnea, oxygen saturation of 92% or less on room air, a requirement for supplemental oxygen, or a need for positive pressure ventilation. The patients were monitored for up to 28 days post-registration for clinical outcomes or were discharged if they recovered. This dataset includes biomarkers recorded over three separate days (0, 3, and 7 days).\n\nThe second dataset, known as Dataset-2, was gathered retrospectively from 375 patients in Wuhan, China, between January 10 and February 18, 2020. The primary objective was to identify valid and relevant clinical markers of mortality risk. Standard case report forms were employed to collect comprehensive medical records, encompassing epidemiological, demographic, clinical, laboratory, and mortality results. This dataset has been previously published by Yan et al. and was approved by the Tongji Hospital Ethics Committee. Among the 375 COVID-19 positive patients in Dataset-2, 174 patients died, while 201 survived. The dataset contains 76 parameters, with common parameters between Dataset-1 and Dataset-2 being used for this study. The parameters from Dataset-2 were normalized to match those in Dataset-1, enabling Dataset-2 to serve as an external validation set.",
  "dataset/splits": "In our study, we utilized two primary datasets for model development and validation. The first dataset, referred to as Dataset-1, was collected from the Emergency Department of a metropolitan and academic hospital in Boston during the first wave of the COVID-19 pandemic. This dataset was used to develop and validate an early death prediction model. The second dataset, Dataset-2, was collected retrospectively from patients in Wuhan, China, and served as an external validation set for our developed model.\n\nFor the model development phase, we divided the training dataset (Day-0 patients’ data from Dataset-1) into three distinct splits: training, validation, and testing. The validation dataset was specifically used for tuning hyperparameters in the machine learning process, while the testing dataset was reserved for evaluating the final model performance. This approach ensured that our model was robust and generalizable.\n\nAdditionally, we used Day-3 and Day-7 patients’ data from Dataset-1 for external validation. This multi-day validation strategy helped us assess the model's performance over time and ensure its reliability in predicting mortality risk.\n\nIn summary, our dataset splits included:\n\n1. **Training Split**: Used for initial model training.\n2. **Validation Split**: Used for hyperparameter tuning.\n3. **Testing Split**: Used for evaluating the final model performance.\n4. **External Validation Splits**: Day-3 and Day-7 patients’ data from Dataset-1, and the entire Dataset-2 for validating the model's generalizability and performance over time and across different populations.",
  "dataset/redundancy": "In our study, we utilized two distinct clinical biomarker datasets from different countries to ensure robustness and generalizability of our findings. The first dataset, referred to as Dataset-1, was collected from the Emergency Department of a metropolitan and academic hospital in Boston during the initial wave of the COVID-19 pandemic. This dataset was used for developing and validating an early death prediction model. The second dataset, Dataset-2, was gathered retrospectively from patients in Wuhan, China, and served as an external validation set.\n\nTo develop the prediction model, Dataset-1 was divided into training, validation, and testing subsets. This division was crucial for ensuring that the model could generalize well to unseen data. The training set was used to fit the model, the validation set was employed for hyperparameter tuning, and the testing set was reserved for evaluating the final model performance. This approach ensured that the training and test sets were independent, thereby preventing data leakage and overfitting.\n\nThe datasets were preprocessed to handle missing data using the multiple imputations using the chained equations (MICE) technique. This method estimates missing values based on other variables in the dataset, preserving the integrity of the data. Additionally, the synthetic minority oversampling technique (SMOTE) was applied to balance the dataset, as the number of survived patients was significantly higher than the number of deceased patients. This balancing step was essential to mitigate bias in the model.\n\nThe distribution of the datasets compares favorably with previously published machine learning datasets in the clinical domain. Both datasets included a comprehensive range of epidemiological, demographic, clinical, and laboratory parameters, ensuring that the models were trained on a rich and diverse set of features. The common parameters between Dataset-1 and Dataset-2 were used for consistency, and the parameters from Dataset-2 were normalized to match those in Dataset-1. This normalization process facilitated the use of Dataset-2 as an external validation set, providing an unbiased assessment of the model's performance.",
  "dataset/availability": "The data used in this study consists of two clinical biomarker datasets from different countries. The first dataset, referred to as Dataset-1, was collected from the Emergency Department of a metropolitan and academic hospital in Boston during the first wave of the COVID-19 pandemic. The second dataset, Dataset-2, was collected retrospectively from patients in Wuhan, China.\n\nThe dataset associated with Dataset-2 has been published alongside an article by Yan et al. This dataset includes information on epidemiological, demographic, clinical, laboratory, and mortality results. The original study was approved by the Tongji Hospital Ethics Committee, ensuring that the data collection and usage adhered to ethical standards.\n\nThe datasets were used to develop and validate an early death prediction model. Dataset-1 was primarily used for model development, while Dataset-2 served as an external validation set. The parameters from Dataset-2 were normalized to match those in Dataset-1, allowing for consistent analysis and validation.\n\nThe datasets include a variety of parameters, with 76 parameters present in Dataset-2. Common parameters between Dataset-1 and Dataset-2 were used for the study, and the normalization process ensured that Dataset-2 could be effectively used for external validation.\n\nThe specific details about the public availability of the datasets, including the data splits used, are not provided. However, the publication of the dataset by Yan et al. suggests that it may be accessible through the associated article or related repositories. The ethical approval and normalization processes ensure the integrity and consistency of the data used in the study.",
  "optimization/algorithm": "The optimization algorithm employed in our study is based on logistic regression, which is a well-established machine-learning algorithm class. Logistic regression is a supervised learning method commonly used for binary classification tasks, making it suitable for our study's objective of predicting patient mortality risk.\n\nThe logistic regression algorithm used is not new; it has been extensively used in clinical investigations and other fields. The decision to use logistic regression was driven by its proven effectiveness in handling binary classification problems and its interpretability, which is crucial in medical applications.\n\nGiven that logistic regression is a widely recognized and validated algorithm, it was appropriate to publish our work in a diagnostics journal rather than a machine-learning journal. Our focus was on the application of this algorithm to develop a nomogram-based scoring technique for predicting patient outcomes, rather than on the innovation of a new machine-learning algorithm. This approach aligns with the journal's emphasis on diagnostic tools and methodologies.",
  "optimization/meta": "The model developed in this study does not function as a meta-predictor. Instead, it relies on a single machine-learning approach, specifically logistic regression, to create a nomogram-based scoring technique. This technique is designed to classify patients into three mortality risk categories: Low, Medium, and High.\n\nThe study employs two datasets: Dataset-1, used for model development and internal validation, and Dataset-2, used for external validation. Dataset-1 includes Day-0 patient data, as well as Day-3 and Day-7 data for additional validation. The training dataset from Dataset-1 is divided into training, validation, and testing subsets to ensure robust model development and evaluation.\n\nThe logistic regression model is trained using the training dataset, with the validation dataset used for hyperparameter tuning and the testing dataset for model evaluation. This process ensures that the training data is independent of the validation and testing data, maintaining the integrity of the model's performance assessment.\n\nThe nomogram-based scoring technique is then developed using the best-trained logistic regression model. This technique is validated using external datasets to confirm its generalizability and reliability across different populations and countries. The performance of the model is evaluated using several metrics, including sensitivity, specificity, precision, accuracy, and F1-score, to ensure its effectiveness in predicting mortality risk.",
  "optimization/encoding": "The data encoding and preprocessing for the machine-learning algorithm involved several key steps. Initially, the first-day data from Dataset-1 was used for model training and validation. This dataset included various clinical features, some of which had missing values. To address this, the multiple imputations using chained equations (MICE) technique was employed. This method estimates missing data using multiple regression models, considering the data form of the missing variables. Binary variables were predicted using logistic regression, while continuous variables were handled through statistical mean matching.\n\nThe dataset also suffered from imbalances, with alive patients being about seven times more frequent than dead patients. To mitigate this, the synthetic minority oversampling technique (SMOTE) was used to balance the data.\n\nFeature selection and reduction were crucial parts of the preprocessing. Twenty different features were checked for correlation, and while most features were not highly correlated, the maximum correlation found was between creatinine and kidney parameters at 0.56. This indicated that no features needed to be removed based on correlation alone. Instead, feature ranking and identifying the best feature combination were necessary for stratifying the dead and survived groups.\n\nThe dataset was then divided into training, validation, and testing subsets. The validation dataset was used for hyperparameter tuning, while the testing dataset was used for model evaluation. The best-trained model was subsequently used to develop a scoring technique to classify patients into three mortality risk categories: Low, Medium, and High. This comprehensive preprocessing ensured that the machine-learning algorithm could effectively identify primary predictors of disease severity and differentiate between patients requiring urgent medical support.",
  "optimization/parameters": "In our study, we utilized five key input parameters for our model, which we refer to as the ALDCC score. These parameters are Age, Lymphocyte count, D-dimer, C-reactive protein (CRP), and Creatinine. The selection of these five variables was based on a rigorous feature selection process. Initially, we considered 20 features present in the original dataset. We employed a feature selection method to identify the most promising features, which resulted in the top five features being Age, Lymphocyte count, D-dimer, Creatinine, and CRP. These variables exhibited the best performance in predicting outcomes and were subsequently used for nomogram creation and scoring technique development and validation. The decision to use these five variables was further supported by the logistic regression classifier, which showed that the five-variable model outperformed models with fewer variables. This selection process ensured that our model was both efficient and effective in predicting patient outcomes.",
  "optimization/features": "In the optimization phase of our study, we utilized a subset of features from the original dataset to develop our predictive model. The original dataset contained 20 features, but through a rigorous feature selection process, we identified the top five features as the most promising for our model. These features were Age, Lymphocyte count, D-dimer, Creatinine, and CRP.\n\nFeature selection was indeed performed to enhance the model's performance and interpretability. This process was conducted using the training dataset only, ensuring that the validation and testing datasets remained unbiased. By focusing on these top features, we aimed to create a more efficient and accurate predictive model for early death risk classification in COVID-19 patients. The selected features were then used to divide the training dataset into training, validation, and testing subsets, facilitating the tuning of hyperparameters and the evaluation of the model's performance.",
  "optimization/fitting": "In our study, we employed a logistic regression model, which is known for its simplicity and effectiveness in binary classification tasks, such as predicting patient outcomes. The number of parameters in our model was not excessively large compared to the number of training points, which helped mitigate the risk of overfitting.\n\nTo further ensure that our model did not overfit, we implemented several strategies. First, we used a feature selection process to identify the most relevant features, reducing the dimensionality of our data and focusing on the most informative variables. This step helped in eliminating noise and irrelevant features that could lead to overfitting. Additionally, we divided our dataset into training, validation, and testing sets. The validation set was crucial for tuning hyperparameters and preventing overfitting by providing an unbiased evaluation of the model's performance during training.\n\nWe also addressed the issue of imbalanced data, which can lead to biased models. We used the Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset, ensuring that the model did not become biased towards the majority class. This technique helped in creating a more representative training set, improving the model's generalizability.\n\nTo rule out underfitting, we carefully selected and engineered features that were relevant to the prediction task. We conducted a thorough statistical analysis to identify significant features and used techniques like multiple imputations to handle missing data, ensuring that the model had access to comprehensive and reliable information. Furthermore, we evaluated the model's performance using various metrics and compared it with other machine learning classifiers to ensure that it captured the underlying patterns in the data effectively.\n\nIn summary, our approach involved feature selection, data balancing, and rigorous validation processes to prevent both overfitting and underfitting, resulting in a robust and reliable logistic regression model for predicting patient outcomes.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was data preprocessing, which included handling missing data through the Multiple Imputations using Chained Equations (MICE) technique. This approach helped to estimate missing values based on other variables in the dataset, thereby preserving important data that might have been lost if rows with missing values were simply deleted.\n\nAdditionally, we addressed the issue of imbalanced data, which can lead to biased models. We used the Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset. This technique is particularly useful when one class is significantly more frequent than the other, as was the case in our study where alive patients were about seven times more frequent than dead patients.\n\nFeature selection and reduction were also crucial steps in our methodology. We evaluated twenty different features to identify correlations and ensure that only the most relevant features were used in our models. This process helped to improve classifier performance by removing highly correlated features that could lead to overfitting.\n\nFurthermore, we utilized cross-validation techniques, specifically five-fold cross-validation, to evaluate the performance of our models. This method involves dividing the dataset into five subsets, training the model on four subsets, and validating it on the remaining subset. This process is repeated five times, with each subset serving as the validation set once. This approach helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.\n\nIn summary, our study incorporated several overfitting prevention techniques, including advanced data imputation, SMOTE for balancing the dataset, feature selection and reduction, and cross-validation. These methods collectively contributed to the development of a robust and reliable prediction model.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the methodology section of our paper. Specifically, we discuss the division of our training dataset into training, validation, and testing subsets, and how the validation dataset was employed for hyper-parameter tuning. The code for the machine learning pipeline, which includes these configurations, is available and can be accessed through the reference provided in the methodology section.\n\nRegarding model files and optimization parameters, while the specific files are not directly linked within the paper, the methodology and results sections provide comprehensive details on the performance metrics and the processes used to develop and validate our models. This includes the use of five-fold cross-validation and the evaluation metrics such as sensitivity, specificity, precision, accuracy, and F1-score.\n\nFor those interested in replicating or building upon our work, the paper references external resources and supplementary materials that offer further insights into the datasets and preprocessing techniques used. Additionally, an application has been created to assist healthcare personnel in predicting early mortality using the developed model, which can be accessed through the provided link. This application includes the scoring results and can serve as a practical implementation of the optimization parameters discussed in the paper.",
  "model/interpretability": "The model developed in this study is not a blackbox, but rather a transparent and interpretable tool designed to assist in clinical decision-making. The core of the model is a nomogram, which is a graphical representation of a statistical predictive model. Nomograms are widely favored in clinical settings due to their user-friendly nature and clear visual interface.\n\nThe nomogram simplifies complex statistical models into an easy-to-understand format. It represents multiple independent variables on a horizontal axis, allowing clinicians to place a patient's specific values on these scales. By tracing a vertical line down to a horizontal score axis, the scores from all independent variables are combined to produce a total score. This total score is then linked to a probability of the outcome of interest, in this case, mortality, which is displayed on a horizontal axis ranging from 0 to 1. A higher score indicates a greater risk of mortality.\n\nOne of the key strengths of this model is its use of logistic regression, a well-established statistical method that provides clear insights into the relationship between the predictors and the outcome. The logistic regression equations used in the model are transparent, showing how each predictor contributes to the overall risk assessment. For example, the log of odds, or linear prediction, is calculated using a formula that includes the coefficients for each predictor, making it straightforward to understand the impact of each variable.\n\nThe model was developed using data from Day-0 of Dataset-1 and validated using data from subsequent days and Dataset-2. This approach ensures that the model is robust and generalizable. The use of Decision Curve Analysis (DCA) further enhances the interpretability by determining the threshold values at which the nomogram becomes clinically relevant.\n\nIn summary, the nomogram-based model is designed to be transparent and interpretable, providing clinicians with a clear and reliable tool for predicting mortality risk. The use of logistic regression and the graphical representation of the nomogram ensures that the model's predictions are easy to understand and apply in clinical practice.",
  "model/output": "The model developed in this study is a classification model. It is designed to predict binary outcomes, specifically the survival or death of patients. The primary technique used is logistic regression, which is a supervised machine learning method tailored for classification tasks. This model estimates the probability of a binary classification problem, making it suitable for determining the likelihood of patient survival or death based on various clinical and biological data.\n\nThe performance of the model was evaluated using several metrics, including accuracy, sensitivity, specificity, precision, and the F1-score. These metrics provide a comprehensive view of the model's effectiveness in correctly classifying patients into the two categories. The model was validated using five-fold cross-validation, ensuring its robustness and generalizability.\n\nA nomogram was also developed as part of the model. This nomogram is a graphical representation of a statistical prognostic model that predicts the probability of a clinical event, such as death, for individual patients. It uses biologic and clinical data, such as age, lymphocyte count, D-dimer levels, CRP, and creatinine, to assign points to each variable. The total point score is then used to estimate the probability of death.\n\nThe model's performance was further validated through internal and external validation curves, which showed that the calibration curve closely matched the ideal model. This indicates that the model's predictions are reliable and accurate. Additionally, the net gain of single predictors, such as age and lymphocyte count, was positive, reinforcing their significance in the prediction model.\n\nIn summary, the model is a classification model that uses logistic regression to predict patient outcomes. It has been thoroughly validated and shown to be effective in stratifying patients based on their risk of death. The accompanying nomogram provides a user-friendly tool for clinicians to estimate individual patient risk.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the machine learning pipeline used in this study is publicly available. This allows other researchers to replicate and build upon the work presented. Additionally, a webpage application has been created to assist healthcare personnel in predicting early mortality using the developed model. This application provides easily accessible ALDCC scoring results, making the tool practical for real-world use. The application is designed to be user-friendly, ensuring that healthcare providers can quickly and efficiently assess patient risk.",
  "evaluation/method": "The evaluation method employed in this study involved a comprehensive two-phase approach: model development and model validation. The initial phase utilized Dataset-1, which comprised Day-0 patient data from an academic hospital in Boston during the first wave of the COVID-19 pandemic. This dataset was divided into training, validation, and testing subsets. The training dataset was used to develop the prediction model, while the validation dataset was crucial for tuning hyperparameters. The testing dataset was then used to evaluate the model's performance.\n\nTo ensure robustness, the study employed five-fold cross-validation, a technique that involves partitioning the data into five subsets, training the model on four subsets, and validating it on the remaining one. This process was repeated five times, with each subset serving as the validation set once. The performance metrics evaluated included sensitivity, specificity, precision, accuracy, and F1-score. These metrics provided a thorough assessment of the model's ability to correctly classify patients into mortality risk categories: Low, Medium, and High.\n\nIn addition to cross-validation, the model was validated using an external dataset, Dataset-2, collected from Wuhan, China. This dataset served as an independent validation set, ensuring that the model's performance was not overfitted to the initial dataset. The parameters from Dataset-2 were normalized to match those in Dataset-1, allowing for a fair comparison and validation.\n\nThe performance of various machine learning classifiers, including KNN, Random Forest, XGBoost, SVM, Extra-tree, and Logistic Regression, was compared using the aforementioned metrics. Logistic Regression emerged as the top-performing model, demonstrating high accuracy and reliability in predicting patient outcomes.\n\nFurthermore, the study developed a nomogram-based scoring technique using multivariate logistic regression analysis. This nomogram graphically represents the statistical prognostic model, predicting the likelihood of clinical events such as death. Each variable in the nomogram is assigned a point value based on its magnitude, and the total point score is matched to an outcome scale. This approach provides a visual and intuitive tool for clinicians to assess patient risk.\n\nIn summary, the evaluation method involved a rigorous combination of cross-validation, independent dataset validation, and the development of a nomogram-based scoring technique. This multifaceted approach ensured the model's reliability and generalizability across different populations and datasets.",
  "evaluation/measure": "In the evaluation of our machine learning models, we employed a comprehensive set of performance metrics to ensure a thorough assessment. These metrics include accuracy, precision, sensitivity (recall), specificity, and the F1-score. Accuracy measures the overall correctness of the model's predictions, while precision indicates the proportion of true positive predictions among all positive predictions. Sensitivity, or recall, reflects the model's ability to identify true positive cases, and specificity measures the proportion of true negative predictions among all negative predictions. The F1-score provides a harmonic mean of precision and recall, offering a balanced view of the model's performance, especially useful when dealing with imbalanced datasets.\n\nAdditionally, we utilized the area under the receiver operating characteristic curve (AUC-ROC) to evaluate the discriminative power of individual features and their combinations. The ROC curve plots the true positive rate against the false positive rate at various threshold settings, and the AUC provides a single scalar value summarizing the performance across all thresholds.\n\nThese metrics are widely recognized and used in the literature for evaluating classification models, particularly in medical and healthcare applications. They provide a robust framework for assessing the model's effectiveness in predicting patient outcomes, such as mortality risk. By reporting these metrics, we aim to offer a transparent and comprehensive evaluation of our models, enabling comparisons with other studies and ensuring the reliability and validity of our findings.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of various machine learning classifiers to ensure the robustness and generalizability of our proposed model. We compared several classifiers, including K-Nearest Neighbors (KNN), Random Forest, XGBoost, Support Vector Machine (SVM), Extra-trees, and Logistic Regression. Each model was assessed using five-fold cross-validation on the training dataset, which consisted of 80% of the data, while the remaining 20% was used for testing.\n\nThe performance metrics used for comparison included sensitivity, specificity, precision, accuracy, and F1-score. These metrics provided a comprehensive evaluation of each model's ability to correctly classify patients into mortality risk categories. Additionally, we used the receiver operating characteristic (ROC) curve to measure the area under the curve (AUC) for individual predictors and their combinations. This allowed us to determine the contribution of different features in stratifying patients who died versus those who survived.\n\nThe comparison also included simpler baselines to ensure that our chosen models were not only complex but also effective. For instance, Logistic Regression, which is a relatively simpler model, showed competitive performance with an accuracy of 0.91 ±0.03 and an F1-score of 0.92 ±0.03. This indicates that while more complex models like Extra-trees and Random Forest also performed well, simpler models can still be effective in certain contexts.\n\nFurthermore, we validated our models using external datasets to ensure that they could generalize to unseen data from different populations and countries. This step was crucial in demonstrating the reliability and applicability of our models in real-world scenarios. The external validation datasets included Day-3 and Day-7 patient data from the same dataset used for training, as well as a completely unseen dataset from a different country.\n\nIn summary, our methods comparison involved a thorough evaluation of multiple machine learning classifiers using cross-validation and external datasets. We ensured that our models were not only complex but also effective by comparing them to simpler baselines and validating their performance on unseen data. This approach provided a robust assessment of our models' capabilities and their potential for real-world application.",
  "evaluation/confidence": "The evaluation of our machine learning models included several performance metrics, each accompanied by confidence intervals to provide a measure of uncertainty. These metrics included precision, sensitivity, specificity, accuracy, and F1-score. For instance, the logistic regression model, which showed high performance, reported precision, sensitivity, and F1-score with confidence intervals of approximately 0.92 ± 0.03, indicating a high level of confidence in these estimates.\n\nStatistical significance was assessed using p-values derived from chi-square tests. Features such as age and lymphocyte count showed statistically significant differences between the death and survival groups, with p-values less than 0.0001. This suggests that these features are reliable indicators and contribute significantly to the model's predictive power.\n\nThe use of five-fold cross-validation further ensured that the results were robust and not due to random chance. This method helps in evaluating the model's performance across different subsets of the data, providing a more reliable estimate of its generalizability.\n\nOverall, the confidence intervals and statistical significance tests support the claim that our method, particularly the logistic regression model, is superior to other evaluated models and baselines. The consistent performance across different metrics and the statistical validation of key features strengthen the reliability of our findings.",
  "evaluation/availability": "The datasets used in this study are publicly available. They were originally published alongside articles by other researchers. The datasets contain clinical data from patients, including epidemiological, demographic, clinical, laboratory, and mortality results. These datasets were used to develop and validate a nomogram-based scoring technique for predicting mortality risk in COVID-19 patients. The datasets were collected from different countries and populations, ensuring a diverse and robust validation of the model. The specific datasets can be accessed through the references provided in the publication."
}