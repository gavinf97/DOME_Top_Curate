{
  "publication/title": "Contrastive Cross-Site Learning With Redesigned Net for COVID-19 CT Classification",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "IEEE Journal of Biomedical and Health Informatics",
  "publication/year": "2020",
  "publication/doi": "10.1109/JBHI.2020.3023246",
  "publication/tags": "- Contrastive learning\n- COVID-19 CT diagnosis\n- Multi-site data heterogeneity\n- Network redesign\n- Deep learning\n- Medical imaging\n- Computed tomography\n- Artificial intelligence\n- Machine learning\n- Cross-site learning",
  "dataset/provenance": "The datasets used in our study are two publicly available COVID-19 CT image datasets. The first dataset, referred to as Site A, is the SARS-CoV-2 dataset. It contains 2482 CT images from 120 patients. Of these images, 1252 are positive for COVID-19, while 1230 are non-COVID but show other types of lung disease manifestations. The spatial sizes of these images vary, ranging from 119 × 104 to 416 × 512 pixels.\n\nThe second dataset, known as Site B, is the COVID-CT dataset. This dataset includes 349 CT images. These datasets are notable because, to the best of our knowledge, they are the only relatively large-scale, high-quality COVID-19 datasets that are currently publicly available for research purposes. The use of these datasets allows for a robust evaluation of our joint learning framework, demonstrating its effectiveness in handling heterogeneous data from different sources.",
  "dataset/splits": "Not applicable.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in our study are publicly available, ensuring transparency and reproducibility. We utilized two COVID-19 CT datasets: SARS-CoV-2 and COVID-CT. The SARS-CoV-2 dataset, denoted as Site A, consists of 2482 CT images from 120 patients, with 1252 positive for COVID-19 and 1230 non-COVID but with other lung disease manifestations. This dataset is accessible via medRxiv. The COVID-CT dataset, denoted as Site B, includes 349 CT images and can be found on arXiv.\n\nBoth datasets are released under licenses that permit their use for research purposes, facilitating open access and collaboration within the scientific community. The availability of these datasets was enforced through their respective hosting platforms, which provide clear guidelines and permissions for data usage. Researchers can access these datasets by adhering to the specified terms and conditions, ensuring ethical and responsible use of the medical imaging data.",
  "optimization/algorithm": "The optimization algorithm used in our work is based on deep learning techniques, specifically convolutional neural networks (CNNs). The class of machine-learning algorithm employed is supervised learning, where the model is trained on labeled data to classify COVID-19 CT images.\n\nThe algorithm is not entirely new; it builds upon the existing COVID-Net architecture, which has been previously proposed for COVID-19 detection from chest radiography images. However, we have redesigned the network architecture and learning strategy to better suit the task of COVID-19 CT image classification. This redesign includes modifications to the network layers and the addition of a contrastive training objective to enhance the learning of domain-invariant semantic features.\n\nThe reason this work was not published in a machine-learning journal is that the primary focus is on the application of these techniques to a specific medical problem—COVID-19 diagnosis using CT images. The innovations lie in the adaptation and improvement of existing deep learning methods for this particular task, rather than the development of a entirely new machine-learning algorithm. The emphasis is on the practical application and the clinical significance of the results, which is more aligned with biomedical and health informatics journals.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "In our study, the data encoding and preprocessing steps were crucial for ensuring that the machine-learning algorithm could effectively learn from the CT images. The CT images used presented notable appearance differences for COVID-19 patients across different severity levels. For instance, mild cases might only contain small lesions, while severe cases could have extensive lung involvement. This variance within the input space posed challenges for the model to find a robust optimal solution.\n\nTo address this, we employed data augmentation techniques such as random cropping and random vertical and horizontal flipping. These techniques helped to mitigate overfitting by increasing the diversity of the training data. Additionally, we used a global averaging pooling layer after extracting high-level features to create compact semantic embeddings. This step significantly reduced the parameters of the output dense layers, further alleviating overfitting issues.\n\nFor the batch normalization process, we utilized domain-specific batch normalization (DSBN) to handle the statistical discrepancies between different datasets. Each site was assigned an individual batch normalization layer, allowing the model to capture domain-specific moving average values of mean and variance. This approach ensured that the feature normalization during the testing phase accurately represented the statistics of each site, thereby improving the model's performance.\n\nThe learning rate was adjusted smoothly using cosine annealing. This method helped to facilitate a smoother learning process, which was beneficial for optimizing the model and reaching a more robust solution. The learning rate at a current epoch was calculated using a cosine annealing formula, which ensured a gradual decrease in the learning rate over the training epochs.\n\nOverall, these preprocessing and encoding steps were designed to enhance the model's ability to learn from heterogeneous datasets, improving its diagnostic accuracy for COVID-19 CT images.",
  "optimization/parameters": "In our model, the number of parameters was significantly reduced by incorporating a global averaging pooling layer after the extracted high-level features. This modification led to a substantial decrease in the parameters of the output dense layers, specifically by a factor of 12. This reduction helps to alleviate overfitting issues, which is crucial for improving the model's generalization performance.\n\nThe learning rate was initialized at 1e-4 and decayed using a cosine annealing strategy. This approach ensures a smooth learning process, which is particularly important given the notable appearance differences in CT images of COVID-19 patients across different severity levels. The cosine annealing method helps the model to explore a robust optimal solution from heterogeneous datasets.\n\nAdditionally, hyper-parameters were empirically adjusted using grid search with a random small subset of the entire dataset. This process involved setting the temperature parameter τ to 0.05 and α to 1.0. These values were chosen to optimize the contrastive loss, which is a key component of our training objective. The contrastive loss helps to regularize the latent space, ensuring that semantic embeddings of samples from the same class are close to each other in the angle space, regardless of the domain, and away from those of different classes.\n\nThe model was trained for a total of 100 epochs with a batch size of 32, containing 16 images from each dataset. To address the imbalance in sample numbers between the two datasets, the smaller dataset was reloaded four times. Data augmentation techniques, including random crop and random vertical and horizontal flips, were employed to mitigate overfitting problems. These strategies collectively contribute to the model's ability to handle the variability and complexity of the input data effectively.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The fitting method employed in this study addresses both overfitting and underfitting concerns through several strategies.\n\nTo mitigate overfitting, a global averaging pooling layer was added after the extracted high-level features. This layer helps to significantly decrease the parameters of the output dense layers, reducing the risk of overfitting. Additionally, data augmentation techniques such as random cropping and random vertical and horizontal flipping were used to increase the diversity of the training data, further helping to prevent overfitting.\n\nThe learning strategy was redesigned to facilitate a smooth learning process. The learning rate was adjusted more smoothly in a cosine annealing manner, which helps the model to reach a relatively robust solution. This approach ensures that the model does not converge too quickly to a suboptimal solution, thereby avoiding underfitting.\n\nThe overall training objective combines cross-entropy loss to assess classification error and contrastive loss to regularize the latent space. This dual-loss approach ensures that the model not only learns to classify the data accurately but also learns robust semantic embeddings, which helps in generalizing well to unseen data.\n\nThe model was trained for 100 epochs with a batch size of 32, containing 16 images from each dataset. The smaller dataset was reloaded four times to balance the sample number between the two datasets, ensuring that the model receives adequate training signals from both datasets.\n\nThe framework was implemented using PyTorch with an Nvidia TITAN Xp GPU, and the model was trained from scratch with the same Adam Optimizer. The learning rate was initialized at 1e-4 and decayed with cosine annealing, which helps in fine-tuning the model parameters effectively.\n\nIn summary, the fitting method employed in this study effectively addresses both overfitting and underfitting through a combination of architectural modifications, learning rate adjustments, and data augmentation techniques. These strategies ensure that the model generalizes well to new data while maintaining high accuracy on the training data.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and enhance the generalization of our model. One key method involved the addition of a global averaging pooling layer after the extraction of high-level features. This layer helps to create compact semantic embeddings, which significantly reduces the number of parameters in the output dense layers, thereby alleviating overfitting issues.\n\nAdditionally, data augmentation techniques such as random cropping and random vertical and horizontal flipping were applied to the training data. These augmentations help to increase the diversity of the training samples, making the model more robust and less likely to overfit to the specific characteristics of the training dataset.\n\nThe use of a contrastive loss function also played a crucial role in regularizing the latent space. This loss function encourages the model to learn domain-invariant semantic features, ensuring that samples of the same class are close to each other in the feature space regardless of the dataset they come from. This approach helps in mitigating the overfitting problem by promoting more generalized feature representations.\n\nFurthermore, the learning rate was adjusted smoothly using a cosine annealing strategy. This method helps in achieving a more stable and effective training process, which can contribute to better generalization performance.\n\nLastly, the framework was implemented using an Nvidia TITAN Xp GPU, and the model was trained from scratch with the Adam Optimizer. The learning rate was initialized at 1e-4 and decayed using cosine annealing, which further aids in preventing overfitting by ensuring that the model does not become too specialized to the training data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule are reported in the publication. The learning rate was initialized at 1e-4 and decayed using cosine annealing. The temperature parameter τ was set to 0.05, and the weighting factor α was set to 1.0. The model was trained for 100 epochs with a batch size of 32, containing 16 images from each dataset. Data augmentation techniques such as random crop and random vertical and horizontal flips were employed to mitigate overfitting.\n\nThe framework was implemented using PyTorch and an Nvidia TITAN Xp GPU. The classiﬁcation model and embedding network were trained from scratch with the same Adam Optimizer. Grid search with a random small subset of the entire dataset was used to empirically adjust the hyper-parameters.\n\nRegarding the availability of model files and optimization parameters, this information is not explicitly detailed in the provided context. Therefore, it is not clear whether these resources are publicly available or under what license they might be distributed.",
  "model/interpretability": "The model's interpretability is a key aspect of our framework, ensuring that it is not a black box. To achieve this, we employ Grad-CAM, a technique that provides visual explanations from deep networks via gradient-based localization. This method generates color maps that highlight the regions in CT images that the model focuses on when making predictions.\n\nFor instance, the suspicious lesion regions are successfully localized across various abnormality patterns, such as bilateral and peripheral ground-glass opacities, and consolidative pulmonary opacity. This visualization consistently shows that the model can identify even mild lesions, demonstrating its potential clinical relevance for COVID-19 image-based computer-assisted diagnosis.\n\nAdditionally, we present typical failure cases where the method might misclassify samples due to wrongly attended regions or fail to distinguish images with unobvious lesions. These visualizations help in understanding the limitations of the model and areas where it can be improved. Overall, the use of Grad-CAM enhances the transparency of our model, making it more interpretable and clinically useful.",
  "model/output": "The model is a classification model designed for COVID-19 diagnosis using CT images. It aims to classify CT images into COVID-19 positive or negative categories. The model's output is a predicted probability map, which is used to assess the classification error through cross-entropy loss. The final loss function combines cross-entropy loss and contrastive loss to enhance the model's ability to learn domain-invariant semantic embeddings. This approach ensures that the semantic embeddings of samples of the same class are close to each other in angle space, regardless of the domain, and away from those of different classes. The model's effectiveness is evaluated using metrics such as accuracy, F1 score, sensitivity, precision, and AUC, demonstrating its robustness in handling heterogeneous datasets from different sources.",
  "model/duration": "The model was trained using an Nvidia TITAN Xp GPU. The training process involved 100 epochs with a batch size of 32, containing 16 images from each dataset. The smaller dataset was reloaded four times to address the imbalance in sample numbers between the two datasets. Data augmentation techniques, such as random crop and random vertical and horizontal flips, were employed to mitigate overfitting. The learning rate was initialized at 1e-4 and decayed using cosine annealing. The training was conducted with grid search on a random small subset of the entire dataset to empirically adjust the hyper-parameters, setting the temperature parameter τ as 0.05 and α as 1.0. However, the exact execution time for the model to run is not specified.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "The evaluation of our method was conducted using two public COVID-19 CT datasets, SARS-CoV-2 and COVID-CT. These datasets were chosen because they are the largest and highest-quality publicly available datasets for COVID-19 research. The SARS-CoV-2 dataset, referred to as Site A, contains 2482 CT images from 120 patients, with an equal distribution of COVID-19 positive and non-COVID images. The COVID-CT dataset, referred to as Site B, includes 349 CT images from 216 patients with COVID-19 and 397 images from 171 patients without COVID-19.\n\nTo ensure a comprehensive evaluation, we performed four-fold cross-validation on both datasets. The images were preprocessed by resizing them to 224x224 pixels and normalizing the intensity values. We used five evaluation metrics to assess the performance of our models: accuracy, F1 score, sensitivity, precision, and AUC. These metrics were reported as the average and standard deviation over three independent runs.\n\nThe evaluation involved comparing our method with baseline settings, including Single and Joint learning schemes, and state-of-the-art joint learning approaches. The Single approach trains a model for each dataset individually, while the Joint approach trains a model using both datasets with naive aggregation. We also compared our method with domain-adaptive approaches like Series-Adapter and Parallel-Adapter, as well as MS-Net, which incorporates domain-specific auxiliary branches and knowledge transfer strategies.\n\nThe results demonstrated that our method outperformed the Single approach in 9 out of 10 metrics on both sites, highlighting its practical value in maximizing data utility from different datasets. Additionally, our method showed statistically significant improvements over the Joint, Single, and SepNorm approaches, as indicated by paired t-tests with p-values smaller than 0.05. Furthermore, our method considerably outperformed state-of-the-art joint learning methods, demonstrating its superiority in exploiting robust representations from heterogeneous datasets.",
  "evaluation/measure": "In our study, we adopted a comprehensive set of evaluation metrics to assess the performance of our COVID-19 CT image classification models. These metrics include Accuracy, F1 score, Sensitivity, Precision, and Area Under the Curve (AUC). These metrics are widely used in the literature for evaluating classification models, particularly in medical imaging, ensuring that our results are comparable with other studies in the field.\n\nAccuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F1 score is the harmonic mean of precision and sensitivity, providing a single metric that balances both concerns. Sensitivity, also known as recall, measures the proportion of actual positives that are correctly identified by the model. Precision measures the proportion of positive identifications that are actually correct. Finally, AUC provides an aggregate measure of performance across all classification thresholds, offering a comprehensive view of the model's ability to distinguish between positive and negative cases.\n\nBy reporting these metrics, we aim to provide a thorough evaluation of our models' performance, highlighting their strengths and areas for improvement. This set of metrics is representative of the standards used in the literature, ensuring that our findings can be contextualized within the broader research community.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated our approach against both simpler baselines and state-of-the-art methods using publicly available datasets. For the baseline comparisons, we considered two settings: Single and Joint. The Single approach trains a model for each dataset individually, while the Joint approach trains a single model using both datasets with naive aggregation. Our method outperformed the Single approach in 9 out of 10 metrics across two sites, demonstrating its effectiveness in maximizing data utility from different datasets to boost diagnosis accuracy.\n\nWe also compared our approach with state-of-the-art joint learning methods, including Series-Adapter, Parallel-Adapter, and MS-Net. The Series-Adapter and Parallel-Adapter methods incorporate domain-adaptive layers to mitigate cross-domain visual discrepancies. MS-Net uses domain-specific auxiliary branches and an online knowledge transfer strategy. Our method outperformed all three state-of-the-art methods on both datasets, highlighting its superiority in exploiting robust representations from heterogeneous datasets. The statistical significance of these improvements was confirmed using paired t-tests, with all p-values being smaller than 0.05.",
  "evaluation/confidence": "To evaluate the confidence in our results, we conducted paired t-tests to analyze the significance of the improvements of our method over various baselines and state-of-the-art approaches. The detailed results of these tests are presented in tables, showing p-values for each comparison. All paired t-tests presented p-values smaller than 0.05, indicating statistically significant improvements of our method on both datasets. This statistical significance supports the claim that our approach is superior to the compared methods. Additionally, we reported the results in the form of average and standard deviation over three independent runs, providing a measure of variability and reliability in our performance metrics. This comprehensive evaluation ensures that our findings are robust and not due to random chance.",
  "evaluation/availability": "Not enough information is available."
}