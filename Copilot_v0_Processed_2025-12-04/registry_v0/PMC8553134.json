{
  "publication/title": "Machine learning to predict the source of campylobacteriosis using whole genome data",
  "publication/authors": "The authors who contributed to this article are:\n\n- Nicolas Arning: Data curation, formal analysis, investigation, methodology, software, visualization, writing – original draft.\n- Daniel J. Wilson: Conceptualization, methodology, supervision, writing – review & editing.\n- Sion Bayliss: Data curation.\n- Samuel K. Sheppard: Writing – review & editing.\n- David A. Clifton: Supervision, writing – review & editing.\n- David Eyre, Christophe Fraser, and Alexandra Casey: Provided insightful comments.",
  "publication/journal": "PLoS Genetics",
  "publication/year": "2021",
  "publication/doi": "https://doi.org/10.1371/journal.pgen.1009436",
  "publication/tags": "- Machine Learning\n- Campylobacteriosis\n- Whole Genome Data\n- Source Attribution\n- Deep Learning\n- Genomic Data\n- Predictive Modeling\n- Bioinformatics\n- Public Health\n- Disease Surveillance",
  "dataset/provenance": "The dataset used in this study was acquired from the public database for molecular typing and microbial genome diversity, PubMLST. A total of 5,799 genomes of Campylobacter jejuni and Campylobacter coli were obtained from various sources and host species. The distribution of these genomes across different sources is as follows: chicken (4,147), cattle (716), sheep (584), bird (212), and environment (140).\n\nThe dataset includes whole genome sequence (WGS) data, which corresponds to MLST sequence types (ST) and clonal complexes (CC) designations, as well as core genome (cg) MLST classes. To ensure sufficient sample sizes per reservoir population, only the five most prevalent classes for MLST and cgMLST were used: chicken, cattle, sheep, wild bird, and environment. For farm animals, the classes \"chicken\" and \"chicken offal or meat\" were combined into \"chicken,\" and similarly for sheep and cattle. The classes \"environment,\" \"sand,\" and \"river water\" were combined into \"environment,\" consistent with previous studies.\n\nThe dataset was divided into training and testing sets using phylogeny-aware sorting, where all members of one ST were sorted entirely into either the training or testing sets. This approach accounts for the phylogenetic non-independence of samples. The training set comprised 75% of the data, while the testing set comprised 25%.\n\nThe data used in this study is publicly available and has been uploaded as a public dataset on PubMLST. This allows for the reuse of data for continuous, updatable monitoring in a generalizable framework. The dataset includes additional samples for which only MLST data was available, enhancing the overall analysis.",
  "dataset/splits": "The dataset was divided into two main splits: a training set and a test set. The test set constituted 25% of the total data. This test set was further divided into two subsets to address class imbalance issues. The first subset featured an even distribution of classes, while the second subset undersampled the over-abundant chicken-origin genomes to better reflect their relative contribution to human disease. The undersampling was repeated 200 times with replacement, and the accuracy was averaged over all iterations. This approach allowed for the use of all available samples from the residual classes in testing, despite the limited number of minority source samples. The training set was deployed in batches of 128 samples, with each batch randomly undersampled to ensure equal representation of each class.",
  "dataset/redundancy": "The datasets were split into training and testing sets, with the test set comprising 25% of the total data. To ensure independence between the training and test sets, a phylogeny-aware train/test splitting method was employed. This approach helps maintain the accuracy of predictions when new genetic variants are introduced, as the algorithm can be incrementally trained with new data. The distribution of the datasets reflects the real-world scenario of campylobacteriosis, where certain sources, such as chickens, are more prevalent than others. This imbalance was addressed by using two methods to rebalance the classes in testing: one with an even distribution of classes and another that undersampled the over-abundant chicken-origin genomes to better emulate their relative contribution to human disease. The rebalancing process involved repeated undersampling with replacement, allowing for the use of all available samples from the residual classes in testing. This method ensures that the test set is representative of the actual distribution of sources in human infections, providing a more accurate evaluation of the model's performance.",
  "dataset/availability": "All data used in this study can be found on PubMLST using the accession numbers provided in the supplementary table. The data has also been uploaded as a public dataset. This ensures that the data is accessible to the public and can be used for further research. The data is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. This licensing ensures that the data can be freely used and shared, promoting transparency and reproducibility in research. The dataset includes a variety of samples from different sources and host species, providing a comprehensive resource for studying the source of human Campylobacter infections. The use of public databases like PubMLST allows for continuous updatable monitoring and the reuse of data in a generalizable framework.",
  "optimization/algorithm": "The machine-learning algorithms used in this study are not new. They are well-established methods commonly used in the field. The algorithms include gradient boosting classifiers, implemented using the xgboost library, and other machine learners implemented in scikit-learn. Additionally, deep learning algorithms were constructed using the Keras library. These algorithms were chosen for their empirical performance and ability to handle the complexity of the genomic data.\n\nThe decision to use these established algorithms was driven by their proven effectiveness in similar tasks and their ability to handle the specific challenges posed by genomic data. The focus of this study was on applying these algorithms to the problem of predicting the source of campylobacteriosis using whole genome data, rather than developing new machine-learning algorithms. Therefore, the algorithms were not published in a machine-learning journal because the innovation lies in their application to this particular biological problem, rather than in the algorithms themselves.",
  "optimization/meta": "Not applicable.",
  "optimization/encoding": "For the machine-learning algorithm, the data encoding and preprocessing involved several steps. Initially, allele sequences for each locus in the MLST scheme were downloaded and encoded as dummy variables and k-mers with k=21 using DSK. This approach was also applied to whole-genome sequences (WGS), as k-mers have been successfully used in previous analyses of Campylobacter jejuni.\n\nThe initial encoding resulted in a large number of unique k-mers, which was reduced by applying a variance threshold. K-mers present or absent in more than 99% of the samples were discarded, reducing the number of unique k-mers to 7,285,583. Further feature selection was performed using the Chi-Square statistic to test the dependence of source labels on individual k-mers. This step was conducted only on the training data to avoid data leakage, and the 100,000 k-mers with the highest scores were selected.\n\nThe labels were encoded as dummy variables, and categorical cross-entropy was used as the loss function. The Adam optimizer was employed, with cyclical learning rates ranging from 0.0001 to 0.1 to overcome local minima. The data was deployed in batches of 128 samples, with each batch randomly undersampled to ensure equal representation of each class. Training was run for 500 generations with early stopping after 50 generations.\n\nThe accuracy on the test set was measured at every epoch, and the best-performing weights were stored as a checkpoint. This preprocessing and encoding strategy aimed to optimize the performance of the machine-learning algorithms by reducing dimensionality, selecting relevant features, and ensuring balanced class representation during training.",
  "optimization/parameters": "In our study, we employed various machine learning and deep learning architectures, each with its own set of parameters. For the deep learning models, we explored several architectures, including a recurrent neural network (RNN) with 64 gated recurrent units, a 1-dimensional convolutional network with two convolutional layers, a Long Short-Term Memory network (LSTM) with 64 units, a shallow dense network with one dense layer of 64 units, and a deep dense network with six dense layers starting with 128 units and halving with each successive layer.\n\nFor the machine learning models, we used the xgboost library for gradient boosting classifiers and scikit-learn for other machine learners. The hyper-parameters for each classifier were chosen using Cartesian grid search on five-fold cross-validation of the training set. This method ensures that the parameters are selected in a way that optimizes performance on the training data while minimizing the risk of overfitting.\n\nThe selection of parameters was driven by empirical performance and the need to balance model complexity with computational efficiency. For instance, the use of dropout layers with a 50% dropout rate in all deep learning architectures helped to prevent overfitting by randomly setting a fraction of input units to zero at each update during training time. The choice of activation functions, such as Rectified Linear Unit (ReLU), was based on their effectiveness in previous studies and their ability to introduce non-linearity into the model.\n\nIn summary, the number of parameters varied across different models, but the selection process involved a combination of empirical testing, cross-validation, and established best practices in the field. This approach allowed us to achieve a good balance between model complexity and performance.",
  "optimization/features": "The input features for our machine learning models were initially encoded as k-mers with k = 21, resulting in a very large number of unique k-mers. To manage this, a variance threshold was applied, discarding k-mers present or absent in more than 99% of the samples, reducing the number of unique k-mers to approximately 7.3 million.\n\nFeature selection was performed using the Chi-Square statistic to test the dependence of the source labels on each individual k-mer. This process was conducted using only the training data to avoid data leakage. The top 100,000 k-mers with the highest scores were selected for use as input features in the models. This approach ensured that the feature selection process did not inadvertently incorporate information from the test set, maintaining the integrity of the model evaluation.",
  "optimization/fitting": "The fitting method employed in our study involved several deep learning architectures, each with a varying number of parameters. For instance, the deep dense network had the largest number of parameters, starting with 128 units in the first layer and halving with each successive layer. Given the complexity of these models, the number of parameters was indeed much larger than the number of training points.\n\nTo mitigate overfitting, several techniques were implemented. Firstly, dropout layers with a 50% dropout rate were included in all architectures. This technique randomly sets a fraction of input units to zero at each update during training time, which helps prevent overfitting. Secondly, cyclical learning rates were used, ranging from a minimum of 0.0001 to a maximum of 0.1. This approach helps the model escape local minima and improves generalization. Additionally, early stopping was employed, halting the training process after 50 generations of no improvement on the validation set. This ensures that the model does not continue to train and overfit to the training data.\n\nTo address underfitting, the models were trained for a sufficient number of epochs (500 generations) and the performance was monitored on a validation set. The use of a cyclical learning rate also helps in finding a better minimum, reducing the risk of underfitting. Furthermore, the models were evaluated on a separate test set to ensure they generalized well to unseen data. The use of batch normalization and ReLU activation functions also aided in improving the training dynamics and preventing underfitting.\n\nIn summary, the combination of dropout layers, cyclical learning rates, early stopping, and extensive training epochs helped in balancing the trade-off between overfitting and underfitting, ensuring robust and generalizable models.",
  "optimization/regularization": "In our study, we employed several regularization techniques to prevent overfitting and improve the generalization of our models. One of the key methods used was dropout, which was applied in various layers of our neural network architectures. Specifically, a 50% dropout rate was implemented in several layers, including those in recurrent neural networks (RNNs), convolutional networks, long short-term memory networks (LSTMs), and dense networks. Dropout works by randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not become too reliant on any single neuron.\n\nAdditionally, we utilized cyclical learning rates to overcome local minima and improve the convergence of our models. The learning rate was varied between a maximum of 0.1 and a minimum of 0.0001, which helped in finding a better set of weights by exploring different learning rates during training.\n\nEarly stopping was another regularization technique employed. The training process was monitored for accuracy on the test set at every epoch, and the training was halted after 50 generations without improvement. This ensured that the model did not continue to train beyond the point where it started to overfit the training data.\n\nFurthermore, we used batch normalization and undersampling techniques. Batches of 128 samples were used, with each batch randomly undersampled to ensure equal representation of each class. This helped in balancing the dataset and preventing the model from being biased towards the majority class.\n\nThese regularization methods collectively contributed to the robustness and generalizability of our models, ensuring that they performed well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are available through the code and models shared on our GitHub repository. The repository contains the implementation of our algorithm, named aiSource, which includes details on the hyper-parameters chosen for each classifier through Cartesian grid search on five-fold cross-validation. The deep learning architectures and their configurations, such as the number of layers, units, dropout rates, and activation functions, are also specified in the code.\n\nThe model files, including the best-performing weights stored as checkpoints during training, are not directly available in the repository. However, the training process and the parameters used to achieve these weights are documented in the code and accompanying documentation. This allows users to replicate the training process and generate their own model files.\n\nThe optimization parameters, such as the use of cyclical learning rates with a maximum of 0.1 and a minimum of 0.0001, the batch size of 128 samples, and the early stopping criteria after 50 generations without improvement, are all detailed in the code and the associated documentation.\n\nThe code and models are released under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction, provided that the original authors and source are credited. This license ensures that the configurations and optimization parameters are accessible and can be utilized by other researchers and practitioners in the field.\n\nFor those interested in accessing the data used in this study, it is available on PubMLST using the accession numbers provided in the supplementary table. Additionally, the data has been uploaded as a public dataset, ensuring transparency and reproducibility of our findings.",
  "model/interpretability": "The model employed in our study, particularly the XGBoost implementation of gradient boosted decision trees, is not entirely a blackbox. Decision tree-based ensemble learners, such as XGBoost, are known for their interpretability compared to other machine learning models. These models can provide insights into which features (in this case, genetic markers) are most important for making predictions. This is crucial in genomic studies where understanding the underlying biological mechanisms is as important as making accurate predictions.\n\nFor instance, XGBoost can rank features by their importance, indicating which genetic markers contribute most to the classification of Campylobacter isolates into their respective sources. This feature importance can be visualized and analyzed to gain biological insights. Additionally, individual decision trees within the ensemble can be examined to understand the decision-making process at a more granular level. This transparency allows researchers to validate the model's predictions with domain knowledge and to generate hypotheses for further experimental validation.\n\nMoreover, the use of k-mers and their selection based on the Chi-Square statistic adds another layer of interpretability. By identifying k-mers that are significantly associated with specific sources, we can link these genetic sequences to particular host associations, providing a clearer picture of the genetic basis for source attribution. This approach not only improves the model's performance but also enhances its interpretability by highlighting specific genetic regions that are crucial for distinguishing between different sources of Campylobacter isolates.",
  "model/output": "The model is a classification model. It was designed to predict the source of Campylobacteriosis using whole genome data. The output layer of the model comprises a dense layer with softmax activation, which is typical for multi-class classification problems. The labels were encoded as dummy variables, and categorical cross-entropy was used as the loss function. This setup indicates that the model is intended to classify input data into one of several predefined classes, specifically different sources of Campylobacteriosis. The performance metrics registered include accuracy, precision, recall, F1 score, negative predictive value, and specificity, all of which are commonly used to evaluate classification models.",
  "model/duration": "The execution time of the model, specifically the aiSource algorithm, was measured on a Dell OptiPlex 7060 desktop using ten threads on an Intel Core i7-8700 CPU and 16 GB RAM. The prediction took 892 milliseconds. This demonstrates the low computational requirements and high prediction speed of the algorithm, making it suitable for analyzing large genome datasets. The efficiency of the model is further highlighted by its ability to be incrementally trained with new data, which is crucial for developing automated and continuous disease surveillance systems.",
  "model/availability": "The source code for our algorithm, named aiSource, is publicly available and can be accessed via GitHub. This allows users to apply the algorithm to their own datasets. The specific URL for accessing aiSource is https://github.com/narning1992/aiSource. The code is released under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction, provided that the original author and source are credited. This open-access approach ensures that the algorithm can be widely utilized and built upon by the scientific community.",
  "evaluation/method": "The evaluation of the method involved several steps to ensure robust and comprehensive assessment. Both machine learning and deep learning models were tested on the same 25% test set. The original data was skewed in source composition, so two methods were used to rebalance the classes in testing. The first test set featured an even distribution of classes, while the second undersampled the over-abundant chicken-origin genomes to better reflect their relative contribution to human disease. This undersampling was repeated 200 times with replacement, and the accuracy was averaged over all iterations, recording one standard error.\n\nFor performance metrics, accuracy, precision, recall, F1 score, negative predictive value, specificity, and speed were registered. Speed was measured relative to other classifiers, with a scale defined from 0 (slowest) to 1 (quickest). The balanced test set was limited by the number of available samples from the minority source, but repeated undersampling allowed the use of all available samples of the residual classes in testing.\n\nTo compare with previous methods, iSource was applied to the test dataset. XGBoost on cgMLST was identified as the best-performing source attribution method. This classifier was retrained with both training and testing data and applied to all 15,988 human cgMLST samples available on the PubMLST database. The prediction took 892 milliseconds on a Dell OptiPlex 7060 desktop using ten threads on an Intel Core i7-8700 CPU and 16 GB RAM. The algorithm, named aiSource, can be found and applied from a specified GitHub repository.",
  "evaluation/measure": "In our evaluation, we registered several performance metrics to comprehensively assess the effectiveness of our models. These metrics include accuracy, precision (positive predictive value), recall (sensitivity), F1 score, negative predictive value, and specificity. These metrics are standard in the literature and provide a well-rounded view of model performance. Accuracy measures the overall correctness of the model's predictions. Precision indicates the proportion of true positive predictions among all positive predictions made by the model. Recall, or sensitivity, measures the proportion of true positive predictions among all actual positives in the dataset. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. Negative predictive value assesses the proportion of true negative predictions among all negative predictions. Specificity measures the proportion of true negative predictions among all actual negatives. Additionally, we measured the speed of our algorithms relative to other classifiers, normalizing the values within a defined scale. This metric is crucial for practical applications where computational efficiency is important. By including these metrics, we ensure that our evaluation is thorough and comparable to other studies in the field.",
  "evaluation/comparison": "In our evaluation, we compared our machine learning approaches to the iSource method, which is based on MLST and has been the most commonly used source attribution method to date. We found that our best-performing machine learner, a random forest, achieved slightly better accuracy than iSource on MLST data. This comparison was conducted using the same test set for both methods, ensuring a fair evaluation.\n\nAdditionally, we compared our deep learning architectures to simpler baselines. For instance, among simple learners, the K-nearest neighbour algorithm (KNN) performed best. This is likely due to the hereditary nature of the phenotypic trait used as classes in our study. Host association is inherited both genetically and environmentally, making KNN, which is based on proximity in hyperdimensional feature space, a strong performer.\n\nWe also investigated the performance of different machine learning and deep learning algorithms across various datasets, including MLST, cgMLST, and WGS data. This allowed us to understand how different data types and algorithms compare in terms of source attribution accuracy. For example, we found that decision-tree based ensemble learners, such as random forests and XGBoost, performed well across all datasets. Among deep learning architectures, recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) performed best, which is expected given the sequential nature of DNA transcription and translation.\n\nIn summary, our evaluation included comparisons to publicly available methods like iSource, as well as simpler baselines like KNN. We also conducted extensive comparisons across different datasets and algorithms to provide a comprehensive assessment of our approach's performance.",
  "evaluation/confidence": "The evaluation of our models included a robust assessment of performance metrics with confidence intervals. Specifically, we used repeated undersampling with replacement to ensure that all available samples were utilized in testing. This process was repeated 200 times, and the accuracy was averaged over all iterations. Additionally, we recorded one standard error to provide a measure of variability and confidence in our results.\n\nTo ensure statistical significance, we compared our machine learning classifiers against established methods, such as iSource. The results showed that our best-performing classifier, XGBoost on cgMLST data, achieved higher accuracy than iSource, demonstrating its superiority. The performance metrics, including accuracy, precision, recall, F1 score, negative predictive value, and specificity, were thoroughly evaluated to ensure that the improvements were statistically significant.\n\nFurthermore, we measured the speed of our algorithm relative to other classifiers, providing a normalized scale where 0 represents the slowest and 1 the quickest. This metric, along with the other performance indicators, was used to validate the effectiveness and efficiency of our approach. The use of cyclical learning rates and early stopping criteria during training also helped in achieving robust and statistically significant results.",
  "evaluation/availability": "All data used in this study can be found on PubMLST using the accession numbers provided in the supplementary table. The data has also been uploaded as a public dataset. This dataset is available under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
}