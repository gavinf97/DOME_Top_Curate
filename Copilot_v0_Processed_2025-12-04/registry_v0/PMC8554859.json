{
  "publication/title": "iTTCA-RF: a robust computational predictor for tumor T cell antigen identification",
  "publication/authors": "The authors who contributed to the article are:\n\n- QZ\n- HG\n- LS\n- SJ\n\nQZ, HG, and LS contributed to the conception of the study. SJ performed the experiment. QZ and HG analyzed the results. SJ and HG wrote and revised the manuscript. All authors read and approved the final manuscript.",
  "publication/journal": "Journal of Translational Medicine",
  "publication/year": "2021",
  "publication/doi": "10.1186/s12967-021-03084-x",
  "publication/tags": "- Tumor T cell antigens\n- Computational prediction\n- Machine learning\n- Feature selection\n- Imbalanced data\n- Random Forest\n- Cross-validation\n- Bioinformatics\n- Cancer vaccine research\n- Protein sequence analysis",
  "dataset/provenance": "The dataset used in this research was collected by Charoenkwan et al. The dataset consists of MHC class I peptides, which were gathered from TANTIGEN and TANTIGEN 2.0 for positive samples. Negative samples were obtained from the IEDB database, ensuring that they had no relationship with any disease. Duplicate peptide sequences were removed to avoid redundancy. The final dataset comprises 592 positive samples and 393 negative samples. This dataset was divided into training and independent test sets, with 80% of the samples used for training and the remaining 20% for testing. This division ensures a robust evaluation of the model's performance.",
  "dataset/splits": "There were two data splits: the training dataset and the independent test dataset. The training dataset consisted of 80% of the total samples, while the independent test dataset consisted of the remaining 20%.\n\nThe total number of samples was 985, with 592 positive samples and 393 negative samples. Therefore, the training dataset contained approximately 788 samples, and the independent test dataset contained approximately 197 samples. The distribution of positive and negative samples in each split was maintained proportionally to the original dataset.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in this study are publicly available. They can be accessed at the following URL: http://lab.malab.cn/~acy/iTTCA. The data includes the benchmark datasets collected by Charoenkwan et al., which were constructed using MHC class I peptides from TANTIGEN and TANTIGEN 2.0 for positive samples, and non-TTCA samples from the IEDB database for negative samples. Duplicate peptide sequences were eliminated to ensure data integrity.\n\nThe data splits used in the study, including the training and independent test datasets, are also part of the publicly available material. The training dataset consists of 80% of the samples, while the independent test dataset consists of the remaining 20%. This split was done randomly to ensure a fair evaluation of the models.\n\nThe data is released under a license that allows for public access and use, facilitating reproducibility and further research by other scientists in the field. The availability of the data ensures that other researchers can verify the results and build upon the findings presented in this study.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is ensemble learning, specifically focusing on tree-based methods. We employed six widely used classifiers to identify the most suitable machine learning algorithm for our task. These classifiers include random forest (RF), support vector machine (SVM), adaboost (AB), logistic regression (LR), bagging, and gradient boosting machine (GBM). Among these, the random forest classifier demonstrated the highest performance across various feature sets and evaluation metrics.\n\nThe machine-learning algorithms used are not new; they are well-established and commonly utilized in the field of machine learning. These algorithms were implemented using the scikit-learn package, which is a popular and reliable library for machine learning in Python. The choice to use these established algorithms was driven by their proven effectiveness and robustness in handling complex classification tasks.\n\nGiven that the algorithms are not novel, it is not applicable to discuss why they were not published in a machine-learning journal. Our focus was on applying these algorithms to a specific biological problem, rather than developing new machine-learning techniques. The optimization of hyper-parameters was conducted using grid search, and the search range was detailed in additional supplementary materials. This approach ensured that the models were finely tuned to achieve optimal performance for the task at hand.",
  "optimization/meta": "The model described in the publication is not a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on hybrid features derived from various feature descriptors, specifically GPSD, GAAPC, and PAAC. These features are used to train a single machine-learning model, namely the Random Forest (RF) classifier.\n\nThe RF classifier was chosen after evaluating six widely used classifiers, including support vector machine (SVM), adaboost (AB), logistic regression (LR), bagging, and gradient boosting machine (GBM). The performance of these classifiers was assessed using tenfold cross-validation (CV) and independent tests. The RF classifier demonstrated superior performance, particularly in handling imbalanced datasets, which is crucial for the task of identifying tumor T cell antigens (TTCA).\n\nThe training data used for the RF classifier was balanced using the SMOTE-Tomek technique, a hybrid-sampling method that combines over-sampling and under-sampling. This approach helps to mitigate the issues of overfitting and loss of key information, ensuring that the model generalizes well to unseen data.\n\nIn summary, the model is not a meta-predictor but rather a single RF classifier trained on hybrid features. The training data is independent, and the model's performance was rigorously evaluated using standard machine-learning techniques.",
  "optimization/encoding": "In our study, we employed several feature representation methods to encode the data for the machine-learning algorithms. Specifically, we utilized four different feature descriptors: Global Protein Sequence Descriptors (GPSD), Adaptive Skip Dipeptide Composition (ASDC), Grouped Amino Acid and Peptide Composition (GAAPC), and Pseudo Amino Acid Composition (PAAC). Each of these methods captures different aspects of the protein sequences, providing a comprehensive set of features for the classifiers.\n\nGPSD encapsulates global sequence information, offering a broad overview of the protein's characteristics. ASDC focuses on dipeptide compositions with adaptive skipping, which helps in capturing local sequence patterns. GAAPC groups amino acids and peptides into categories, reducing the dimensionality while retaining essential information. PAAC incorporates sequence-order information, which is crucial for understanding the structural and functional properties of proteins.\n\nTo handle the issue of redundant information and overfitting, we applied the Maximum Relevance Maximum Distance (MRMD) feature selection method. MRMD evaluates the correlation between features and the target class using the Pearson correlation coefficient and assesses redundancy among features using distance metrics such as Euclidean distance, cosine distance, and the Tanimoto coefficient. This approach ensures that the selected features are highly relevant to the class labels and have minimal redundancy, thereby improving the model's performance.\n\nAdditionally, we addressed the challenge of data imbalance, which is common in bioinformatics and other fields. We employed the SMOTE-Tomek hybrid-sampling method to balance the training dataset. SMOTE generates synthetic samples for the minority class, while Tomek's links remove borderline samples that can cause overfitting. This combination helps in maintaining the integrity of the data while effectively balancing the classes.\n\nThe encoded features were then fed into six widely used classifiers: Random Forest (RF), Support Vector Machine (SVM), AdaBoost (AB), Logistic Regression (LR), Bagging, and Gradient Boosting Machine (GBM). These classifiers were implemented using the scikit-learn package, and their hyperparameters were optimized through grid search. The performance of these classifiers was evaluated using tenfold cross-validation (CV) and independent tests, with metrics such as balanced accuracy (BACC), true positive rate (TPR), true negative rate (TNR), and the area under the receiver operating characteristic curve (AUC) to ensure a comprehensive assessment of the models.",
  "optimization/parameters": "In the optimization process, the number of parameters used in the model was determined through a feature selection strategy. Initially, a hybrid feature set consisting of GPSD, GAAPC, and PAAC was used, resulting in a 365-dimensional feature vector. To identify the optimal feature subset, the Maximum Relevance Maximum Distance (MRMD) algorithm was employed to rank these features based on their importance. Subsequently, the Incremental Feature Selection (IFS) strategy was applied to systematically evaluate different feature subsets. A total of 365 Random Forest (RF) models were trained, each with a feature subset containing 1, 2, 3, ..., up to 365 features. The performance of these models was assessed using five metrics: balanced accuracy (BACC), area under the receiver operating characteristic curve (AUC), sensitivity (Sn), specificity (Sp), and Matthew's correlation coefficient (MCC). The tenfold cross-validation (CV) BACC scores indicated that the model's performance improved significantly as more features were added, particularly when the feature dimension was less than 60. Beyond this point, the performance gains plateaued. The optimal feature subset was found to contain 263 features, achieving the highest tenfold CV BACC of 83.71%. This subset was selected as the final optimal feature space, named F263, for building a robust and generalizable model.",
  "optimization/features": "In the optimization process, the input features used were derived from a combination of four single descriptors: GPSD, ASDC, GAAPC, and PAAC. Initially, a 365-dimensional hybrid feature vector was created by combining these descriptors. Feature selection was performed using the MRMD algorithm, which ranks features based on their importance and redundancy. This process was conducted using only the training set to ensure that the selection was unbiased and to prevent data leakage.\n\nThe MRMD algorithm considers two main factors: the correlation between the feature and the target class vector, measured by the Pearson correlation coefficient, and the redundancy between features, determined by distance formulas such as Euclidean distance, cosine distance, and the Tanimoto coefficient. Features with a higher Pearson correlation coefficient and lower redundancy were selected.\n\nFollowing the feature ranking, the incremental feature selection (IFS) strategy was applied to determine the optimal feature vector space for the random forest (RF) classifier. A total of 365 RF models were trained on feature subsets with dimensions ranging from 1 to 365. The performance of these models was evaluated using five metrics: balanced accuracy (BACC), area under the curve (AUC), sensitivity (Sn), specificity (Sp), and Matthews correlation coefficient (MCC).\n\nThe tenfold cross-validation (CV) BACC scores increased sharply as more features were added, particularly when the feature dimension was less than 60. The scores then approached a slowly fluctuating rising plateau. The model achieved the maximum tenfold CV BACC of 83.71% when the feature dimension reached 263. Therefore, the top 263-dimensional feature subset, named F263, was selected as the final optimal feature space for constructing the predictive model. This subset was chosen for its good robustness and generalization capabilities.",
  "optimization/fitting": "In our study, we employed a robust feature selection strategy to mitigate the risk of overfitting, which is a common concern when the number of features is large relative to the number of training samples. We initially extracted a comprehensive set of features, totaling 365 dimensions, from a combination of GPSD, GAAPC, and PAAC descriptors. To address the potential issue of overfitting, we utilized the Minimal Redundancy-Maximal Relevance-Dependency (MRMD) algorithm to rank these features based on their importance and relevance to the target classification. This step ensured that we retained features that were strongly correlated with the class labels while minimizing redundancy.\n\nFollowing the feature ranking, we applied the Incremental Feature Selection (IFS) strategy. This involved training a series of Random Forest (RF) models, each with an increasing number of top-ranked features, ranging from 1 to 365. By evaluating the performance of these models using tenfold cross-validation (CV), we observed that the balanced accuracy (BACC) scores increased sharply as more features were added, up to a point where the performance plateaued. This indicated that the model was not overfitting to the training data, as the addition of more features did not significantly improve the performance beyond a certain threshold.\n\nThe optimal feature subset was determined to be 263 dimensions, which achieved the highest tenfold CV BACC of 83.71%. This subset was selected as it provided a good balance between model complexity and performance, ensuring that the model generalized well to unseen data. Additionally, we used the t-distributed stochastic neighbor embedding (t-SNE) visualization method to validate the effectiveness of the optimal feature set. The t-SNE plots showed that the optimal feature subset (F263) provided a clearer separation between the positive and negative samples compared to the individual feature descriptors, further confirming the robustness of our feature selection process.\n\nTo further ensure the model's generalization capability, we employed the SMOTE-Tomek hybrid-sampling method to balance the training dataset. This approach helped to address the issue of data imbalance, which can lead to underfitting of the minority class. By combining over-sampling and under-sampling techniques, we were able to create a more representative training set, thereby improving the model's ability to generalize to new, unseen data. The significant improvement in performance metrics after applying SMOTE-Tomek, particularly for the RF classifier, demonstrated the effectiveness of this strategy in mitigating both overfitting and underfitting.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the robustness of our models. One of the key strategies we used was the hybrid-sampling method SMOTE-Tomek. This approach combines synthetic minority over-sampling technique (SMOTE) and Tomek’s links (Tomek). SMOTE helps to balance the dataset by generating synthetic samples for the minority class, while Tomek’s links remove borderline samples that are close to each other but belong to different classes. This combination effectively addresses the shortcomings of overfitting associated with SMOTE and the loss of key information caused by Tomek when used individually.\n\nAdditionally, we utilized feature selection methods to mitigate overfitting. Specifically, we applied the Maximum Relevance Maximum Distance (MRMD) algorithm to sort features based on their importance and relevance to the target class. This step helped in reducing the dimensionality of the feature space and eliminating redundant or irrelevant features, which can otherwise lead to overfitting.\n\nFurthermore, we employed cross-validation techniques, particularly tenfold cross-validation, to evaluate and optimize our models. This method ensures that the model is trained and tested on different subsets of the data, providing a more reliable estimate of its performance and reducing the risk of overfitting.\n\nIn summary, our approach to preventing overfitting involved a combination of data balancing through SMOTE-Tomek, feature selection using MRMD, and robust model evaluation through tenfold cross-validation. These techniques collectively contributed to the development of more accurate and generalizable predictive models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule for the classifiers used in this study are available. The hyper-parameters were optimized using grid search, and the search range is detailed in Additional file 1: Table S1. This table provides the specific parameters and their ranges that were explored during the optimization process.\n\nThe model files and optimization parameters are not explicitly mentioned as being available for download. However, the methods and results are thoroughly documented in the supplementary materials, which include additional tables and figures that provide detailed insights into the optimization process and the performance of the models.\n\nThe datasets used in this study are publicly available. They can be accessed at the provided URL: http://lab.malab.cn/~acy/iTTCA. This ensures that other researchers can replicate the experiments and validate the findings.\n\nRegarding the license, the publication does not specify a particular license for the datasets or the supplementary materials. However, the datasets are openly accessible, and the supplementary materials are provided as part of the publication, which is typically under the terms of the journal's open-access policy. For specific licensing details, one would need to refer to the journal's policies or contact the authors directly.",
  "model/interpretability": "The model iTTCA-RF, which we developed, is not entirely a black-box model. While it leverages machine learning techniques, specifically the Random Forest (RF) classifier, we have taken steps to enhance its interpretability. The RF classifier itself is considered more interpretable compared to other complex models like deep neural networks because it provides feature importance scores. These scores indicate the contribution of each feature to the model's predictions, allowing us to understand which features are most influential in distinguishing tumor T cell antigens (TTCA) from non-TTCA.\n\nTo further improve interpretability, we employed feature selection techniques such as Maximum Relevance Maximum Distance (MRMD) and Incremental Feature Selection (IFS). These methods helped us identify the most relevant features from a hybrid set that included Global Protein Sequence Descriptors (GPSD), Grouped Amino Acid and Peptide Composition (GAAPC), and Pseudo Amino Acid Composition (PAAC). By selecting the top 263 features, we not only improved the model's performance but also made it easier to understand which specific features are critical for the predictions.\n\nAdditionally, we used the t-distributed Stochastic Neighbor Embedding (t-SNE) visualization technique to validate the effectiveness of the selected feature set. This visualization method allowed us to observe how well the features separate TTCA from non-TTCA in the feature space, providing a visual interpretation of the model's decision boundaries.\n\nIn summary, while iTTCA-RF is a machine learning model, it incorporates several techniques to enhance its interpretability. The use of feature importance scores, feature selection methods, and visualization techniques like t-SNE makes it more transparent and easier to understand the underlying mechanisms of its predictions.",
  "model/output": "The model is a classification model. It is designed to predict tumor T cell antigens (TTCA) from non-TTCA. The model employs various feature descriptors and classifiers to achieve this. The performance of the model is evaluated using metrics such as balanced accuracy (BACC), sensitivity (Sn), specificity (Sp), and Matthew's correlation coefficient (MCC). The random forest (RF) classifier consistently showed the best performance among the six classifiers investigated, which include support vector machine (SVM), adaboost (AB), logistic regression (LR), bagging, and gradient boosting machine (GBM). The model's performance was significantly improved after using the SMOTE-Tomek resampling technique to balance the positive and negative samples. The best-performing model used a combination of GPSD, GAAPC, and PAAC features, achieving a BACC of 83.03%, Sn of 88.69%, Sp of 77.38%, and MCC of 0.665. The model's output is the classification of sequences into TTCA or non-TTCA categories.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the iTTCA-RF model is not explicitly mentioned as being released. However, a user-friendly online server has been developed to facilitate the use of the model. This web server, accessible at http://lab.malab.cn/~acy/iTTCA, allows users to identify whether their protein sequences are TTCA or non-TTCA. The server accepts input in FASTA format and provides identification results directly on the webpage. Users can also download relevant data and contact the authors through the homepage. The web server is designed to be convenient and accessible, ensuring that researchers can easily utilize the model without needing to implement it locally.",
  "evaluation/method": "In our study, we employed two commonly used methods to evaluate the performance of our models: tenfold cross-validation (CV) and independent tests. These methods were chosen to ensure a thorough and unbiased assessment of our predictor's capabilities.\n\nTenfold cross-validation was used to evaluate and optimize the model. This process involves dividing the dataset into ten equal parts, or folds. The model is then trained on nine of these folds and tested on the remaining fold. This procedure is repeated ten times, with each fold serving as the test set once. The results from these ten iterations are then averaged to produce a single estimation. This method helps to ensure that the model's performance is consistent and not dependent on a particular subset of the data.\n\nIn addition to cross-validation, we also conducted independent tests. For this, we split our dataset into training and testing sets. The training set, which comprised 80% of the samples, was used to train the model, while the remaining 20% served as the independent test set. This approach allows us to assess the model's ability to generalize to new, unseen data.\n\nFor the binary classification task, we used confusion-matrix-based metrics to measure the predictor's performance. These metrics include accuracy (ACC), true negative rate (TNR)/specificity (Sp), true positive rate (TPR)/sensitivity (Sn), and Matthew's correlation coefficient (MCC). However, since our dataset was imbalanced, we primarily relied on balanced accuracy (BACC) to evaluate the overall performance of the models. BACC takes into account the imbalance in the dataset and provides a more accurate measure of the model's performance.\n\nFurthermore, we employed the area under the receiver operating characteristic (auROC, also called AUC) curve. The AUC provides a single scalar value that represents the quality of the model's predictions. A higher AUC indicates better performance.\n\nIn summary, our evaluation method involved a combination of tenfold cross-validation and independent tests, along with a set of performance metrics tailored to handle imbalanced datasets. This comprehensive approach allowed us to thoroughly assess the effectiveness of our predictor.",
  "evaluation/measure": "In our study, we employed several performance metrics to evaluate the effectiveness of our models, particularly focusing on the challenges posed by imbalanced datasets. The metrics we used are commonly reported in the field of protein prediction and are considered representative of the state-of-the-art.\n\nFor binary classification tasks, we utilized confusion-matrix-based metrics, which include accuracy (ACC), true negative rate (TNR) or specificity (Sp), true positive rate (TPR) or sensitivity (Sn), and Matthew's correlation coefficient (MCC). However, due to the imbalanced nature of our datasets, we found that accuracy (ACC) did not perform well. Therefore, we opted for balanced accuracy (BACC) to provide a more accurate measure of our models' overall performance. BACC takes into account the imbalance by calculating the average of sensitivity and specificity, ensuring that the performance on both the minority and majority classes is considered.\n\nAdditionally, we employed the area under the receiver operating characteristic (auROC or AUC) curve. This metric provides a comprehensive view of the model's performance across all classification thresholds, making it a valuable tool for evaluating predictive models.\n\nThese metrics—BACC, AUC, Sn, Sp, and MCC—were used to assess the performance of our models both in tenfold cross-validation (CV) and independent tests. This approach ensures that our evaluation is robust and that our models are generalizable to new, unseen data. The use of these metrics aligns with established practices in the literature, providing a clear and comparable benchmark for our work.",
  "evaluation/comparison": "In our study, we conducted a thorough comparison of our proposed method, iTTCA-RF, with existing tools designed to discriminate between tumor T cell antigens (TTCA) and non-TTCA. Specifically, we compared iTTCA-RF with two previously published classifiers: TTAgP1.0 and iTTCA-Hybrid. This comparison was performed on the same training and testing datasets to ensure a fair evaluation.\n\nThe performance metrics used for this comparison included balanced accuracy (BACC), area under the receiver operating characteristic curve (AUC), sensitivity (Sn), specificity (Sp), and Matthew's correlation coefficient (MCC). These metrics were evaluated using both tenfold cross-validation (CV) and independent tests.\n\nThe results, summarized in a comparison table, showed that iTTCA-RF outperformed both TTAgP1.0 and iTTCA-Hybrid in terms of BACC, AUC, and MCC for both the tenfold CV and independent tests. For instance, iTTCA-RF achieved a BACC of 83.71% and an AUC of 0.894 in the tenfold CV, which were higher than the corresponding values for TTAgP1.0 and iTTCA-Hybrid. Similarly, in the independent test, iTTCA-RF maintained superior performance with a BACC of 73.14% and an AUC of 0.780.\n\nAdditionally, we evaluated the performance of individual feature descriptors and hybrid feature descriptors using the random forest (RF) classifier. The tenfold CV results indicated that the hybrid feature descriptor, obtained through the MRMD and IFS strategies, significantly improved the classification performance compared to individual feature descriptors. This improvement was evident in the t-SNE distribution maps, where the hybrid feature subset provided a clearer distinction between TTCA and non-TTCA samples.\n\nIn summary, our comparison with publicly available methods demonstrated that iTTCA-RF offers superior performance in identifying TTCA and non-TTCA samples. The use of hybrid feature descriptors and advanced machine learning techniques contributed to the enhanced accuracy and reliability of our proposed method.",
  "evaluation/confidence": "Not enough information is available.",
  "evaluation/availability": "Not applicable."
}