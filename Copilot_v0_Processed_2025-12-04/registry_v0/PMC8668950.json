{
  "publication/title": "Protein embeddings and deep learning predict binding residues for various ligand classes",
  "publication/authors": "The authors who contributed to the article are:\n\n- Maria Littmann, who prepared the dataset, implemented and evaluated the final method bindEmbed21 and its two components bindEmbed21DL and bindEmbed21HBI, and performed the major part of manuscript writing and figure generation.\n\n- Michael Heinzinger, who provided ProtT5 embeddings and helped with the original setup of the Deep Learning architecture forming the basis of bindEmbed21DL.\n\n- Christian Dallago, who facilitated the integration of bindEmbed21 into bio_embeddings.\n\n- Konstantin Weissenow, who helped with the original setup of the Deep Learning architecture forming the basis of bindEmbed21DL.\n\n- Burkhard Rost, who supervised and guided the work over the entire time and proofread the manuscript.",
  "publication/journal": "Scientific Reports",
  "publication/year": "2021",
  "publication/doi": "10.1038/s41598-021-03431-4",
  "publication/tags": "- Protein binding prediction\n- Deep learning\n- Homology-based inference\n- Protein embeddings\n- Artificial Intelligence\n- Bioinformatics\n- Computational biology\n- Machine learning\n- Protein structure\n- Ligand binding\n- Protein sequence analysis\n- Binding residue prediction\n- Metal ion binding\n- Nucleic acid binding\n- Small molecule binding",
  "dataset/provenance": "The dataset used in this study was sourced from BioLiP, a database that provides structural and functional annotations for protein-ligand interactions. At the time of data accession, BioLiP contained 104,733 structures with sufficient resolution and binding annotations, which could be mapped to 14,894 unique protein sequences in UniProt. To ensure non-redundancy, the dataset was clustered using UniqueProt with a strict cutoff of HVAL < 0, resulting in a significant reduction to 1,314 proteins. This cutoff ensured that no pair of proteins in the dataset shared more than 20% pairwise sequence identity over 250 aligned residues.\n\nThe final dataset was split into a development set (DevSet1014) consisting of 1,014 proteins, and a test set (TestSet300) with 300 proteins. The development set was further divided into training and validation subsets to optimize model weights and hyperparameters. The test set was designed to allow maximum overlap with the development set of a previous method, bindPredictML, which was trained on enzymes and DNA-binding proteins. This overlap was achieved by including 225 proteins from the original dataset of bindPredictML and adding an additional 75 proteins to slightly adjust for any imbalances.\n\nAdditionally, a new and independent test set (TestSetNew46) was created by extracting sequences with binding annotations added to BioLiP after the initial dataset was built. This set initially contained 1,592 proteins but was reduced to 46 proteins after applying the same redundancy reduction criteria. This new test set highlighted the importance of complementing experimental data with in silico predictions, as it showed that only a small fraction of new experiments provided completely novel insights into binding residues.\n\nThe dataset primarily focused on predicting the binding of metal ions, nucleic acids, and small ligands, excluding peptides. The development set included 13,999 binding residues and 156,684 non-binding residues, while the test set contained 5,869 binding residues and 56,820 non-binding residues. The strict redundancy reduction criteria ensured that the dataset was non-redundant, which is crucial for training robust machine learning models.",
  "dataset/splits": "We utilized three distinct data splits in our study: a development set, a test set, and an additional independent test set. The development set, referred to as DevSet1014, comprises 1,014 proteins with a total of 13,999 binding residues and 156,684 non-binding residues. This set was used for optimizing model weights and hyperparameters. The primary test set, TestSet300, consists of 300 proteins with 5,869 binding residues and 56,820 non-binding residues. This set was designed to evaluate the model's performance on unseen data.\n\nAdditionally, we created a new and independent test set, TestSetNew46, to further validate our model. This set includes 46 proteins with 575 binding residues and 5,652 non-binding residues. The proteins in this set were selected from sequences added to BioLiP after our initial data set was built, ensuring that they were not part of the development or primary test sets. This additional test set helps to assess the model's generalizability and robustness to new, unseen data.\n\nThe distribution of data points across these sets varies by ligand class. For metal ions, the development set has 455 proteins, the primary test set has 122 proteins, and the new test set has 15 proteins. For nucleic acids, the numbers are 108, 66, and 10 proteins, respectively. For small molecules, the development set contains 606 proteins, the primary test set has 220 proteins, and the new test set includes 25 proteins. These distributions reflect the availability of data for each ligand class and ensure a comprehensive evaluation of the model's performance across different types of binding residues.",
  "dataset/redundancy": "The datasets were constructed to ensure a non-redundant split between training and test sets, aiming to prevent information leakage. Initially, a strict cutoff of HVAL<0 was applied using UniqueProt11, reducing the dataset from 14,894 to 1,314 proteins. This cutoff ensured that no pair of proteins in the dataset shared a common binding annotation.\n\nThe final set of 1,314 proteins was split into a development set (DevSet1014) with 1,014 proteins and a test set (TestSet300) with 300 proteins. The development set was used for optimizing model weights and hyperparameters, while the test set was used for evaluating the model's performance.\n\nTo ensure the independence of the training and test sets, further redundancy reduction was performed. This involved assessing the performance of reduced versions of TestSet300 using stricter HVAL cutoffs and E-value cutoffs. The F1 score did not change significantly except for an E-value cutoff of 10, where it dropped by five percentage points but remained within the confidence interval of the full set's performance. This indicated that the redundancy reduction at HVAL=0 yielded a non-redundant dataset without information leakage between the train and test sets.\n\nThe distribution of the datasets compares favorably to previously published machine learning datasets. For instance, the test set had more proteins binding to nucleic acids and small molecules than the development set, which could indicate that binding residues are better defined and easier to predict for enzymes than for other proteins in the development set. Additionally, an independent test set (TestSetNew46) was created from recent annotations, and the model's performance on this set agreed with both the original test and validation sets, further validating the dataset's independence and robustness.",
  "dataset/availability": "The data used in this study is publicly available. The final dataset of 1314 proteins was split into a development set (DevSet1014) with 1014 proteins and a test set (TestSet300) with 300 proteins. Additionally, a new independent test set (TestSetNew46) was created with 46 proteins. Details about the data splits and the redundancy reduction process are provided in the supplementary material, specifically in Supplementary Table S12 and Supplementary Section 2.1 of the Supporting Online Material.\n\nThe data is licensed under a Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source, a link to the Creative Commons license is provided, and any changes made are indicated. The images or other third-party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and the intended use is not permitted by statutory regulation or exceeds the permitted use, permission must be obtained directly from the copyright holder.\n\nTo ensure compliance with the license, users must adhere to the terms specified in the Creative Commons Attribution 4.0 International License. This includes providing proper attribution, linking to the license, and indicating any changes made to the original work. The supplementary material and the main text provide detailed information on the data splits and the methods used, ensuring transparency and reproducibility.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is deep learning, specifically convolutional neural networks (CNNs). This class of algorithms is well-established and widely used in various fields, including bioinformatics.\n\nThe algorithm employed is not entirely new, as it builds upon existing deep learning techniques. However, its application to the specific problem of predicting binding residues for various ligand classes is novel. The method, named bindEmbed21DL, leverages protein embeddings generated by the ProtT5 model, which is a transformer-based language model pre-trained on large-scale protein sequence data.\n\nThe reason this algorithm was not published in a machine-learning journal is that the primary focus of our work is on its application in bioinformatics rather than the development of new machine-learning techniques. The innovation lies in the creative use of existing deep learning methods to solve a specific biological problem, rather than in the creation of a new algorithm. Our study demonstrates the effectiveness of embeddings in outperforming traditional multiple sequence alignment (MSA)-based predictions for binding residue prediction. This highlights the potential of transfer learning and pre-trained models in bioinformatics, where labeled data can be scarce.",
  "optimization/meta": "The final method, bindEmbed21, is indeed a meta-predictor that combines two distinct approaches: homology-based inference (HBI) and machine learning (ML). The ML component, bindEmbed21DL, utilizes embeddings from ProtT5 to predict binding residues. HBI, on the other hand, relies on the assumption that sequence-similar proteins share common functions and binding residues.\n\nThe meta-predictor operates by first attempting to predict binding residues using HBI. If an experimentally annotated sequence-similar protein is available, HBI is used for the prediction. If not, the ML component, bindEmbed21DL, is employed. This combination aims to leverage the strengths of both methods, with HBI providing high accuracy for a subset of proteins and ML offering broader applicability.\n\nRegarding the independence of training data, it is clear that the data used for HBI and ML are distinct. HBI relies on experimental annotations from sequence-similar proteins, while ML is trained on a separate dataset of protein embeddings. This ensures that the training data for each component is independent, reducing the risk of data leakage and overfitting. The combination of these methods has been shown to improve performance, particularly in terms of recall, and to reduce overprediction for all ligand classes.",
  "optimization/encoding": "In our study, we utilized embeddings generated from pre-trained protein language models, specifically ProtT5, to encode protein sequences. These embeddings are fixed-length, per-protein representations that capture the intricate details of protein sequences. For each residue in a protein, ProtT5 generates a 1024-dimensional embedding, which serves as the input for our machine-learning model, bindEmbed21DL.\n\nThe embeddings were fed into a shallow, two-layer Convolutional Neural Network (CNN). This CNN is designed to predict whether each residue in a protein binds to one of three ligand classes: metal ions, nucleic acids (DNA or RNA), or small molecules. The output of the CNN for each residue indicates the likelihood of it being a binding residue for each of the three ligand classes or non-binding.\n\nTo ensure the robustness of our predictions, we also trained a binary version of our model, bindEmbed21DL-binary. This version uses the same dataset and architecture but predicts only whether a residue is binding or non-binding, without distinguishing between ligand classes. The binary model employs a dropout rate of 50% and a weight of 4.2 for the positive class (binding residues) to handle class imbalances effectively.\n\nAdditionally, we integrated a homology-based inference component, bindEmbed21HBI, which combines the de novo predictions from bindEmbed21DL with homology-based information. This integration further enhances the performance of our binding residue predictions.\n\nThe use of embeddings allows for fast and easy predictions for any protein sequence, as these embeddings can be readily extracted from the ProtT5 model. This approach enables scalable and efficient binding residue predictions across a wide range of proteins.",
  "optimization/parameters": "The model employs a two-layer Convolutional Neural Network (CNN) architecture. The first CNN layer consists of 128 feature channels with a kernel size of 5. The second layer has three feature channels, also with a kernel size of 5. The parameters of the model, including the number of feature channels, learning rate, kernel size, and dropout rate, were optimized using an exhaustive grid search. This process ensured that the selected parameters yielded the best performance for the task of binding residue prediction. The dropout rate was set to 70%, which helps in regularizing the model and preventing overfitting. The learning rate for the Adamax optimizer was set to 0.01. These parameters were chosen to balance the trade-off between precision and recall, given the substantial class imbalance between binding and non-binding residues.",
  "optimization/features": "The input features for our model, bindEmbed21DL, are derived from ProtT5-XL-UniRef50, a protein language model. This model generates fixed-length vector representations for each residue in a protein sequence. Specifically, we extract 1024-dimensional vectors for each residue from the last hidden layer of ProtT5. These vectors serve as the sole input features for our model, resulting in f = 1024 features per residue.\n\nFeature selection was not performed in the traditional sense, as we utilized embeddings generated by a pre-trained language model. The features are inherently selected through the training process of the language model on a vast dataset of protein sequences. This approach ensures that the features are rich and informative, capturing complex patterns in protein sequences.\n\nThe embeddings were generated using a model trained on unlabeled protein sequences, ensuring that there is no risk of information leakage or overfitting to a specific task during pre-training. The use of these embeddings as input features allows our model to leverage the learned representations from the language model, which have been shown to be effective for various protein-related tasks.",
  "optimization/fitting": "The fitting method employed in our study utilized a combination of homology-based inference and machine learning techniques to predict binding residues. The machine learning component, bindEmbed21DL, was trained on a dataset that underwent stringent redundancy reduction to prevent information leakage between training and test sets. This reduction significantly decreased the number of available protein sequences, ensuring that the model did not overfit to the training data.\n\nTo address the potential for overfitting, we implemented several measures. First, we used a strict homology value (HVAL) cutoff of 0, which resulted in a dataset where no pairs of proteins shared common binding annotations. This approach ensured that the model did not memorize specific binding patterns from the training set. Additionally, we performed cross-validation and evaluated the model on independent test sets, including a newly annotated set (TestSetNew46), to assess its generalizability. The performance metrics, such as the F1 score, were consistent across different datasets, indicating that the model did not overfit to the training data.\n\nConversely, to rule out underfitting, we compared the performance of bindEmbed21DL with a binarized version trained solely on the distinction of binding versus non-binding. The results showed that bindEmbed21DL performed similarly to its binarized counterpart, suggesting that the model was sufficiently complex to capture the underlying patterns in the data. Furthermore, the model's performance was evaluated using various metrics, including precision, recall, F1 score, and Matthews Correlation Coefficient, which provided a comprehensive assessment of its predictive accuracy.\n\nThe number of parameters in the model was not excessively large compared to the number of training points, thanks to the stringent redundancy reduction. This balance ensured that the model could generalize well to new data without overfitting or underfitting. The use of embeddings, which capture complex relationships in the data, further enhanced the model's ability to make accurate predictions. Overall, the fitting method was designed to achieve a robust balance between model complexity and generalization, ensuring reliable performance on unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our model, bindEmbed21DL. One key approach was the use of cross-validation, which helps to assess the model's performance on different subsets of data, thereby reducing the risk of overfitting to a specific dataset. We observed that performance values were generally higher for the validation set than for the test set, except for binding to metal ions. This discrepancy was within the confidence intervals, indicating that our model's performance was consistent across different datasets.\n\nAdditionally, we created an independent test set from recent annotations to further validate our model's performance. This new test set helped us establish that our performance estimates were statistically significant and reflective of what could be expected for future proteins submitted for prediction.\n\nWe also implemented a reliability index (RI) to help users focus on the most precise and reliable predictions. This index ranges from 0 to 9, with higher values indicating more reliable predictions. By using a cutoff of 0.95, we achieved high precision, suggesting that our model can identify binding residues with a high degree of accuracy.\n\nFurthermore, we compared our method with a binarized version of bindEmbed21DL, which was trained solely on the distinction of binding vs. non-binding. The performance of bindEmbed21DL was similar to its binarized version, indicating that our model is robust and not overly complex.\n\nIn summary, our use of cross-validation, independent test sets, and a reliability index, along with comparisons to simpler models, helped us prevent overfitting and ensure that our model's performance is generalizable to new data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are available. All data, the source code, and the trained model are accessible via GitHub. The specific URL for access is https://github.com/Rostlab/bindPredict. The embeddings can be generated using the bio_embeddings pipeline. Additionally, the method bindEmbed21 and its components bindEmbed21DL and bindEmbed21HBI are publicly available through bio_embeddings. Users can apply the combined method or run its components independently, allowing for binding residue predictions without the need for any alignment method. The license under which these resources are available is the Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source, a link to the Creative Commons license is provided, and any changes made are indicated.",
  "model/interpretability": "The model bindEmbed21DL, which we developed, is not entirely a black-box model. While it does utilize deep learning techniques, specifically a two-layer Convolutional Neural Network (CNN), we have taken steps to ensure some level of interpretability.\n\nFirstly, the model uses ProtT5 embeddings as input, which are fixed-length vector representations for each residue in a protein sequence. These embeddings are generated by a protein Language Model (pLM) trained on a vast amount of unlabeled protein sequences. This means that the features learned by the pLM can be transferred to the task of binding residue prediction, providing some level of interpretability.\n\nSecondly, we have analyzed the internal representations from the first CNN layer. By comparing the representations of falsely predicted binding residues with those of correct predictions, we found evidence that highly reliable, not yet observed predictions clustered with those of experimental annotations. This suggests that the model is not just making arbitrary predictions, but is learning meaningful patterns from the data.\n\nAdditionally, we have defined a reliability index (RI) based on the probability score, which ranges from 0 (unreliable) to 9 (very reliable). This allows users to focus on the most precise and reliable predictions, providing a way to interpret the model's output.\n\nHowever, it's important to note that while these steps provide some level of interpretability, the model is still largely based on deep learning techniques, which are known for their lack of transparency. Therefore, while we can provide some insights into how the model makes predictions, we cannot fully explain the complex interactions and patterns that the model has learned.",
  "model/output": "The model, bindEmbed21DL, is a classification model. It predicts whether a residue in a protein binds to one of three ligand classes—metal ions, nucleic acids, or small molecules—or if it is non-binding. The model uses embeddings generated from protein sequences as input and outputs a probability for each residue indicating its likelihood of binding to each of the three ligand classes or being non-binding. The architecture consists of a two-layer Convolutional Neural Network (CNN) that processes these embeddings to make the predictions. Additionally, a binary version of the model, bindEmbed21DL-binary, simplifies the task to distinguishing between binding and non-binding residues, achieving a slightly higher F1 score but not significantly different from the multi-class model. The performance metrics, such as precision, recall, and F1 score, are used to evaluate the model's classification accuracy.",
  "model/duration": "The model, bindEmbed21DL, was designed for efficiency and speed. Predictions for all human proteins were completed within 80 minutes using a single Xeon machine equipped with 400 GB RAM, 20 cores, and a Quadro RTX 8000 GPU with 48 GB vRAM. This process involved two main steps: generating embeddings and making predictions. The embedding generation took approximately 40 minutes, while the prediction phase also required around 40 minutes. This setup allowed for fast predictions, with each protein sequence taking about 0.2 seconds to process. This efficiency makes bindEmbed21DL suitable for large-scale applications, such as analyzing complete proteomes.",
  "model/availability": "The source code for the method bindEmbed21, including its components bindEmbed21DL and bindEmbed21HBI, is publicly available via GitHub. This allows users to access and utilize the code for their own research or applications. Additionally, the trained model and all relevant data are also provided on the same platform. The embeddings necessary for the predictions can be generated using the bio_embeddings pipeline. Users have the flexibility to apply the combined method or run its components independently, without the need for any alignment method. This accessibility ensures that the method can be easily integrated into various workflows and adapted to different research needs.",
  "evaluation/method": "The evaluation of our method, bindEmbed21DL, involved several key steps and datasets to ensure robust and unbiased performance assessment. We primarily used cross-validation, where the dataset was split into training, validation, and test sets. Performance values were higher for the validation set than for the test set, except for binding to metal ions, due to hyper-parameter optimization on the validation set. The test set included more proteins binding to nucleic acids and small molecules, which allowed bindEmbed21DL to achieve higher values on the test set for these classes.\n\nTo further validate our method, we created an independent test set from recent annotations, consisting of 46 unique proteins out of a total of 1592 new proteins. This new test set helped us establish that our performance estimates were statistically significant and reflective of what could be expected for future proteins submitted for prediction.\n\nPerformance was captured using four per-residue measures: precision, recall, F1 score, and Matthews Correlation Coefficient (MCC). These measures were calculated for each protein individually, and then the mean and symmetric 95% confidence intervals were computed over the resulting distribution. We also compared our method to a recent binding method, bindPredictML, which relies on multiple sequence alignments (MSAs). Our embeddings clearly outperformed MSA-based predictions, demonstrating the effectiveness of our approach.\n\nAdditionally, we assessed the performance of our method using different redundancy reduction techniques, such as HVAL and E-value cutoffs. These assessments ensured that our dataset split represented an unbiased split without information leakage between the training and test sets. The F1 score remained within the confidence interval of the performance for the entire set, indicating that our redundancy reduction methods were effective.\n\nIn summary, our evaluation method involved a combination of cross-validation, independent test sets, and redundancy reduction techniques to ensure a thorough and unbiased assessment of bindEmbed21DL's performance.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the effectiveness of our method. These metrics include recall, precision, F1 score, and Matthews Correlation Coefficient (MCC). Recall, also known as sensitivity, measures the proportion of actual positives that are correctly identified. Precision indicates the proportion of predicted positives that are actually correct. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. The MCC offers a more nuanced view by considering true and false positives and negatives, providing a value between -1 and 1, where 1 indicates perfect prediction.\n\nAdditionally, we calculated negative recall, negative precision, and negative F1 score, which focus on the performance regarding non-binding residues. These metrics are crucial for understanding how well our method identifies residues that do not participate in binding.\n\nWe also introduced CovOneBind, which measures the fraction of proteins for which at least one residue is predicted as binding. This metric is particularly useful for experimentalists who submit single sequences and want to gauge the likelihood of binding. Conversely, CovNoBind indicates the fraction of proteins for which neither predictions nor experiments detected binding, computed for different ligand classes.\n\nThe choice of these metrics is representative of standard practices in the literature, ensuring that our evaluation is both rigorous and comparable to other studies in the field. By using these metrics, we aim to provide a clear and comprehensive assessment of our method's performance, highlighting its strengths and areas for potential improvement.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our method, bindEmbed21DL, against other publicly available methods on benchmark datasets. We specifically compared bindEmbed21DL to methods that predict binding residues, rather than just binding pockets or cavities. Several methods were excluded from comparison for various reasons, such as being template-based, focusing on different types of binding (e.g., DNA or RNA), or not being available for local installation.\n\nOne of the methods we compared against was ProNA2020, which uses multiple sequence alignments (MSAs) to predict binding. Our method, which uses embeddings only, achieved a statistically significant higher performance than ProNA2020 in terms of F1 score for XNA (DNA or RNA) binding, although ProNA2020 had a higher recall. We also compared bindEmbed21DL to ZincBindPredict and PredZinc for zinc-binding predictions. bindEmbed21DL-metal achieved a similar performance to PredZinc but provided predictions for more proteins.\n\nAdditionally, we compared bindEmbed21DL to bindPredictML, another MSA-based method. bindEmbed21DL outperformed bindPredictML by 13 percentage points in F1 score when using annotations from BioLiP. This comparison highlights the strength of our embeddings-only approach over traditional MSA-based methods.\n\nWe also considered simpler baselines, such as template-based methods, but excluded them because our method is designed to work in scenarios where templates are not available. This makes our method more broadly applicable, especially for proteins without known structural templates.\n\nIn summary, we performed a thorough comparison of bindEmbed21DL with other publicly available methods on benchmark datasets, demonstrating its competitive performance and broader applicability.",
  "evaluation/confidence": "In our evaluation, we calculated symmetric 95% confidence intervals (CIs) for each performance measure to provide a range within which the true performance values are likely to fall. These CIs were initially calculated assuming a normal distribution of the performance values, which was justified by the sufficiently large sample size. To ensure the robustness of this assumption, we also computed bootstrapped CIs by repeatedly resampling the dataset and recalculating the performance metrics. The results from both methods were consistent, confirming that the normal distribution assumption is valid.\n\nThe performance metrics, including precision, recall, F1 score, and Matthews Correlation Coefficient (MCC), were evaluated for different datasets: a validation set (DevSet1014), a fixed test set (TestSet300), and an additional test set (TestSetNew46). The error bars in our performance plots indicate the 95% CIs, showing the variability and reliability of our estimates.\n\nFor the validation set, our method achieved an F1 score of 39 ± 2% for predicting any binding residue. Interestingly, the test set showed a slightly higher F1 score of 43 ± 2%, while the additional test set had a similar F1 score of 37 ± 6%. The overlapping confidence intervals suggest that the differences in performance across these datasets are not statistically significant, indicating consistent performance across different data splits.\n\nTo assess the statistical significance of our method's superiority, we compared it with other approaches, particularly those based on multiple sequence alignments (MSAs). Our embeddings-based method clearly outperformed MSA-based predictions, as evidenced by the higher performance metrics and non-overlapping confidence intervals in many cases. This comparison was facilitated by using a subset of the test set that was compatible with the other method's requirements.\n\nIn summary, our evaluation includes robust confidence intervals for all performance metrics, ensuring that our claims about the method's performance are statistically sound. The consistent results across different datasets and the superior performance compared to baselines and other methods provide strong evidence of our method's effectiveness.",
  "evaluation/availability": "The raw evaluation files are not explicitly mentioned as available. However, all data, the source code, and the trained model are publicly accessible via GitHub. The specific repository can be found at https://github.com/Rostlab/bindPredict. Additionally, embeddings can be generated using the bio_embeddings pipeline. The methods bindEmbed21, bindEmbed21DL, and bindEmbed21HBI are also publicly available through bio_embeddings, allowing users to apply the combined method or run its components independently. This means that while the raw evaluation files may not be directly available, the tools and data necessary to reproduce the evaluations are accessible. The software is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source."
}