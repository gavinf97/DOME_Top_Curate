{
  "publication/title": "COVID-19 Screening in Chest X-Ray Images Using Lung Region Priors",
  "publication/authors": "The authors who contributed to the article are:\n\n- Jianpeng An\n- Qing Cai\n- Zhiyong Qu\n- Zhongke Gao\n\nNot sure about the specific contributions of each author.",
  "publication/journal": "IEEE Journal of Biomedical and Health Informatics",
  "publication/year": "2021",
  "publication/doi": "10.1109/JBHI.2021.3104629",
  "publication/tags": "- COVID-19\n- Chest X-ray\n- Lung segmentation\n- Deep learning\n- Medical imaging\n- Domain adaptation\n- Multi-appearance classification\n- Transfer learning\n- Pneumonia screening\n- Multi-scale adversarial learning",
  "dataset/provenance": "The datasets used in our study are publicly available and include lung mask labels. These datasets are the Japanese Society of Radiological Technology dataset, denoted as JSRT, the Montgomery County chest X-Ray set, denoted as MC, and the Shenzhen chest X-ray set, denoted as SC. The input chest X-ray (CXR) images from these datasets are resized to 256 × 256 pixels for both the source and target domains. We constructed six cross-domain combinations, such as J → M, J → C, and M → C, to validate our models. Each dataset was randomly split into training and testing sets according to an 80%/20% division.\n\nFor the COVID-19 classification dataset, we evaluated our model on a public database that combines several databases of normal, viral pneumonia, and COVID-19 CXRs. The appearance images of OC and HPV are resized to 1024 × 1024, and HLS images are resized to 256 × 256. We used 5-fold cross-validation for model evaluation. The datasets used have been previously utilized by the community for similar studies, ensuring that our results are comparable and reproducible.",
  "dataset/splits": "For the lung segmentation datasets, we used three publicly available datasets: the Japanese Society of Radiological Technology dataset (JSRT), the Montgomery County chest X-Ray set (MC), and the Shenzhen chest X-ray set (SC). Each dataset was randomly split into training and testing sets according to an 80%/20% division. This means that 80% of the data from each dataset was used for training the models, while the remaining 20% was used for testing.\n\nFor the COVID-19 classification dataset, we used a public dataset that combines several databases of normal, viral pneumonia, and COVID-19 chest X-rays. We employed 5-fold cross-validation for model evaluation. This involves dividing the dataset into five parts, or folds. The model is trained on four of these folds and tested on the remaining fold. This process is repeated five times, with each fold serving as the test set once. This approach ensures that every data point is used for both training and testing, providing a more robust evaluation of the model's performance.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in our study are publicly available. For lung segmentation, we utilized three datasets: the Japanese Society of Radiological Technology dataset (JSRT), the Montgomery County chest X-Ray set (MC), and the Shenzhen chest X-ray set (SC). These datasets include lung mask labels and are accessible from their respective sources. The input chest X-ray (CXR) images were resized to 256 × 256 for both the source and target domains.\n\nFor COVID-19 classification, we evaluated our model on a public dataset that combines several databases of normal, viral pneumonia, and COVID-19 CXRs. The appearance images of OC and HPV were resized to 1024 × 1024, while HLS images were resized to 256 × 256. This dataset is available online and includes contributions from various sources, ensuring a diverse and representative sample for training and evaluation.\n\nThe datasets were split into training and testing sets according to an 80%/20% division for segmentation tasks, and 5-fold cross-validation was used for the classification task. This approach ensures that the data splits are consistent and reproducible, allowing other researchers to validate our findings. The datasets are released under licenses that permit their use for research purposes, and we adhered to these licenses in our study.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on adversarial learning, specifically using a generator-discriminator framework. The generator network, denoted as G, is responsible for learning common feature distributions between the source and target domains. The discriminator network, D, is trained to classify the segmentation outputs corresponding to either the source or target domain. This adversarial process encourages the generator to produce outputs that are indistinguishable from the target domain, thereby improving the segmentation performance.\n\nThe adversarial learning approach used is not entirely new, as it builds upon established techniques in domain adaptation and generative models. However, the specific application and integration of multi-scale adversarial learning within the semantic prediction space for lung segmentation in chest X-ray images is novel. This adaptation is tailored to handle the unique challenges posed by medical imaging data, particularly in the context of COVID-19 screening.\n\nThe reason this work was published in a biomedical and health informatics journal rather than a machine-learning journal is due to the specific focus on medical applications. The primary contribution lies in the application of advanced machine-learning techniques to solve real-world medical problems, specifically in the domain of COVID-19 screening using chest X-ray images. The integration of multi-scale adversarial learning with semantic prediction space adaptation demonstrates a significant improvement in lung segmentation accuracy, which is crucial for accurate diagnosis and treatment planning. This application-driven approach highlights the practical benefits of the proposed method in a clinical setting, making it more relevant to a biomedical and health informatics audience.",
  "optimization/meta": "The model employs a meta-predictor approach, integrating multiple appearance images to enhance the classification of pulmonary diseases, including COVID-19. The meta-predictor, referred to as MA-Net, aggregates outputs from three sub-networks, each specialized in processing different appearance images derived from chest X-ray (CXR) scans. These appearances include the original CXR image (OC), the high-pass filtered version (HPV), and the high-lighted segmented lung region (HLS).\n\nThe sub-networks are trained individually on their respective appearance images and then combined at the decision level. This fusion of multi-appearance images leverages the strengths of each appearance type, leading to improved classification performance. The meta-predictor uses a softmax function to integrate the outputs from the sub-networks, ensuring that the final classification decision is based on a comprehensive analysis of the lung region's morphological characteristics.\n\nThe training data for each sub-network is independent, as they are derived from different transformations of the same CXR images. This independence ensures that the meta-predictor benefits from diverse feature representations, enhancing its robustness and accuracy in classifying pneumonia diseases, including COVID-19.",
  "optimization/encoding": "The data encoding process involved generating multi-appearance chest X-ray (CXR) images based on lung region priors. The original CXR image was characterized as the original appearance (OC). Using the segmented lung region from the OC image, two additional appearances were created: the high-lighted lung segment (HLS) appearance, where pixel values of the lung region were set to 0, and the high-lighted pulmonary vessels (HPV) appearance, where pixel values of the non-lung region were set to 0. These appearances were then used as inputs for the multi-appearance network (MA-Net) to improve the classification accuracy for COVID-19 screening. The process ensured that the model could leverage different visual characteristics of the lung region to enhance its diagnostic capabilities.",
  "optimization/parameters": "In our study, the model architecture primarily consists of an encoder-decoder network based on the U-Net model for the segmentation task and DenseNet for the classification task. The segmentation network employs multiple discriminators to receive feedback at the semantic prediction space via multi-scale features. Each encoder stage in the segmentation network includes two convolutional layers, batch normalization, leaky-ReLU activation, and a max-pooling layer. The decoder stages are constructed with an upsampling layer, two convolution layers, batch normalization, and leaky-ReLU activation. The discriminator consists of five 4 × 4 convolutional layers with a stride of 2 and varying numbers of feature maps.\n\nFor the classification network, DenseNet is used as the backbone model, which includes a 7 × 7 convolutional layer with a 3 × 3 max-pooling, and four dense blocks with a transition layer between contiguous dense blocks. The output of the last average pooling layer in each sub-network is concatenated, followed by two extra fully connected layers for the three-category classification task.\n\nThe specific number of parameters (p) in the model was not explicitly stated, but the architecture details provide insight into the complexity and the number of layers involved. The selection of these parameters was guided by the need to achieve high performance in both segmentation and classification tasks. The use of established architectures like U-Net and DenseNet, along with multi-scale adversarial learning and focal loss, ensures that the model can handle the complexities of medical image analysis effectively.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "Not applicable",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One key method used was early stopping. We set the patience for the early stop scheme to 20, which means that if the model's performance on the validation set did not improve for 20 consecutive epochs, the training process would be halted. This approach helps to prevent the model from overfitting to the training data by stopping the training when the model starts to memorize the training examples rather than generalizing from them.\n\nAdditionally, we utilized a polynomial decay strategy for the learning rate. The initial learning rate was set to 1 × 10−4 for the classification network and was decreased using a polynomial decay with a power of 0.9. This gradual reduction of the learning rate helps the model to converge more smoothly and avoids large updates that could lead to overfitting.\n\nFor the segmentation network, we used the Adam optimizer with an initial learning rate of 2 × 10−4, which was also subject to polynomial decay. This optimizer is known for its adaptive learning rate properties, which can help in stabilizing the training process and preventing overfitting.\n\nFurthermore, we employed data augmentation techniques to increase the diversity of the training data. By generating multi-appearance images (OC, HPV, and HLS) from the original chest X-ray images, we effectively augmented the dataset, making the model more robust and less likely to overfit to the specific patterns in the training data.\n\nIn summary, our regularization methods included early stopping, learning rate decay, and data augmentation, all of which contributed to preventing overfitting and improving the generalization performance of our models.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules used in our experiments are detailed within the publication. Specifically, for the segmentation network, we trained each cross-domain combination model for 500 epochs with a batch size of 4. The Adam optimizer was employed with an initial learning rate of 2 ×10−4, which was decreased using polynomial decay with a power of 0.9. For the classification network, the batch size was set to 2, and the maximum number of epochs was 100. The min-batch stochastic gradient descent was used with an initial learning rate of 1 ×10−4. To prevent overfitting, an early stopping scheme with a patience of 20 was implemented.\n\nThe model files and optimization parameters are not explicitly provided in the publication. However, the implementation details, including the frameworks and hardware used, are specified. All experiments were conducted using the PyTorch framework on two NVIDIA RTX 2080Ti GPUs. The evaluation metrics, such as the Dice Similarity Coefficient (DSC) for segmentation and accuracy, F1-score, recall, and precision for classification, are also clearly defined.\n\nRegarding the availability and licensing of the configurations and parameters, the publication does not provide direct links or repositories for downloading the specific model files or optimization parameters. However, the methods and configurations are described in sufficient detail to allow replication of the experiments by other researchers. For access to the datasets used, references to the publicly available datasets are provided, which can be accessed according to their respective licensing agreements.",
  "model/interpretability": "The model presented in this work is not a black-box but rather a transparent system designed to enhance interpretability in medical imaging, particularly for COVID-19 screening using chest X-ray (CXR) images. The transparency of the model is achieved through several key components and methodologies.\n\nFirstly, the model utilizes lung region priors to generate multi-appearance images, which are then used for classification. This approach leverages the morphological characteristics of the lung region, making the decision-making process more interpretable. By focusing on specific lung regions, the model can highlight areas of interest that are crucial for diagnosing pulmonary diseases, including COVID-19.\n\nSecondly, the model employs an encoder-decoder network based on the U-Net architecture for lung segmentation. This network architecture is well-known for its ability to provide detailed and precise segmentation maps, which are essential for understanding the spatial distribution of abnormalities in the lung. The use of multi-scale features in the semantic prediction space further enhances the model's ability to capture fine details, making the segmentation results more interpretable.\n\nAdditionally, the classification network, MA-Net, aggregates information from three different appearances of the lung region: OC (Original Chest X-ray), HPV (High-Pass Filtered), and HLS (Hue, Lightness, Saturation). By concatenating the outputs of these sub-networks, the model can integrate diverse visual information, leading to more robust and interpretable classification results. The use of DenseNet as the backbone model ensures that the network can effectively learn and represent complex patterns in the data.\n\nFurthermore, the model incorporates focal loss to handle class imbalance and hard samples, which are common in medical diagnosis. This loss function helps in balancing the weights of different classes, making the model's predictions more reliable and interpretable. The use of focal loss also ensures that the model pays more attention to difficult cases, which is crucial for accurate diagnosis.\n\nIn summary, the model's transparency is achieved through the use of lung region priors, multi-scale feature extraction, and the integration of multiple appearances. These components work together to provide clear and interpretable results, making the model a valuable tool for COVID-19 screening and other pulmonary disease diagnoses.",
  "model/output": "The model is a classification model. It is designed to classify chest X-ray (CXR) images for COVID-19 screening. The model, referred to as MA-Net, utilizes multi-appearance images derived from the lung region to improve classification accuracy. It contains three sub-networks, each associated with a different appearance of the lung region: original CXR (OC), high-lighted lung segment (HLS), and high-pass filtered version (HPV). These sub-networks are trained separately and their outputs are concatenated for the final classification task, which involves categorizing the images into different classes, including COVID-19. The model employs DenseNet as the backbone for each sub-network, and the final classification is performed using fully connected layers.",
  "model/duration": "Not enough information is available.",
  "model/availability": "Not enough information is available.",
  "evaluation/method": "In our evaluation, we employed several metrics and techniques to assess the performance of our methods. For lung segmentation, we used the Dice Similarity Coefficient (DSC) to evaluate segmentation performance. DSC is a common choice in unsupervised domain adaptation medical segmentation tasks. It is defined as:\n\nDSC (A,B) = 2 ×|A ∩B| / (|A|+ |B|)\n\nwhere A and B represent the lung region of prediction and ground truth, respectively.\n\nFor classification evaluation, we used accuracy, F1-score, recall, and precision. These metrics are defined as follows:\n\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\n\nF1 = 2 × (Precision × Recall) / (Precision + Recall)\n\nRecall = TP / (TP + FN)\n\nPrecision = TP / (TP + FP)\n\nWe conducted experiments using the PyTorch framework on two NVIDIA RTX 2080Ti GPUs. For the segmentation network, each cross-domain combination model was trained for 500 epochs with a batch size of 4. The Adam optimizer was used with an initial learning rate of 2 ×10−4, which was decreased using polynomial decay with a power of 0.9.\n\nFor the classification network, we set the batch size to 2 and the maximum number of epochs to 100. Min-batch stochastic gradient descent with an initial learning rate of 1 ×10−4 was chosen for training. To avoid overfitting, we set the patience of the early stop scheme to 20.\n\nWe validated our MS-AdaNet using three publicly available datasets: the Japanese Society of Radiological Technology dataset (JSRT), the Montgomery County chest X-Ray set (MC), and the Shenzhen chest X-ray set (SC). The input CXR images were resized to 256 ×256 for both source and target domains. Six cross-domain combinations were constructed, and each dataset was randomly split into training and testing sets according to an 80%/20% division.\n\nFor the COVID-19 classification dataset, we evaluated our MA-Net on a public dataset that combines several databases of normal, viral pneumonia, and COVID-19 CXRs. The appearance images of OC and HPV were resized to 1024 ×1024, and HLS were resized to 256 ×256. We used 5-fold cross-validation for model evaluation.\n\nTo demonstrate the effectiveness of our methods, we designed comparative experiments and ablation studies. We compared our MS-AdaNet with other unsupervised domain adaptation methods, including CycleGAN, MUNIT, AdaptSegNet, and SeUDA. We also evaluated the impact of transfer learning and the effectiveness of multi-appearance classification. The results showed that our proposed methods achieved satisfactory performance in lung segmentation and COVID-19 screening tasks.",
  "evaluation/measure": "For the evaluation of our models, we employed a set of performance metrics that are widely recognized and used in the literature for both segmentation and classification tasks.\n\nFor segmentation, we used the Dice Similarity Coefficient (DSC). This metric is commonly used in medical image segmentation tasks due to its robustness in measuring the overlap between predicted and ground truth regions. The DSC ranges from 0 to 1, where 1 indicates perfect overlap.\n\nFor classification, we reported accuracy, F1-score, recall, and precision. Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. Recall, also known as sensitivity, measures the proportion of actual positives that are correctly identified. Precision, also known as positive predictive value, measures the proportion of positive identifications that are actually correct.\n\nThese metrics provide a comprehensive evaluation of our models' performance, ensuring that we capture both the effectiveness of segmentation and the accuracy of classification. The choice of these metrics aligns with standard practices in the field, making our results comparable to other studies in medical image analysis.",
  "evaluation/comparison": "In the evaluation of our methods, we conducted a comprehensive comparison with publicly available methods on benchmark datasets. Specifically, we validated our MS-AdaNet using three publicly available datasets: the Japanese Society of Radiological Technology dataset (JSRT), the Montgomery County chest X-Ray set (MC), and the Shenzhen chest X-ray set (SC). These datasets include lung mask labels and were used to construct six cross-domain combinations for training and testing.\n\nFor the classification task, we evaluated our MA-Net on a public dataset that combines several databases of normal, viral pneumonia, and COVID-19 chest X-rays. This dataset was used to assess the performance of our method in classifying different types of pneumonia, including COVID-19.\n\nIn addition to comparing with state-of-the-art methods, we also performed comparisons with simpler baselines. For instance, we conducted experiments to demonstrate the effectiveness of domain adaptation by comparing supervised training on the target domain (as an upper bound) and training the segmentation model directly on the source domain without adaptation (as a lower bound). These comparisons helped us understand the impact of our domain adaptation techniques on lung segmentation performance.\n\nFurthermore, we evaluated the impact of using different backbone networks in our MA-Net. We found that using DenseNet-121 as the backbone achieved the best performance, highlighting the importance of choosing an appropriate architecture for the classification task.\n\nOverall, our evaluation included comparisons with both advanced methods and simpler baselines, providing a thorough assessment of our approach's effectiveness in lung segmentation and COVID-19 classification.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of our methods includes statistical significance tests to ensure the reliability of our results. For instance, when comparing our MS-AdaNet with other unsupervised domain adaptation methods, a t-test was conducted to indicate the statistical significance of the differences observed. This test showed that the differences between our method and others were statistically significant (p< 0.05), providing strong evidence that our approach outperforms the compared methods.\n\nIn addition to statistical significance, our evaluation metrics include mean and standard deviation values. For example, the Dice Similarity Coefficient (DSC) performance for lung segmentation is presented with mean and standard deviation, offering a clear view of the variability and consistency of our results across different datasets and domain combinations.\n\nFurthermore, the comparative results in classification tasks, such as those shown in various tables, include metrics like accuracy, precision, recall, and F1-score. These metrics are crucial for understanding the performance of our models in different scenarios and for different types of pneumonia, including COVID-19. The inclusion of these detailed performance metrics allows for a comprehensive evaluation of our methods' effectiveness and robustness.",
  "evaluation/availability": "The evaluation of our proposed framework involved several datasets and metrics, but specific details about the availability of raw evaluation files are not provided. The datasets used for lung segmentation included the Japanese Society of Radiological Technology dataset (JSRT), the Montgomery County chest X-Ray set (MC), and the Shenzhen chest X-ray set (SC). These datasets are publicly available and were used to validate the performance of our MS-AdaNet model. The input CXR images were resized to 256 × 256 for both source and target domains.\n\nFor the COVID-19 classification dataset, a public database was used, which combined several databases of normal, viral pneumonia, and COVID-19 CXRs. The appearance images of OC and HPV were resized to 1024 × 1024, and HLS were resized to 256 × 256. The evaluation metrics included the Dice Similarity Coefficient (DSC) for segmentation performance and accuracy, F1-score, recall, and precision for classification performance.\n\nThe experiments were conducted using the PyTorch framework on two NVIDIA RTX 2080Ti GPUs. The segmentation network was trained for 500 epochs with a batch size of 4, using the Adam optimizer with an initial learning rate of 2 × 10−4. The classification network was trained with a batch size of 2 and a maximum of 100 epochs, using min-batch stochastic gradient descent with an initial learning rate of 1 × 10−4.\n\nThe results of the lung segmentation and COVID-19 classification tasks demonstrated the effectiveness of our framework. The MS-AdaNet model achieved an average DSC of 90.6% for lung segmentation, which is slightly below the supervised results but significantly better than the no-adaptation results. The MA-Net model showed improved performance in COVID-19 classification when using multi-appearance images compared to single-appearance images.\n\nHowever, the specific availability of raw evaluation files, including how and where they can be accessed, is not detailed. Therefore, it is not clear whether these files are publicly released or under what license they might be available."
}