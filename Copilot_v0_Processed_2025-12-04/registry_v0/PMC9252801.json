{
  "publication/title": "Biological Structure and Function",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Nature Biotechnology",
  "publication/year": "2022",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Protein localization\n- Multi-label classification\n- Deep learning\n- Bioinformatics\n- Machine learning\n- Subcellular localization\n- Protein prediction\n- Attention mechanisms\n- Transformer models\n- Performance metrics",
  "dataset/provenance": "The dataset used in this study is primarily derived from the SwissProt database, which is a curated protein sequence database. To generate high-quality data partitions, a procedure was adopted to ensure label-balanced splits for 5-fold cross-validation. This procedure guarantees that each pair of train and test folds does not share sequences with a global sequence identity greater than 30%, as determined using ggsearch36 from the FASTA package.\n\nAdditionally, the Human Protein Atlas (HPA) project provides annotations for 78,136 proteins. The HPA independent dataset was constructed through several steps:\n\n1. Homology reduction to ensure a maximum of 30% sequence identity to the entire SwissProt dataset, resulting in 23,422 proteins.\n2. Selection of only \"Enhanced\" or \"Supported\" annotations to improve label reliability, leaving 5,523 proteins.\n3. Clustering of sequences with a 90% identity threshold and selecting the centroids to reduce correlated errors. This step also involved removing Peroxisome and Lysosome/Vacuole localizations due to their limited numbers, resulting in 2,445 proteins.\n4. Further removal of sequences with greater than 30% sequence identity to any sequence from the multi-label eukaryote dataset used to train Fuel-mLoc, leaving 1,717 proteins.\n\nThe dataset includes proteins categorized into one or multiple of the following ten locations: Cytoplasm, Nucleus, Extracellular, Cell membrane, Mitochondrion, Plastid, Endoplasmic reticulum, Lysosome/Vacuole, Golgi apparatus, and Peroxisome. The details of the sublocation mapping and the number of proteins in each category are provided in supplementary tables.\n\nThe HPA dataset is used for independent validation and ensures that no sequences have more than 30% global sequence identity with the SwissProt Localization dataset. This dataset is crucial for evaluating the performance of various subcellular localization prediction methods.",
  "dataset/splits": "In our study, we utilized two primary datasets for evaluation: the SwissProt dataset and the Human Protein Atlas (HPA) dataset.\n\nFor the SwissProt dataset, we generated high-quality data partitions using a procedure that ensures label-balanced splits for 5-fold cross-validation. This means the dataset was divided into five parts, or folds. Each fold was used once as a test set while the remaining four folds were used for training. This process was repeated five times, with each fold serving as the test set once. The procedure ensures that each pair of train and test folds does not share sequences that have a global sequence identity greater than 30%.\n\nThe HPA dataset was constructed through several steps to ensure reliability and reduce correlated errors. Initially, it contained annotations for 78,136 proteins. After homology reduction, selection of reliable annotations, clustering of sequences, and removal of specific localizations, the dataset was narrowed down to 1,717 proteins. This dataset was used as an independent test set to validate our models.\n\nThe distribution of proteins in the SwissProt dataset across different subcellular locations is detailed in a supplementary table. For instance, the nucleus location contains 9,720 proteins, while the peroxisome location contains 304 proteins. The HPA dataset, being an independent test set, does not have a direct split but is used to evaluate the generalizability of our models.\n\nIn summary, we used a 5-fold cross-validation approach for the SwissProt dataset and an independent test set from the HPA dataset. The SwissProt dataset was divided into five folds, each containing a balanced representation of different subcellular locations. The HPA dataset, after rigorous filtering, consisted of 1,717 proteins and was used for independent validation.",
  "dataset/redundancy": "In our study, we focused on generating high-quality data partitions for the SwissProt dataset to ensure robust and reliable model training and evaluation. We adopted a procedure described by Gíslason et al. to create label-balanced splits for 5-fold cross-validation. This method ensures that each pair of training and test folds does not share sequences with a global sequence identity greater than 30%, as determined using ggsearch36 from the FASTA package. This step is crucial to prevent data leakage and ensure that the model's performance is a true reflection of its ability to generalize to unseen data.\n\nFor the Human Protein Atlas (HPA) independent dataset, we followed a series of steps to construct a reliable test set. First, we performed homology reduction to ensure a maximum of 30% sequence identity to the entire SwissProt dataset using USEARCH. This left us with 23,422 proteins. Next, we selected only \"Enhanced\" or \"Supported\" annotations to improve the reliability of the labels, reducing the dataset to 5,523 proteins. We then clustered sequences with a 90% identity threshold and selected the centroids using USEARCH to reduce measuring correlated errors. Additionally, we removed sequences with greater than 30% sequence identity to any sequence from the multi-label eukaryote dataset used to train Fuel-mLoc, resulting in a final set of 1,717 proteins.\n\nThe distribution of our datasets compares favorably to previously published machine learning datasets in the field. By ensuring that our training and test sets are independent and by enforcing strict homology reduction, we aim to provide a more accurate assessment of model performance. This approach helps in mitigating overfitting and ensures that the models can generalize well to new, unseen data.",
  "dataset/availability": "The datasets used in our study are publicly available. We curated three main datasets: two for subcellular localization and one for sorting signals. The subcellular localization datasets include a cross-validation set derived from the SwissProt database and an independent test set from the Human Protein Atlas (HPA) project. The sorting signals dataset contains experimentally verified annotations compiled from the literature.\n\nThe SwissProt localization dataset was extracted from the UniProt database release 2021_03. The proteins were filtered to include only eukaryotic, non-fragment sequences encoded in the nucleus, with more than 40 amino acids and experimentally annotated subcellular localizations. These proteins are categorized into ten locations: Cytoplasm, Nucleus, Extracellular, Cell membrane, Mitochondrion, Plastid, Endoplasmic reticulum, Lysosome/Vacuole, Golgi apparatus, and Peroxisome.\n\nThe HPA dataset provides subcellular localization of human proteins using confocal microscopy. We considered only \"Enhanced\" and \"Supported\" annotations for our independent test set to ensure reliability. This dataset was ensured to have no sequences with more than 30% global sequence identity with the SwissProt localization dataset.\n\nThe sorting signals dataset includes annotations for signal peptides, transmembrane domains, mitochondrial transit peptides, chloroplast transit peptides, thylakoidal lumen composite transit peptides, nuclear localization signals, nuclear export signals, and peroxisome targeting signals. These annotations were compiled from various literature sources and filtered to exclude proteins not present in our SwissProt localization dataset.\n\nAll datasets are available in a public forum, and the specific details and access information can be found in the supplementary materials. The data splits used for cross-validation and independent testing are also provided, ensuring reproducibility. The datasets are released under a license that allows for academic and research use, with proper citation of the original work. The enforcement of data usage is managed through the licensing agreements and the provision of detailed documentation on how the datasets were curated and partitioned.",
  "optimization/algorithm": "The optimization algorithm employed in our work leverages transformer models, which are a class of machine-learning algorithms known for their effectiveness in handling sequential data. Specifically, we utilize a transformer encoder as part of our architecture. This choice is driven by the transformer's ability to capture long-range dependencies in protein sequences, which is crucial for predicting subcellular localization.\n\nThe algorithm is not entirely new but has been adapted and optimized for our specific task. The transformer architecture has been widely used and studied in the field of natural language processing and has more recently been applied to biological sequences. Our contribution lies in the way we fine-tune and integrate the transformer with additional components, such as an attention-pooling layer and regularization techniques, to enhance its performance for protein localization prediction.\n\nThe decision to publish this work in a bioinformatics journal rather than a machine-learning journal is motivated by the application domain. Our primary focus is on advancing the state-of-the-art in protein subcellular localization prediction, which is a critical problem in bioinformatics. While the underlying machine-learning techniques are well-established, their application to biological data and the specific innovations we introduce are of significant interest to the bioinformatics community. Additionally, the biological insights and practical applications of our method are more directly relevant to researchers in this field.",
  "optimization/meta": "The model described in this publication does not function as a meta-predictor. It does not use data from other machine-learning algorithms as input. Instead, it relies on transformer models, specifically ESM12, ESM1b, and ProtT5, which are trained on large-scale protein datasets. These models are used to predict protein localization and are evaluated using various metrics such as accuracy, Jaccard index, MicroF1, MacroF1, and Matthews Correlation Coefficient.\n\nThe training and evaluation processes involve cross-validation datasets and independent test sets, ensuring that the data used for training and testing are independent. This independence is crucial for assessing the model's performance and generalizability. The datasets are carefully partitioned to avoid any overlap between training and test sets, ensuring that the model's predictions are not biased by the training data.\n\nThe performance of the model is compared against other tools like YLoc+, DeepLoc 1.0, and Fuel-mLoc. The comparisons are made on various metrics and datasets, including the Human Protein Atlas (HPA) dataset, to provide a comprehensive evaluation of the model's capabilities. The results demonstrate that the model outperforms other methods in several metrics, indicating its effectiveness in protein localization prediction.",
  "optimization/encoding": "The data encoding process involved using transformer models to generate vector representations for each residue in the input sequences. Specifically, three models were evaluated: ESM12, ESM1b, and ProtT5. These models output a vector representation for each token (residue) in the sequence.\n\nThe per-token representations were then combined using an attention mechanism. This involved computing a scalar score for each token by taking the dot-product of the representation with a learnable vector. This learnable vector was tuned using supervised learning, incorporating both subcellular localization labels and sequence annotations of sorting signals.\n\nTo account for signals being present in contiguous sets of residues, the scalar scores were smoothed along the sequence using a 1D Gaussian filter of width 5, clipped at one standard deviation. The attention weights over the sequence were then computed using the softmax function on the smoothed scores, ensuring they summed to 1. The final output representation was the attention-weighted sum of the token representations.\n\nThis attention-pooled representation vector served as the input to the prediction stage. The attention weights, which were visualized on the webserver, along with the prediction of sorting signals, provided a better understanding of the model's predictions.\n\nFor the multi-label localization and signal type prediction, the prediction stage consisted of two multi-layer perceptron (MLP) classifier heads. The first head was trained along with the learnable vector from the attention step for the ten-class multi-label subcellular localization task. A second head was trained after freezing the rest of the parameters for the nine-class sorting signal prediction task. These classifiers output a probability for each label.\n\nThe maximum sequence length used for training was 1022 for the ESM models and 4000 for ProtT5. Proteins that exceeded these lengths had the middle portion of their sequence removed to retain the ends. This approach was chosen to accommodate the limitations of the ESM1b model architecture.",
  "optimization/parameters": "In our implementation, we utilized three distinct transformer models, each with a varying number of parameters. The first model is a 12-layer ESM model with 84 million parameters. The second is a more extensive 33-layer ESM model containing 650 million parameters. The third model is the ProtT5-XL-UniRef50, which has 3 billion parameters. These models were chosen based on their established performance in protein sequence analysis and their capacity to handle the complexity of the data.\n\nThe selection of these models was driven by the need to balance computational efficiency and predictive accuracy. The 12-layer ESM model offers a lighter computational load, making it suitable for initial experiments and rapid prototyping. The 33-layer ESM model provides a middle ground, offering improved accuracy without the extensive computational requirements of the largest model. The ProtT5-XL-UniRef50 model, with its 3 billion parameters, was included to push the boundaries of what is achievable in terms of predictive performance, albeit at a higher computational cost.\n\nThe maximum sequence length used for training was set to 1022 for the ESM models due to architectural limitations, particularly with the ESM1b model. For the ProtT5 model, the maximum sequence length was set to 4000. Proteins exceeding these lengths had their middle portions removed to retain the sequence ends, ensuring that critical information at the termini was preserved. This approach was necessary to accommodate the varying sequence lengths in our dataset while maintaining the models' computational feasibility.",
  "optimization/features": "The input features for our models are derived from protein sequences. Specifically, we utilize three different transformer models: ESM12, ESM1b, and ProtT5. These models process the entire sequence of amino acids to generate embeddings that capture the evolutionary and structural information of the proteins.\n\nFeature selection in the traditional sense is not applicable here, as the transformer models inherently extract relevant features from the raw sequence data. The models are trained end-to-end, meaning they learn to identify and utilize the most informative parts of the sequence during training. This approach leverages the power of deep learning to automatically determine which sequence elements are most predictive of protein localization.\n\nThe training process ensures that the models generalize well to unseen data by using cross-validation techniques. This involves splitting the data into training and validation sets multiple times, ensuring that the model's performance is evaluated on different subsets of the data. This method helps in preventing overfitting and ensures that the features learned are robust and generalizable.\n\nIn summary, the input features are the raw protein sequences, and the models learn to extract relevant features during training without the need for explicit feature selection. The training process is designed to ensure that the models perform well on unseen data, validating the robustness of the learned features.",
  "optimization/fitting": "The fitting method employed in our study involves a transformer encoder and an attention-pooling layer, which together have a substantial number of parameters. The transformer encoder, in particular, is known for its high capacity due to the large number of parameters it contains. This can potentially lead to overfitting, especially when the number of training points is limited.\n\nTo mitigate overfitting, several strategies were implemented. Firstly, different learning rates were used for the transformer encoder and the attention-pooling, classification layer. The learning rate for the transformer encoder was set to 5×10−6 if finetuning, and 0 otherwise, while the learning rate for the attention-pooling and classification layer was set to 5×10−5. This approach helps in controlling the update steps of the model parameters, preventing the model from fitting the noise in the training data.\n\nAdditionally, the training was terminated after a fixed number of epochs to ensure that the models always overfit based on the randomly sampled validation set. The maximum number of epochs was set to 5 in the case of finetuning and 15 for the frozen models. The final model was selected based on the best validation loss over all epochs, which helps in selecting a model that generalizes well to unseen data.\n\nMixed-precision and model sharding techniques were also utilized to efficiently fine-tune the models. These techniques help in reducing the memory footprint and computational requirements, allowing for more effective training of large models.\n\nTo rule out underfitting, the model's performance was evaluated on a validation set that was randomly sampled from the training data. The use of a validation set helps in monitoring the model's performance during training and ensures that the model is not too simplistic to capture the underlying patterns in the data.\n\nThe final loss is a weighted combination of three losses: the multi-label loss, the attention supervision loss, and the regularization loss. The attention supervision loss and the regularization loss are scaled by 0.1 to ensure that the secondary losses do not dominate the multi-label loss. This approach helps in balancing the different components of the loss function and ensures that the model is not underfitted to any particular component.\n\nIn summary, the fitting method employed in our study involves a transformer encoder and an attention-pooling layer with a large number of parameters. To mitigate overfitting, different learning rates were used, the training was terminated after a fixed number of epochs, and mixed-precision and model sharding techniques were utilized. To rule out underfitting, the model's performance was evaluated on a validation set, and the final loss is a weighted combination of three losses.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and enhance the robustness of our models. One key method involved the use of a Discrete-Cosine Transform (DCT)-based prior regularization. This approach helped to mitigate artifacts caused by discontinuities at the sequence ends, which are inherent when using Fourier transforms due to their assumption of signal periodicity. By smoothing the raw attention scores with a Gaussian 1-D convolutional filter and padding the attention on both sides, we were able to compute a regularization loss that effectively penalized unwanted patterns.\n\nAdditionally, we incorporated sorting signal supervision, which utilized a normalized KL-divergence loss between the attention and annotated signals. This ensured that the attention mechanisms aligned well with known biological signals, thereby improving the interpretability and stability of the training process.\n\nTo further control overfitting, we used a weighted combination of multiple loss terms. The final loss function was a blend of the multi-label localization loss, the attention supervision loss, and the DCT-based regularization loss. The attention supervision loss and the regularization loss were scaled down to ensure they did not dominate the primary multi-label loss, maintaining a balanced training regime.\n\nMoreover, we implemented early stopping based on validation loss, terminating training after a fixed number of epochs. This ensured that the models did not overfit to the training data. We also utilized mixed-precision and model sharding techniques during training, which helped in efficiently fine-tuning the models without compromising on performance. These techniques collectively contributed to the development of robust and generalizable models.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, and model files are not explicitly detailed in the provided information. However, some optimization parameters and training details are mentioned.\n\nThe training process utilized different learning rates for the transformer encoder and the attention-pooling, classification layer. Specifically, a learning rate of 5×10−6 was used for fine-tuning the transformer encoder, while a learning rate of 5×10−5 was applied to the attention-pooling and classification layer. The training was terminated after a fixed number of epochs to ensure overfitting based on a randomly sampled validation set. The maximum number of epochs was 5 for fine-tuning and 15 for frozen models. The final model was selected based on the best validation loss over all epochs.\n\nMixed-precision and model sharding techniques were employed to efficiently fine-tune the models. The PyTorch-lightning library and hardware provided by Google Colaboratory GPUs, including 1 and 2 Tesla V100s, were used for training and testing.\n\nRegarding the availability of these configurations and parameters, specific details about where to access them or their licensing are not provided. Therefore, it is not clear if these configurations are publicly available or under what terms they might be accessible.",
  "model/interpretability": "The model is not a black box; it incorporates mechanisms to enhance interpretability, particularly through the use of attention layers. These layers help quantify the relevance of attention to sorting signals, making the model's decision-making process more transparent.\n\nSeveral metrics are used to measure the interpretability of attention. These include:\n\n* Importance in signal: This metric evaluates the total attention mass present within the signal, indicating how much the model focuses on relevant parts of the input data.\n* Signal over background: This metric compares the average attention value within the signal to the average value outside the signal, highlighting the model's ability to distinguish between relevant and irrelevant information.\n* Metric Entropy: This metric measures the entropy of the attention, normalized by the information length of the protein. Lower values indicate that the attention mass is concentrated on fewer residues, making the model's focus more interpretable.\n* KL-Divergence: This metric assesses the distributional dissimilarity between the signal and attention, providing insights into how well the attention correlates with the actual signal.\n\nAdditionally, the model includes supervision and regularization losses for the attention layer. These losses significantly improve interpretability by making the signals more prominent compared to the background, placing more attention within the signal, and reducing KL-Divergence, which implies better correlation between attention and the signal. This approach ensures that the model's attention mechanisms are not only effective but also interpretable, providing clear insights into how the model processes and prioritizes information.",
  "model/output": "The model is a multi-label classification model designed for predicting protein subcellular localization and sorting signals. It employs transformer-based protein language models to generate vector representations for each residue in an input protein sequence. These representations are then combined using an attention mechanism, which helps in identifying important regions in the sequence that may correspond to sorting signals.\n\nThe model's prediction stage consists of two multi-layer perceptron (MLP) classifier heads. The first head is trained to predict ten classes of subcellular localization labels. The second head is trained to predict nine classes of sorting signal types. Both heads output probabilities for each label, which are then used to make predictions. The model uses a weighted focal loss for training, which helps in handling class imbalances and improving the performance on rare classes.\n\nThe output of the model includes predicted subcellular localization labels and sorting signal types, along with their corresponding prediction scores. These predictions are visualized on a webserver, where users can download the results as a comma-separated file (CSV). The webserver also provides attention values and logo-like plots that highlight important regions in the sequence for localization prediction. These visualizations serve as guidelines for users to understand the model's predictions better. Additionally, specialized tools like SignalP or TargetP can be used for a more detailed and accurate analysis of the predicted sorting signals.",
  "model/duration": "The execution time for the model varies depending on the sequence length and the specific transformer model used. For short sequences with an average length of 104, the model load time is approximately 11 seconds for one model and 27 seconds for another. The prediction time per sequence is about 0.83 seconds for one model and 3.93 seconds for the other, while the plot time per sequence is around 2.38 seconds and 2.57 seconds, respectively.\n\nFor long sequences with an average length of 400, the model load time remains similar, at approximately 11 seconds and 26 seconds. However, the prediction time per sequence increases to about 3.29 seconds for one model and 7.33 seconds for the other. The plot time per sequence also rises significantly, to around 7.94 seconds and 7.97 seconds, respectively.\n\nIt is important to note that the model load time is constant regardless of the number of input sequences, while the prediction and plot times scale linearly with the number of sequences. This information provides a clear understanding of the time requirements for running the model with different sequence lengths and models.",
  "model/availability": "The source code for the model is not publicly released. However, the model is accessible through a web server. This web server is free and open to all users, with no login requirement. It accepts a maximum of 500 input sequences in the FASTA format. Users can submit their sequences, and the model will provide predictions along with an attention plot that visualizes the regions of the input sequence used for predictions. This plot can help identify potential sorting signals. Users can be notified via email when the results are ready, or they can wait on the page for automatic redirection once the results are available. The web server also allows users to download the prediction summary as a comma-separated file (CSV), which includes the predicted subcellular localization and sorting signals. Additionally, the image or attention values of each plot can be downloaded separately.",
  "evaluation/method": "The evaluation of the method involved a comprehensive approach using both cross-validation and independent datasets. For the cross-validation dataset, a procedure was adopted to generate label-balanced splits, ensuring that each pair of train and test folds did not share sequences with a global sequence identity greater than 30%. This was determined using ggsearch36, part of the FASTA package.\n\nAdditionally, an independent dataset was constructed from the Human Protein Atlas project. This dataset underwent several steps to ensure high quality, including homology reduction to maintain a maximum of 30% sequence identity to the entire SwissProt dataset. Only \"Enhanced\" or \"Supported\" annotations were selected to improve label reliability. Sequences were clustered with a 90% identity threshold, and centroids were selected to reduce correlated errors. Localizations with few instances, such as Peroxisome and Lysosome/Vacuole, were removed. Furthermore, sequences with greater than 30% sequence identity to any sequence from the multi-label eukaryote dataset used to train Fuel-mLoc were excluded.\n\nThe performance of the method was quantified using several metrics, including accuracy, Jaccard index, MicroF1, and MacroF1 scores. These metrics provided a comprehensive evaluation of the classification performance on the datasets. The method was compared against other tools, such as YLoc+, DeepLoc 1.0, Fuel-mLoc, and LAProtT5, which have public web servers or easily available local implementations. The outputs of these methods were mapped to the ten classes used in the work to ensure a fair comparison. Detailed results and insights from the experiments are provided in supplementary tables and sections.",
  "evaluation/measure": "In our evaluation, we employed a comprehensive set of performance metrics to assess the effectiveness of our multi-label prediction models. These metrics include:\n\n* **Number of predicted labels**: This metric is averaged over all predictions and provides insight into the bias of the predictor.\n* **Accuracy**: This metric requires the exact prediction of locations. Given that our dataset is skewed towards proteins with single localization, this metric favors single-label predictors.\n* **Jaccard Index**: This measures the overlap between the actual and predicted labels over their union.\n* **MicroF1 and MacroF1**: These are F1 score variants. MicroF1 considers the total number of true positives, false negatives, and false positives, while MacroF1 is computed for each class and then averaged, giving equal emphasis to rare and frequent classes.\n* **Matthews Correlation Coefficient (MCC)**: This metric is measured for each class and requires the model to perform well on all four confusion matrix entries. It is particularly useful for handling imbalanced datasets, which is a common issue in subcellular localization prediction.\n\nThese metrics collectively provide a robust evaluation of our models' performance, ensuring that we capture various aspects of prediction quality. The use of MCC, in particular, helps mitigate the issues arising from class imbalance, which is a significant concern in multi-label classification tasks. The inclusion of both MicroF1 and MacroF1 ensures that we account for both the overall performance and the performance on individual classes, respectively. This set of metrics is representative of the standards used in the literature for evaluating multi-label classification models in bioinformatics.",
  "evaluation/comparison": "In our evaluation, we conducted a thorough comparison of our methods with several publicly available tools on benchmark datasets. Specifically, we chose YLoc+, DeepLoc 1.0, Fuel-mLoc, and LAProtT5 for comparison, as these tools have either public webservers or easily accessible local implementations. To ensure a fair comparison, we mapped the locations predicted by these methods to the ten classes used in our work. Additionally, we reduced the Fuel-mLoc database by about 2% to remove close homologs to the test set, further ensuring a fair evaluation.\n\nFor sequence-based methods, we used the standalone versions of the predictors, selecting the appropriate model (animal, plant, or fungi) without the GO-terms option. DeepLoc 1.0 was retrained using the same procedure as originally described. This approach allowed us to benchmark sequence-based methods more reliably, as proper homology-partitioning between the training and test sets ensures that the performance evaluation is a good estimate for unseen proteins.\n\nOn the other hand, GO-based methods rely on local-alignment scores using a large database of indirect localization labels, such as GO terms. To address potential homology \"leakage,\" which can lead to performance overestimation, we reimplemented Fuel-mLoc, changing only the database used. We excluded sequences with a sequence-identity match of greater than 30% with the ProSeq-GO database, based on global sequence identity measured using USEARCH. This modification ensured that our comparison was fair and that the results were not inflated by homology leakage.\n\nWe also considered simpler baselines, such as single-location predictors, to provide a baseline for comparison. These predictors can have an average number of predicted labels less than one, as they may fall outside the true labels on the independent test set. This comparison helped us understand the performance improvements offered by multi-label predictors.\n\nIn summary, our evaluation included a comprehensive comparison with publicly available methods and simpler baselines on benchmark datasets, ensuring a fair and thorough assessment of our methods' performance.",
  "evaluation/confidence": "The evaluation of our method includes a comprehensive set of performance metrics, each accompanied by confidence intervals to provide a clear understanding of the variability and reliability of the results. These metrics include Accuracy, Jaccard index, MicroF1, MacroF1, and Matthews Correlation Coefficient (MCC). The confidence intervals are computed to ensure that the performance claims are statistically significant and not due to random chance.\n\nFor instance, in the quantitative comparison of interpretable attention, metrics such as KL Divergence and Importance in Signal are presented with their respective standard deviations. This allows for a robust assessment of the model's performance across different localization classes, such as Signal Peptide (SP), Transmembrane (TM), Mitochondrion (MT), Chloroplast (CH), Thylakoid (TH), Nuclear Localization Signal (NLS), Nuclear Export Signal (NES), Peroxisomal Targeting Signal (PTS), and Glycosylphosphatidylinositol (GPI).\n\nThe statistical significance of our results is further supported by cross-validation on multiple datasets, including the HPA benchmark. The performance of our method, DeepLoc 2.0, is compared against other tools like YLoc+, DeepLoc 1.0, Fuel-mLoc, and LAProtT5. The results demonstrate that DeepLoc 2.0 outperforms these tools on several metrics, indicating a superior and statistically significant performance.\n\nAdditionally, the use of MCC as a primary metric ensures that the evaluation is not biased by class imbalances, which is a common issue with accuracy-based metrics. The thresholds for each output label are computed by maximizing the MCC on the training data, providing a balanced and reliable measure of performance.\n\nIn summary, the evaluation of our method is thorough and statistically sound, with confidence intervals and cross-validation results supporting the claim that DeepLoc 2.0 is superior to other tools and baselines.",
  "evaluation/availability": "The raw evaluation files for our study are not publicly available. The evaluation process involved specific datasets and models that are integral to our research but are not released as part of the public materials. This decision is made to maintain the integrity of the evaluation process and to prevent potential misuse of the data. However, we provide detailed descriptions and results of our evaluations in the supplementary materials, allowing other researchers to understand and potentially replicate our methods. The supplementary materials include tables and figures that outline the performance metrics and comparisons with other tools, ensuring transparency in our evaluation process."
}