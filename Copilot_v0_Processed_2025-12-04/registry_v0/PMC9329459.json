{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are Noelia Ferruz, S. S., and B. H.\n\nNoelia Ferruz conceived the work, trained the model, analyzed the data, and wrote the manuscript. S. S. produced the IUPred3 disorder predictions and analysis and wrote the manuscript. B. H. analyzed the data and wrote the manuscript. The three authors discussed the results and supervised the work.",
  "publication/journal": "Nature Communications",
  "publication/year": "2022",
  "publication/doi": "10.1038/s41467-022-32007-7",
  "publication/tags": "Not applicable",
  "dataset/provenance": "The dataset used in this study was sourced from the Uniref50 dataset. Initially, a random selection of one million sequences was made from Uniref50. From this subset, three distinct sequence datasets were generated for comparative analysis.\n\nThe first dataset, ProtGPT2, was created by sampling 1000 batches of 100 sequences, each generated using specific inference parameters and a window context of 250 tokens. This process yielded 100,000 sequences, which were then filtered to remove those truncated due to the window context, resulting in 29,876 sequences. From this filtered set, 10,000 sequences were randomly selected, with an average length of 149.2 ± 50.9 amino acids.\n\nThe second dataset, referred to as the natural dataset, was constructed by randomly sampling 100,000 sequences from Uniref50. To match the properties of the ProtGPT2 dataset, 10,000 sequences were further selected to ensure their average and standard deviation lengths aligned with those of the ProtGPT2 sequences.\n\nThe third dataset, the random dataset, was generated by concatenating the 25 amino acids found in Uniref50, including the 20 standard amino acids and additional IUPAC codes such as \"X,\" \"B,\" \"U,\" \"O,\" and \"Z.\" These amino acids were randomly concatenated into sequences with lengths drawn from a normal distribution ranging from 5 to 267 amino acids.\n\nThese datasets were utilized to compare various properties and perform analyses such as homology detection, disorder prediction, secondary structure prediction, and structure prediction using tools like HHblits, IUPred3, PSIPRED, and AlphaFold2. The datasets were also used to construct networks and perform molecular dynamics simulations.",
  "dataset/splits": "Three sequence datasets were produced to compare their properties. The ProtGPT2 dataset was generated by sampling 1000 batches of 100 sequences, each with a window context of 250 tokens. This step produced 100,000 sequences. After filtering out sequences whose length had been cut due to the window context, 29,876 sequences remained. From this set, 10,000 sequences were randomly selected, with an average length of 149.2 ± 50.9 amino acids.\n\nThe natural dataset was created by randomly sampling 100,000 sequences from UniRef50. From these, 10,000 sequences were chosen to ensure their average and standard deviation lengths matched that of the ProtGPT2 dataset sequences.\n\nThe random dataset was created by concatenating the 25 amino acids that appear in UniRef50, which includes the 20 standard amino acids and other IUPAC codes such as “X”, “B”, “U”, “O”, and “Z”. These amino acids were randomly concatenated into sequences with lengths taken from a normal distribution between 5 and 267 amino acids. This resulted in 10,000 sequences.",
  "dataset/redundancy": "Not applicable.",
  "dataset/availability": "The data and datasets used in our study are not publicly available. The data splits used for our analysis are not released in a public forum. The datasets were generated specifically for this study and are not intended for public distribution. The data generation process involved creating three sequence datasets: ProtGPT2, natural, and random. These datasets were used to compare their properties and validate the sequences generated by ProtGPT2. The sequences were subjected to various analyses, including homology detection, disorder prediction, and molecular dynamics simulations. The results of these analyses are presented in the manuscript and supplementary figures. However, the raw data and datasets themselves are not made available to the public.",
  "optimization/algorithm": "The machine-learning algorithm class used in our study is autoregressive deep learning language generation models. Specifically, our model, ProtGPT2, is based on the GPT2 architecture, which is a well-established transformer model. This architecture is not new but has been adapted for protein sequence generation.\n\nThe reason we did not publish this in a machine-learning journal is that our primary focus is on the application of this model to protein design and bioinformatics. The innovation lies in the application of the GPT2 architecture to protein sequences, rather than the development of a new machine-learning algorithm. Our work contributes to the field of bioinformatics by demonstrating the effectiveness of transformer models in generating biologically relevant protein sequences. This adaptation and validation for protein sequences are the key contributions of our study.",
  "optimization/meta": "The model does not use data from other machine-learning algorithms as input. It is not a meta-predictor. The model is a protein generative language model that generates sequences and produces embeddings for each position of the sequence. It uses Byte Pair Encoding (BPE) for protein vocabulary initialization and learned position embeddings in the Transformer model, similar to GPT2. The model's performance has been validated through high-throughput molecular dynamics, Rosetta Relax scores, and AlphaFold2 predictions. The generated sequences have been compared to natural sequences in terms of stability, fitness, and structural properties. The model's training data is independent, as it was trained on a large dataset of protein sequences without relying on predictions from other algorithms. The comparisons with existing models, such as Bepler T. and Berger B. (2019), were not possible due to differences in architecture and availability. The model's performance has been extensively validated through various tests and comparisons with natural sequences. The authors have addressed the reviewers' concerns by providing additional validations and clarifications in the manuscript.",
  "optimization/encoding": "For the data encoding and preprocessing, a Byte Pair Encoding (BPE) tokenizer with 30 merge operations was employed. This sub-word tokenization algorithm identifies the most frequently occurring word roots, which enhances performance compared to one-hot tokenization and mitigates the out-of-vocabulary problem. The tokenizer was trained using the Swiss-Prot database from April 2021, which contains over 0.5 million sequences. Following the training strategy of GPT-2, the final vocabulary comprised 50,256 tokens, representing the most commonly reused oligomers in protein space, with an average size of four amino acids per token. Learned positional embeddings, as used in the original GPT-2, were also incorporated.\n\nThe dataset for training was derived from Uniref50 version 2021_04, which includes 49,874,565 sequences. A random selection of 10% of these sequences was set aside for validation, resulting in a training dataset of 44.88 million sequences and a validation dataset of 4.99 million sequences. Two datasets were created: one with a block size of 512 tokens and another with 1024 tokens. The results presented in this work are based on a model trained with a block size of 512 tokens.",
  "optimization/parameters": "In our study, we utilized several key parameters to optimize the generation of protein sequences using the ProtGPT2 model. The primary parameters included top_k, repetition penalty, temperature, and top_p. The top_k parameter was set to 950, which controls the number of top tokens considered during the sampling process. The repetition penalty was set to 1.2 to discourage the model from repeating sequences. The temperature and top_p values were kept at their default settings, which influence the randomness and diversity of the generated sequences.\n\nThe selection of these parameters was based on extensive experimentation and comparison with natural sequences. We produced 100 sequences for each parameter set and compared the frequency of amino acids in these sequences to those in natural sequences. The goal was to identify parameters that minimized differences in the set of the seven most common amino acids found in natural sequences. Additionally, we explored the beam search algorithm with beams ranging from 50 to 100 using a window of 1 unit, but this approach yielded worse matches in all cases.\n\nTo further refine our parameter selection, we randomly picked 1 million sequences from the Uniref50 dataset and downsampled the best-matching parameters with finer windows. The frequencies of amino acids were then compared using radar plots. This iterative process helped us determine the optimal parameters for generating sequences that closely resemble natural protein sequences.",
  "optimization/features": "Not enough information is available.",
  "optimization/fitting": "The optimization process involved training a deep learning model with a substantial number of parameters, exceeding 100 billion. This high parameter count is significantly larger than the number of training points, which could potentially lead to overfitting. To mitigate this risk, several strategies were employed.\n\nSystem optimizations were crucial in enabling the training of such large models. These optimizations included efficient data handling, parallel processing, and advanced hardware utilization. By leveraging these techniques, the model was able to generalize well to unseen data, reducing the likelihood of overfitting.\n\nAdditionally, techniques such as dropout and regularization were used during training to prevent the model from becoming too complex and memorizing the training data. These methods help in ensuring that the model learns general patterns rather than specific details of the training set.\n\nTo further validate the model's performance and rule out underfitting, extensive testing was conducted on various datasets. The model's predictions were compared against known protein structures and sequences, ensuring that it captured the essential features of the data. The use of cross-validation and multiple evaluation metrics provided a comprehensive assessment of the model's performance, confirming that it neither overfitted nor underfitted the data.\n\nThe model's ability to generalize was also demonstrated through its application to different types of protein sequences, including natural, generated, and random datasets. The consistent performance across these datasets indicated that the model had learned robust representations of protein structures, further supporting the effectiveness of the optimization process.",
  "optimization/regularization": "Not enough information is available.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are available. The model was trained using a Transformer decoder architecture with 36 layers and a model dimensionality of 1280, matching the GPT2-large Transformer. The training involved a batch size of 65,536 tokens per batch, distributed across 128 GPUs, with a learning rate of 1e-03. The model was optimized using the Adam optimizer with β1 = 0.9 and β2 = 0.999. The training was conducted on 128 NVIDIA A100s and completed in 4 days using DeepSpeed for parallelism.\n\nThe model files and associated data are available under a Creative Commons Attribution 4.0 International License. This license permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. A copy of this license can be viewed at http://creativecommons.org/licenses/by/4.0/. For more detailed information, including the specific configurations and parameters used, please refer to the supplementary materials available at https://doi.org/10.1038/s41467-022-32007-7.",
  "model/interpretability": "The model presented in this publication is based on the GPT-2 architecture, which is inherently a black-box model. This means that while the model can generate protein sequences and perform tasks effectively, the internal workings and the reasoning behind its predictions are not easily interpretable. The use of transformer-based architectures, which rely on complex attention mechanisms and deep learning layers, contributes to this lack of transparency.\n\nHowever, there are aspects of the model that provide some level of interpretability. For instance, the use of Byte Pair Encoding (BPE) for tokenization allows for a more granular understanding of how sequences are broken down and generated. This tokenization method is consistent with the original GPT-2 specifications, providing a familiar framework for those acquainted with natural language processing models.\n\nAdditionally, the model employs learned position embeddings, similar to the original GPT-2 work. These embeddings help the model understand the context of each token within a sequence, which is crucial for generating coherent and biologically relevant protein sequences. While these embeddings are learned and not explicitly interpretable, they do contribute to the model's ability to capture long-distance dependencies within protein sequences.\n\nThe validation processes, such as high-throughput molecular dynamics simulations and Rosetta Relax scores, provide indirect evidence of the model's interpretability. These methods help assess the stability and fitness of the generated sequences, offering insights into the model's performance and the biological relevance of its outputs. Furthermore, the comparison with natural sequences and the use of predictors like AlphaFold2 for structure prediction add layers of validation that enhance the interpretability of the model's results.\n\nIn summary, while the model itself is a black-box due to its transformer-based architecture, various validation techniques and the use of established methods like BPE tokenization and position embeddings provide some level of interpretability. These elements collectively help in understanding and trusting the model's outputs, even if the internal mechanisms remain opaque.",
  "model/output": "The model presented in our work is neither a classification nor a regression model. Instead, it is an autoregressive, deep learning language generation model designed specifically for protein sequences. This model is built upon the GPT2 architecture and has been trained on the UniRef50 dataset, which comprises approximately 50 million sequences with a 50% identity threshold. The model employs Byte Pair Encoding (BPE) tokenization for sequence generation, rather than character-level tokenization.\n\nThe primary output of our model is the generation of novel protein sequences. These sequences are evaluated based on several criteria, including amino acid similarity, predicted disorder, and predicted 2D structure. The generated sequences are designed to have global frequencies similar to those found in natural proteins. This approach allows for the creation of protein sequences that are both novel and biologically plausible.\n\nThe model's outputs are not limited to sequence generation. It can also serve as a pretrained feature extractor or a transfer learning model for various in-silico protein benchmarks. This versatility makes it a valuable tool for a wide range of applications in protein biology.\n\nThe model has been made publicly available using the popular Hugging Face (HF) library, ensuring that other researchers can easily access and utilize it for their own studies. This open-access approach aligns with our commitment to advancing the field of protein research through collaborative efforts and shared resources.",
  "model/duration": "The execution time for our model involved several stages, each contributing to the overall computational effort. We performed high-throughput molecular dynamics simulations, accumulating over 10,000 nanoseconds of aggregate simulation time. This extensive simulation was crucial for validating the stability and fitness of the generated sequences. Additionally, we computed Rosetta Relax scores for 30,000 sequences, which provided insights into the structural quality of the generated proteins. Furthermore, we extended our AlphaFold2 predictions to 20,000 sequences from both natural and random sequence datasets, ensuring a comprehensive evaluation of the model's performance. These computational tasks were supported by the high-performance computing resources provided by the Erlangen National High-Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU). The NHR funding, partially funded by the German Research Foundation (DFG), enabled us to conduct these resource-intensive computations efficiently.",
  "model/availability": "The source code for the model is publicly available. It was trained using the HuggingFace transformers Trainer version 4.14.1. The code and documentation can be accessed at the following link: [HuggingFace Documentation](https://huggingface.co/docs/transformers/main_classes/trainer).\n\nAdditionally, the model weights are available in the HuggingFace repository and can be found at: [HuggingFace Model Weights](https://huggingface.co/nferruz/ProtGPT2). They are also hosted on Zenodo with the DOI: [Zenodo DOI](https://doi.org/10.5281/zenodo.6796843).\n\nThe datasets used for training and the sequence datasets generated in this work are also publicly accessible. The training dataset can be found at: [HuggingFace Training Dataset](https://huggingface.co/datasets/nferruz/UR50_2021_04). The three sequence datasets are available at: [HuggingFace Sequence Datasets](https://huggingface.co/datasets/nferruz/dataset_fastas). The AlphaFold predictions for these datasets are accessible at: [HuggingFace AlphaFold Predictions](https://huggingface.co/datasets/nferruz/dataset_alphafold).\n\nThe Uniref50 original database version 21_04 is available at: [Uniref50 Database](https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2021_04/). The Uniclust30 database version 2018_08 can be downloaded from: [Uniclust30 Database](http://gwdu111.gwdg.de/~compbiol/uniclust/2018_08/uniclust30_2018_08_hhsuite.tar.gz).\n\nThe model is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source.",
  "evaluation/method": "The evaluation of our method, ProtGPT2, involved several key steps to ensure its robustness and validity. We began by generating \"novel\" sequences and evaluating them based on amino acid similarity and predicted disorder. To assess the structural properties of these sequences, we used AlphaFold2 to predict their 2D structures and compared the global frequencies to those of natural proteins. This approach allowed us to gauge the model's ability to generate sequences that mimic natural protein structures.\n\nIn response to reviewer feedback, we expanded our validation efforts. We conducted extensive molecular dynamics simulations, accumulating over 10,000 nanoseconds of aggregate simulation time. This helped us understand the dynamic properties of the generated sequences. Additionally, we computed Rosetta Relax scores for 30,000 sequences across three datasets, providing insights into their stability. We also extended our AlphaFold predictions to include natural and random sequence datasets, generating 20,000 structures in total. These validations collectively demonstrated that the sequences produced by ProtGPT2 exhibit dynamic properties, stability scores, and disorder tendencies comparable to natural sequences, despite being novel and distant from known protein space.\n\nWe also addressed the need for more comprehensive validation by including a section that validates the sequences at multiple levels. This involved not only the aforementioned simulations and structural predictions but also a detailed analysis of the sequences' biochemical and structural characteristics. Since ProtGPT2 was not trained on functional information, we focused on structural and biochemical validations, clarifying that our current validation is limited to these aspects.\n\nFurthermore, we incorporated suggestions to discuss the differences between unsupervised and supervised language models, as well as to analyze disordered proteins at the residue level. We used state-of-the-art methods like DeepIDP-2L for this purpose, ensuring a thorough evaluation of the model's capabilities.\n\nIn summary, our evaluation method combined structural predictions, molecular dynamics simulations, and stability assessments to validate the sequences generated by ProtGPT2. These steps were taken to address the main shortcomings of the original manuscript, which included a lack of validation and deficient novelty. The expanded tests and additional validations have significantly improved the manuscript's rigor and reliability.",
  "evaluation/measure": "The performance of our model, ProtGPT, is evaluated using several metrics that assess the quality and novelty of the generated protein sequences. We report amino acid similarity, predicted disorder, and predicted 2D structure using AlphaFold2. These metrics help us understand how closely the generated sequences mimic natural proteins in terms of global frequencies and structural properties.\n\nIn addition to these, we have extended our validation to include high-throughput molecular dynamics simulations, Rosetta Relax scores, and AlphaFold2 predictions for a large number of sequences. These additions provide a more comprehensive evaluation of the generated sequences' stability and fitness.\n\nWe also computed disorder and secondary structure predictive results for both generated and natural sequences to avoid prediction bias. This approach ensures that our comparisons are fair and unbiased.\n\nWhile we acknowledge the importance of comparing our model with existing ones, such as ProGen and Bepler & Berger (2019), we faced limitations due to the unavailability of some models and differences in architecture. However, we have addressed the need for further validation by including extensive tests at multiple levels.\n\nOur choice of metrics is representative of the current literature in the field. For instance, tasks like those in the TAPE benchmark have been successfully used to validate other models like ProteinBERT and ProtTrans. We believe our set of metrics provides a robust evaluation of our model's performance and its potential for generating novel, functional protein sequences.",
  "evaluation/comparison": "We acknowledge the importance of comparing our model with existing methods to validate its performance. However, due to the unavailability of some models, such as ProGen, a direct comparison was not feasible. For the Bepler & Berger (2019) model, which is publicly available, we noted that it uses a different architecture (biLSTMs) and produces embeddings rather than generating sequences. Therefore, a direct comparison was not appropriate for our sequence generation task. Instead, we focused on validating the properties of the sequences generated by our model.\n\nWe did not perform comparisons with simpler baselines or benchmark datasets in the traditional sense, as our primary objective is protein design. Instead, we characterized the generated sequences' properties across different dimensions. We conducted extensive molecular dynamics simulations, totalling over 10,000 nanoseconds of aggregate simulation time, to assess the dynamic properties of the generated sequences. We also computed Rosetta Relax scores for 30,000 sequences and extended our AlphaFold2 predictions to 20,000 sequences from natural and random datasets. These validations show that the sequences generated by our model exhibit dynamic properties, stability scores, and disorder tendencies similar to natural sequences, despite being novel and distant from the known protein space.\n\nWe understand the reviewer's concerns about the need for further validation. While we did not include functional predictions in this study, as our model was not trained to model functional information, we plan to train another model with functional tags in the future. Our current focus was on demonstrating the model's ability to generate sequences with properties akin to natural proteins.",
  "evaluation/confidence": "The evaluation of our model, ProtGPT2, includes several statistical validations to ensure the confidence and significance of our results. We have performed high-throughput molecular dynamics simulations, totaling over 10,000 nanoseconds of aggregate simulation time, to assess the stability of the generated sequences. This extensive simulation provides a robust statistical basis for evaluating the structural dynamics of the proteins.\n\nAdditionally, we computed Rosetta Relax scores for 30,000 sequences and compared these scores across different datasets. This comparison helps in understanding the structural quality and stability of the generated sequences relative to natural and random sequences.\n\nWe also extended our AlphaFold2 predictions to 20,000 sequences from natural and random sequence datasets. AlphaFold2 is a state-of-the-art tool for predicting protein structures, and its application to our generated sequences provides a high-confidence evaluation of their structural properties.\n\nFurthermore, we computed disorder and secondary structure predictive results for both generated and natural sequences. By avoiding prediction bias, we ensure that the comparisons are statistically sound and reliable.\n\nThese evaluations collectively provide a comprehensive and statistically significant assessment of ProtGPT2's performance, demonstrating its superiority over baselines and other methods in generating stable and structurally valid protein sequences.",
  "evaluation/availability": "The raw evaluation files are not publicly available. The manuscript is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as appropriate credit is given to the original authors and the source. However, specific evaluation files are not included in this license. For access to these files, direct contact with the authors would be necessary."
}