{
  "publication/title": "Not enough information is available",
  "publication/authors": "The authors who contributed to the article are:\n\n- **Martin Habeck**: Provided AlphaFold2 structures, relative solvent accessibility values, embeddings, and SETH’s predictions. Performed MMSeqs2 clustering and drafted the code available on GitHub and Google Colab. Trained and tested all models and analyzed the data with the help of DI.\n\n- **Dominik Ilzhoefer**: Wrote the first manuscript draft and generated the figures. Refined the manuscript iteratively with MH and BR.\n\n- **Burkhard Rost**: Helped in study planning, design, and setting the focus. Refined the manuscript iteratively with MH and DI.",
  "publication/journal": "Frontiers in Bioinformatics",
  "publication/year": "2022",
  "publication/doi": "10.3389/fbinf.2022.1019597",
  "publication/tags": "- Protein disorder prediction\n- Intrinsically disordered proteins\n- Machine learning models\n- Protein language models\n- AlphaFold2\n- CheZOD scores\n- Protein embeddings\n- Supervised learning\n- Regression models\n- Protein sequence analysis",
  "dataset/provenance": "The dataset used in this study consists of two main subsets: a training set and a test set. The training set, dubbed CheZOD1174, contains 1,174 proteins with a total of 132,545 residues. The average length of these proteins is approximately 113 residues, which is about 3 to 4 times shorter than most existing proteins. The test set, dubbed CheZOD117, contains 117 sequences with a total of 13,069 residues, with an average length of 112 residues.\n\nThe CheZOD scores in these sets range from -5.6 to 16.2. A threshold of eight is used to differentiate between disorder (CheZOD score ≤ 8) and order (CheZOD score > 8). The training set has an over-representation of ordered residues (72% ordered), while the test set has a higher prevalence of disordered residues (31% ordered).\n\nThe test set used in this study is the same as the one published alongside ODiNPred, which has been used to evaluate 26 disorder prediction methods. This allows for a direct comparison of the results. However, the training data published and used for ODiNPred was altered to reduce the overlap between training and testing. This was done to avoid overestimating performance through pairs of proteins with too similar sequences between the training and testing sets.\n\nThe datasets are available in online repositories. The training set (CheZOD1174) and test set (CheZOD117) can be found at https://github.com/Rostlab/SETH. The predictions of SETH for Swiss-Prot and the human proteome are available at https://doi.org/10.5281/zenodo.6673817.",
  "dataset/splits": "Two data splits were created: a training set and a test set. The training set, named CheZOD1174, contains 1,174 proteins with a total of 132,545 residues. The average protein length in this set is 113 residues. The test set, named CheZOD117, contains 117 sequences with a total of 13,069 residues, and the average protein length is 112 residues.\n\nThe distribution of CheZOD scores in these sets ranges from -5.6 to 16.2. A threshold of eight is used to differentiate between disorder (CheZOD score ≤ 8) and order (CheZOD score > 8). The training set has an over-representation of ordered residues, with 72% of the residues being ordered. In contrast, the test set has a prevalence of disordered residues, with only 31% of the residues being ordered. This discrepancy in the distribution of ordered and disordered residues between the training and test sets serves as an additional safeguard against overestimating performance.",
  "dataset/redundancy": "To ensure the independence of training and test sets, we constructed non-redundant subsets. This involved building position-specific scoring matrices (PSSMs) from multiple sequence alignments (MSAs) for proteins in the test set using MMSeqs2. We then removed any protein from the training set that had more than 20% pairwise sequence identity (PIDE) with any test set profile, using a bi-directional coverage threshold of 80%. This process was designed to avoid overestimating performance due to similar sequences between the training and testing sets.\n\nThe training set, dubbed CheZOD1174, contained 1,174 proteins with a total of 132,545 residues, while the test set, CheZOD117, contained 117 sequences with 13,069 residues. The test set was not altered from the one published alongside ODiNPred, which has been used to evaluate 26 disorder prediction methods. This allows for a direct comparison of results. However, the training data was modified to reduce overlap between the training and testing sets.\n\nThe distribution of CheZOD scores in both sets ranged from -5.6 to 16.2. A threshold of eight was used to differentiate between disorder (CheZOD score ≤ 8) and order (CheZOD score > 8). The training set had an over-representation of ordered residues (72% ordered), while the test set had more disordered residues (31% ordered). This discrepancy in distributions provided an additional safeguard against overestimating performance, as artificial intelligence models tend to optimize for similar distributions in training and test sets.\n\nThe redundancy reduction process was crucial to correctly estimate performance. We removed 151 sequences from the ODiNPred’s training set that had over 20% pairwise sequence identity with proteins in the test set, based on alignments with 80% coverage. This step ensured that the training and test sets were independent and that the performance estimates were reliable.",
  "dataset/availability": "The datasets used in this study are publicly available. The training set, referred to as CheZOD1174, and the test set, referred to as CheZOD117, can be accessed through the online repository hosted on GitHub at https://github.com/Rostlab/SETH. This repository provides the necessary data for replication and further analysis by other researchers.\n\nThe data is released under a license that allows for open access and use, facilitating the reproducibility of the results presented in this study. The specific licensing details can be found on the GitHub repository page.\n\nTo ensure the integrity and consistency of the datasets, redundancy reduction techniques were employed. These techniques involved constructing non-redundant subsets by building profiles from multiple sequence alignments and removing proteins with high sequence similarity between the training and testing sets. This process was enforced using the MMSeqs2 tool with specific parameters to control the percentage of pairwise sequence identity and coverage thresholds.\n\nAdditionally, the predictions made by the SETH model for the Swiss-Prot database and the human proteome are available at https://doi.org/10.5281/zenodo.6673817. This provides further resources for researchers interested in exploring the model's performance on different protein datasets.\n\nThe availability of these datasets and the accompanying code on GitHub ensures that the methods and results can be independently verified and built upon by the scientific community.",
  "optimization/algorithm": "The machine-learning algorithms used in our study include regression models such as SETH, ANN, and LinReg, as well as a classification model, LogReg. These algorithms are not new; they are well-established in the field of machine learning. SETH, in particular, was implemented using PyTorch, utilizing Conv2d for convolutional layers, MSELoss as the loss function, and Adam as the optimizer. The choice of these algorithms was driven by their effectiveness in handling the specific problem of predicting protein disorder, rather than by the novelty of the algorithms themselves.\n\nThe decision to use these established algorithms in a bioinformatics context, rather than publishing them in a machine-learning journal, is due to the specific application and the domain expertise required to interpret the results. The focus of our work is on the biological significance and the practical application of these models in predicting protein disorder, which is a critical area of study in bioinformatics. The optimization and tuning of these models were tailored to the unique challenges and data characteristics of protein disorder prediction, ensuring that the models are both accurate and efficient in this specific context.",
  "optimization/meta": "The models we developed do not function as meta-predictors. They do not use data from other machine-learning algorithms as input. Instead, they are trained directly on the CheZOD scores, which are continuous chemical shift Z-scores. Our models include SETH, a convolutional neural network (CNN), ANN, a simple artificial neural network, LinReg, a linear regression model, and LogReg, a logistic regression model. These models were designed to be simple to gain speed and avoid overfitting.\n\nSETH, in particular, was implemented in PyTorch using Conv2d for the convolutional layers, MSELoss as the loss function, and Adam as the optimizer. The model was trained on regression tasks, aiming to predict the degree of residue order more accurately than binary classification methods.\n\nThe training and test sets were created through redundancy reduction, which resulted in substantial differences in the distributions of CheZOD scores. This imbalance might explain why our supervised regression models mildly over-predicted the degree of residue order compared to the raw embedding values.\n\nThe simplicity of our models was proxied by the number of free parameters. While our top-performing models did not reach the simplicity of earlier IDR prediction methods, we constrained their size to ensure they remained efficient. The comparison to one-hot encodings demonstrated the benefit of increasing model complexity by inputting high-dimensional pLM embeddings.\n\nIn summary, our models are not meta-predictors but rather standalone machine-learning algorithms trained on continuous data to predict disorder in proteins. The training data was carefully curated to minimize information leakage and ensure independent training and testing sets.",
  "optimization/encoding": "The data encoding process involved transforming protein sequences into distributed vector representations, known as embeddings, using five different protein language models (pLMs). These models included SeqVec, ProtBERT, ESM-1b, ProtT5, and ProSE. Each model utilized distinct architectures and training datasets to encode the sequences.\n\nSeqVec, based on the ELMo algorithm, employs a stack of bi-directional long short-term memory (LSTM) cells trained on a non-redundant version of UniProt. ProtBERT, derived from the BERT algorithm, was trained on the Big Fantastic Database (BFD), which contains over 2.1 billion protein sequences. ESM-1b, conceptually similar to ProtBERT, uses a stack of Transformer encoder modules and was trained on UniRef50. ProtT5, based on the T5 sequence-to-sequence model, was trained on BFD and fine-tuned on UniRef50. ProSE consists of LSTMs trained on a large set of unlabeled protein sequences from UniRef90 and additionally on predicting intra-residue contacts and structural similarity from labeled proteins in SCOPe.\n\nThe embeddings were extracted from the last hidden layer of each pLM. For ProtT5, the per-residue embeddings were derived from the last attention layer of the model’s encoder-side using half-precision. The resulting output for each input residue was a single vector, yielding an LxN-dimensional matrix, where L represents the protein length and N denotes the embedding dimension. The dimensions varied across models: 1,024 for SeqVec, ProtBERT, and ProtT5; 1,280 for ESM-1b; and 6,165 for ProSE.\n\nAdditionally, for comparison purposes, standard one-hot encodings were used. These encodings represent each residue by a 20-dimensional vector, corresponding to the 20 standard amino acids. Each position in the vector is binary, with a one indicating the presence of the encoded amino acid and a zero otherwise. The special case of an unknown amino acid (\"X\") was encoded as a 20-dimensional vector containing only zeros.\n\nThese embeddings and one-hot encodings served as input features for the machine-learning algorithms, enabling the prediction of CheZOD scores and the classification of residues as ordered or disordered.",
  "optimization/parameters": "In our study, we optimized four models to predict disorder, each with a different number of parameters. The models included linear regression (LinReg), a multi-layer artificial neural network (ANN), a two-layer convolutional neural network (SETH), and logistic regression (LogReg). The simplicity of these models was deliberately maintained to avoid overfitting and to ensure speed.\n\nFor SETH, implemented in PyTorch, we used Conv2d for the convolutional layers, MSELoss as the loss function, and Adam as the optimizer with a learning rate of 0.001 and amsgrad activated. The model was padded to receive one output per residue, and all random seeds were set to 42 for reproducibility. The hyper-parameters were optimized using a validation set, resulting in a kernel size of (5,1), 28 output channels in the first convolutional layer, the Tanh activation function between the two convolutional layers, and a weight decay parameter of 0.001 in the optimizer.\n\nThe selection of parameters for SETH was based on hyper-parameter optimization, where we chose the model with the most constraints that performed best on the validation set. This approach ensured that the model was neither too simple nor too complex, balancing performance and generalization.\n\nFor the other models (LinReg, ANN, and LogReg), details are provided in the supplementary material. The number of parameters in these models varied, but they were all kept relatively simple to maintain computational efficiency and to avoid overfitting.\n\nIn summary, the number of parameters in our models was carefully selected through hyper-parameter optimization and validation, ensuring that they were neither too simple nor too complex. This approach allowed us to achieve a good balance between performance and generalization.",
  "optimization/features": "The input features for our models were derived from high-dimensional protein language model (pLM) embeddings. Specifically, we utilized the ProtT5 embeddings, which consist of 1,024 dimensions. This means that the number of features (f) used as input is 1,024.\n\nFeature selection was indeed performed to simplify one of our models, LinReg, resulting in a variant called LinReg1D. This selection process identified that dimension 295 of the ProtT5 embeddings carried a significant portion of the signal, ranging from 86% to 96% of the entire 1,024-dimensional vector. Therefore, LinReg1D was trained using this single dimension, demonstrating that even with a much-reduced feature set, the model could still outperform more complex methods.\n\nThe feature selection was conducted using the training set only, ensuring that the test set remained independent and unbiased. This approach helped in maintaining the integrity of our evaluation metrics and preventing any data leakage that could artificially inflate performance estimates.",
  "optimization/fitting": "The models used in this work were deliberately kept simple to avoid overfitting and to gain speed. The number of free parameters in our top-performing models, such as SETH, ANN, LinReg, and LogReg, did not reach the level of complexity seen in earlier IDR prediction methods or recent adaptations of AlphaFold2 predictions. This simplicity helped in constraining the size of our models, making them less prone to overfitting.\n\nTo further mitigate overfitting, several techniques were employed. For instance, the training and test sets were constructed to be non-redundant, ensuring that proteins with too similar sequences were not present in both sets. This was achieved by removing proteins from the training set that had more than 20% pairwise sequence identity (PIDE) with any test set profile, using bi-directional coverage. Additionally, the training set was constructed such that all protein pairs had less than 50% PIDE, providing an additional safeguard against overfitting.\n\nThe models were also trained using techniques such as early stopping and hyperparameter optimization. For example, SETH was implemented in PyTorch using convolutional layers, mean squared error loss, and the Adam optimizer with a learning rate of 0.001. Early stopping was applied after 10 epochs without improvement, and hyperparameter optimization was performed to select the best-performing model with the most constraints. This approach helped in finding a balance between model complexity and performance, ensuring that the models did not overfit the training data.\n\nTo address underfitting, the models were trained on high-dimensional protein language model (pLM) embeddings, which provided rich information about the sequences. This approach demonstrated the benefit of increasing model complexity by inputting high-dimensional embeddings, as it outperformed simpler methods that used one-hot encodings. Additionally, the models were evaluated on a diverse test set, CheZOD117, which had been used to evaluate 26 disorder prediction methods, enabling a direct comparison of the results.\n\nIn summary, the models were designed to be simple yet effective, with techniques such as non-redundant dataset construction, early stopping, and hyperparameter optimization employed to mitigate overfitting. The use of high-dimensional embeddings and evaluation on a diverse test set helped in ensuring that the models did not underfit the data.",
  "optimization/regularization": "In our work, several techniques were employed to prevent overfitting and ensure the robustness of our models. Firstly, we deliberately kept the models simple. This approach not only gained speed but also helped in avoiding overfitting. The models we optimized for disorder prediction included linear regression (LinReg), a multi-layer artificial neural network (ANN), a two-layer convolutional neural network (SETH), and logistic regression (LogReg). Among these, three models (LinReg, ANN, and SETH) were trained on regression tasks, while LogReg was trained for binary classification.\n\nFor the SETH model, implemented in PyTorch, we used Conv2d for the convolutional layers, MSELoss as the loss function, and Adam as the optimizer with a learning rate of 0.001 and amsgrad activated. We also padded the inputs to ensure one output per residue and set all random seeds to 42 for reproducibility. Additionally, we used early-stopping after 10 epochs without improvement and hyper-parameter optimization to select the best-performing model with the most constraints. This included a kernel size of (5,1), 28 output channels in the first convolutional layer, the Tanh activation function between the two convolutional layers, and a weight decay parameter of 0.001 in the optimizer.\n\nMoreover, we constrained the size of our models to avoid overfitting. For instance, we simplified LinReg to LinReg1D based on one of the 1,024 dimensions of ProtT5, which carried a significant portion of the signal. This simplification reached the level of very basic predictors but still outperformed many complex methods.\n\nWe also addressed the issue of information leakage by being conservative with our threshold for identity (PIDE). We only removed proteins that were aligned with 80% coverage, which meant there might still be some minor information leakage. However, this leakage was considered negligible as none of the aligned proteins lay above the HSSP-curve. This conservative approach helped in balancing the trade-off between losing proteins for training/testing and over-estimating performance.\n\nIn summary, our regularization methods included model simplicity, early-stopping, hyper-parameter optimization, and conservative thresholding to prevent overfitting and ensure reliable performance.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our work are detailed within the publication. Specifically, for the SETH model, we provided details such as the kernel size, number of output channels, activation function, and weight decay parameter. The random seeds were set to 42 for reproducibility, and the training-validation split was 90-10%. The optimization process included early-stopping after 10 epochs without improvement.\n\nModel files and optimization parameters are not explicitly mentioned as being available for download. However, the implementation details for SETH, including the use of PyTorch, Conv2d layers, MSELoss, and the Adam optimizer with amsgrad, are thoroughly described. This information should enable replication of the model and its training process.\n\nRegarding the availability and licensing of the configurations and parameters, the publication does not specify where these can be directly accessed or under what license they are provided. However, the methods and details are openly described in the text, allowing researchers to implement and use them according to standard academic practices.",
  "model/interpretability": "The models developed in this study, including SETH, ANN, LinReg, and LogReg, are primarily designed for predictive performance rather than interpretability. They can be considered as black-box models, especially the more complex ones like SETH and ANN. These models use high-dimensional embeddings from protein language models (pLMs) as inputs, which are not easily interpretable by humans. The embeddings capture complex patterns in the data, but the specific features that contribute to the predictions are not straightforward to extract or understand.\n\nHowever, some aspects of the models can be considered more transparent. For instance, the linear regression model (LinReg) is inherently more interpretable because it provides a direct relationship between the input features and the output predictions. Each coefficient in the linear model represents the contribution of a specific embedding dimension to the prediction of the CheZOD score. This allows for a clearer understanding of how different parts of the input data influence the model's output.\n\nAdditionally, the simplification of LinReg to LinReg1D, which uses a single dimension of the ProtT5 embeddings, further enhances interpretability. By focusing on a single dimension that carries a significant portion of the signal, LinReg1D makes it easier to understand the relationship between the input features and the predictions. This simplification does not significantly compromise the model's performance, as it still outperforms many more complex methods.\n\nIn summary, while the more complex models like SETH and ANN are black-box models, the linear regression models offer a degree of transparency. The use of high-dimensional embeddings and the complexity of the neural network architectures make it challenging to interpret the exact mechanisms by which these models make predictions. However, the linear models provide a more straightforward way to understand the contributions of different input features to the output predictions.",
  "model/output": "The models we developed for disorder prediction include both regression and classification approaches. Specifically, three of our models—LinReg, ANN, and SETH—were trained using regression to predict continuous CheZOD scores, which measure the degree of residue disorder. These models aim to capture the nuances of disorder rather than simply classifying residues as ordered or disordered. On the other hand, LogReg was trained as a binary classifier, distinguishing between disordered (CheZOD score ≤8) and ordered (CheZOD score >8) residues. This classification approach is more straightforward but may not capture the subtle variations in disorder as effectively as the regression models. Among the regression models, SETH, which stands for a two-layer convolutional neural network, consistently showed superior performance across various criteria. It was implemented in PyTorch with specific configurations to ensure speed and avoid overfitting. The models were deliberately kept simple to maintain efficiency and reproducibility, with all random seeds set to 42. The training and validation datasets were split randomly from CheZOD1174, with 90% used for training and 10% for validation and early-stopping. The performance of these models was evaluated using metrics such as the Spearman correlation coefficient and the area under the receiver operating characteristic curve.",
  "model/duration": "The execution time for the model, specifically the SETH method, was evaluated on different hardware configurations to assess its efficiency. The primary evaluation was conducted on a machine equipped with two AMD EPYC™ ROME 7352 CPUs, each with 24/48 cores running at 2.30 GHz, 256 GB of DDR4-3200 MHz ECC RAM, and an RTX A6000 GPU with 48GB of RAM. This setup was used to predict the human proteome, consisting of 20,352 proteins, and the Swiss-Prot database, which includes 566,969 proteins. The runtime measurements included all necessary steps: loading ProtT5, loading the SETH model checkpoint, reading sequences from FASTA files, creating embeddings, generating predictions, and writing the results to a file.\n\nAdditionally, the model's performance was benchmarked on a smaller GPU, a single NVIDIA GeForce RTX 3060 with 12 GB of vRAM, to demonstrate its capability to run efficiently on more modest hardware. Furthermore, the speed was tested on an AMD Ryzen 5 5500U CPU for the test set CheZOD117, showing that SETH can be run without a GPU for smaller datasets, albeit with some cost in speed. This flexibility allows SETH to be deployed on various machines, including those with GPUs having at least 8 GB of RAM, making it accessible for use in environments like Google Colab.",
  "model/availability": "The source code for SETH is publicly available for download on GitHub. It can be accessed at the following URL: https://github.com/Rostlab/SETH. This repository contains the implementation details and allows users to run the algorithm on their local machines.\n\nAdditionally, for those who prefer not to set up the environment on their own machines, SETH is also available for online execution. This can be accessed via a shared Google Colab notebook, which can be found at: https://colab.research.google.com/drive/1vDWh5YI_BPxQg0ku6CxKtSXEJ25u2wSq?usp=sharing. This notebook provides a convenient way to run SETH without the need for any local setup.\n\nThe predictions generated by SETH for the Swiss-Prot database and the human proteome are also publicly available. These datasets can be accessed at: https://doi.org/10.5281/zenodo.6673817. This ensures that users have access to the results and can verify or build upon the findings presented in the study.\n\nThe training and test datasets used in this study, specifically CheZOD1174 and CheZOD117, are also available in online repositories. They can be found at: https://github.com/Rostlab/SETH. This allows for reproducibility and further research using the same datasets.\n\nThe software is released under a license that permits its use, modification, and distribution, adhering to the principles of open science and collaborative research.",
  "evaluation/method": "The evaluation of our method, SETH, was conducted using a comprehensive approach to ensure robustness and comparability with existing methods. We followed a previous analysis to maintain consistency in evaluation measures and test sets, allowing for direct comparison with other state-of-the-art methods. The test set used was CheZOD117, which is a well-established dataset in the field.\n\nWe estimated the Spearman correlation (ρ) and its 95% confidence interval (CI) over 1,000 bootstrap sets. For each bootstrap set, a random sample of the size of the test set was drawn with replacement from the test set. The Spearman correlation was calculated for each sampled set, and the final ρ was derived from averaging over those 1,000 values. The 95% CI was estimated by computing the standard deviation of the ρ over the sampled sets and multiplying it by 1.96.\n\nAdditionally, the Area Under the Curve (AUC) and its 95% CI were estimated for each model using the same bootstrapping procedure. The AUC requires binarized ground truth class labels, so continuous CheZOD scores were binarized using a threshold of eight (disorder CheZOD score ≤ 8 and order CheZOD score > 8) for the calculation of the AUC.\n\nWe also plotted the receiver operating characteristic (ROC) curve for our models (SETH, LinReg/LinReg1D, ANN, and LogReg), as well as for AlphaFold2’s pLDDT. This visual representation helps in understanding the performance of the models across different thresholds.\n\nThe evaluation included a comparison with various existing methods such as AUCPred, DisEMBL, DISOPRED2, DISOPRED3, DISpro, DynaMine, DISPROT/VSL2b, ESpritz, GlobPlot, IUPred, MetaDisorder, MFDp2, PrDOS, RONN, s2D, SPOT-Disorder, fIDPnn, and ODiNPred. Results for some of these methods were obtained using publicly available web-servers, while others were custom-generated by the program’s developers.\n\nIn summary, the evaluation method involved a rigorous bootstrapping procedure to estimate Spearman correlation and AUC, along with a direct comparison with a wide range of existing methods using a consistent test set and evaluation measures.",
  "evaluation/measure": "In our evaluation, we employed several performance metrics to comprehensively assess the effectiveness of our models. One of the primary metrics we reported is the Spearman correlation coefficient (ρ), which measures the rank correlation between the predicted and ground truth values. This metric is particularly useful for evaluating the performance of models that predict continuous values, such as our regression models (SETH, ANN, LinReg, and LinReg1D).\n\nAdditionally, we used the area under the receiver operating characteristic curve (AUC) to evaluate the performance of our models in a binary classification setting. The AUC provides a single scalar value that summarizes the model's ability to distinguish between ordered and disordered residues, which is crucial for understanding the model's discriminative power.\n\nWe also plotted the receiver operating characteristic (ROC) curve for our models, as well as for AlphaFold2’s pLDDT, to visually compare their performance. This visualization helps in understanding how well the models perform across different threshold settings.\n\nTo ensure that our set of metrics is representative and comparable to the literature, we followed established practices in the field of disorder prediction. The use of Spearman correlation and AUC is standard in evaluating regression and classification models, respectively. Furthermore, we compared our models' performance with state-of-the-art methods using the same test set, which allows for a fair and direct comparison.\n\nIn summary, the performance metrics we reported—Spearman correlation coefficient, AUC, and ROC curves—are well-established and widely used in the literature. These metrics provide a comprehensive evaluation of our models' performance in both regression and classification tasks, ensuring that our results are representative and comparable to other studies in the field.",
  "evaluation/comparison": "A comparison to publicly available methods was performed on benchmark datasets. Specifically, the performance of our methods was added to a recent method comparison using the same performance metrics and test set, CheZOD117. This comparison included various state-of-the-art methods, such as ODiNPred, fDPnn, and ADOPT ESM-1b, which also utilize protein language model embeddings. Additionally, the performance of SPOT-Disorder2 was evaluated on a subset of the test set, as it failed to run for one protein.\n\nOur methods, including SETH, ANN, LinReg, and LogReg, were compared against these state-of-the-art methods. When considering the mean ρ, our methods SETH and ANN numerically outperformed all others, both those not using multiple sequence alignments (MSAs) and those using MSAs. Statistically significant differences at the 95% confidence interval were observed, with our methods significantly outperforming all others except for ODiNPred and ADOPT ESM-1b. When evaluating based on the mean AUC, SETH and the simplistic LinReg outperformed all other evaluated methods. However, due to the already high AUC levels of many methods, the absolute improvement of our models to state-of-the-art methods in terms of AUC was often not statistically significant.\n\nA comparison to simpler baselines was also performed. The simplicity of a machine learning model was proxied by the number of free parameters. Our top-performing models, SETH, ANN, LinReg, and LogReg, did not reach the simplicity of earlier IDR prediction methods like NORS or IUPred, or recent adaptations of AlphaFold2 predictions. However, we constrained the size of our models and demonstrated the benefit of increasing model complexity by inputting high-dimensional protein language model embeddings. A simplified version of LinReg, called LinReg1D, was created based on one of the 1,024 dimensions of ProtT5, which carried a significant portion of the signal. This extremely reduced model outperformed most complex methods not using MSAs and fell short only compared to the two best-performing methods using MSAs. Nonetheless, LinReg1D performed significantly worse compared to our other methods.",
  "evaluation/confidence": "The evaluation of our methods included the estimation of the Spearman correlation (ρ) and its 95% confidence interval (CI) over 1,000 bootstrap sets. This approach allowed us to assess the robustness and reliability of our correlation measurements. For each bootstrap set, a random sample of the size of the test set was drawn with replacement, and the ρ was calculated for each sampled set. The final ρ was derived from averaging over those 1,000 values, and the 95% CI was estimated by computing the standard deviation of the ρ over the sampled sets and multiplying it by 1.96. This method ensures that our performance metrics are accompanied by confidence intervals, providing a measure of the uncertainty associated with our estimates.\n\nAdditionally, the Area Under the Curve (AUC) and its 95% CI were estimated for each model using the same bootstrapping procedure. The AUC requires binarized ground truth class labels, and continuous CheZOD scores were binarized using a threshold of eight. This binarization process allowed us to calculate the AUC and its associated confidence interval, further enhancing the statistical rigor of our evaluation.\n\nThe performance of our models was compared to several state-of-the-art methods, including AUCPred, DisEMBL, DISOPRED2, DISOPRED3, DISpro, DynaMine, DISPROT/VSL2b, ESpritz, GlobPlot, IUPred, MetaDisorder, MFDp2, PrDOS, RONN, s2D, SPOT-Disorder, fIDPnn, and ODiNPred. This comprehensive comparison allowed us to contextualize the performance of our methods within the broader landscape of disorder prediction tools.\n\nWhile our methods, particularly SETH, demonstrated strong performance, it is important to note that the statistical significance of the performance differences between methods can depend on various factors. The bootstrapping procedure and the inclusion of confidence intervals provide a foundation for assessing the statistical significance of our results. However, a more detailed statistical analysis would be required to definitively claim that our methods are superior to others and baselines in a statistically significant manner.",
  "evaluation/availability": "The raw evaluation files for our models are publicly available. Specifically, the predictions made by our method, SETH, for the Swiss-Prot database and the human proteome can be accessed via the Zenodo repository. The datasets used in this study, including the training set (CheZOD1174) and the test set (CheZOD117), are also available in online repositories. These datasets can be found on GitHub. Additionally, SETH is available for download on GitHub and can be executed online without requiring any setup on the user's machine. This online execution is facilitated through a Google Colab notebook, which provides an accessible way to run the model and evaluate its performance."
}