{
  "publication/title": "Not enough information is available.",
  "publication/authors": "The authors who contributed to the article are:\n\n- Xiaobao Yang\n- Shenghua Gao\n- Fang Bai\n- Z.D.\n- F.B.\n\nThe contributions of each author are as follows:\n\n- Z.D. was involved in the initial project exploration with other researchers.\n- F.B. designed the whole project, revised the manuscript, and solved the problems encountered together with S.G.\n- S.G. worked with F.B. to solve the problems encountered during the project.\n- Xiaobao Yang, Shenghua Gao, and Fang Bai are the main points of contact for correspondence and requests for materials.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2022",
  "publication/doi": "10.1038/s41467-022-34807-3",
  "publication/tags": "- PROTACs\n- Deep Learning\n- Protein Degradation\n- Machine Learning\n- Data Analysis\n- Experimental Validation\n- Model Training\n- Hyperparameter Tuning\n- Antibodies\n- Biochemical Research\n\nNot sure if these tags are the ones provided in the published article.",
  "dataset/provenance": "The dataset used in this study comprises PROTACs (Proteolysis Targeting Chimeras) collected from various literature sources. The total sample size is approximately 4000 PROTACs. However, only around 2800 of these have corresponding experimental data that could be utilized for developing our method. These PROTACs were gathered to investigate their degradation capabilities, with a focus on those that have experimentally determined degradation capacities. The dataset includes a mix of active and inactive samples, with the original ratio being 988 active to 1844 inactive samples. Various sampling techniques, such as under-sampling, over-sampling, and normal-sampling, were employed to address data imbalance issues. The dataset was split into training, validation, and test sets using random allocation to ensure robust model training and evaluation. Additionally, a single-blind test was conducted using 16 ER PROTACs synthesized by experimental collaborators to assess the model's generalization ability, achieving an accuracy of 68.75%. The dataset does not include human participants, and all experimental data were collected and labeled accordingly.",
  "dataset/splits": "In our study, we utilized a dataset consisting of approximately 2800 PROTACs with experimentally determined degradation capacity. For tuning the hyperparameters of the model, the entire dataset was randomly split into training, validation, and test sets at a ratio of 8:1:1. This means that 80% of the data was used for training, 10% for validation, and 10% for testing.\n\nFor the actual training of the model, the dataset was split into training and test sets at a ratio of 8:2. This implies that 80% of the data was used for training, and 20% was reserved for testing. The data was shuffled during the training process to ensure randomness and to prevent any potential biases.\n\nAdditionally, we conducted experiments to investigate the influence of data balance on the entire dataset. We employed various sampling methods, including under-sampling, normal-sampling, and over-sampling. Under-sampling involved deleting some redundant inactive samples, resulting in an equal number of active and inactive samples (988:988). Normal-sampling used the original samples, maintaining the ratio of 988 active to 1844 inactive samples. Over-sampling involved repeatedly sampling part of the active samples to achieve a balanced ratio of 1844:1844. These methods were used to train the model three times each, and the average predicting accuracy and AUROC were obtained for each method.",
  "dataset/redundancy": "The dataset used in this study consists of approximately 2832 biologically independent samples, which were collected from various literature sources. Initially, around 4000 PROTACs were gathered, but only those with experimentally determined degradation capacity were included, resulting in the final dataset size.\n\nFor hyperparameter tuning, the dataset was randomly split into training, validation, and test sets at a ratio of 8:1:1. This split was done to ensure that the model's performance could be evaluated on unseen data during the tuning process. After optimizing the hyperparameters, the entire dataset was then randomly divided into a training set and a test set at a ratio of 8:2. This larger test set was chosen to reduce the standard deviation between different training trials, providing a more robust evaluation of the model's performance.\n\nTo enforce independence between the training and test sets, the data was shuffled during the training process. This shuffling helps to ensure that the model does not learn any specific order or pattern in the data, thereby enhancing its generalization capability.\n\nThe distribution of active and inactive samples in the original dataset was 988:1844. To address data imbalance, various sampling methods were employed, including under-sampling, over-sampling, and weighted loss. Under-sampling involved deleting redundant inactive samples to achieve a balanced ratio of 988:988. Over-sampling, on the other hand, involved repeatedly sampling part of the active samples to achieve a balanced ratio of 1844:1844. Additionally, the weight of the loss corresponding to the active samples was multiplied by a factor of 2, while the weight of the loss corresponding to the inactive samples remained unchanged. These methods were used to mitigate the impact of data imbalance and improve the model's performance.\n\nThe experimental results showed that the over-sampling method performed the best, followed by normal-sampling and under-sampling. This suggests that taking full advantage of the limited data, as done in over-sampling, leads to better model performance. However, the improvements in performance due to these efforts were marginal, leading to the adoption of the default settings (normal-sampling and normal-loss) for the training of the final model.\n\nIn summary, the dataset was carefully split and shuffled to ensure independence between the training and test sets. Various sampling methods were employed to address data imbalance, with over-sampling showing the best performance. The final model was trained using the default settings, balancing performance and computational cost.",
  "dataset/availability": "The data used in this study, including the data splits, are not publicly released in a forum. The study collected nearly all reported PROTACs from the literature, with a sample size of around 4000. However, only approximately 2800 of these have corresponding experimental data that were used for developing the method. The data splits were determined based on the requirements for testing the deep learning model or verifying the conclusions. For tuning the hyperparameters of the model, the dataset was randomly split into training, validation, and test sets at a ratio of 8:1:1. For training the model, the dataset was split into training and test sets at a ratio of 8:2, with the data shuffled during the training process.\n\nThe study did not involve human participants, and the data used were collected from existing literature and experimental results. The methods and materials used in the study are described in detail in the manuscript, and the results were repeated three times to ensure reproducibility. The study also performed a single-blind test to predict the degradation capability of 16 ER PROTACs, which were not reported elsewhere and were synthesized by experimental collaborators. The accuracy achieved in this test was 68.75%.\n\nThe study did not provide specific information on how the data availability was enforced, as the data were not publicly released. However, the methods and materials used in the study are described in detail in the manuscript, and the results were repeated three times to ensure reproducibility. The study also performed a single-blind test to validate the predictive capability of the model. The study was conducted in accordance with ethical guidelines and regulations, and the authors declare no competing interests.",
  "optimization/algorithm": "The optimization algorithm used in our study is the Adam optimizer. This is a well-established method for stochastic optimization, known for its efficiency and effectiveness in training deep learning models. It is not a new algorithm; it was introduced by Kingma and Ba in 2015 and has since become a standard choice in the field of machine learning.\n\nThe Adam optimizer combines the advantages of two other extensions of stochastic gradient descent. Specifically, it computes adaptive learning rates for each parameter, which helps in handling sparse gradients on noisy problems. This makes it particularly suitable for training neural networks, including the graph convolutional networks (GCNs) used in our DeepPROTACs model.\n\nThe reason the Adam optimizer was not published in a machine-learning journal is that it is a widely recognized and extensively used algorithm in the machine learning community. Its development and initial publication were sufficient to establish its credibility and utility. Since then, it has been implemented in various deep learning frameworks, including PyTorch, which we used for our experiments.\n\nThe Adam optimizer was chosen for its ability to handle sparse gradients and its efficiency in converging to optimal solutions. It uses a learning rate of 0.0001, with β1 set to 0.9 and β2 set to 0.999. These hyperparameters were selected based on empirical performance and are commonly used in practice. The objective function for our binary classification task is cross-entropy, which is well-suited for this type of problem.",
  "optimization/meta": "The DeepPROTACs model does not function as a meta-predictor. It is a standalone deep learning model that does not rely on the outputs of other machine-learning algorithms as input. The model is designed to predict the degradation capacity of PROTACs by processing the graph representations of the POI, E3 ligase, and PROTAC molecules.\n\nThe architecture of DeepPROTACs includes graph convolutional layers, max pooling layers, and fully connected layers. It processes the embeddings of the POI pocket, E3 pocket, warhead, E3 ligand, and linker SMILES. These components are concatenated and fed into a multi-layer perceptron (MLP) to produce the final output.\n\nThe training data for DeepPROTACs is split into training, validation, and test sets with specific ratios. The model's performance is evaluated on these sets, ensuring that the data used for training is independent of the data used for validation and testing. This independence is crucial for assessing the model's generalization capability.\n\nThe optimization of the model involves tuning various parameters, such as batch size, number of GCN layers, pocket size, and inclusion of bond type encoding. These optimizations are performed to enhance the model's accuracy and AUROC on the validation set. The final model is trained using the Adam optimizer with specific learning rates and other hyperparameters.\n\nIn summary, DeepPROTACs is a deep learning model that does not depend on other machine-learning algorithms for its predictions. It processes molecular data directly and is optimized through careful tuning of its parameters to achieve high performance.",
  "optimization/encoding": "In our study, data encoding and preprocessing were crucial steps to prepare the input for our machine-learning algorithm. The PROTAC molecules were converted into SMILES (Simplified Molecular Input Line Entry System) strings, which were then encoded using a specific character frequency code. This encoding table assigned a unique code to each character in the SMILES notation, facilitating the conversion of molecular structures into a format suitable for neural network processing.\n\nFor the protein structures, we extracted the binding pockets from both the target protein (POI) and the E3 ligase. These pockets were then converted into graph representations, where nodes represented atoms and edges represented bonds. This graph-based representation allowed the model to capture the spatial and structural information of the proteins.\n\nThe binding pocket size was optimized to 5 Å around the ligand, as this size was found to perform best in terms of model accuracy. This pocket size includes the first and second coordination shell residues around the ligand, providing a balance between capturing relevant interaction information and avoiding irrelevant noise.\n\nAdditionally, we included bond type encoding in our preprocessing pipeline, which improved the model's performance. This encoding provides more detailed information about the molecular structure, helping the model to better understand the chemical properties of the PROTAC molecules.\n\nThe preprocessing of the data also involved handling data imbalance. We investigated the effects of different sampling methods, including normal-sampling, under-sampling, and over-sampling. Over-sampling was found to perform the best, as it made full use of the limited active samples. However, considering the balance between performance and computational cost, we adopted the default settings with normal-sampling for the final model training.\n\nIn summary, our data encoding and preprocessing steps involved converting PROTAC molecules into encoded SMILES strings, representing protein binding pockets as graphs, optimizing the pocket size, including bond type encoding, and addressing data imbalance through over-sampling. These steps were essential in preparing the data for our machine-learning algorithm and achieving optimal model performance.",
  "optimization/parameters": "The model utilizes several key parameters that were optimized to enhance its performance. The selection of these parameters was based on extensive experimentation and validation.\n\nThe model employs two consecutive Graph Convolutional Network (GCN) layers, which was determined to be the optimal number after investigating the effect of varying GCN layer numbers. A pocket size of 5 Å was found to perform best, as it balances the inclusion of relevant residues around the ligand while minimizing noise from irrelevant residues. The batch size was set to 1, as other values resulted in performance reduction. The inclusion of a max pooling layer and bond type encoding was also found to improve the model's performance.\n\nThe model was trained using the Adam optimizer with a learning rate of 0.0001, β1 of 0.9, and β2 of 0.999. The objective function used was cross-entropy for binary classification. These hyperparameters were selected based on their effectiveness in achieving high accuracy and robustness in the model's predictions.\n\nThe dataset was split into training, validation, and test sets at a ratio of 8:1:1. The training was finished at epoch 30 under the circumstance of batch size 1. The final model achieved an average accuracy of 77.15% on the validation set and 77.46% on the test set, demonstrating its reliability and generalizability.",
  "optimization/features": "The DeepPROTACs model utilizes a combination of features derived from the protein and ligand components of the ternary complex. For the protein features, the auto cross-covariance (ACC) method is employed, transforming a protein sequence into an 18-bit vector. This method captures the structural and sequence information of the proteins involved, specifically the target protein (POI) and the E3 ligase.\n\nFor the ligand features, two different representations are used: MACCS keys and Morgan fingerprints. MACCS keys represent a small molecule with a 166-bit vector, while Morgan fingerprints use a 1024-bit vector. These representations encode the chemical structure and properties of the PROTAC molecule, including the warhead, linker, and E3 ligand.\n\nThe features of the ternary complex are constructed by concatenating the features of the target protein, the E3 ligase, and the PROTAC molecule. This concatenation results in a comprehensive feature set that includes both protein and ligand information, enabling the model to capture the interactions and relationships within the ternary complex.\n\nFeature selection was not explicitly performed in the traditional sense. Instead, the model leverages the inherent structure of the data by using specific encoding methods for proteins and ligands. The ACC method for proteins and the MACCS keys and Morgan fingerprints for ligands were chosen based on their ability to represent the relevant biological and chemical information. The use of these encoding methods ensures that the most relevant features are included in the model without the need for additional feature selection steps.\n\nThe training, validation, and test sets were split at a ratio of 8:1:1, ensuring that the model's performance is evaluated on unseen data. This split was done randomly, and the same split was used consistently across different experiments to maintain comparability. The features used in the training set were the same as those used in the validation and test sets, ensuring that the model's performance is evaluated on the same feature space.",
  "optimization/fitting": "The model was trained using a dataset split into training, validation, and test sets at an 8:1:1 ratio. This split ensures that the model has a sufficient number of training points to learn from, while also having separate validation and test sets to evaluate its performance and generalize to unseen data.\n\nTo address overfitting, several strategies were employed. First, the model's architecture was carefully designed, including the use of two consecutive GCN layers, which was found to be optimal. Additionally, techniques such as max pooling and bond type encoding were included to improve the model's performance. The use of a validation set during training allowed for early stopping based on the validation loss, preventing the model from overfitting to the training data. Furthermore, the model's performance was evaluated on a separate test set, ensuring that it generalizes well to unseen data.\n\nUnderfitting was addressed by thoroughly tuning the hyperparameters, including the learning rate, batch size, and pocket size. The optimal hyperparameters were determined through extensive experimentation, as documented in the relevant tables. The use of techniques like over-sampling and weighted loss also helped in handling data imbalance, ensuring that the model could learn from all available data effectively. The final model achieved an average accuracy of 77.15% on the validation set and 77.46% on the test set, indicating that it was neither overfitting nor underfitting.",
  "optimization/regularization": "In our study, several techniques were employed to prevent overfitting and improve the model's generalization performance. One of the key strategies involved data sampling methods to address data imbalance. We experimented with under-sampling, over-sampling, and weighted loss techniques. Under-sampling involved deleting redundant inactive samples to balance the dataset, while over-sampling involved repeatedly sampling part of the active samples. The over-sampling method performed the best, indicating that it effectively utilized the limited data available. Additionally, we used a weighted loss function where the weight of the loss corresponding to the active samples was multiplied by a factor of 2, while the weight of the loss corresponding to the inactive samples remained unchanged. This approach slightly improved both accuracy and AUROC, demonstrating its effectiveness in mitigating the impact of data imbalance.\n\nAnother regularization technique involved the use of a max pooling layer and bond type encoding, which improved the model's performance. The final model was trained using optimized parameters, including a learning rate of 0.0001, β1 of 0.9, and β2 of 0.999, with the Adam optimizer. The objective function used was cross-entropy for binary classification. These choices collectively helped in preventing overfitting and enhancing the model's predictive capability.",
  "optimization/config": "The hyper-parameter configurations and optimization schedule used in our study are detailed within the publication. Specifically, the dataset was split into training, validation, and test sets at an 8:1:1 ratio. The model was trained with a batch size of 1 for 30 epochs, achieving an average accuracy of 77.15% on the validation set. Various batch sizes were tested, but they resulted in performance reductions.\n\nThe network architecture includes two consecutive graph convolutional layers followed by a max pooling layer. The pocket size of 5 Å was found to be optimal, as larger sizes introduced noise. The inclusion of a max pooling layer and bond type encoding improved the model's performance.\n\nThe final model was trained using the Adam optimizer with a learning rate of 0.0001, β1 of 0.9, and β2 of 0.999. The objective function used was cross-entropy for binary classification.\n\nThe detailed output dimensions of each layer and hyper-parameters are listed in Table 1 of the publication. The model files and specific optimization parameters are not explicitly mentioned as being available for download. However, the implementation details and frameworks used, such as PyTorch and PyTorch Geometric, are specified, allowing for potential replication of the model. The publication is open-access under the Nature Communications license, which permits sharing and adaptation under certain conditions.",
  "model/interpretability": "The DeepPROTACs model is not inherently transparent and can be considered a black-box model. This is primarily due to the complex architecture involving graph convolutional networks (GCNs) and bidirectional LSTM layers, which are known for their ability to capture intricate patterns but are often difficult to interpret directly.\n\nThe model processes input data through multiple layers of graph convolutions and LSTMs, which transform the input features into high-dimensional representations. These transformations are not straightforward to interpret, as they involve complex interactions and non-linear activations. The final output is obtained through fully connected layers, which further abstract the input features, making it challenging to trace back the decision-making process.\n\nHowever, the model's architecture does provide some level of interpretability through its components. For instance, the use of GCNs allows for the extraction of local structural information from the protein pockets and ligands, which can be visualized and analyzed. Similarly, the LSTM layers process the linker SMILES sequences, capturing sequential dependencies that can be inspected to some extent.\n\nAdditionally, the model's performance metrics, such as accuracy and AUROC, provide quantitative measures of its effectiveness. These metrics, along with the model's predictions on specific datasets, can offer insights into its strengths and weaknesses. For example, the model's ability to predict degradation capacities of PROTACs with varying accuracy rates for different targets (e.g., ER, EZH2, STAT3, eIF4E, and FLT-3) demonstrates its capability to generalize across different scenarios.\n\nIn summary, while the DeepPROTACs model is not fully transparent, its architecture and performance metrics offer some level of interpretability. The use of GCNs and LSTMs allows for the extraction and analysis of local structural and sequential information, providing insights into the model's decision-making process.",
  "model/output": "The model is designed for binary classification. It predicts whether a given PROTAC molecule will result in good degradation (classified as 1) or bad degradation (classified as 0) of the target protein. The output of the model is a binary value indicating the degradation capacity of the PROTAC.\n\nThe model's performance is evaluated using metrics such as accuracy, AUROC (Area Under the Receiver Operating Characteristic curve), true positive rate, and precision rate. For instance, the average accuracy rate on the test set can reach up to 77.95%, and the AUROC can be as high as 0.8470. These metrics demonstrate the model's capability to distinguish between good and bad degradation outcomes effectively.\n\nThe final output is obtained by concatenating the embeddings from different components of the PROTAC molecule and feeding them into a multi-layer perceptron (MLP) with two fully connected layers. This architecture allows the model to integrate information from the protein pockets, ligands, and linker SMILES to make accurate predictions.\n\nThe model's predictions have been validated on various PROTAC targets, including VHL, EZH2, STAT3, eIF4E, and FLT-3, showing consistent performance across different datasets. For example, the model successfully predicted the degradation capacities of 11 out of 16 PROTACs that recruited VHL to destroy estrogen receptor (ER), achieving a prediction accuracy of 68.75%. This validation further confirms the model's reliability and generalizability.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the network model constructed and trained in this study is not publicly released. However, a related tool, DeepPROTACs, is available. DeepPROTACs (v1.0.0) can be accessed via Zenodo at https://doi.org/10.5281/zenodo.7269561. This tool is provided under the terms specified in the Zenodo repository, ensuring that users can utilize it for their research needs. For those interested in running the algorithm, this resource serves as a valuable starting point, although it may not cover the entire methodology described in the publication.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and generalization. We began by splitting our dataset into training, validation, and test sets using random allocation. For hyperparameter tuning, we used an 8:1:1 ratio, while for model training, we employed an 8:2 ratio. Data shuffling was implemented during the training process to enhance the model's learning capability.\n\nTo assess the reproducibility of our results, we repeated our experiments three times under consistent conditions, achieving successful outcomes each time. This demonstrated the method's reliability.\n\nFor evaluating the generalization ability of our model, we conducted a single-blind test. This involved predicting the degradation capability of 16 ER PROTACs that were not previously reported and were synthesized by our experimental collaborators. The model achieved an accuracy of 68.75%, indicating its potential to generalize to new, unseen data.\n\nAdditionally, we performed ablation experiments to understand the contribution of different components of the PROTACs to the model's predictions. These components included the ligase pocket, E3 ligand, POI pocket, warhead, and linker, among others. The results of these experiments are detailed in our supplementary materials.\n\nWe also inspected data balance by conducting experiments with different sampling strategies, including under-sampling, normal-sampling, over-sampling, and weighted-loss approaches. These strategies helped us to understand how the balance of active and inactive samples affects the model's performance.\n\nThe evaluation results on different targets, such as EZH2, STA T3, eIF4E, and FLT-3, showed varying accuracies, providing insights into the model's performance across diverse biological contexts. The specific accuracies for these targets are available in our supplementary tables.",
  "evaluation/measure": "In our evaluation of the DeepPROTACs model, we focused on several key performance metrics to ensure a comprehensive assessment of its predictive capabilities. The primary metrics reported include accuracy and the Area Under the Receiver Operating Characteristic Curve (AUROC). Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. AUROC, on the other hand, provides a single scalar value that represents the quality of the model's predictions across all classification thresholds, making it a robust metric for evaluating binary classifiers.\n\nIn addition to accuracy and AUROC, we also calculated the true positive rate and precision rate. The true positive rate, also known as recall or sensitivity, indicates the proportion of actual positives that are correctly identified by the model. Precision, meanwhile, measures the proportion of positive identifications that are actually correct. These metrics are crucial for understanding the model's sensitivity and precision, which are particularly important in applications where the costs of false positives and false negatives differ significantly.\n\nThe reported metrics are representative of standard practices in the field of machine learning and computational biology. Accuracy and AUROC are widely used in the literature to evaluate the performance of predictive models, especially in binary classification tasks. The inclusion of true positive rate and precision further enhances the robustness of our evaluation, providing a more nuanced understanding of the model's strengths and weaknesses.\n\nOur model achieved an average accuracy of 77.46% and an AUROC of 0.8531 on a test set that constituted 10% of the entire dataset. These results are impressive when compared to other models, such as Support Vector Machine (SVM) and Random Forest (RF), which used different fingerprints. For instance, the SVM model with Morgan fingerprints showed improved performance over the MACCS keys but still lagged behind the DeepPROTACs model, particularly in terms of AUROC. The RF model, with an average accuracy approaching 70% and an AUROC around 0.80, also did not surpass the performance of the SVM and DeepPROTACs models.\n\nIn subsequent experiments, we adopted an 8:2 training-to-test split to further evaluate the model's predictive capability on a larger test set. The DeepPROTACs model maintained high performance, achieving 77.95% accuracy and 0.8470 AUROC on the test set, which comprised 20% of the entire dataset. The true positive rate and precision rate were calculated to be 85.37% and 80.98%, respectively, demonstrating the model's good sensitivity and precision.\n\nIn summary, the performance metrics reported in our study are comprehensive and representative of best practices in the field. They provide a clear and robust evaluation of the DeepPROTACs model's predictive capabilities, highlighting its superior performance compared to other models.",
  "evaluation/comparison": "In the \"Methods Comparison\" subsection, we evaluated the performance of our DeepPROTACs model against other machine learning models, specifically Support Vector Machines (SVM) and Random Forest (RF), using different molecular fingerprints. For SVM, we compared the performance using Morgan fingerprints and MACCS keys. The results indicated that the SVM model with Morgan fingerprints showed significant improvement over the one using MACCS keys. However, both SVM models were outperformed by our DeepPROTACs model, particularly in terms of the Area Under the Receiver Operating Characteristic Curve (AUROC).\n\nThe RF model achieved an average accuracy of around 70% and an AUROC of approximately 0.80, which were not superior to the performances of the SVM and DeepPROTACs models. This comparison highlights the robustness and superior predictive capability of our DeepPROTACs model.\n\nAdditionally, we constructed two alternative models to further validate the effectiveness of our approach. These models treated different components of the PROTAC molecules, such as the warhead, linker, and E3 ligand, separately. The results from these comparisons reinforced the superiority of our DeepPROTACs model in predicting the degradation capability of PROTACs.",
  "evaluation/confidence": "The evaluation of our method, DeepPROTACs, includes a comprehensive assessment of its performance metrics. We have provided average accuracy and AUROC (Area Under the Receiver Operating Characteristic Curve) values for DeepPROTACs and compared them with other models such as Support Vector Machine (SVM) and Random Forest (RF). The metrics for DeepPROTACs show superior performance with an average accuracy of 77.46% and an AUROC of 0.8531. These values are higher than those of the SVM and RF models, indicating that DeepPROTACs outperforms these baseline methods.\n\nTo ensure the reliability of our results, we conducted multiple experiments and validated our findings through statistical tests. Specifically, we used paired t-tests with two-sided comparisons, which did not involve adjustments for multiple comparisons. This statistical approach helps to confirm that the observed differences in performance are not due to random chance.\n\nAdditionally, we performed ablation experiments to understand the contribution of different components of our model. These experiments involved systematically removing or modifying parts of the model to see how it affected performance. The results of these ablation studies provide further confidence in the robustness and effectiveness of our method.\n\nWe also addressed data balance issues by conducting experiments with different sampling techniques, including under-sampling, normal-sampling, over-sampling, and weighted-loss approaches. These experiments helped to ensure that our model's performance was not biased by imbalanced data and that the results were generalizable.\n\nOverall, the performance metrics for DeepPROTACs are supported by rigorous statistical analysis and experimental validation, providing a high level of confidence in the superiority of our method over other baselines.",
  "evaluation/availability": "Not enough information is available."
}