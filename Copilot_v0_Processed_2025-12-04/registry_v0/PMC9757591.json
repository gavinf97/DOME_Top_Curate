{
  "publication/title": "Prediction of plasmid and chromosome sequences",
  "publication/authors": "The authors who contributed to this article are:\n\n- Xiaohui Zou, who contributed to conceptualization, data curation, formal analysis, funding acquisition, investigation, methodology, resources, software, visualization, and writing the original draft and review & editing.\n- Marcus Nguyen, who contributed to conceptualization, methodology, software, supervision, and writing the review & editing.\n- Jamie Overbeek, who contributed to conceptualization, supervision, and validation, and writing the review & editing.\n- Bin Cao, who contributed to funding acquisition, resources, and writing the review & editing.\n- James J. Davis, who contributed to conceptualization, formal analysis, funding acquisition, investigation, methodology, project administration, resources, and writing the original draft and review & editing.",
  "publication/journal": "PLoS ONE",
  "publication/year": "2022",
  "publication/doi": "10.1371/journal.pone.0279280",
  "publication/tags": "- Machine Learning\n- Plasmid Classification\n- Chromosomal Sequences\n- Bioinformatics\n- Neural Networks\n- Sequence Analysis\n- K-mer Frequency\n- Model Performance\n- Cross-Validation\n- Genetic Exchange\n- Horizontal Gene Transfer\n- Contig Sequences\n- Subsequence Sampling\n- Model Accuracy\n- Bioinformatic Tools\n- Sequence Fragmentation\n- Plasmid Prediction\n- Genetic Annotation\n- Model Tuning\n- Sequence Classification",
  "dataset/provenance": "The dataset used in this study was sourced from the Bacterial and Viral Bioinformatic Resource Center (BV-BRC), previously known as the PAThosystems Resource Integration Center (PATRIC) database. This database is a well-known repository for genomic data, providing high-quality sequences that are essential for bioinformatic analyses.\n\nThe dataset comprises plasmid and chromosome sequences, which were collected based on a good quality score. This ensures that the data used is reliable and suitable for building robust machine learning models. The specific number of data points is not explicitly mentioned, but the dataset was split into training, testing, and validation sets in a 7:2:1 ratio, respectively. This splitting strategy is common in machine learning to ensure that models are trained, tested, and validated on distinct subsets of data, thereby reducing the risk of overfitting.\n\nThe data used in this study is publicly available and has been utilized by the community in previous research. This accessibility is crucial for reproducibility and for building upon existing work. The dataset is held at BV-BRC, and all code and models developed for this study are available on GitHub, further facilitating transparency and collaboration within the scientific community.",
  "dataset/splits": "The dataset was divided into three primary splits: training, testing, and validation. The split ratio used was 7:2:1, respectively. This means that 70% of the data was used for training the models, 20% for testing, and 10% for validation. Additionally, a holdout set was exempted from this split and used for final evaluations.\n\nFor further robustness, a 10-fold cross-validation was performed. In this process, the dataset (excluding the holdout data) was divided into 10 equal parts. In each of the ten rounds, one part was used for testing, one part for validation, and the remaining eight parts for training. This ensured that every part of the dataset was used for training, testing, and validation across the different rounds, helping to assess the model's performance and sensitivity to the input training set.",
  "dataset/redundancy": "The dataset used in this study was split into three parts: training, testing, and validation. The split ratio was 7:2:1, respectively. This means that 70% of the data was used for training the models, 20% for testing, and 10% for validation. The holdout set was exempted from this split and was used for final evaluation.\n\nTo ensure independence between the training and test sets, a holdout set was maintained separately. This holdout set was not used during the training or validation phases, ensuring that the models were evaluated on completely unseen data. Additionally, 10-fold cross-validation was performed to assess the models' accuracy and sensitivity to the input training set. In this process, the dataset (excluding the holdout data) was divided into 10 equal parts. In each of the ten rounds, one part was used for testing, one part for validation, and the remaining eight parts for training. This method ensured that each part of the dataset was used for training, testing, and validation, reducing the risk of bias.\n\nThe distribution of the dataset in this study compares favorably to previously published machine learning datasets for similar tasks. By using a 7:2:1 split ratio and maintaining a holdout set, the study aimed to provide a robust evaluation of the models' performance. The use of 10-fold cross-validation further ensured that the models were thoroughly tested and validated, enhancing the reliability of the results.",
  "dataset/availability": "The data used in this study is publicly available. All datasets were retrieved from the Bacterial and Viral Bioinformatic Resource Center (BV-BRC), formerly known as the PAThosystems Resource Integration Center (PATRIC) database. The data is freely accessible and can be found at the BV-BRC website.\n\nAdditionally, all code and models developed for this study are available on GitHub. This includes the scripts used for data processing, model training, and evaluation. The repository can be accessed at the provided GitHub link.\n\nThe work is made available under the Creative Commons CC0 public domain dedication. This means that the data, code, and models can be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. There are no restrictions on the use of the data or the models, and no specific license is required to access or use them.\n\nTo ensure the reproducibility of the results, the dataset was split into training, testing, and validation sets using a 7:2:1 ratio, respectively. This split was applied to the dataset excluding the holdout set, which was used for final evaluation. The holdout set was not used in the training or validation process to provide an unbiased assessment of the model's performance.",
  "optimization/algorithm": "The machine-learning algorithm class used in this study is primarily neural networks, with additional models including logistic regression, random forests, and XGBoost. The neural network model employed is not entirely new, as it leverages established architectures and techniques from TensorFlow 2.0. However, the specific implementation and its application to the classification of plasmid and chromosomal sequences are novel contributions of this study.\n\nThe neural network model consists of seven fully connected layers with varying numbers of hidden neurons, utilizing rectified linear unit (ReLU) activation functions in the hidden layers and a sigmoid activation function in the output layer. Dropout layers and L2 weight regularization are applied to mitigate overfitting. The model is trained using batch inputs with a batch size of 256 over 200 epochs, with hyperparameters tuned using validation data.\n\nThe decision to use a neural network was driven by its ability to capture complex patterns in the data, which is crucial for distinguishing between plasmid and chromosomal sequences. The neural network's performance was compared against other established machine-learning algorithms, such as logistic regression and random forests, to ensure robustness and accuracy.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of this study is on the biological application and the specific problem of classifying plasmid and chromosomal sequences, rather than the development of new machine-learning techniques. The neural network serves as a tool to achieve the biological objectives of the study, demonstrating its practical utility in a real-world scenario. The study provides a framework for identifying and characterizing plasmid DNA in next-generation sequencing (NGS) datasets, which has important implications for human health.",
  "optimization/meta": "A meta-predictor was developed using a voting classifier strategy. This approach involved selecting three randomly chosen sequence fragments from each contig in the holdout set and classifying each using a tuned neural network model. The accuracy was then reported based on different voting requirements: a single vote, 2 out of 3 votes, and 3 out of 3 votes.\n\nThe meta-predictor leverages the outputs of multiple machine-learning algorithms, specifically the neural network model, to make final predictions. This method enhances the robustness and accuracy of the classification by aggregating the results from multiple sequence fragments.\n\nRegarding the independence of the training data, it is implied that the data used for training the individual models and the meta-predictor is independent. The holdout set, from which the sequence fragments are drawn, is distinct from the training and validation datasets used in the 10-fold cross-validation process. This ensures that the meta-predictor's performance is evaluated on unseen data, maintaining the integrity of the validation process.",
  "optimization/encoding": "In our study, we employed k-mer frequency matrices as the primary feature set for our machine-learning models. Specifically, we utilized 6-mers, which are subsequences of length 6, to capture nucleotide composition and specific sequence signatures. The k-mer frequency matrix was scaled using the StandardScaler from the sklearn package to ensure that the features were standardized, which is crucial for the performance of many machine-learning algorithms.\n\nFor the neural network model, the k-mer frequency matrix was fed into a fully connected network implemented in TensorFlow 2.0. The network consisted of seven layers with varying numbers of hidden neurons: 256, 256, 128, 128, 32, 10, and 1. The output layer used a sigmoid activation function, while the remaining layers utilized a rectified linear unit (ReLU) activation function. Dropout layers were applied to the first four layers with dropout rates of 0.4, 0.4, 0.5, and 0.2, respectively, to mitigate overfitting. Additionally, L2 weight regularization with a value of 0.0001 was applied to the first four layers to further reduce overfitting.\n\nThe dataset was split into training, testing, and validation sets. Initially, the dataset (excluding the holdout data) was divided into 10 equal parts for 10-fold cross-validation. In each round, one part was used for testing, one part for validation, and the remaining eight parts for training. This approach ensured that each part of the dataset was used for training, testing, and validation, providing a robust assessment of model performance.\n\nFor the final model evaluation, the dataset was split by a 7:2:1 ratio for model training, testing, and validation, respectively. This split was chosen to ensure that the model had sufficient data for training while also having adequate data for testing and validation.\n\nIn summary, the data encoding involved the use of 6-mer frequency matrices, which were scaled and fed into a neural network with specific architectural choices to optimize performance and reduce overfitting. The dataset was carefully split to ensure comprehensive training, testing, and validation processes.",
  "optimization/parameters": "In our study, we optimized several machine learning models using different sets of parameters. For the logistic regression model, we tuned three hyperparameters: penalty type, regularization strength (C value), and solver type. The penalty type was chosen between 'l1' and 'l2', the C value ranged from 0.2 to 3 with a step size of 0.2, and the solver type was selected between 'sag' and 'liblinear'. The final model used an 'l2' penalty, the 'liblinear' solver, and a C value of 1.0, which provided the best accuracy.\n\nFor the random forest model, we focused on three hyperparameters: the number of trees (n_estimators), maximum depth of the trees (max_depth), and the criterion for splitting nodes (criterion). The number of trees ranged from 20 to 400, the maximum depth ranged from 10 to 100 with a step size of 10, and the criterion was chosen between 'gini' and 'entropy'. The final random forest model consisted of 200 trees, with the remaining parameters set to their default values.\n\nThe XGBoost model was optimized by tuning three hyperparameters: learning rate (eta), maximum depth of the trees (max_depth), and the objective function (objective). The learning rate ranged from 0 to 1 with a step size of 0.05, the maximum depth ranged from 5 to 50 with a step size of 2, and the objective function was chosen between 'binary:logistic' and 'binary:hinge'. The final XGBoost model used a learning rate of 0.1, the 'binary:logistic' objective, and default values for the remaining parameters.\n\nFor the neural network model, we used a fixed architecture with seven layers containing 256, 256, 128, 128, 32, 10, and 1 hidden neurons, respectively. Dropout layers with varying rates were applied to the first four layers to reduce overfitting. Additionally, L2 weight regularization with a value of 0.0001 was applied to the first four layers. The model was trained using a batch size of 256 for 200 epochs, with model hyperparameters tuned using validation data. Adjustments to the number of layers and dropout rates had minimal impact on model performance.",
  "optimization/features": "The input features used in our models were k-mer frequencies. Specifically, we utilized 6-mers as features, which resulted in 2,080 canonical k-mers. This choice was made because, while increasing the k-mer length from 6 to 9 slightly improved the accuracy of the logistic regression model, it also significantly increased computational complexity. The 6-mer models were more efficient to compute and provided a good balance between accuracy and computational cost.\n\nFeature selection was not explicitly performed in the traditional sense, as we relied on the inherent properties of k-mers to capture the necessary information for classification. The k-mer frequencies were computed from the sequence data, and no additional feature selection techniques were applied beyond this.\n\nThe k-mer frequency matrix was scaled using StandardScaler from the sklearn package before being input into the neural network models. This scaling was done to ensure that the features were on a similar scale, which can help improve the performance and convergence of the neural network models. The scaling was performed using the training set only, ensuring that the validation and test sets remained unseen during this preprocessing step. This approach helps to prevent data leakage and ensures that the model's performance is a true reflection of its generalization capability.",
  "optimization/fitting": "In our study, we employed several machine learning models, including logistic regression, random forest, XGBoost, and a neural network, to classify plasmid and chromosomal sequences. The number of parameters varied significantly across these models.\n\nFor the logistic regression model, we tuned hyperparameters such as penalty, C value, and solver. The final model used an L2 penalty, the liblinear solver, and a maximum of 1,000 iterations. Given the relatively small number of hyperparameters, overfitting was not a major concern for this model.\n\nThe random forest model had a larger number of parameters, particularly with the number of trees set to 200 and other parameters tuned through grid search. To mitigate overfitting, we used 10-fold cross-validation, which helped in assessing the model's performance and generalizability. This approach ensured that the model was not overly complex and could generalize well to unseen data.\n\nThe XGBoost model also had a substantial number of parameters, with hyperparameters like eta, max_depth, and objective tuned through grid search. Similar to the random forest, we used 10-fold cross-validation to prevent overfitting. The final model used an objective of \"binary:logistic\" and an eta of 0.1, which were determined to be the best parameters through extensive testing.\n\nThe neural network model had the most parameters, with a fully connected architecture consisting of seven layers and various dropout rates and L2 regularization applied to the first four layers. To address overfitting, we implemented dropout layers with specific rates and L2 weight regularization. Additionally, we used batch inputs with a batch size of 256 and trained the model for 200 epochs. The model's performance was validated using 10-fold cross-validation, ensuring that it did not overfit to the training data.\n\nUnderfitting was addressed by carefully tuning the hyperparameters and ensuring that the models were complex enough to capture the underlying patterns in the data. For instance, the neural network's architecture was designed with multiple layers and neurons to capture intricate relationships. The use of 10-fold cross-validation also helped in identifying and correcting underfitting by providing a robust evaluation of the model's performance.\n\nIn summary, we employed a combination of regularization techniques, cross-validation, and hyperparameter tuning to balance the complexity of our models, ensuring that they neither overfit nor underfit the data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting in our models. For the neural network model, we utilized dropout layers and L2 weight regularization. Dropout layers were applied to the first four layers with varying dropout rates, specifically 0.4, 0.4, 0.5, and 0.2, respectively. This technique helps to prevent the model from becoming too reliant on any single neuron by randomly setting a fraction of the input units to zero at each update during training time. Additionally, L2 weight regularization with a value of 0.0001 was applied to the first four layers. L2 regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function, which helps to keep the weights small and reduces the risk of overfitting. These regularization methods were crucial in improving the generalization performance of our neural network model.",
  "optimization/config": "All code and models are available in a public repository. This includes the hyper-parameter configurations, optimization schedule, and model files. The repository is licensed under the Creative Commons CC0 public domain dedication, which means that the work is free of all copyright and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The repository can be accessed at the following URL: https://github.com/BV-BRC-dependencies/zou-plasmid-prediction. Additionally, all data used in the study are publicly available and held at the Bacterial and Viral Bioinformatics Resource Center (BV-BRC), which can be accessed at https://www.bv-brc.org/.",
  "model/interpretability": "The models employed in this study, particularly the neural network and XGBoost models, are generally considered black-box models. These models do not inherently provide clear, interpretable insights into how they make predictions. The neural network, for instance, consists of multiple layers with complex interactions between neurons, making it difficult to trace the decision-making process. Similarly, XGBoost, while powerful, relies on ensemble learning techniques that aggregate the results of many decision trees, obscuring the individual contributions of each tree.\n\nThe logistic regression and random forest models, on the other hand, offer more transparency. Logistic regression provides coefficients for each feature, indicating the direction and magnitude of their influence on the prediction. Random forests can be interpreted by examining the importance of each feature across all trees in the forest. However, even these models can become less interpretable as the number of features and trees increases.\n\nTo enhance interpretability, techniques such as SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) could be employed. These methods can approximate the behavior of complex models locally, providing insights into which features are most influential for specific predictions. However, such techniques were not explicitly used in this study.\n\nIn summary, while some of the models used offer a degree of interpretability, the neural network and XGBoost models are largely black-box, requiring additional tools for interpretation.",
  "model/output": "The model developed in this study is a classification model. It is designed to distinguish between plasmid and chromosomal sequences, which are two distinct classes. The model was trained using various machine learning algorithms, including logistic regression, random forest, XGBoost, and a fully connected neural network. Each of these models was evaluated using 6-mer frequencies as features, with sequence fragments of either 2kb or 5kb in length. The neural network model, in particular, demonstrated the highest accuracy, achieving an average accuracy of 85.22% ± 2.32% for 5kb sequence fragments. This model was further refined using a voting strategy, where each contig in the holdout set was classified based on the majority vote of three randomly selected sequence fragments. This approach improved the model's accuracy to 92.08% ± 2.16%, indicating its robustness and effectiveness in classifying plasmid and chromosomal sequences. The model's performance was assessed using metrics such as accuracy, F1 score, precision, and recall, and it was compared with existing methods like PlasFlow and PlasClass. The results showed that the neural network model outperformed these published methods, particularly for 5kb fragment classification.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms used in this study is publicly available. It can be accessed via a GitHub repository. Additionally, all the data utilized in this research is publicly held and can be found at the Bacterial and Viral Bioinformatics Resource Center (BV-BRC).\n\nThe work is released under the Creative Commons CC0 public domain dedication, which allows for free reproduction, distribution, transmission, modification, and use by anyone for any lawful purpose. This open-access approach ensures that the tools and findings are accessible to the broader scientific community for further research and development.",
  "evaluation/method": "The evaluation of our method involved several rigorous steps to ensure its robustness and accuracy. We employed 10-fold cross-validation to assess the performance of each model. This process involved dividing the dataset into 10 equal parts, using one part for testing, one for validation, and the remaining eight for training. Each part was used in turn for testing and validation, ensuring that every part of the dataset was evaluated. This approach helped to mitigate bias and provided a comprehensive assessment of the model's accuracy and sensitivity to the input training set.\n\nIn each round of cross-validation, we drew receiver operating characteristic (ROC) curves and calculated the area under the curve (AUC) using the test dataset. This allowed us to evaluate the model's performance in distinguishing between plasmid and chromosomal sequences.\n\nAdditionally, we developed a voting classifier to further enhance accuracy. For each contig in the holdout set, we selected three randomly chosen sequence fragments and classified each using the tuned neural network model. We then reported the accuracy based on different voting strategies: requiring a single vote, 2 out of 3 votes, and 3 out of 3 votes. This voting strategy improved the accuracy significantly, demonstrating the effectiveness of ensemble methods in classification tasks.\n\nWe also compared our models with published methods, such as PlasFlow and PlasClass, using metrics like accuracy, F1 score, precision, and recall. This comparison provided a benchmark for our models' performance and highlighted their strengths and areas for improvement.\n\nFurthermore, we analyzed misclassified subsequences to understand the challenges in classifying plasmid and chromosomal sequences. This analysis included examining the protein annotations of misclassified subsequences and exploring the genetic exchange between replicons. This step was crucial in identifying potential improvements for future models.\n\nOverall, our evaluation method combined cross-validation, voting classifiers, and comparative analysis to ensure a thorough and unbiased assessment of our models' performance.",
  "evaluation/measure": "In our evaluation, we primarily focused on accuracy as our key performance metric. This was chosen because it provides a straightforward measure of how often the models correctly classify plasmid and chromosomal sequences. We reported the average accuracy over ten folds of cross-validation, along with the standard deviation, to give a sense of the model's consistency and robustness.\n\nIn addition to accuracy, we also considered the area under the receiver operating characteristic curves (AUCs). For our neural network model, the AUCs for each fold of the 10-fold cross-validation were consistent, ranging from 0.94 to 0.95. This indicates that the model is stable and performs well across different subsets of the data.\n\nWe also compared our models to published methods like PlasFlow and PlasClass, evaluating them using accuracy, F1 score, precision, and recall. This allowed us to benchmark our approach against existing techniques and demonstrate its effectiveness.\n\nFurthermore, we explored the impact of sequence sampling on model performance. We found that using multiple randomly selected sequence fragments per contig improved accuracy, with a \"best out of three\" voting strategy yielding the highest results. This approach was evaluated using both random forest and neural network classifiers, with the neural network model achieving an average accuracy of 92.08% ± 2.16% when using this strategy.\n\nOverall, our set of performance metrics is representative of common practices in the literature. Accuracy, AUC, F1 score, precision, and recall are widely used in machine learning evaluations, particularly in classification tasks. By including these metrics, we provide a comprehensive assessment of our models' performance and their potential for real-world applications.",
  "evaluation/comparison": "In our evaluation, we compared our models to two publicly available methods, PlasClass and PlasFlow, which are machine learning-based classifiers for predicting the plasmid or chromosomal origin of contig sequences. These methods use similar machine learning approaches but were trained on different datasets and sequence lengths.\n\nWe assessed these tools using the same test data in our 10-fold cross-validation. The comparison was based on several metrics, including accuracy, F1 score, precision, and recall. PlasFlow achieved an accuracy of 71.43% ± 4.37%, while PlasClass achieved an accuracy of 78.10% ± 3.28%. Our neural network model, which used a voting strategy, outperformed both methods with an accuracy of 92.08% ± 1.93%.\n\nAdditionally, we compared our models to simpler baselines such as logistic regression and random forest models. The neural network model consistently showed higher accuracy, indicating its robustness and effectiveness in classifying plasmid and chromosomal sequences. The comparison to these baselines and publicly available methods provides a comprehensive evaluation of our approach, demonstrating its superiority in terms of performance metrics.",
  "evaluation/confidence": "The evaluation of our models included a comprehensive assessment of performance metrics with confidence intervals. For instance, the average accuracy for the neural network model using 6-mers and 5kb sequence fragments was reported as 85.22% ± 2.32%. This indicates the mean accuracy along with the standard deviation, providing a measure of the variability and reliability of the results.\n\nStatistical significance was also considered in our evaluations. The use of 10-fold cross-validation ensured that each part of the dataset was used for both training and testing, reducing the risk of overfitting and providing a robust estimate of model performance. The consistent area under the receiver operating characteristic curves (AUCs) for the neural network model, ranging from 0.94 to 0.95, further supports the stability and reliability of our approach.\n\nAdditionally, the improvement in accuracy when using multiple sequence fragments per contig was statistically significant. For example, the random forest model's accuracy improved from 83.98% ± 2.03% to 88.11% ± 1.55% when using a \"best out of three\" voting strategy. This indicates that the observed improvements are not due to random chance but reflect a genuine enhancement in model performance.\n\nOverall, the inclusion of confidence intervals and the use of cross-validation techniques provide a strong basis for claiming that our method is superior to others and baselines. The statistical significance of our results further supports the robustness and generalizability of our approach.",
  "evaluation/availability": "All code and models used in this study are publicly available on GitHub. The data utilized for the evaluation is also publicly accessible and can be found at the Bacterial and Viral Bioinformatic Resource Center (BV-BRC). The work is released under the Creative Commons CC0 public domain dedication, which allows for free reproduction, distribution, transmission, modification, and use by anyone for any lawful purpose. This open-access approach ensures that the evaluation materials are readily available to the scientific community for further research and validation."
}