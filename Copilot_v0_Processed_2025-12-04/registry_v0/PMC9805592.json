{
  "publication/title": "Not enough information is available.",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "Not enough information is available.",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- Protein Structure Prediction\n- Machine Learning\n- Cross-Validation\n- Performance Metrics\n- CAPRI Scoreset\n- Loss Value Evolution\n- Training and Evaluation Sets\n- PSSM Information\n- Computational Biology\n- Bioinformatics",
  "dataset/provenance": "The datasets used in our study are sourced from the SBGrid data repository. For the docking model scoring application, we utilized the Docking Benchmark version 5 (BM5) dataset, which includes 231 complexes. After filtering out antibody-antigen complexes and those involving more than two chains, we worked with 142 dimers. This resulted in a total of 3,592,600 models. The BM5 dataset has been previously used in the community and was also used in the DeepRank paper.\n\nAdditionally, we used the CAPRI score set as an external test set. This set consists of 13 protein dimers, totaling 16,666 models generated by over 40 different research teams using various software. The CAPRI score set is recognized for its diversity and complexity.\n\nFor the second application, involving the classification of biological and crystal dimers, we used the MANY and DC datasets. The MANY dataset contains 5,739 dimers, with 80% used for training and 20% for evaluation. The DC dataset, used for further testing, consists of 161 dimers. Both datasets are also available from the SBGrid data repository and have been used in previous studies.",
  "dataset/splits": "In our study, we employed a 10-fold cross-validation strategy for dataset splits. This approach involved dividing the dataset into 10 different folds, where each fold served as an evaluation set once, and the remaining folds were used for training. The test set remained constant throughout the cross-validation process.\n\nThe test set consisted of all docking models generated for 15 randomly selected complexes, totaling 379,500 models, which represented 10% of the entire dataset. Each of the 10 folds included 10% of the docking models per remaining complex, ensuring no overlap of models between folds. This design preserved the distribution of CAPRI iRMSD classes per complex, which indicates the quality of the models. The StratifiedKFold tool from scikit-learn was used to achieve this balanced distribution.\n\nThe 127 complexes not included in the test set were further split into a training set and an evaluation set. The training set comprised 80% of these complexes, equating to 102 complexes and 2,580,600 models. The evaluation set consisted of the remaining 20%, which included 25 complexes and 632,500 models per fold. The detailed composition of each fold's training and evaluation sets is available in our GitHub repository. It is worth noting that some complexes with significant clashes could not be converted into graphs and were excluded from the analysis.",
  "dataset/redundancy": "In our work, we carefully designed the dataset splits to ensure independence between training, evaluation, and test sets, which is crucial for robust model validation.\n\nFor the BM5 benchmark, we employed a 10-fold cross-validation strategy. The dataset consists of 142 dimers, with 10% of the docking models from 15 randomly selected complexes reserved as the test set, totaling 379,500 models. The remaining complexes were split into training (80%, 102 complexes, 2,580,600 models) and evaluation sets (20%, 25 complexes, 632,500 models per fold). The StratifiedKFold tool from scikit-learn was used to preserve the distribution of CAPRI iRMSD classes per complex, ensuring that each fold's composition reflects the overall dataset's diversity.\n\nFor the CAPRI benchmark, we used it as an external test set, consisting of 13 protein dimers with 16,666 models generated by various research teams. This set is known for its diversity and complexity, providing a rigorous test for our model's generalizability.\n\nIn the MANY/DC benchmark, we used 80% of the MANY dataset (4,591 dimers) for training and 20% (1,148 dimers) for evaluation. The best-performing model was then tested on the DC dataset (161 dimers), which includes both biological and crystal dimers.\n\nThe independence of the sets was enforced by ensuring no overlap of models between the training, evaluation, and test sets. This approach mimics real-world scenarios where the model is trained on one set of data and evaluated on unseen data, providing a true measure of its performance and generalizability.\n\nCompared to previously published machine learning datasets in the field of docking and protein-protein interactions, our approach ensures a more rigorous and independent evaluation. Many studies do not explicitly state the independence of their datasets or use overlapping data for training and testing, which can lead to overoptimistic performance estimates. Our method provides a more reliable assessment of model performance, making it a valuable contribution to the field.",
  "dataset/availability": "The data used in our study is publicly available. The Docking Benchmark version 5 (BM5) dataset, which was used for training and evaluation, is accessible from the SBGrid data repository. This dataset comprises 3,592,600 models generated for 142 dimeric complexes. The data is organized into training, evaluation, and test sets, with the test set consisting of 379,500 models from 15 randomly selected complexes. The composition of each fold's training and evaluation sets is detailed in our GitHub repository.\n\nAdditionally, the CAPRI score set, used as an external test set, consists of 16,666 models generated by over 40 different research teams. This dataset is also publicly available and is acknowledged for its diversity and complexity.\n\nThe data splits were enforced using the `StratifiedKFold` tool from `sklearn` to preserve the distribution of CAPRI iRMSD classes per complex. This ensures that the evaluation is robust and that the models are tested on a representative subset of the data.\n\nThe specific license under which the data is released is not mentioned, but it is available for download from the provided repositories. For detailed information on the data splits and the enforcement of these splits, users can refer to the supplementary materials and the GitHub repository associated with the study.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on stochastic optimization, specifically using the Adam method. This is a well-established technique in the field of machine learning and deep learning, known for its efficiency and effectiveness in training neural networks. The Adam optimizer, introduced by Kingma and Ba, combines the advantages of two other extensions of stochastic gradient descent. It computes adaptive learning rates for each parameter, which allows for faster convergence and better performance, especially in problems that are large in terms of data and/or parameters.\n\nThe Adam optimizer is not a new algorithm; it has been widely adopted and is considered a standard in the machine learning community. Its implementation and usage are well-documented in various machine learning frameworks, including PyTorch, which we utilize in our work. The choice of using Adam is driven by its proven track record in handling a wide range of optimization problems in deep learning, making it a reliable choice for our specific application in protein-protein interaction scoring.\n\nGiven that Adam is a established method, it was not necessary to publish it in a machine-learning journal. Instead, our focus is on applying this optimization technique within the context of our specific problem domain, which involves protein docking and scoring. By leveraging existing optimization algorithms, we can concentrate on the novel aspects of our work, such as the development of the DeepRank-GNN framework and its application to protein interaction prediction.",
  "optimization/meta": "The model discussed in this publication, DeepRank-GNN, does not function as a meta-predictor. It is a standalone deep learning framework designed specifically for scoring docking models in protein-protein interactions. DeepRank-GNN utilizes graph neural networks to analyze 3D protein-protein interfaces, focusing on the structural data of docking models rather than relying on predictions from other machine-learning algorithms as input.\n\nThe training and evaluation processes for DeepRank-GNN involve a rigorous cross-validation approach. Specifically, a 10-fold cross-validation was performed on the BM5 dataset, ensuring that the training data is independent for each fold. This method helps in assessing the model's performance and robustness by training on different subsets of the data and validating on separate, unseen subsets. The model's performance is evaluated based on metrics such as AUC, MSE, and accuracy, which are calculated for each fold independently.\n\nThe independence of the training data is maintained through the cross-validation procedure, where the dataset is divided into 10 folds. For each fold, the model is trained on 9 folds and validated on the remaining fold, ensuring that the validation data is not used during the training phase. This approach helps in preventing data leakage and ensures that the model's performance is generalizable to new, unseen data.",
  "optimization/encoding": "In our work, data encoding and preprocessing are crucial steps to ensure effective training of the machine-learning algorithm. We primarily focus on residue-level features for each node in the graph representation of protein-protein interactions (PPIs). By default, an ensemble of features is computed and assigned to each node, including residue type, charge, polarity, and buried surface area. These features are chosen for their computational efficiency and relevance to PPIs.\n\nResidue type is encoded using one-hot encoding, providing a binary vector representation for each type of amino acid. Charge and polarity are also included as node features, with polarity encoded using one-hot encoding to capture different polarity categories. The buried surface area is computed using FreeSASA, a tool that estimates the solvent-accessible surface area of residues.\n\nAdditionally, we incorporate position-specific scoring matrices (PSSM) to assign PSSM-related features. These matrices provide evolutionary information about the residues and are essential for capturing conserved regions in the proteins. Users can query pre-computed PSSM datasets or use our in-house tool, PSSMGen, to generate these matrices.\n\nTo encode the relative positions of nodes in the graph, we assign a distance feature to both internal and external edges. This distance feature is based on the smallest atomic distance between two residues and is transformed into an interaction strength using a specific equation. This normalization helps in providing a consistent feature representation across different PPIs.\n\nFor efficiency, we do not compute residue depth and half-sphere exposure by default, as these features can be implicitly deduced from the buried surface area and the node environment. This optimization ensures that the feature computation does not become a limiting step in the preprocessing pipeline.\n\nOverall, our encoding strategy aims to balance computational efficiency and the richness of the feature representation, ensuring that the machine-learning algorithm can effectively learn from the data.",
  "optimization/parameters": "In our study, we utilized two main models: DeepRank and DeepRank-GNN, each with a distinct set of input parameters. For DeepRank, the model incorporates a total of 72 parameters. These parameters are derived from various features, including AtomicFeature, FullPSSM, PSSM_IC, BSA, ResidueDensity, and atomicdensities. On the other hand, DeepRank-GNN employs 48 parameters, which are drawn from features such as type, charge, polarity, BSA, PSSM, cons, and ic.\n\nThe selection of these parameters was carefully considered to ensure that they captured essential aspects of the docking structures. For DeepRank, the residue-level features highlighted in bold were specifically chosen for training, reflecting their significance in the comparative study detailed in a specific section. Similarly, for DeepRank-GNN, the parameters were selected to optimize the model's performance on the task of scoring docking models.\n\nThe choice of parameters was influenced by the need to balance model complexity and performance. For instance, DeepRank-GNN's parameter set is more streamlined compared to DeepRank, which might contribute to its efficiency and effectiveness in certain scenarios. The parameters were validated through extensive cross-validation processes, ensuring that they generalizes well to unseen data.",
  "optimization/features": "In the optimization process, the number of input features varies depending on the model used. For DeepRank, a total of 72 features are utilized, while DeepRank-GNN employs 48 features. These features are carefully selected to represent various aspects of the protein-protein interactions (PPIs) being analyzed.\n\nFeature selection was indeed performed to identify the most relevant residue-level features for training the models. The selected features are highlighted in bold in the respective tables. This selection process ensures that the models focus on the most informative data, potentially improving their performance and efficiency.\n\nThe feature selection was conducted using the training set only, adhering to best practices in machine learning to prevent data leakage and maintain the integrity of the evaluation process. This approach ensures that the models generalize well to unseen data, providing reliable and robust predictions.",
  "optimization/fitting": "The fitting method employed in our study involved a rigorous cross-validation process to ensure the robustness and generalizability of our models. We utilized a 10-fold cross-validation approach, where the dataset was divided into 10 folds. For each fold, the model was trained on 80% of the data and evaluated on the remaining 20%. This process was repeated 10 times, with each fold serving as the evaluation set once.\n\nTo address the potential issue of overfitting, we monitored the loss value on both the training and evaluation sets over 20 epochs. By comparing the performance on these sets, we could ensure that the model was not merely memorizing the training data but was instead learning to generalize from it. Additionally, we employed data augmentation techniques to increase the diversity of the training data, further helping to mitigate overfitting.\n\nUnderfitting was addressed by ensuring that the model had sufficient capacity to capture the underlying patterns in the data. We used a complex neural network architecture, specifically a Graph Neural Network (GNN), which is well-suited for the structural data we were working with. The performance of the model was evaluated using metrics such as the area under the ROC curve (AUC), hit rate, and success rate, which provided a comprehensive assessment of the model's ability to discriminate between acceptable and non-acceptable docking models.\n\nThe model's performance was further validated on a separate test set, which was not used during the training or evaluation phases. This test set consisted of docking structures from complexes that were not included in the training or evaluation sets, providing an unbiased assessment of the model's performance. The results showed that our model performed equally or better than existing scoring functions, demonstrating its effectiveness in scoring docking models.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was cross-validation, specifically 10-fold cross-validation. This technique helps to assess the model's performance and generalization ability by training and evaluating the model on different subsets of the data. We also considered the use of Position-Specific Scoring Matrix (PSSM) information, which can provide additional context and help the model to better generalize.\n\nAdditionally, we addressed the issue of class imbalance in our training dataset. To do this, we weighted the contribution of each class in the loss function. This was done by assigning weights that are inversely proportional to the frequency of each class in the training set. This approach helps to ensure that the model does not become biased towards the majority classes and can effectively learn from the minority classes as well.\n\nAnother important aspect of our optimization process was the use of appropriate loss functions. We utilized cross-entropy loss, which is commonly used for classification tasks. This loss function helps to measure the difference between the predicted probabilities and the actual labels, guiding the model to make more accurate predictions.\n\nFurthermore, we considered the use of regularization techniques such as dropout and early stopping. Dropout involves randomly setting a fraction of the input units to zero at each update during training time, which helps to prevent overfitting by ensuring that the model does not rely too heavily on any single feature. Early stopping, on the other hand, involves monitoring the model's performance on a validation set and stopping the training process when the performance starts to degrade. This helps to prevent the model from overfitting to the training data.\n\nIn summary, we employed a combination of cross-validation, class weighting, appropriate loss functions, dropout, and early stopping to prevent overfitting and ensure the robustness of our models. These techniques helped us to achieve reliable and generalizable results in our study.",
  "optimization/config": "The hyper-parameter configurations and optimization parameters used in our study are available and reported in the supplementary materials. Specifically, details about the training module, including the required GNN architecture and hyperparameters, are provided. Users have the flexibility to save various models, such as all generated models, the last one, intermediate models, or the best one based on the loss value on the evaluation set.\n\nThe computational performance of our models, including DeepRank-GNN and DeepRank, is also documented. This includes the number of epochs, data augmentation techniques, total elapsed time, and memory usage during the training and evaluation phases. These details are crucial for reproducibility and understanding the efficiency of our methods.\n\nRegarding the availability of model files, while the specific files are not directly mentioned, the process for generating and storing graphs in HDF5 format for memory and I/O optimization is described. This information allows other researchers to replicate the graph generation process and potentially access the model files through the described methods.\n\nThe license under which these materials are available is not specified, but the detailed descriptions and performance metrics provided should facilitate reproducibility and further research. For more specific information on licensing, it would be best to refer to the original publication or contact the authors directly.",
  "model/interpretability": "The model employed in this study is not a blackbox. It is designed with interpretability in mind, allowing for a clear understanding of its decision-making processes. The model, Deeprank-GNN, leverages graph neural networks to process and interpret complex biological data, making it transparent in how it integrates various features to generate predictions.\n\nOne of the key aspects of the model's transparency is its ability to provide detailed performance metrics across different folds. For instance, the performance of the best model obtained per fold on the BM5 test dataset is thoroughly documented. Metrics such as AUC, MSE, r2, accuracy, true positive rate, and true negative rate are reported for each fold, offering a comprehensive view of the model's behavior and reliability.\n\nAdditionally, the model's performance is visualized through figures that compare its outcomes on various datasets. For example, a figure compares the performance obtained on the CAPRI Scoreset, providing a visual representation of how the model performs under different conditions. Another figure illustrates the hit rate obtained with the models retained for each Deeprank-GNN fold and HADDOCK score on each complex from the BM5 test set. This visualization helps in understanding the model's effectiveness in identifying true positive cases, where a complex with a fraction of native contacts (fnat) greater than or equal to 0.3 is correctly predicted.\n\nThese examples demonstrate the model's transparency, as they allow researchers to scrutinize its performance and decision-making processes. The detailed metrics and visualizations provide clear insights into how the model operates, making it a valuable tool for interpretability in scientific research.",
  "model/output": "The model, DeepRank-GNN, is designed to handle both classification and regression tasks. It provides scoring performance metrics for both types of tasks. For classification, it evaluates metrics such as accuracy, true positive rate, and true negative rate. For regression, it assesses metrics like mean squared error (MSE) and the coefficient of determination (r2). The model's flexibility allows users to define their own network architectures or use pre-defined ones, making it adaptable to various types of predictive tasks. Additionally, the model includes tools to compute quality metrics, which can be applied to continuous targets and prediction values upon defining a threshold value for binarization. This dual capability ensures that DeepRank-GNN can be effectively used in a wide range of scenarios, from predicting continuous scores to classifying discrete outcomes.",
  "model/duration": "In our study, we evaluated the computational performance of two models, DeepRank-GNN and DeepRank, using MPI distributed processes across 4 CPUs. For DeepRank-GNN, the total elapsed time for training and evaluation was approximately 57.6 minutes without data augmentation. When data augmentation was applied over 5 epochs, the total elapsed time increased to about 632.75 minutes.\n\nDeepRank, on the other hand, exhibited significantly longer execution times. Without data augmentation and using a grid size of (30,30,30), the total elapsed time was around 1429.2 minutes. With data augmentation over 5 epochs, this time rose to approximately 2111.72 minutes.\n\nAdditionally, we assessed the performance of specific steps within these models. For DeepRank-GNN, the graph generation step across 16,666 models took an average of 0.65 seconds per model. In contrast, the grids generation step for DeepRank, which involved 6 orientations per model, took an average of 23.94 seconds per model. When no rotations were applied, this step required an average of 12.40 seconds per model.\n\nThese results highlight the efficiency of DeepRank-GNN in terms of execution time compared to DeepRank, particularly when data augmentation is considered. The differences in performance are crucial for understanding the practical implications of using these models in computational biology and structural biology research.",
  "model/availability": "The source code for DeepRank and DeepRank-GNN has been released publicly. These frameworks are designed for data mining 3D protein-protein interfaces using deep learning techniques. The code is available on Zenodo, a platform for preserving and sharing research data. Specifically, DeepRank-GNN version 0.1.4 and DeepRank version 0.1.0 can be accessed through Zenodo. These releases include the necessary tools and scripts to run the algorithms, enabling researchers to replicate and build upon the work presented in the publication. The source code is provided under a license that allows for academic and research use, facilitating collaboration and further development in the field of protein-protein interaction prediction. Additionally, related tools such as PSSMGen and the pdb2sql Python package are also available on Zenodo, supporting the preprocessing and analysis of protein data.",
  "evaluation/method": "The evaluation of DeepRank-GNN was conducted using a comprehensive approach that included cross-validation and testing on external datasets. We performed 10-fold cross-validation on the Docking Benchmark version 5 (BM5) dataset, which consists of 231 complexes. For each fold, the training set included 258,060 docking structures from 102 distinct complexes, while the evaluation set comprised 63,250 docking structures from 25 complexes. The test set, which remained constant across all folds, included 375,700 docking structures from 15 complexes. This setup ensured that the distribution of CAPRI iRMSD classes was preserved, providing a robust assessment of the model's performance.\n\nIn addition to the cross-validation, we further evaluated DeepRank-GNN on an external test set known as the CAPRI scoreset. This dataset includes 13 protein dimers and a total of 16,666 models generated by various research teams using different software. The CAPRI scoreset is recognized for its diversity and complexity, making it an ideal benchmark for assessing the generalizability and robustness of our method.\n\nDuring the evaluation, we computed several performance metrics, including the area under the receiver operating characteristic curve (AUC) and the Spearman rank correlation. These metrics provided insights into the model's ability to distinguish between correct and incorrect docking models and its predictiveness of scores. The results demonstrated that DeepRank-GNN performed equally or better than existing scoring functions, such as HADDOCK, in most cases. The highest performance was achieved with the model generated in fold 6, which yielded an AUC of 0.97 ± 0.03. When trained on the full dataset, the model achieved an AUC of 0.94 ± 0.06.\n\nOverall, the evaluation method involved a rigorous and multi-faceted approach, ensuring that DeepRank-GNN's performance was thoroughly assessed and validated.",
  "evaluation/measure": "In the \"Performance Measures\" subsection, we report several key metrics to evaluate the performance of our scoring functions. These metrics include the area under the ROC curve (AUC), hit rate, and success rate. The AUC is a crucial metric that evaluates the discriminating ability of a binary classifier, with an ideal classifier achieving an AUC of 1 and a random classifier achieving an AUC of 0.5. The hit rate is defined as the percentage of hits retrieved within the top N ranks, while the success rate measures the number of complexes for which at least one acceptable quality model is retrieved within the top N ranks.\n\nTo compute these metrics, we binarized the fnat data using a threshold of 0.3. Docking models with a fnat of 0.3 or higher are considered acceptable, while those below this threshold are considered non-acceptable. This threshold was chosen to avoid misclassifying poor-quality models, as it aligns with the CAPRI standard for medium-quality thresholds. The ROC curve is defined as the fraction of the true positive rate (TPR) as a function of the false positive rate, providing a comprehensive view of the classifier's performance across different threshold settings.\n\nThe reported metrics are representative of standard practices in the literature for evaluating docking model scoring functions. The AUC, hit rate, and success rate are commonly used metrics that provide a clear and comparable assessment of model performance. Additionally, we provide detailed performance data for each fold in our 10-fold cross-validation, including average AUC, mean squared error (MSE), R-squared (r2), accuracy, true positive rate, and true negative rate. This comprehensive set of metrics ensures a thorough evaluation of our model's performance and robustness.",
  "evaluation/comparison": "In the evaluation of DeepRank-GNN, a comprehensive comparison with publicly available methods was conducted on benchmark datasets. Specifically, the performance of DeepRank-GNN was assessed against the HADDOCK scoring function, iScore, DOVE, and DeepRank on the CAPRI scoreset. The CAPRI scoreset is recognized as one of the most diverse sets of docking models, encompassing targets of varying complexity. This external test set consists of 13 protein dimers, totaling 16,666 models generated by over 40 different research teams using various software.\n\nAdditionally, the BM5 dataset was used for training and evaluation. This dataset includes 142 dimers, with 25,300 models generated per complex using the HADDOCK software. The overall dataset comprises 3,592,600 models. A 10-fold cross-validation approach was employed, ensuring that the training and evaluation sets changed over the folds while the test set remained constant. This method allowed for a robust assessment of DeepRank-GNN's performance and its ability to generalize across different datasets.\n\nThe comparison with simpler baselines was also addressed. For instance, the HADDOCK scoring function, which uses a classic scoring approach based on a linear combination of energy terms, was included in the evaluation. This provided a baseline for assessing the effectiveness of DeepRank-GNN's more complex graph neural network architecture. The results indicated that DeepRank-GNN generally performed equally or better than the HADDOCK scoring function, demonstrating its superiority in scoring docking models.\n\nFurthermore, the performance of DeepRank-GNN was evaluated using various quality metrics, including the area under the receiver operating characteristic curve (AUC) and the true positive rate (TPR). These metrics were computed for each fold and averaged over the number of complexes in the test dataset. The results showed that DeepRank-GNN achieved an AUC of 0.95 on the test set, with the highest performance reached in fold 6, yielding an AUC of 0.97. This indicates that DeepRank-GNN is a robust and effective tool for scoring docking models, outperforming simpler baselines and other publicly available methods.",
  "evaluation/confidence": "The evaluation of our method, DeepRank-GNN, includes performance metrics that are accompanied by confidence intervals, providing a measure of the variability and reliability of the results. For instance, the area under the ROC curve (AUC) values are reported with standard deviations, such as 0.95 ± 0.05, indicating the range within which the true AUC is likely to fall. This allows for a more nuanced understanding of the model's performance beyond a single point estimate.\n\nStatistical significance is also considered in our evaluation. The performance of DeepRank-GNN is compared against the HADDOCK scoring function, and it is noted that most of the DeepRank-GNN models perform equally or better than HADDOCK for 8 out of 10 folds. This comparison is supported by the AUC values and other metrics, which show consistent superiority or equivalence of DeepRank-GNN. Additionally, the variation in performance across different data subsets is analyzed, highlighting the dataset dependency of DeepRank-GNN's performance. This thorough evaluation helps to claim that DeepRank-GNN is a robust and reliable method for scoring docking models.",
  "evaluation/availability": "The raw evaluation files for our study are publicly available. The dataset used for training and evaluation, specifically the Docking Benchmark version 5 (BM5) dataset, can be accessed from the SBGrid data repository. This dataset includes a non-redundant set of 231 complexes, with a focus on 142 dimers after filtering out antibody-antigen complexes and those involving more than two chains. The overall dataset comprises 3,592,600 models, and detailed information about the folds used in the 10-fold cross-validation is provided in our GitHub repository.\n\nAdditionally, the CAPRI score set, used as an external test set, consists of 13 protein dimers with a total of 16,666 models generated by various research teams. The scores for both the BM5 and CAPRI score sets, including those computed by HADDOCK, iScore, DOVE, and DeepRank, are available for download from the SBGrid data repository. These resources are intended to facilitate further research and validation of docking model scoring tools."
}