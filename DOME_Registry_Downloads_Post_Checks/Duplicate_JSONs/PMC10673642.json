{
  "publication/title": "Imputation method for single-cell RNA-seq data using neural topic model",
  "publication/authors": "The authors who contributed to the article are:\n\n- **Y.Q.** (Full name not provided) - Conceived and developed the study. Completed the scNTImpute workflow and wrote the main manuscript text.\n- **L.L.** (Full name not provided) - Conceived and developed the study.\n- **S.H.** (Full name not provided) - Reviewed and contributed to all versions of the manuscript text.\n- **L.T.** (Full name not provided) - Reviewed and contributed to all versions of the manuscript text.\n\nAll authors read and approved the final manuscript.",
  "publication/journal": "GigaScience",
  "publication/year": "2023",
  "publication/doi": "Not enough information is available.",
  "publication/tags": "- scNTImpute\n- scRNA-seq\n- Transfer learning\n- Neural topic model\n- Imputation\n- Single-cell RNA sequencing\n- Clustering performance\n- Path enrichment analysis\n- Scalability\n- Efficiency\n- Human pancreatic islet\n- Mouse pancreatic islet\n- Gene expression\n- Dropout values\n- Similarity matrix\n- ARI\n- NMI\n- MI\n- RI\n- Cosine similarity\n- Fowlkes–Mallows score",
  "dataset/provenance": "The datasets used in this study are publicly available and have been utilized in previous research. The human pancreatic islet data can be accessed from the Gene Expression Omnibus (GEO) or the EMBL-EBI database with accession codes GSE81076, GSE85241, GSE86469, E-MTAB-5061, and GSE84133. The mouse pancreatic islet data is available under the GEO accession code GSE84133. Additionally, other datasets such as the human brain data (GSE67835), Chung et al. dataset (GSE75688), Hrvatin dataset (GSE59739), Romanov et al. dataset (GSE74672), and Deng et al. dataset (GSE45719) were also referenced. These datasets were obtained using the inDrop method, a droplet-based single-cell RNA sequencing technique. The specific number of data points is not detailed, but the datasets are comprehensive and have been used in the community for various analyses.",
  "dataset/splits": "Not enough information is available.",
  "dataset/redundancy": "Not applicable",
  "dataset/availability": "The data utilized in this study are publicly accessible. All single-cell RNA sequencing (scRNA-seq) datasets employed are available through the Gene Expression Omnibus (GEO) database. Specific datasets include human brain data with accession number GSE67835, the Chung et al. dataset with accession code GSE75688, the Hrvatin dataset with GEO accession code GSE59739, the Romanov et al. dataset with GEO accession code GSE74672, the Deng et al. dataset with GEO accession code GSE45719, and mouse pancreatic islet data with GEO accession code GSE84133. Additionally, human pancreatic islet data can be found in both the GEO and EMBL-EBI databases under accession codes GSE81076, GSE85241, GSE86469, E-MTAB-5061, and GSE84133. All supplementary data are available in the GigaScience repository, GigaDB.\n\nThe datasets are freely available for public use, adhering to the terms and conditions specified by the respective databases. This ensures that other researchers can access and utilize the data for further studies, promoting reproducibility and transparency in scientific research.",
  "optimization/algorithm": "The optimization algorithm employed in our work is based on variational inference, a well-established class of machine-learning algorithms. This approach is not entirely new but has been adapted and implemented specifically for our neural topic model in the context of single-cell RNA sequencing (scRNA-seq) data imputation.\n\nVariational inference is a method used to approximate complex probability distributions, making it suitable for high-dimensional data like scRNA-seq. In our model, we use a two-layer feed-forward neural network to estimate the parameters of the variational posterior distribution. This network helps in approximating the true posterior distribution of the latent cell topic mixtures, which is computationally challenging to solve directly in high-dimensional space.\n\nThe reason this algorithm was not published in a machine-learning journal is that our focus is on the application of this method to biological data, specifically scRNA-seq data imputation. The innovation lies in how we have tailored variational inference to handle the unique characteristics of scRNA-seq data, such as sparsity and high dimensionality. Our work demonstrates the practical utility of this approach in improving the accuracy and efficiency of scRNA-seq data imputation, which is crucial for biological research.\n\nThe optimization process involves minimizing the evidence lower bound (ELBO), which includes a likelihood term and a regularization term involving the Kullback-Leibler (KL) divergence. This ensures that the variational posterior distribution closely approximates the true posterior distribution, leading to more accurate imputation of dropout values in the data. The use of variational inference in our model allows for scalable and efficient imputation, making it suitable for large scRNA-seq datasets.",
  "optimization/meta": "The model discussed in this publication does not function as a meta-predictor. It is designed to handle single-cell RNA-seq data using a neural topic model, focusing on imputation and transfer learning across datasets. The model does not rely on data from other machine-learning algorithms as input. Instead, it directly processes scRNA-seq data to improve clustering performance and handle dropout events.\n\nThe core of the model involves training on reference datasets and applying the learned parameters to target datasets, even when the datasets do not share the same cell types. This approach leverages the similarity in data distributions between different datasets, such as those from human and mouse pancreatic islets. The model's effectiveness is demonstrated through various metrics, including ARI, NMI, MI, and RI, which show significant improvements in imputation and clustering performance.\n\nThe transfer learning capability of the model is a key feature, allowing it to be trained on one dataset and applied to another, thereby enhancing its versatility and applicability across different biological contexts. This transfer learning is validated through extensive testing and visualization, ensuring that the model maintains high performance and stability.\n\nThe scalability and efficiency of the model are also highlighted, as it can handle datasets with varying numbers of genes, exhibiting a linear increase in running time relative to the number of genes. This makes the model practical for large-scale applications in single-cell RNA-seq data analysis.",
  "optimization/encoding": "The data encoding process began with the input of a single-cell RNA-seq gene count expression matrix, where rows represented cells and columns represented genes. Prior to encoding, data filtering and quality control were performed to ensure the integrity of the dataset. The goal was to obtain an imputation matrix with the same dimensionality as the original count matrix.\n\nEach sample (cell) and each gene in the matrix were normalized separately to facilitate subsequent work. This normalization resulted in two matrices: one normalized by cell (Y_C) and another by gene (Y_G). The gene-normalized matrix (Y_G) was then log10 transformed, and a pseudo-count of 1.01 was added to generate the final matrix (Y). This transformation helped to prevent large observations from having a significant impact, eliminated heteroscedasticity issues, and provided greater flexibility for modeling by transforming the values into continuity.\n\nThe addition of pseudo-counts was crucial to avoid infinite values of the parameters during later model training. This encoding process ensured that the data was appropriately pre-processed and ready for input into the machine-learning algorithm, enabling effective imputation and analysis of single-cell RNA-seq data.",
  "optimization/parameters": "In our model, the number of parameters, denoted as p, is primarily determined by the architecture of the neural networks employed. Specifically, we use a two-layer feed-forward neural network to estimate the sufficient statistics of the proposed distribution of cell topic mixtures. The parameters of this network, including weights and biases, contribute significantly to the total number of parameters in the model.\n\nThe selection of the number of parameters was guided by the complexity of the data and the need for the model to capture the underlying patterns effectively. We conducted experiments with different network architectures and found that a two-layer feed-forward neural network provided a good balance between model complexity and performance. The specific number of neurons in each layer was chosen based on empirical results and validation on held-out datasets.\n\nAdditionally, the variational inference method we employed involves optimizing the evidence lower bound (ELBO), which includes terms for the likelihood function and the Kullback-Leibler (KL) divergence. The parameters of the approximate posterior distribution, which are estimated by the neural network, play a crucial role in this optimization process.\n\nOverall, the number of parameters in our model is designed to be flexible and scalable, allowing it to handle datasets with varying numbers of genes and cells efficiently. This design ensures that the model can generalize well to new data while maintaining computational efficiency.",
  "optimization/features": "The input features for our model are the genes present in the single-cell RNA sequencing (scRNA-seq) datasets. The number of genes, and thus the number of input features, varies depending on the dataset used. We tested our model on datasets containing 1k, 2k, 5k, 10k, and 15k genes to validate its scalability and efficiency.\n\nFeature selection was not explicitly performed as part of our methodology. Instead, we utilized all available genes in the datasets for imputation. The model is designed to handle high-dimensional data typical of scRNA-seq experiments, where the number of genes can be very large.\n\nThe datasets used for training and testing were carefully chosen to ensure similarity in their distributions, which is crucial for the transfer learning approach employed. This similarity was demonstrated through statistical measures such as mean, variance, and standard deviation, as well as visualizations of probability distributions. By ensuring that the datasets have similar characteristics, we aimed to enhance the model's performance in transfer learning tasks.",
  "optimization/fitting": "In our optimization process, we employed a variational inference method to approximate the true posterior distribution of latent variables in our neural topic model. This approach is particularly useful when dealing with high-dimensional data, such as single-cell RNA-seq data, where the number of parameters can indeed be much larger than the number of training points.\n\nTo address the risk of over-fitting, we incorporated a regularization term in our evidence lower bound (ELBO) optimization. This term involves the Kullback-Leibler (KL) divergence between the approximate posterior distribution and the prior distribution. By minimizing this divergence, we encourage the approximate posterior to stay close to the prior, effectively preventing the model from fitting the noise in the data.\n\nAdditionally, we used a two-layer feed-forward neural network to estimate the sufficient statistics of the proposed distribution of cell topic mixtures. This network was trained to optimize the ELBO, which includes both the likelihood of the observed data and the KL divergence term. By doing so, we ensure that the model generalizes well to unseen data, rather than merely memorizing the training data.\n\nTo rule out under-fitting, we validated our model on multiple datasets and compared its performance with other imputation methods. The results showed that our model achieved competitive or superior performance in terms of clustering metrics such as ARI, AMI, and NMI. Furthermore, we conducted transfer learning experiments, where the model trained on one dataset was applied to another, demonstrating its robustness and ability to generalize across different datasets.\n\nIn summary, our fitting method carefully balances the complexity of the model with the need for generalization, using variational inference and regularization to prevent over-fitting, and validation on multiple datasets to ensure that under-fitting is not an issue.",
  "optimization/regularization": "In our optimization process, we employed a regularization technique to prevent overfitting. Specifically, we used the Kullback-Leibler (KL) divergence as a regularization term. This term measures the difference between the approximate posterior distribution and the prior distribution. By minimizing this divergence, we encourage the variational posterior distribution to approximate the prior distribution, which helps in preventing overfitting. This regularization is integrated into the evidence lower bound (ELBO) optimization process, ensuring that our model generalizes well to unseen data.",
  "optimization/config": "The hyper-parameter configurations, optimization schedule, model files, and optimization parameters are not explicitly detailed in the provided information. However, the source code for the project, named scNTImpute, is available on GitHub. The project homepage is https://github.com/qiyueyang-7/scNTImpute. This repository likely contains the necessary details for reproducing the experiments, including hyper-parameter settings, optimization schedules, and model configurations. The project is licensed under the MIT License, which allows for free use, modification, and distribution. For specific optimization parameters and schedules, one would need to refer to the documentation or code within the repository.",
  "model/interpretability": "The scNTImpute model, while leveraging deep learning techniques, is designed with interpretability in mind. Unlike traditional deep learning models that are often considered \"black boxes,\" scNTImpute incorporates a neural topic model that allows for a more transparent understanding of the underlying features learned from the data.\n\nOne of the key aspects of scNTImpute's interpretability is its use of topic modeling. This approach enables the extraction of meaningful topics from the single-cell RNA-seq data, which can be linked to known biological pathways. For instance, topics derived from human islet data have been shown to be enriched in pathways related to pancreatic function, such as insulin receptor recycling and signaling. This enrichment analysis provides a clear biological context for the topics identified by the model, making it easier to interpret the results.\n\nAdditionally, the model's ability to perform pathway enrichment analysis on the topics allows researchers to explore the relevance of these topics to currently known gene pathways. This feature enhances the interpretability of the deep features extracted by the model from a biological perspective. By understanding the underlying connections between cells, genes, and pathways, researchers can gain insights into the biological significance of the data.\n\nFurthermore, the model's use of neural networks to learn the mixture model parameters of gene expression distribution helps in understanding the true state of the expression data. This allows for a more direct interpretation of which gene transcripts are affected by dropout events, thereby improving the overall interpretability of the model's outputs.\n\nIn summary, scNTImpute's use of neural topic modeling and pathway enrichment analysis makes it a more interpretable model compared to traditional deep learning approaches. This interpretability is crucial for researchers to understand the biological significance of the data and to make informed decisions based on the model's outputs.",
  "model/output": "The model is primarily focused on imputation rather than classification or regression. It is designed to handle single-cell RNA sequencing (scRNA-seq) data, addressing the issue of dropout events where true gene expression is not captured. The model uses a neural topic model to infer the mixture of cell topics and the parameters of a mixture model for gene expression data. This allows it to identify and impute dropout values, effectively recovering biologically significant gene expression from single-cell datasets.\n\nThe output of the model includes imputed gene expression matrices, which can be visualized using techniques like UMAP. The model's performance is evaluated using various metrics such as adjusted Rand index (ARI), Rand index (RI), normalized mutual information (NMI), mutual information (MI), cosine similarity (CS), and Fowlkes–Mallows score (FMS). These metrics help in assessing the accuracy and effectiveness of the imputation process.\n\nAdditionally, the model's output can be used for downstream analyses such as clustering and pathway enrichment. For instance, the imputed data can be clustered using methods like Leiden clustering, and the topics inferred by the model can be analyzed for their biological relevance to known genetic pathways. This provides insights into the functional implications of the imputed gene expression data.\n\nThe model also supports transfer learning, allowing it to be trained on one dataset and applied to another, even if the datasets do not share the same cell types. This capability enhances the model's versatility and applicability to different scRNA-seq datasets. The transfer learning process has been shown to improve various imputation indicators, demonstrating the model's robustness and generalizability.",
  "model/duration": "The execution time of our model, scNTImpute, was thoroughly evaluated to ensure its scalability and efficiency. We tested the model on datasets containing varying numbers of genes—specifically, 1k, 2k, 5k, 10k, and 15k genes. The results showed a linear increase in running time relative to the number of genes. This linear relationship indicates that as the dataset size grows, the model's runtime scales predictably, making it suitable for practical applications where the number of genes and cells is limited. This scalability is particularly advantageous for single-cell RNA sequencing (scRNA-seq) datasets, where the model's performance remains robust and efficient.",
  "model/availability": "The source code for the scNTImpute project is publicly available. It can be accessed via the project's homepage on GitHub. The project is licensed under the MIT License, which allows for free use, modification, and distribution of the software, both in personal and commercial projects.\n\nThe software is platform-independent and is programmed in Python. To run the algorithm, users need to have Python 3.7 installed, along with specific libraries such as numpy 1.21 and pandas 1.3. Additionally, conda is required for managing the environment.\n\nThe project is identified with the Research Resource Identifier (RRID) SCR_024395 and the BiotoolsID scNTImpute. These identifiers can be used to find more information about the software in relevant databases.",
  "evaluation/method": "The evaluation of our method, scNTImpute, involved a comprehensive assessment using various real datasets and multiple evaluation metrics. We performed imputation experiments on several published datasets, including those from Chung et al., Deng et al., and others, to validate the robustness and stability of our model.\n\nFor evaluation, we utilized several metrics such as Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), Rand Index (RI), and Mutual Information (MI). Additionally, we employed other indexes like cosine similarity (CS) and Fowlkes–Mallows score (FMS) to assess the imputation quality. These metrics provided a thorough evaluation of how well scNTImpute recovered true biological information from sparse single-cell data.\n\nWe compared scNTImpute with several existing imputation models, including MAGIC, DCA, DeepImpute, SAUCIE, scIGANs, scImpute, SCVI, and others. The evaluation results, visualized in figures and tables, demonstrated that scNTImpute consistently outperformed these models across various metrics. For instance, in the Chung et al. dataset, scNTImpute achieved the highest scores in both CS and FMS, with a significant gap in FMS compared to other methods.\n\nTo further validate the model's performance, we applied scNTImpute to diverse real scRNA-seq datasets, including temporal datasets involving mouse preimplantation embryonic development. We also included advanced imputation methods like AE-TPGG, scGNN, scISR, and scScope for comparison. The results showed that scNTImpute achieved the highest scores in ARI and NMI metrics, followed by the newly added AE-TPGG imputation method.\n\nMoreover, we conducted transfer learning experiments using human pancreatic islet (HP) and mouse pancreatic islet (MP) datasets. The transfer learning effect was evident, with scNTImpute achieving high ARI scores in both datasets. This demonstrated the model's ability to generalize across different species and datasets.\n\nIn summary, the evaluation of scNTImpute involved rigorous testing using multiple real datasets and evaluation metrics. The results consistently showed that scNTImpute outperformed existing imputation models, highlighting its effectiveness in recovering true biological information from sparse single-cell data.",
  "evaluation/measure": "In the evaluation of our imputation model, we employed a comprehensive set of performance metrics to ensure a thorough assessment. These metrics include Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), Rand Index (RI), and Mutual Information (MI). These metrics are widely used in the literature for evaluating the performance of imputation methods in single-cell RNA sequencing (scRNA-seq) data.\n\nARI measures the similarity between the predicted and true cluster assignments, adjusted for chance. NMI quantifies the amount of information obtained about one clustering given another clustering, normalized to account for the amount of information contained in each clustering. RI assesses the agreement between two data clusterings by considering all pairs of samples and determining whether they are assigned to the same or different clusters in both clusterings. MI measures the amount of information obtained about one clustering given another clustering, without normalization.\n\nAdditionally, we used cosine similarity (CS) and Fowlkes–Mallows score (FMS) for evaluating the imputation of RNA-seq data. CS measures the cosine of the angle between two vectors, providing a measure of their similarity. FMS evaluates the similarity between two clusterings by considering the number of true positive and true negative pairs.\n\nFor clustering evaluation, we also adopted adjusted mutual information (AMI), which is a corrected version of MI that accounts for chance. This metric is commonly used to assess the quality of clustering results.\n\nThe use of these metrics allows for a robust evaluation of our imputation model's performance, ensuring that it can effectively recover true biological information from sparse single-cell data. The metrics provide a comprehensive view of the model's ability to accurately impute missing values and improve the clustering of cell subpopulations.",
  "evaluation/comparison": "In the evaluation of our model, scNTImpute, we conducted extensive comparisons with various publicly available methods on benchmark datasets to assess its performance. We evaluated scNTImpute using multiple real scRNA-seq datasets, including the Chung et al. dataset, a temporal scRNA-seq dataset involving mouse preimplantation embryonic development, and additional datasets such as the human pancreatic islet and Romanov et al. datasets.\n\nFor the Chung et al. dataset, we compared scNTImpute with several existing imputation models using different evaluation metrics, including cosine similarity (CS) and Fowlkes–Mallows score (FMS). The results, visualized in figures and tables, demonstrated that scNTImpute outperformed other methods, particularly in the FMS metric, where it showed a significant advantage.\n\nWe also applied scNTImpute to a temporal scRNA-seq dataset involving mouse preimplantation embryonic development. This dataset includes single cells from various developmental stages. We compared scNTImpute with a new imputation method called AE-TPGG using evaluation metrics such as Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI). The visualization of the imputation results indicated that scNTImpute achieved the highest scores in both metrics, followed by AE-TPGG.\n\nAdditionally, we evaluated scNTImpute on two more real datasets: the human pancreatic islet dataset and the Romanov et al. dataset. We compared scNTImpute with three advanced imputation methods: scScope, scGNN, and scISR. The evaluation, using the ARI metric, showed that scNTImpute achieved ARI scores close to 0.7 on both datasets, outperforming the other imputation methods. This further confirmed scNTImpute's effectiveness in recovering true biological information from sparse single-cell data.\n\nTo validate the robustness and stability of scNTImpute, we applied it to more diverse real scRNA-seq datasets and included several advanced imputation methods for comparison. These methods included AE-TPGG, scGNN, scISR, and scScope. We also introduced variations of the MAGIC method, MAGIC-C and MAGIC-N, to understand how the form of the data affects the imputation.\n\nIn summary, our comparisons with publicly available methods on benchmark datasets demonstrated that scNTImpute consistently outperformed other imputation models across various evaluation metrics and datasets. This comprehensive evaluation highlights the superior performance and robustness of scNTImpute in handling real scRNA-seq data.",
  "evaluation/confidence": "In our evaluation, we utilized several metrics to assess the performance of our imputation method, scNTImpute, including ARI, NMI, RI, and MI. These metrics provide a quantitative measure of how well the imputed data recovers the true biological information from sparse single-cell RNA-seq data.\n\nTo ensure the reliability of our results, we conducted extensive experiments on multiple real datasets, including human brain, human pancreatic islet, and mouse preimplantation embryonic development data. The consistent high performance of scNTImpute across these diverse datasets indicates the robustness and generalizability of our method.\n\nWe also compared scNTImpute with several other state-of-the-art imputation methods, such as scGGAN, DCA, MAGIC, and DeepImpute. The results showed that scNTImpute achieved the highest scores in most of the evaluation metrics, demonstrating its superiority over existing methods.\n\nIn addition to the primary metrics, we also employed other evaluation indexes like cosine similarity (CS) and Fowlkes–Mallows score (FMS) to further validate the performance of scNTImpute. The results showed that our model performed the best in both CS and FMS, especially in FMS, where a significant gap was observed compared to other imputation methods.\n\nTo assess the statistical significance of our results, we calculated P values and fold change (FC) for the enrichment levels of topics in known human genetic pathways. By converting P values to −log10(P value), we found that most of our topics were below the set threshold, indicating significant differences and confirming the biological relevance of the topics identified by scNTImpute.\n\nFurthermore, we conducted transfer learning experiments using cross-species datasets, such as human pancreatic islet (HP) and mouse pancreatic islet (MP) datasets. The results showed that scNTImpute achieved high ARI scores in both direct imputation and transfer learning tasks, further validating the stability and effectiveness of our model.\n\nOverall, the performance metrics used in our evaluation are reliable and statistically significant, providing strong evidence that scNTImpute is a superior method for single-cell RNA-seq data imputation.",
  "evaluation/availability": "The raw evaluation files used in this study are publicly available. All the data utilized for evaluation can be accessed through the Gene Expression Omnibus (GEO) database. Specifically, the datasets include human brain data with the accession number GSE67835, the Chung et al. dataset with the accession code GSE75688, the Hrvatin dataset with the GEO accession code GSE59739, the Romanov et al. dataset with the GEO accession code GSE74672, the Deng et al. dataset with the GEO accession code GSE45719, and mouse pancreatic islet data with the GEO accession code GSE84133. Additionally, human pancreatic islet data are available at GEO or the EMBL-EBI database with accession codes GSE81076, GSE85241, GSE86469, E-MTAB-5061, and GSE84133. All supplementary data are available in the GigaScience repository, GigaDB. The project's source code is available on GitHub under the MIT License, ensuring open access and reproducibility. The project homepage is https://github.com/qiyueyang-7/scNTImpute."
}