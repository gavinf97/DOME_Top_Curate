{
  "publication/title": "CREMA: a meta-learner for plant lncRNA prediction",
  "publication/authors": "Not enough information is available.",
  "publication/journal": "BMC Genomics",
  "publication/year": "2018",
  "publication/doi": "10.1186/s12864-018-4691-5",
  "publication/tags": "- Long non-coding RNA\n- lncRNA prediction\n- Machine learning\n- Gradient boosting\n- Random forest\n- Transcript classification\n- Bioinformatics\n- Plant genomics\n- Empirical validation\n- Non-coding RNA",
  "dataset/provenance": "The dataset used in this study consists of both positive and negative data for training machine learning models to predict long non-coding RNAs (lncRNAs). The positive data remained constant across all training sets and comprised 436 unique, validated lncRNA sequences. These sequences were sourced from two separate lncRNA databases: lncRNAdb v2.0 and lncRNAdisease. The lncRNAdb v2.0 was accessed on November 25, 2016, and lncRNAdisease was accessed on February 15, 2017. These databases include validated lncRNAs, with a majority from animal systems and only a few from plant species.\n\nThe negative data for each training set included sequences from four different species: Homo sapiens, Arabidopsis thaliana, Mus musculus, and Oryza sativa. These species were chosen because they are well-represented in the available validated lncRNA data. The sequences for H. sapiens were downloaded from Ensembl on December 19, 2016. A. thaliana sequences were obtained from Araport v11 on December 16, 2016. M. musculus and O. sativa sequences were both downloaded from Ensembl on March 28, 2017. To ensure that the negative training data did not include lncRNAs, tRNAs, or rRNAs, these types of sequences were removed using data from RNAcentral v6, accessed on March 28, 2017.\n\nEight different training sets were created using various combinations of negative data from the aforementioned species. These sets were denoted as \"A\" and \"B\" and were randomly chosen from the transcript sequences of each species. The training datasets were used in both random forest and gradient boosting methods, resulting in a total of 16 preliminary models. The diversity in training datasets was intended to maximize model diversity, which is crucial for the subsequent ensemble models.",
  "dataset/splits": "In our study, we utilized eight different training sets, each with distinct combinations of negative data from multiple species. These sets were used to construct eight different models. The positive data, consisting of 436 unique, validated lncRNA sequences, remained constant across all training sets. The negative data varied, including sequences from Homo sapiens, A. thaliana, Mus musculus, and Oryza sativa. Sets denoted “A” and “B” were randomly chosen from the transcript sequences of each species and remained constant throughout the training sets. The number of sequences from each species in the negative data varied across the training sets, with some sets including 3000 sequences from H. sapiens and others including different combinations of sequences from the other species. For example, one training set included 3000 sequences from H. sapiens, 1000 from M. musculus, and 3000 from O. sativa. Another set included 3000 sequences from H. sapiens and 3000 from A. thaliana. The distribution of data points in each split was designed to maximize model diversity, which is crucial for the subsequent ensemble models. The specific details of the negative data for each training set are described in Table 1.",
  "dataset/redundancy": "Not enough information is available.",
  "dataset/availability": "The datasets used in this study are not publicly available in a forum. The positive training dataset consisted of 436 unique, validated lncRNA sequences downloaded from two separate lncRNA databases: lncRNAdb v2.0 and lncRNAdisease. These sequences were heavily populated by animal systems and included only six plant lncRNA sequences.\n\nThe negative data for each training set consisted of sequences from four different species: Homo sapiens, A. thaliana, Mus musculus, and Oryza sativa. These sequences were downloaded from Ensembl and Araport. To ensure that lncRNA, tRNAs, and rRNAs were removed from the negative training data, these types of sequences were downloaded from RNAcentral v6 and then removed from the dataset.\n\nThe data splits used for training and testing the models were not released in a public forum. The study used eight different training sets with different combinations of negative data from multiple species to construct eight different models. These training datasets were used in both random forest and gradient boosting methods, for a total of 16 preliminary models. The variety of training datasets was used to maximize model diversity, a requirement for the proceeding ensemble models.\n\nThe study also used Zea mays protein coding sequences as negative data in the construction and/or testing of each ensemble model for consistency through models. These sequences were downloaded from EnsemblPlants.\n\nThe study does not provide information on how the availability of the datasets was enforced. The datasets were used internally for the construction and evaluation of the models, and there is no mention of a public release or a specific license for the datasets.",
  "optimization/algorithm": "The machine-learning algorithm class used is ensemble learning, specifically a stacking generalizer model based on gradient boosting models. This approach combines multiple learners into a single model, which helps to avoid overfitting and encourages generalization of the classifier. The stacking generalizer model was constructed from eight stochastic gradient boosting models, which were chosen for their predictive strengths.\n\nThis machine-learning algorithm is not entirely new, as ensemble methods and gradient boosting are well-established techniques in the field of machine learning. However, the specific application of these methods to the prediction of long non-coding RNAs (lncRNAs) in plants is novel. The algorithm was developed and tested on plant transcript sequences, and it offers several advantages over existing methods.\n\nThe reason this algorithm was not published in a machine-learning journal is that the focus of the work is on its application to biological data, specifically the prediction of lncRNAs in plants. The development and testing of the algorithm were driven by the need to improve the accuracy and efficiency of lncRNA prediction in plants, rather than by a desire to contribute to the field of machine learning per se. The algorithm's performance was evaluated using plant-specific data, and the results were compared to those of other plant lncRNA prediction tools. The algorithm's open-source availability allows for flexible inclusion of validated lncRNAs as knowledge of this class of RNA improves, making it a valuable tool for researchers in the field of plant genomics.",
  "optimization/meta": "The meta-predictor described in this study is a stacking ensemble model, which indeed uses data from other machine-learning algorithms as input. Specifically, it integrates outputs from multiple stochastic gradient boosting models. This approach leverages the strengths of individual models by combining their predictions to improve overall accuracy and robustness.\n\nThe ensemble methods explored include four types: a majority vote model, an arithmetic mean of scores model, a geometric mean of scores model, and the stacking ensemble model. The stacking model, which was found to be the best performing, is constructed from a logistic regression of the outputs of the individual gradient boosting models. This meta-learner is trained on a dataset comprising known protein-coding genes and validated long non-coding RNAs (lncRNAs) from Zea mays, ensuring that the training data is independent and diverse.\n\nThe individual gradient boosting models were trained on different datasets to maintain diversity and avoid overfitting. Each model was calibrated using hyper-parameter tuning through grid search and nested cross-validation, ensuring optimal performance. The final stacking model was evaluated using 10-fold cross-validation, and its performance was compared to other ensemble methods using metrics such as accuracy, sensitivity, specificity, and the area under the curve (AUC).\n\nThe use of a stacking ensemble model allows for a more nuanced and accurate prediction of lncRNAs by combining the strengths of multiple machine-learning approaches. This method provides a numerical score for each prediction, helping researchers prioritize their validation efforts on the most likely candidates. Additionally, this approach avoids the arbitrary definitions of lncRNAs used in some databases, offering a more flexible and comprehensive prediction framework.",
  "optimization/encoding": "For the machine-learning algorithm, the data underwent several preprocessing steps and encoding methods to ensure optimal model performance. Initially, eleven features were extracted from the transcript sequences. These features included mRNA length, ORF length, GC content, Fickett score, hexamer score, alignment identity in the SwissProt database, length of alignment in the SwissProt database, the proportion of alignment length to mRNA length, the proportion of alignment length to ORF length, the presence of transposable elements, and the sequence percent divergence from transposable elements.\n\nTo extract these features, a combination of custom Python scripts and established software tools was employed. For instance, CPAT was used to derive the Fickett and hexamer scores. Diamond, configured in \"more-sensitive\" mode with specific parameters, was utilized to quantify transcript sequence alignments to curated protein sequences in the SwissProt database. RepeatMasker, run with default settings and specified for Eukaryota, was used to extract information on transcription element-related features.\n\nAfter feature extraction, recursive feature elimination was performed to identify and remove uninformative features. Features related to transposable elements were found to decrease prediction accuracy and were thus excluded. This process resulted in the retention of nine key features: mRNA length, ORF length, GC content, Fickett score, hexamer score, alignment identity, length of alignment, the ratio of alignment length to mRNA length, and the ratio of alignment length to ORF length.\n\nThese nine features were then used to construct individual random forest and gradient boosting models. For gradient boosting, hyper-parameter tuning was conducted using grid search and 30 iterations of 4-fold nested cross-validation to optimize learning rate, max depth, subsample, and the number of estimators. Random forest models were constructed with default parameters, except for setting the number of estimators to 5000 and the minimum samples per leaf to 20.\n\nThe preprocessed data, encoded with the selected features, was then used to train and evaluate the models. The performance of these models was assessed using 10-fold cross-validation, measuring accuracy, specificity, sensitivity, and the area under the curve (AUC). This rigorous preprocessing and encoding ensured that the machine-learning algorithms could effectively learn from the data and make accurate predictions.",
  "optimization/parameters": "In our study, we utilized a total of 11 features for model construction. These features were selected based on their relevance to the classification task and were extracted using a combination of custom Python scripts and established software tools.\n\nTo ensure optimal performance, we employed hyper-parameter tuning for our gradient boosting models. This process involved a grid search with 30 iterations of 4-fold nested cross-validation. The hyper-parameters that were tuned included learning rate, maximum depth, subsample, and the number of estimators. The specific values considered for these hyper-parameters were:\n\n* Learning rate: 0.02, 0.04, 0.06, 0.08, 0.1\n* Maximum depth: 4, 6, 8, 10\n* Subsample: 0.2, 0.4, 0.6, 0.8, 1\n* Number of estimators: 100, 500, 1000\n\nFor the random forest models, we kept most hyper-parameters at their default settings, with the exception of `n_estimators`, which was set to 5000, and `min_samples_leaf`, which was set to 20. These adjustments were made to enhance the model's performance and stability.\n\nThe selection of these parameters and their values was driven by the need to maximize model diversity and accuracy. The use of different negative training datasets further contributed to this diversity, ensuring that our models were robust and generalizable.",
  "optimization/features": "In the optimization process, feature selection was indeed performed. Initially, eleven features were considered for model construction. These features included mRNA length, ORF length, GC%, Fickett score, hexamer score, alignment identity in the SwissProt database, length of alignment in the SwissProt database, the proportion of alignment length to mRNA length, the proportion of alignment length to ORF length, the presence of transposable elements, and sequence percent divergence from transposable elements.\n\nHowever, after evaluating the features using recursive feature elimination, it was determined that the features related to transposable elements were uninformative and reduced the accuracy of the models. Therefore, these features were removed, leaving nine features for implementation in the individual random forest and gradient boosting models. These final features were mRNA length, ORF length, GC%, Fickett score, hexamer score, alignment identity, length of alignment, alignment length to mRNA length ratio, and alignment length to ORF length ratio.\n\nThe feature selection process was conducted using the training data only, ensuring that the models were trained and evaluated on independent data. This approach helped to maximize model diversity and improve the overall performance of the ensemble models.",
  "optimization/fitting": "The fitting method employed in this study involved the use of both gradient boosting and random forest algorithms, each with a set of hyper-parameters tuned through grid search and nested cross-validation. This approach ensured that the models were neither overfitting nor underfitting the data.\n\nFor gradient boosting, hyper-parameters such as learning rate, max depth, subsample, and number of estimators were optimized using a grid search with 30 iterations of 4-fold nested cross-validation. This method helps in selecting the best combination of hyper-parameters that generalize well to unseen data, thereby mitigating the risk of overfitting. The use of nested cross-validation ensures that the model's performance is evaluated on data that was not used in the hyper-parameter tuning process, providing a more reliable estimate of the model's generalization capability.\n\nFor random forest models, the hyper-parameters were kept constant across all models, with the exception of the number of estimators and minimum samples per leaf. The number of estimators was set to 5000, and the minimum samples per leaf were set to 20. This configuration helps in building robust models that are less prone to overfitting, as increasing the number of trees generally improves the model's ability to capture the underlying patterns in the data without memorizing the training set.\n\nTo rule out underfitting, the models were evaluated using multiple performance metrics, including accuracy, specificity, sensitivity, and the area under the curve (AUC). All models achieved high accuracy, specificity, and AUC values, indicating that they were able to capture the essential patterns in the data. The sensitivity values, although varying, were within an acceptable range, suggesting that the models were not too simplistic to capture the complexity of the data.\n\nIn summary, the fitting method involved careful tuning of hyper-parameters and the use of cross-validation techniques to ensure that the models were neither overfitting nor underfitting the data. The evaluation metrics further confirmed the models' ability to generalize well to unseen data.",
  "optimization/regularization": "In our study, we employed several techniques to prevent overfitting and ensure the robustness of our models. One of the key methods used was recursive feature elimination. This process helped us identify and remove uninformative features, such as those related to transposable elements, which were found to decrease prediction accuracy. By retaining only the most relevant features, we enhanced the models' ability to generalize to new data.\n\nAdditionally, we utilized nested cross-validation during the hyper-parameter tuning process for our gradient boosting models. This involved performing a grid search with 30 iterations of 4-fold nested cross-validation. This technique helps in selecting the best hyper-parameters while providing an unbiased estimate of model performance, thereby reducing the risk of overfitting.\n\nFor the random forest models, we set specific hyper-parameters, such as `n_estimators = 5000` and `min_samples_leaf = 20`, which helped in building diverse and robust models. The use of a large number of estimators ensures that the model captures a wide range of patterns in the data, while the `min_samples_leaf` parameter prevents the model from becoming too complex and overfitting to the training data.\n\nFurthermore, we evaluated our models using 10-fold cross-validation, which provides a comprehensive assessment of their performance across different subsets of the data. This approach helps in ensuring that the models are not overly tailored to the specific training data and can perform well on unseen data.",
  "optimization/config": "The hyper-parameter configurations and optimization schedules for the models are detailed in the publication. Specifically, for gradient boosting models, the hyper-parameters such as learning rate, max depth, subsample, and number of estimators were tuned using a grid search with 30 iterations of 4-fold nested cross-validation. The chosen hyper-parameters for each model are summarized in a table, providing clear insights into the optimization process.\n\nFor random forest models, the hyper-parameters remained constant across all models, with the exception of n_estimators set to 5000 and min_samples_leaf set to 20. This consistency ensures reproducibility and allows for direct comparison between different models.\n\nThe models were evaluated using metrics such as accuracy, specificity, sensitivity, and area under the curve (AUC) through 10-fold cross-validation. These evaluation metrics are crucial for understanding the performance of the models and are reported in detail.\n\nRegarding the availability of model files and optimization parameters, the publication does not explicitly mention where these files can be accessed or under what license they are provided. However, the detailed reporting of hyper-parameter configurations and evaluation metrics ensures that the optimization process is transparent and reproducible. Researchers interested in replicating or building upon this work can use the provided information to configure their own models and optimization schedules.",
  "model/interpretability": "The models employed in this study, specifically gradient boosting and random forest, are generally considered to be more interpretable compared to many other machine learning algorithms, but they are not entirely transparent. These models can provide insights into feature importance, which helps in understanding the contribution of each feature to the final prediction.\n\nFor instance, in the gradient boosting models, the importance of features such as mRNA length, ORF length, GC%, Fickett score, and hexamer score can be quantified. This allows researchers to see which features are most influential in predicting whether a transcript is a long non-coding RNA (lncRNA) or not. Similarly, random forest models can also rank features by their importance, providing a clear indication of which characteristics of the transcripts are most predictive.\n\nHowever, the exact decision-making process within these models is still somewhat opaque. The models use complex combinations of these features, and the interactions between them are not always straightforward to interpret. This means that while we can understand the relative importance of different features, the precise way in which the model combines these features to make a prediction is not immediately clear.\n\nIn summary, while these models offer valuable insights into feature importance, they are not entirely transparent. The interpretability comes from the ability to see which features are most important, but the exact decision-making process remains a black box.",
  "model/output": "The model is a classification model designed to predict long non-coding RNAs (lncRNAs) in plants. It employs ensemble learning techniques, combining multiple machine learning algorithms to improve predictive performance. Specifically, it uses random forest and gradient boosting models, which are both supervised learning algorithms typically used for classification tasks. The ensemble methods explored include majority vote, arithmetic mean of scores, geometric mean of scores, and a stacking ensemble model constructed from a logistic regression of model outputs. The final model, which was determined to be the best performing, is a stacking ensemble model built from gradient boosting model outputs. This model was evaluated using metrics such as accuracy, specificity, sensitivity, and the Matthews correlation coefficient (MCC), indicating its effectiveness in classifying lncRNAs.",
  "model/duration": "Not enough information is available.",
  "model/availability": "The source code for the models and algorithms described in this publication is not explicitly mentioned as being publicly released. Therefore, it is not available for direct download or use.\n\nNo executable, web server, virtual machine, or container instance is provided for running the algorithm. The implementation details are not publicly accessible, and thus, there is no method to run the algorithm as described in the publication without further information or direct contact with the authors.\n\nThe license under which the software might be released is not specified, as the source code and any related executables or services are not publicly available.",
  "evaluation/method": "The evaluation of the models involved several rigorous steps to ensure their performance and reliability. Initially, individual stochastic gradient boosting and random forest models were evaluated using 10-fold cross-validation. This process assessed the models based on key metrics such as accuracy, specificity, sensitivity, and the area under the curve (AUC). These metrics provided a comprehensive view of the models' performance, with all models achieving accuracy, specificity, and AUC measures of at least 0.94. However, sensitivity values varied more widely, ranging from 0.40 to 0.725.\n\nTo further enhance the evaluation, ensemble approaches were employed. These included arithmetic mean, geometric mean, majority vote, and a logistic regression meta-learner. The ensemble models were constructed by combining the outputs of the individual models, leveraging their diversity to improve overall prediction accuracy. The performance of these ensemble models was also evaluated using 10-fold cross-validation, with the results summarized in a table that included AUC, Matthews correlation coefficient (MCC), accuracy, sensitivity, and specificity.\n\nAdditionally, hyper-parameter tuning was conducted for the gradient boosting models using a grid search and 30 iterations of 4-fold nested cross-validation. This process ensured that the models were optimized for the best possible performance. The random forest models were constructed with default hyper-parameters, except for n_estimators set to 5000 and min_samples_leaf set to 20.\n\nThe evaluation also compared the number of predicted long non-coding RNAs (lncRNAs) using the stacking method against another method, GreeNC. This comparison showed that the stacking method identified fewer lncRNAs in certain species but also identified new putative lncRNAs that had not been predicted by GreeNC.\n\nOverall, the evaluation method was thorough and multifaceted, ensuring that the models were robust and reliable for predicting lncRNAs.",
  "evaluation/measure": "In the evaluation of our models, we reported several key performance metrics to comprehensively assess their effectiveness. These metrics include the Area Under the Curve (AUC), Matthews Correlation Coefficient (MCC), accuracy, sensitivity, and specificity. These metrics were chosen because they provide a well-rounded view of model performance, covering aspects such as the model's ability to distinguish between positive and negative classes, its overall correctness, and its performance in identifying true positives and true negatives.\n\nThe AUC measures the model's ability to discriminate between classes, providing a single scalar value that summarizes the performance across all classification thresholds. MCC is particularly useful for imbalanced datasets as it takes into account true and false positives and negatives, giving a balanced measure of the quality of binary classifications. Accuracy provides an overall measure of the model's correctness, while sensitivity (recall) and specificity focus on the model's performance in identifying positive and negative instances, respectively.\n\nThese metrics are widely used in the literature for evaluating machine learning models, particularly in bioinformatics and genomics. They allow for a direct comparison with other studies and ensure that our evaluation is representative and rigorous. By reporting these metrics, we aim to provide a clear and comprehensive understanding of our models' performance, enabling other researchers to replicate and build upon our work.",
  "evaluation/comparison": "To evaluate the performance of our ensemble models, we compared our predictions to those from an established lncRNA database, GreeNC. This database uses a transcript filtering method rather than a machine learning approach. Transcripts in GreeNC must meet specific criteria to be identified as putative lncRNAs, including being larger than 200 nucleotides, having an open reading frame (ORF) smaller than 120 amino acids, not having a hit in the SwissProt database, and not being classified as another type of functional RNA.\n\nWe downloaded transcript sequences for three species: *Arabidopsis thaliana*, *Oryza sativa*, and *Eutrema salsugineum*. Using our stacking ensemble model, we predicted 1310, 856, and 198 lncRNAs in these species, respectively. Of these, 66.6%, 51.9%, and 82.8% had been previously predicted by GreeNC. Notably, our method identified fewer lncRNAs compared to GreeNC—1700, 4381, and 1471 fewer in *A. thaliana*, *O. sativa*, and *E. salsugineum*, respectively. However, our stacking learner also identified 438, 412, and 34 putative lncRNAs that had not been predicted by GreeNC in *A. thaliana*, *O. sativa*, and *E. salsugineum*, respectively.\n\nThis comparison highlights the strengths and differences between our machine learning approach and the transcript filtering method used by GreeNC. While GreeNC identifies a larger number of lncRNAs, our method provides unique predictions that could be valuable for further study. The evaluation measures, including accuracy, specificity, and AUC, were similar across our ensemble approaches, but the stacking model constructed from gradient boosting outputs performed best in terms of sensitivity and Matthews correlation coefficient (MCC). This model was used for the remainder of the study.",
  "evaluation/confidence": "Evaluation Confidence\n\nThe evaluation of the models involved several statistical measures to ensure the robustness and reliability of the results. For the individual random forest and gradient boosting models, performance metrics such as accuracy, specificity, sensitivity, and AUC were calculated using 10-fold cross-validation. This method helps in assessing the model's performance across different subsets of the data, providing a more reliable estimate of its generalizability.\n\nFor the ensemble methods, including the majority vote, arithmetic mean, geometric mean, and logistic regression stacking models, similar evaluation metrics were used. The arithmetic mean, geometric mean, and majority vote models were evaluated by comparing their outputs to true labels. The logistic regression stacking model was evaluated using 10-fold cross-validation scores. This approach ensures that the model's performance is not overestimated and provides a more accurate assessment of its predictive power.\n\nThe statistical significance of the results was further supported by the use of grid search and nested cross-validation for hyper-parameter tuning. This process helps in selecting the optimal hyper-parameters that maximize the model's performance, reducing the risk of overfitting and ensuring that the results are statistically significant.\n\nAdditionally, the comparison of the stacking generalizer to the GreeNC database involved calculating the overlap of predictions. This comparison provides a real-world validation of the model's performance, showing that it can identify lncRNAs that have not been predicted by other methods. The use of multiple evaluation metrics and statistical techniques ensures that the results are robust and reliable, providing confidence in the superiority of the proposed method.",
  "evaluation/availability": "Not enough information is available."
}