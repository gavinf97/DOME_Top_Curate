{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf6507b",
   "metadata": {},
   "source": [
    "# DOME Registry Pre-Repair Analysis\n",
    "\n",
    "This notebook is designed to analyze the DOME registry data, search for specific papers, and cross-reference user metadata before any DSW (Daily Study Workflow) repair operations are performed. It provides tools to verify the state of annotations and curator mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9302af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dome-Recommendations-Annotated-Articles_20250202.tsv...\n",
      "Enriching data with Europe PMC API...\n",
      "Processing 188/188: PMID 24977146\n",
      "Enrichment complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Informazioni cronologiche</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Journal name</th>\n",
       "      <th>Publication year</th>\n",
       "      <th>DOME version</th>\n",
       "      <th>Provenance</th>\n",
       "      <th>Dataset splits</th>\n",
       "      <th>Redundancy between data splits</th>\n",
       "      <th>Availability of data</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>...</th>\n",
       "      <th>Evaluation method</th>\n",
       "      <th>Performance measures</th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Availability of evaluation</th>\n",
       "      <th>Indirizzo email</th>\n",
       "      <th>EPMC_title</th>\n",
       "      <th>EPMC_authors</th>\n",
       "      <th>EPMC_pub_year</th>\n",
       "      <th>EPMC_doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3/23/2022 16:36:18</td>\n",
       "      <td>33465072</td>\n",
       "      <td>PLoS Comput Biol.</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes, N_pos: 13128  N_neg: 32766</td>\n",
       "      <td>5-fold cross-validation</td>\n",
       "      <td>Correlation Feature Selection is used.</td>\n",
       "      <td>yes, described at S1 table: https://deposition...</td>\n",
       "      <td>Naive Bayes, SVM, Random Forests</td>\n",
       "      <td>...</td>\n",
       "      <td>5-fold cross validation</td>\n",
       "      <td>ROC  curves and AUC values</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>andrewhatos@gmail.com</td>\n",
       "      <td>Genome-wide prediction of topoisomerase IIβ bi...</td>\n",
       "      <td>Martínez-García PM, García-Torres M, Divina F,...</td>\n",
       "      <td>2021</td>\n",
       "      <td>10.1371/journal.pcbi.1007814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/28/2022 0:44:11</td>\n",
       "      <td>33679869</td>\n",
       "      <td>Front Genet.</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>public databases: 901 samples from The Cancer ...</td>\n",
       "      <td>10-fold cross validation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no, they claim: \"Publicly available datasets w...</td>\n",
       "      <td>Spathial, Random forest, LASSO</td>\n",
       "      <td>...</td>\n",
       "      <td>10-fold cross validation</td>\n",
       "      <td>AUCs for 2-, 3-, and 5-year OS were 0.527, 0.5...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>andrewhatos@gmail.com</td>\n",
       "      <td>Construction and Comprehensive Analyses of a M...</td>\n",
       "      <td>Sun S, Fei K, Zhang G, Wang J, Yang Y, Guo W, ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.3389/fgene.2020.617174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/28/2022 16:40:10</td>\n",
       "      <td>34419924</td>\n",
       "      <td>EBioMedicine.</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>samples from Stanford Health Care and Stanford...</td>\n",
       "      <td>The final analysis included for discovery coho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conatact (hoganca@stanford.edu) is provided ri...</td>\n",
       "      <td>gradient boosted decision trees and random for...</td>\n",
       "      <td>...</td>\n",
       "      <td>novel experiments.</td>\n",
       "      <td>AUC, sensitivity, specificity</td>\n",
       "      <td>costeffective comare to PCR tests and could be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes, https://github.com/stanfordmlgroup/influe...</td>\n",
       "      <td>andrewhatos@gmail.com</td>\n",
       "      <td>Nasopharyngeal metabolomics and machine learni...</td>\n",
       "      <td>Hogan CA, Rajpurkar P, Sowrirajan H, Phillips ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>10.1016/j.ebiom.2021.103546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3/29/2022 23:01:05</td>\n",
       "      <td>34112769</td>\n",
       "      <td>Nat Commun  .</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Clinical data collected by the authors N_neg 3...</td>\n",
       "      <td>N_pos: Discovery 227 and Validation 77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All data included in this study is available u...</td>\n",
       "      <td>composite model  with several stepp with diffe...</td>\n",
       "      <td>...</td>\n",
       "      <td>independent dataset</td>\n",
       "      <td>Accuracy (94.81% for the C4 prediction model, ...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>andrewhatos@gmail.com</td>\n",
       "      <td>A new molecular classification to drive precis...</td>\n",
       "      <td>Soret P, Le Dantec C, Desvaux E, Foulquier N, ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>10.1038/s41467-021-23472-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3/28/2022 12:23:43</td>\n",
       "      <td>32915751</td>\n",
       "      <td>IEEE J Biomed Health Inform.</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>two public COVID-19 CT datasets:   -   https:/...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>preprocessed data: https://drive.google.com/fi...</td>\n",
       "      <td>deep learning, novel approach</td>\n",
       "      <td>...</td>\n",
       "      <td>four-fold cross-validation</td>\n",
       "      <td>Accuracy, F1 score, Sensitivity, Precision, AUC</td>\n",
       "      <td>firstly comparision to COVID-Net method https:...</td>\n",
       "      <td>outperforming the original COVID-Net trained o...</td>\n",
       "      <td>no</td>\n",
       "      <td>andrewhatos@gmail.com</td>\n",
       "      <td>Contrastive Cross-Site Learning With Redesigne...</td>\n",
       "      <td>Wang Z, Liu Q, Dou Q.</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.1109/jbhi.2020.3023246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Informazioni cronologiche      PMID                   Journal name  \\\n",
       "0        3/23/2022 16:36:18  33465072              PLoS Comput Biol.   \n",
       "1         3/28/2022 0:44:11  33679869                   Front Genet.   \n",
       "2        3/28/2022 16:40:10  34419924                  EBioMedicine.   \n",
       "3        3/29/2022 23:01:05  34112769                  Nat Commun  .   \n",
       "4        3/28/2022 12:23:43  32915751   IEEE J Biomed Health Inform.   \n",
       "\n",
       "   Publication year  DOME version  \\\n",
       "0              2021           1.0   \n",
       "1              2021           1.0   \n",
       "2              2021           1.0   \n",
       "3              2021           1.0   \n",
       "4              2020           1.0   \n",
       "\n",
       "                                          Provenance  \\\n",
       "0                    yes, N_pos: 13128  N_neg: 32766   \n",
       "1  public databases: 901 samples from The Cancer ...   \n",
       "2  samples from Stanford Health Care and Stanford...   \n",
       "3  Clinical data collected by the authors N_neg 3...   \n",
       "4  two public COVID-19 CT datasets:   -   https:/...   \n",
       "\n",
       "                                      Dataset splits  \\\n",
       "0                            5-fold cross-validation   \n",
       "1                           10-fold cross validation   \n",
       "2  The final analysis included for discovery coho...   \n",
       "3             N_pos: Discovery 227 and Validation 77   \n",
       "4                                                 no   \n",
       "\n",
       "           Redundancy between data splits  \\\n",
       "0  Correlation Feature Selection is used.   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                      no   \n",
       "\n",
       "                                Availability of data  \\\n",
       "0  yes, described at S1 table: https://deposition...   \n",
       "1  no, they claim: \"Publicly available datasets w...   \n",
       "2  Conatact (hoganca@stanford.edu) is provided ri...   \n",
       "3  All data included in this study is available u...   \n",
       "4  preprocessed data: https://drive.google.com/fi...   \n",
       "\n",
       "                                           Algorithm  ...  \\\n",
       "0                   Naive Bayes, SVM, Random Forests  ...   \n",
       "1                     Spathial, Random forest, LASSO  ...   \n",
       "2  gradient boosted decision trees and random for...  ...   \n",
       "3  composite model  with several stepp with diffe...  ...   \n",
       "4                      deep learning, novel approach  ...   \n",
       "\n",
       "            Evaluation method  \\\n",
       "0     5-fold cross validation   \n",
       "1    10-fold cross validation   \n",
       "2          novel experiments.   \n",
       "3        independent dataset    \n",
       "4  four-fold cross-validation   \n",
       "\n",
       "                               Performance measures   \\\n",
       "0                         ROC  curves and AUC values   \n",
       "1  AUCs for 2-, 3-, and 5-year OS were 0.527, 0.5...   \n",
       "2                      AUC, sensitivity, specificity   \n",
       "3  Accuracy (94.81% for the C4 prediction model, ...   \n",
       "4    Accuracy, F1 score, Sensitivity, Precision, AUC   \n",
       "\n",
       "                                          Comparison  \\\n",
       "0                                                NaN   \n",
       "1                                                 no   \n",
       "2  costeffective comare to PCR tests and could be...   \n",
       "3                                                 no   \n",
       "4  firstly comparision to COVID-Net method https:...   \n",
       "\n",
       "                                          Confidence  \\\n",
       "0                                                NaN   \n",
       "1                                                no    \n",
       "2                                                NaN   \n",
       "3                                                 no   \n",
       "4  outperforming the original COVID-Net trained o...   \n",
       "\n",
       "                          Availability of evaluation        Indirizzo email  \\\n",
       "0                                                NaN  andrewhatos@gmail.com   \n",
       "1                                                 no  andrewhatos@gmail.com   \n",
       "2  yes, https://github.com/stanfordmlgroup/influe...  andrewhatos@gmail.com   \n",
       "3                                                 no  andrewhatos@gmail.com   \n",
       "4                                                 no  andrewhatos@gmail.com   \n",
       "\n",
       "                                          EPMC_title  \\\n",
       "0  Genome-wide prediction of topoisomerase IIβ bi...   \n",
       "1  Construction and Comprehensive Analyses of a M...   \n",
       "2  Nasopharyngeal metabolomics and machine learni...   \n",
       "3  A new molecular classification to drive precis...   \n",
       "4  Contrastive Cross-Site Learning With Redesigne...   \n",
       "\n",
       "                                        EPMC_authors EPMC_pub_year  \\\n",
       "0  Martínez-García PM, García-Torres M, Divina F,...          2021   \n",
       "1  Sun S, Fei K, Zhang G, Wang J, Yang Y, Guo W, ...          2020   \n",
       "2  Hogan CA, Rajpurkar P, Sowrirajan H, Phillips ...          2021   \n",
       "3  Soret P, Le Dantec C, Desvaux E, Foulquier N, ...          2021   \n",
       "4                              Wang Z, Liu Q, Dou Q.          2020   \n",
       "\n",
       "                       EPMC_doi  \n",
       "0  10.1371/journal.pcbi.1007814  \n",
       "1     10.3389/fgene.2020.617174  \n",
       "2   10.1016/j.ebiom.2021.103546  \n",
       "3    10.1038/s41467-021-23472-7  \n",
       "4     10.1109/jbhi.2020.3023246  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_epmc_metadata(pmid):\n",
    "    \"\"\"\n",
    "    Fetches Title, Authors, PubYear, and DOI from Europe PMC API for a given PMID.\n",
    "    \"\"\"\n",
    "    if not pmid or str(pmid) == 'nan':\n",
    "        return None, None, None, None\n",
    "        \n",
    "    url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "    params = {\n",
    "        'query': f'EXT_ID:{pmid} SRC:MED',\n",
    "        'format': 'json',\n",
    "        'resultType': 'core'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        result_list = data.get('resultList', {}).get('result', [])\n",
    "        \n",
    "        if result_list:\n",
    "            top_result = result_list[0]\n",
    "            title = top_result.get('title', '')\n",
    "            authors = top_result.get('authorString', '')\n",
    "            pub_year = top_result.get('pubYear', '')\n",
    "            doi = top_result.get('doi', '')\n",
    "            return title, authors, pub_year, doi\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching PMID {pmid}: {e}\")\n",
    "        \n",
    "    return None, None, None, None\n",
    "\n",
    "# Load the TSV\n",
    "tsv_file = \"Dome-Recommendations-Annotated-Articles_20250202.tsv\"\n",
    "print(f\"Loading {tsv_file}...\")\n",
    "df_recs = pd.read_csv(tsv_file, sep='\\t')\n",
    "\n",
    "# Ensure columns exist\n",
    "new_cols = ['EPMC_title', 'EPMC_authors', 'EPMC_pub_year', 'EPMC_doi']\n",
    "for col in new_cols:\n",
    "    if col not in df_recs.columns:\n",
    "        df_recs[col] = None\n",
    "\n",
    "# Iterate and Enrich\n",
    "print(\"Enriching data with Europe PMC API...\")\n",
    "total = len(df_recs)\n",
    "\n",
    "for index, row in df_recs.iterrows():\n",
    "    pmid = row.get('PMID')\n",
    "    \n",
    "    # Skip if already filled (optional, but good for retries)\n",
    "    # or if PMID is missing\n",
    "    if pd.isna(pmid):\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing {index + 1}/{total}: PMID {pmid}\", end='\\r')\n",
    "    \n",
    "    title, authors, year, doi = fetch_epmc_metadata(pmid)\n",
    "    \n",
    "    df_recs.at[index, 'EPMC_title'] = title\n",
    "    df_recs.at[index, 'EPMC_authors'] = authors\n",
    "    df_recs.at[index, 'EPMC_pub_year'] = year\n",
    "    df_recs.at[index, 'EPMC_doi'] = doi\n",
    "    \n",
    "    # Be nice to the API\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"\\nEnrichment complete.\")\n",
    "df_recs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f7e097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading users from dome_users_20260130.json...\n",
      "User OIDs mapped successfully.\n",
      "Saved enriched data with User OIDs to Dome-Recommendations-Annotated-Articles_20250202_Enriched.tsv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1. Integrate User OIDs\n",
    "users_file = \"dome_users_20260130.json\"\n",
    "try:\n",
    "    print(f\"Loading users from {users_file}...\")\n",
    "    with open(users_file, 'r', encoding='utf-8') as f:\n",
    "        users_data = json.load(f)\n",
    "        \n",
    "    # Create Email -> OID mapping\n",
    "    email_to_oid = {}\n",
    "    for u in users_data:\n",
    "        email = u.get('email')\n",
    "        oid = u.get('_id', {}).get('$oid')\n",
    "        if email and oid:\n",
    "            email_to_oid[email.strip()] = oid\n",
    "            \n",
    "    # Apply to DataFrame\n",
    "    # Target column is 'Indirizzo email' based on file inspection\n",
    "    if 'Indirizzo email' in df_recs.columns:\n",
    "        df_recs['User_OID'] = df_recs['Indirizzo email'].apply(lambda x: email_to_oid.get(str(x).strip(), 'Unknown'))\n",
    "        print(\"User OIDs mapped successfully.\")\n",
    "    else:\n",
    "        print(\"Warning: 'Indirizzo email' column not found in TSV.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error mapping users: {e}\")\n",
    "\n",
    "# 2. Reorder columns (EPMC info after PMID)\n",
    "cols = list(df_recs.columns)\n",
    "if 'PMID' in cols:\n",
    "    pmid_idx = cols.index('PMID')\n",
    "    \n",
    "    # Remove EPMC cols if they are in the list currently\n",
    "    active_new_cols = [c for c in new_cols if c in cols]\n",
    "    for col in active_new_cols:\n",
    "        cols.remove(col)\n",
    "        \n",
    "    # Insert them back after PMID\n",
    "    for i, col in enumerate(active_new_cols):\n",
    "        cols.insert(pmid_idx + 1 + i, col)\n",
    "        \n",
    "    df_recs = df_recs[cols]\n",
    "\n",
    "# 3. Save (Overwriting/Creating the single enriched file)\n",
    "output_file = \"Dome-Recommendations-Annotated-Articles_20250202_Enriched.tsv\"\n",
    "df_recs.to_csv(output_file, sep='\\t', index=False)\n",
    "print(f\"Saved enriched data with User OIDs to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ee62d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming TSV columns to match DOME JSON schema...\n",
      "Detected 44 standard schema keys from JSON.\n",
      "Dropping 11 unwanted columns: ['optimization/done', 'publication/skip', 'publication/done', 'evaluation/done', 'dataset/done', 'model/done', 'optimization/skip', 'evaluation/skip', 'dataset/skip', 'model/skip', 'DOME version']\n",
      "Saved schema-aligned file to v1_Dome-Recommendations-Schema_Aligned.tsv\n",
      "Columns: ['timestamp', 'PMID', 'EPMC_title', 'EPMC_authors', 'EPMC_pub_year', 'EPMC_doi', 'publication/journal', 'publication/year', 'dataset/provenance', 'dataset/splits', 'dataset/redundancy', 'dataset/availability', 'optimization/algorithm', 'optimization/meta', 'optimization/encoding', 'optimization/parameters', 'optimization/features', 'optimization/fitting', 'optimization/regularization', 'Availability of  configuration', 'model/interpretability', 'model/output', 'Execution time ', 'model/availability', 'evaluation/method', 'Performance measures ', 'evaluation/comparison', 'evaluation/confidence', 'evaluation/availability', 'user_email', 'User_OID', 'publication/doi', 'reviewState', 'model/duration', 'optimization/config', 'publication/authors', 'update', 'publication/pmid', 'uuid', 'publication/title', 'publication/updated', 'evaluation/measure', 'publication/tags', 'public', 'shortid']\n"
     ]
    }
   ],
   "source": [
    "# 4. Standardize Column Names and Sync with JSON Schema\n",
    "\n",
    "raw_reviews_file = \"dome_review_raw_human_20260128.json\"\n",
    "\n",
    "# Known TSV -> DOME JSON Key Mapping\n",
    "# Based on the TSV headers provided and DOME schema conventions\n",
    "column_map = {\n",
    "    'Journal name': 'publication/journal',\n",
    "    'Publication year': 'publication/year',\n",
    "    \n",
    "    'Provenance': 'dataset/provenance',\n",
    "    'Dataset splits': 'dataset/splits',\n",
    "    'Redundancy between data splits': 'dataset/redundancy',\n",
    "    'Availability of data': 'dataset/availability',\n",
    "    \n",
    "    'Algorithm': 'optimization/algorithm',\n",
    "    'Meta-predictions': 'optimization/meta',\n",
    "    'Data encoding': 'optimization/encoding',\n",
    "    'Parameters': 'optimization/parameters',\n",
    "    'Features': 'optimization/features',\n",
    "    'Fitting': 'optimization/fitting',\n",
    "    'Regularization': 'optimization/regularization',\n",
    "    'Availability of configuration': 'optimization/config',\n",
    "    \n",
    "    'Interpretability': 'model/interpretability',\n",
    "    'Output': 'model/output',\n",
    "    'Execution time': 'model/duration',\n",
    "    'Availability of software': 'model/availability',\n",
    "    \n",
    "    'Evaluation method': 'evaluation/method',\n",
    "    'Performance measures': 'evaluation/measure',\n",
    "    'Comparison': 'evaluation/comparison',\n",
    "    'Confidence': 'evaluation/confidence',\n",
    "    'Availability of evaluation': 'evaluation/availability',\n",
    "    \n",
    "    # Metadata/Extra\n",
    "    'Informazioni cronologiche': 'timestamp',\n",
    "    'Indirizzo email': 'user_email'\n",
    "}\n",
    "\n",
    "# 1. Rename Columns\n",
    "print(\"Renaming TSV columns to match DOME JSON schema...\")\n",
    "df_recs.rename(columns=column_map, inplace=True)\n",
    "\n",
    "# 2. Extract Full Schema from JSON\n",
    "# We flatten the keys from the first few records to get a list of all possible \"section/field\" keys\n",
    "all_json_keys = set()\n",
    "try:\n",
    "    with open(raw_reviews_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    for entry in data[:10]: # Check first 10 to cover bases\n",
    "        for section, content in entry.items():\n",
    "            if isinstance(content, dict):\n",
    "                for field in content.keys():\n",
    "                    if not field.startswith('$'): # Skip mongo internal keys\n",
    "                        all_json_keys.add(f\"{section}/{field}\")\n",
    "            else:\n",
    "                if not section.startswith('_'):\n",
    "                    all_json_keys.add(section)\n",
    "                    \n",
    "    print(f\"Detected {len(all_json_keys)} standard schema keys from JSON.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON schema: {e}\")\n",
    "    # Fallback default keys if file read fails\n",
    "    all_json_keys = {\n",
    "        'publication/title', 'publication/authors', 'publication/doi', 'publication/year', 'publication/journal',\n",
    "        'publication/tags',\n",
    "        'dataset/provenance', 'dataset/splits', 'dataset/redundancy', 'dataset/availability',\n",
    "        'optimization/algorithm', 'optimization/meta', 'optimization/encoding', 'optimization/parameters', \n",
    "        'optimization/features', 'optimization/fitting', 'optimization/regularization', 'optimization/config',\n",
    "        'model/interpretability', 'model/output', 'model/duration', 'model/availability',\n",
    "        'evaluation/method', 'evaluation/measure', 'evaluation/comparison', 'evaluation/confidence', 'evaluation/availability'\n",
    "    }\n",
    "\n",
    "# 3. Add Missing Schema Columns\n",
    "for key in all_json_keys:\n",
    "    if key not in df_recs.columns:\n",
    "        df_recs[key] = None # Add as empty\n",
    "\n",
    "# 4. Remove unwanted columns\n",
    "# Filter for exact columns ending in '/done' or '/skip'\n",
    "cols_to_drop = [c for c in df_recs.columns if c.endswith('/skip') or c.endswith('/done')]\n",
    "\n",
    "if 'DOME version' in df_recs.columns:\n",
    "    cols_to_drop.append('DOME version')\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping {len(cols_to_drop)} unwanted columns: {cols_to_drop}\")\n",
    "    df_recs.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Re-Save with Version prefix\n",
    "output_file_schema = \"v1_Dome-Recommendations-Schema_Aligned.tsv\"\n",
    "df_recs.to_csv(output_file_schema, sep='\\t', index=False)\n",
    "print(f\"Saved schema-aligned file to {output_file_schema}\")\n",
    "print(\"Columns:\", list(df_recs.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40d65f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging EPMC data into main schema columns...\n",
      "Merged EPMC_title -> publication/title\n",
      "Merged EPMC_authors -> publication/authors\n",
      "Merged EPMC_doi -> publication/doi\n",
      "Dropped source columns: ['EPMC_title', 'EPMC_authors', 'EPMC_doi']\n",
      "Saved final merged file to v4_Dome-Recommendations-Final_Merged.tsv\n",
      "Final Column Order: ['user_email', 'User_OID', 'timestamp', 'PMID', 'publication/title', 'publication/authors', 'publication/journal', 'publication/year', 'EPMC_pub_year', 'publication/doi', 'publication/tags', 'dataset/provenance', 'dataset/splits', 'dataset/redundancy', 'dataset/availability', 'optimization/algorithm', 'optimization/meta', 'optimization/encoding', 'optimization/parameters', 'optimization/features', 'optimization/fitting', 'optimization/regularization', 'optimization/config', 'model/interpretability', 'model/output', 'model/duration', 'model/availability', 'evaluation/method', 'evaluation/measure', 'evaluation/comparison', 'evaluation/confidence', 'evaluation/availability', 'Availability of  configuration', 'Execution time ', 'Performance measures ', 'reviewState', 'update', 'publication/pmid', 'uuid', 'publication/updated', 'public', 'shortid']\n"
     ]
    }
   ],
   "source": [
    "# 5. Merge EPMC Data and Finalize Order\n",
    "\n",
    "print(\"Merging EPMC data into main schema columns...\")\n",
    "\n",
    "# List of merge pairs: (Source, Target)\n",
    "merge_pairs = [\n",
    "    ('EPMC_title', 'publication/title'),\n",
    "    ('EPMC_authors', 'publication/authors'),\n",
    "    ('EPMC_doi', 'publication/doi')\n",
    "]\n",
    "\n",
    "for src, tgt in merge_pairs:\n",
    "    if src in df_recs.columns and tgt in df_recs.columns:\n",
    "        # Use EPMC data to fill/overwrite\n",
    "        # If you only want to fill missing values, use .fillna() instead\n",
    "        # Here we overwrite as per instructions which implies using the fetched data\n",
    "        df_recs[tgt] = df_recs[src]\n",
    "        print(f\"Merged {src} -> {tgt}\")\n",
    "    else:\n",
    "        print(f\"Skipping merge {src} -> {tgt} (Column missing)\")\n",
    "\n",
    "# Cleanup: Drop the temporary EPMC columns that were merged\n",
    "cols_to_drop = [src for src, tgt in merge_pairs]\n",
    "df_recs.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "print(f\"Dropped source columns: {cols_to_drop}\")\n",
    "\n",
    "\n",
    "# Define the canonical field order based on DOME JSON structure\n",
    "field_order = [\n",
    "    # Metadata (Note: EPMC columns removed/moved)\n",
    "    'user_email', 'User_OID', 'timestamp', 'PMID',\n",
    "    \n",
    "    # Publication\n",
    "    'publication/title', \n",
    "    'publication/authors', \n",
    "    'publication/journal', \n",
    "    'publication/year', \n",
    "    'EPMC_pub_year', # Moved here for ordering\n",
    "    'publication/doi',\n",
    "    'publication/tags',\n",
    "    \n",
    "    # Dataset\n",
    "    'dataset/provenance', \n",
    "    'dataset/splits', \n",
    "    'dataset/redundancy', \n",
    "    'dataset/availability',\n",
    "    \n",
    "    # Optimization\n",
    "    'optimization/algorithm', \n",
    "    'optimization/meta', \n",
    "    'optimization/encoding', \n",
    "    'optimization/parameters', \n",
    "    'optimization/features', \n",
    "    'optimization/fitting', \n",
    "    'optimization/regularization', \n",
    "    'optimization/config',\n",
    "    \n",
    "    # Model\n",
    "    'model/interpretability', \n",
    "    'model/output', \n",
    "    'model/duration', \n",
    "    'model/availability',\n",
    "    \n",
    "    # Evaluation\n",
    "    'evaluation/method', \n",
    "    'evaluation/measure', \n",
    "    'evaluation/comparison', \n",
    "    'evaluation/confidence', \n",
    "    'evaluation/availability'\n",
    "]\n",
    "\n",
    "# Append any remaining columns that weren't explicitly ordered (just in case)\n",
    "for col in df_recs.columns:\n",
    "    if col not in field_order:\n",
    "        field_order.append(col)\n",
    "\n",
    "# Reindex the DataFrame\n",
    "final_cols = [c for c in field_order if c in df_recs.columns]\n",
    "df_recs = df_recs[final_cols]\n",
    "\n",
    "# Final Save\n",
    "output_file_final = \"v4_Dome-Recommendations-Final_Merged.tsv\"\n",
    "df_recs.to_csv(output_file_final, sep='\\t', index=False)\n",
    "print(f\"Saved final merged file to {output_file_final}\")\n",
    "print(\"Final Column Order:\", list(df_recs.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c938ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing ID and Timestamp columns...\n",
      "Dropping placeholder 'publication/pmid' column...\n",
      "Renaming 'PMID' -> 'publication/pmid'...\n",
      "Dropping placeholder 'update' column...\n",
      "Renaming 'timestamp' -> 'update'...\n",
      "Saved standardized data to v5_Dome-Recommendations-Standardized_Columns.tsv\n",
      "Current Columns: ['user_email', 'User_OID', 'update', 'publication/pmid', 'publication/title', 'publication/authors', 'publication/journal', 'publication/year', 'EPMC_pub_year', 'publication/doi', 'publication/tags', 'dataset/provenance', 'dataset/splits', 'dataset/redundancy', 'dataset/availability', 'optimization/algorithm', 'optimization/meta', 'optimization/encoding', 'optimization/parameters', 'optimization/features', 'optimization/fitting', 'optimization/regularization', 'optimization/config', 'model/interpretability', 'model/output', 'model/duration', 'model/availability', 'evaluation/method', 'evaluation/measure', 'evaluation/comparison', 'evaluation/confidence', 'evaluation/availability', 'Availability of  configuration', 'Execution time ', 'Performance measures ', 'reviewState', 'uuid', 'publication/updated', 'public', 'shortid']\n"
     ]
    }
   ],
   "source": [
    "# 6. Standardization: Map Legacy IDs and Timestamps\n",
    "\n",
    "print(\"Standardizing ID and Timestamp columns...\")\n",
    "\n",
    "# Check current columns to avoid errors\n",
    "cols = df_recs.columns\n",
    "\n",
    "# 1. Handle PMID -> publication/pmid\n",
    "# Drop the empty placeholder column if it exists\n",
    "if 'publication/pmid' in cols:\n",
    "    print(\"Dropping placeholder 'publication/pmid' column...\")\n",
    "    df_recs.drop(columns=['publication/pmid'], inplace=True)\n",
    "\n",
    "# Rename the actual data column\n",
    "if 'PMID' in cols:\n",
    "    print(\"Renaming 'PMID' -> 'publication/pmid'...\")\n",
    "    df_recs.rename(columns={'PMID': 'publication/pmid'}, inplace=True)\n",
    "else:\n",
    "    print(\"Warning: 'PMID' column not found.\")\n",
    "\n",
    "# 2. Handle timestamp -> update\n",
    "# Drop the empty placeholder column if it exists\n",
    "if 'update' in cols:\n",
    "    print(\"Dropping placeholder 'update' column...\")\n",
    "    df_recs.drop(columns=['update'], inplace=True)\n",
    "\n",
    "# Rename the actual data column\n",
    "if 'timestamp' in cols:\n",
    "    print(\"Renaming 'timestamp' -> 'update'...\")\n",
    "    df_recs.rename(columns={'timestamp': 'update'}, inplace=True)\n",
    "else:\n",
    "    print(\"Warning: 'timestamp' column not found.\")\n",
    "\n",
    "# Save Version 5 (stacking on previous v4)\n",
    "output_file_v5 = \"v5_Dome-Recommendations-Standardized_Columns.tsv\"\n",
    "df_recs.to_csv(output_file_v5, sep='\\t', index=False)\n",
    "print(f\"Saved standardized data to {output_file_v5}\")\n",
    "print(\"Current Columns:\", list(df_recs.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating legacy data to schema columns...\n",
      "Legacy column 'Availability of configuration' not found in dataframe.\n",
      "Legacy column 'Execution time' not found in dataframe.\n",
      "Legacy column 'Performance measure' not found in dataframe.\n",
      "Saved migrated data to v6_Dome-Recommendations-Migrated_Legacy_Data.tsv\n",
      "Current Columns: ['user_email', 'User_OID', 'update', 'publication/pmid', 'publication/title', 'publication/authors', 'publication/journal', 'publication/year', 'EPMC_pub_year', 'publication/doi', 'publication/tags', 'dataset/provenance', 'dataset/splits', 'dataset/redundancy', 'dataset/availability', 'optimization/algorithm', 'optimization/meta', 'optimization/encoding', 'optimization/parameters', 'optimization/features', 'optimization/fitting', 'optimization/regularization', 'optimization/config', 'model/interpretability', 'model/output', 'model/duration', 'model/availability', 'evaluation/method', 'evaluation/measure', 'evaluation/comparison', 'evaluation/confidence', 'evaluation/availability', 'Availability of  configuration', 'Execution time ', 'Performance measures ', 'reviewState', 'uuid', 'publication/updated', 'public', 'shortid']\n"
     ]
    }
   ],
   "source": [
    "# 7. Standardization: Data Migration for Config, Duration, and Measure\n",
    "\n",
    "print(\"Migrating legacy data to schema columns...\")\n",
    "\n",
    "# List of migration mappings: (Legacy Source Column, Target Schema Column)\n",
    "# precise names taken from dataframe columns\n",
    "migration_map = [\n",
    "    ('Availability of  configuration', 'optimization/config'),\n",
    "    ('Execution time ', 'model/duration'),\n",
    "    ('Performance measures ', 'evaluation/measure')\n",
    "]\n",
    "\n",
    "for legacy_col, target_col in migration_map:\n",
    "    # Ensure source exists\n",
    "    if legacy_col in df_recs.columns:\n",
    "        # We want to fill the target with legacy data. \n",
    "        # If target exists, we overwrite. If not, we rename (less likely if schema enforced, but safe).\n",
    "        if target_col in df_recs.columns:\n",
    "            print(f\"Migrating '{legacy_col}' -> '{target_col}'...\")\n",
    "            df_recs[target_col] = df_recs[legacy_col]\n",
    "            \n",
    "            # Drop the legacy column\n",
    "            df_recs.drop(columns=[legacy_col], inplace=True)\n",
    "            print(f\"Dropped legacy column '{legacy_col}'\")\n",
    "        else:\n",
    "             print(f\"Target column '{target_col}' missing. Renaming '{legacy_col}' to '{target_col}'.\")\n",
    "             df_recs.rename(columns={legacy_col: target_col}, inplace=True)\n",
    "    else:\n",
    "        print(f\"Legacy column '{legacy_col}' not found in dataframe.\")\n",
    "\n",
    "# Save Version 6\n",
    "output_file_v6 = \"v6_Dome-Recommendations-Migrated_Legacy_Data.tsv\"\n",
    "df_recs.to_csv(output_file_v6, sep='\\t', index=False)\n",
    "print(f\"Saved migrated data to {output_file_v6}\")\n",
    "print(\"Current Columns:\", list(df_recs.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
