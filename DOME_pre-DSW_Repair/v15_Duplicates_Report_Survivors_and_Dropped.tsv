Review_Reason	bs_completeness	update	publication/title	publication/doi	_id/$oid	dataset/availability	dataset/provenance	dataset/redundancy	dataset/splits	dataset/done	dataset/skip	evaluation/availability	evaluation/comparison	evaluation/confidence	evaluation/measure	evaluation/method	evaluation/done	evaluation/skip	model/availability	model/duration	model/interpretability	model/output	model/done	model/skip	optimization/algorithm	optimization/config	optimization/encoding	optimization/features	optimization/fitting	optimization/meta	optimization/parameters	optimization/regularization	optimization/done	optimization/skip	user/$oid	publication/pmid	publication/pmcid	publication/updated	publication/authors	publication/journal	publication/year	publication/done	publication/skip	publication/tags	public	created/$date	updated/$date	uuid	reviewState	shortid
DOI_MATCH 	45		A Novel Dataset for Nuclei and Tissue Segmentation in Melanoma with baseline nuclei segmentation and tissue segmentation benchmarks	000	678fa69f5a9877fd97a7d5dc	The training dataset is available on zenodo (https://zenodo.org/records/10940194) with a CC-BY 4.0 licence. 	"The dataset consists of 155 primary and 155 metastatic melanoma manually selected ROIs, scanned at 40Ã— magnification (0.22 Âµm/px) with a size of 1024 Ã— 1024 pixels. For these ROIs, annotations of both tissue and nuclei are supplied, as well as a context ROI of 5120 Ã— 5120 pixels centered around the ROI. Annotations were created by a medical expert and checked and corrected by a dermatopathologist.  All cases were digitized in a large melanoma referral center, however 76 cases are revisions or consultations originating from other treatment hospitals. Annotations are in the .GeoJSON format, making annotations easily visualizable with the opensource pathology image viewer Qupath. 

The following nuclei categories were used:  tumor, stroma, vascular endothelium, histiocyte, melanophage, lymphocyte, plasma cell, neutrophil, apoptotic and epithelium and the following tissue categories were used: tumor, stroma, epidermis, necrosis, blood vessel, and background. A total of 147 739 nuclei are manually annotated. "	The sets were split randomly. As each sample is a histological specimen from a different patient the training and test dataset are independent. 	"The PUMA dataset contains 103 primary melanoma ROIs and 103 metastatic melanoma ROIs in the training set, with a total of 97,429 nuclei. 
The test set includes 52 primary melanoma ROIs and 52 metastatic melanoma ROIs, comprising 50,490 nuclei. 

In the manuscript the distribution of metastatic sites, nuclei and tissue annotations is plotted in the data description paragraph. "	4.0	0.0	https://github.com/tueimage/PUMA-challenge-eval-track1, https://github.com/tueimage/PUMA-challenge-eval-track2	"Yes, a comparison with publicly available methods has been performed (NN192, Hover-Net and Hover-NeXt trained on the PanNuke dataset). 
No publicly available method exists able at segmenting skin tissue on H&E stained histopathology images. "	The results are bootstrapped for generation of confidence intervals. This is not performed in the final public evaluation code for the PUMA challenge due to reducing computational burden. The difference in performance between the used models is statistically significant. 	"For the calculation of the F1 score, the center distance between the predicted nuclei and the ground truth nuclei was used. For each ground truth nucleus, predictions within a 15 pixel (3.3 Î¼m) were identified. This radius is smaller than the average size of lymphocytes, which form the smallest nuclei in the dataset. Matching was performed based on the highest predictive score (if available) or the shortest distance. After matching, the ground truth was censored until all ground truth nuclei were either matched or classified as a false negative. Using the identified true positives, false positives, and false negatives, precision (all correct predictions divided by all predictions) and recall (all correct predictions divided by all ground truth nuclei) were calculated. The class F1 score was computed as the harmonic mean of precision and recall, ranging from 0 to 1. Finally to compare models, micro F1  (aggregation of TP, FP, and FN over all classes, followed by F1 score calculation) and Average F1  (the average of class F1 scores) were calculated [40]. Results are shown with a 95% confidence interval which is calculated through bootstrapping the samples. 

Both models and intra- and interobserver agreement were evaluated using the DICE score. The DICE score is a harmonic mean between 0 and 1, in which 1 is a perfect segmentation prediction, and 0 is no correct prediction. This can result in inflated high average DICE scores if a tissue class is only present in a few samples, as the DICE score is 1 in the case of a correct absent prediction. To accommodate this, we calculated not only the average DICE score over all samples but also the micro average DICE. This is the DICE score for all predictions concatenated along one axis, resulting in a prediction mask of 1024 Ã— 96.256 pixels. Results are shown with a 95% confidence interval which is generated through bootstrapping the sample results. 

Metrics are in line with literature (Maier-Hein, L., Reinke, A., Godau, P. et al. Metrics reloaded: recommendations for image analysis validation. Nat Methods 21, 195â€“212 (2024). https://doi.org/10.1038/s41592-023-02151-z) "	Independent dataset	5.0	0.0	https://github.com/tueimage/PUMA-challenge-baseline-track1, https://github.com/tueimage/PUMA-challenge-baseline-track2	Less than 30 seconds	The model outputs segmentation annotations, which are fully interpretable by pathologists. 	Segmentation	4.0	0.0	"In this study, two algorithm classes are used. The first class consists of models for nuclei instance segmentation, while the second class includes semantic segmentation models for medical image analysis. The nuclei instance segmentation algorithms offer advantages over classic models due to post-processing techniques like watershed segmentation, which help separate densely packed nuclei. Additionally, these algorithms can be applied to whole slide images instead of tiles. 

For tissue annotation semantic segmentation, two models were used. The first is nnU-Net, which is designed for supervised medical image segmentation. The second is Mask2Former2, a transformer segmentation algorithm which can be used for supervised semantic segmentation. For this experiment the backbone was replaced by a foundation model that is better able at feature extraction from histopathology images.


"	The original training code of the models used was not adjusted. Weights for the baseline solution of the PUMA challenge are supplied on Zenodo (https://zenodo.org/records/13881999). 	Annotations are supplied in the .geoJSON format. For training of Hover-Net and Hover-Next the .geoJSON and .TIFF images were recoded to numpy array masks. For tissue segmentation the .geoJSON were encoded to .PNG masks. 	No explicit feature selection was performed. The models used the raw input from the images without reducing or selecting specific features. Therefore, the entire dataset, including both tissue and nuclei information, was used for training without separate feature selection steps.	In this study, due to the use of different models, itâ€™s hard to directly compare p (parameters) and f (input features) with the number of training points. The models were used to establish a baseline without detailed tuning. Overfitting was countered by 5-fold cross-validation and the use of an independent test set, though specific model-related techniques were not addressed.	No	The models in this research used default configurations for the number of parameters. The goal was to establish a baseline for the dataset, providing a reference point for the PUMA challenge. The community is then invited to improve upon these baseline models in future work. No manual tuning or selection of the number of parameters (p) was performed.	To decrease the risk of overfitting we used data-augmentation and 5-fold cross validation to define a training and validation set in the original training set. We took the mean of these models on the independent test set. 	7.0	1.0	678fa631edf7b65039c6cf91	-			Mark Schuiveling, Hong Liu, Daniel Eek,  Gerben E. Breimer,  Karijn P.M. Suijkerbuijk, Willeke A.M. Blokx,  Mitko Veta	GigaScience	2024	0.0	6.0		1.0	2025-01-21T13:52:31.149Z	2025-01-31T11:18:02.800Z	59a6767b-171a-443e-933d-91ad24c967f7	undenfined	45xadyd7wx
TITLE_MATCH DOI_MATCH 	44		Assessing infant risk of cerebral palsy with video-based motion tracking	https://doi.org/10.1101/2024.11.06.24316844	6786955b69b4c4f2642362e8	Pre-processed data has been made available on the OSF pre-registration site: https://osf.io/gztmd/	"Data were collected as part of standard clinical care by team members of the CHOP site of the US CP Early Detection and Intervention Network in a REDcap database between May 2019 and December 2023. This included the secure uploading of iPad- or iPhone-recorded videos, General Movement Assessments (GMA) scores and demographic information.  Exclusions were applied to intubated patients, those under the influence of sedation medications, within a week post-operative, on ECMO support, or diagnosed with myelomeningocele. For all infants who were between 10-20 weeks post-term age (corrected for preterm birth, if applicable) at the time of a clinic visit, and whose parents or legal guardians agreed to video recording for clinical care, clinicians captured a 1-2 minute video of the infant lying in supine.  

Data classes were defined according to the GMA scores: 
1 - Fidgety (Nneg = 820) 
2 - Absent Fidgety (Npos = 108) 
3 - Atypical Fidgety (excluded = 3) 

The full dataset comprised 1060 infants. 129 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (â€œatypical fidgetyâ€) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of â€œabsent fidgetyâ€ infants. 

"	"The true holdout set (n = 187) was randomly sampled from the dataset, stratified by GMA score, prior to any algorithm development, and pre-registered on OSF. 

"	The full dataset comprised 1060 infants. 129 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (â€œatypical fidgetyâ€) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of â€œabsent fidgetyâ€ infants. The remaining 931 videos were split into an analysis set (744) and a lock box holdout set (187) (Figure 1.). The analysis set was further split into train/val/test sets (558, 93, 93), each of which had a 12% representation of the â€œabsent fidgetyâ€ movement type. The splits were stratified to preserve the ratios of boy/girl infants, as well as age, and race/ethnicity. There was a total recording duration of 60-120 s per infant.	4.0	0.0	All model outputs and CV scores are available. 	No other publicly available methods or datasets are available. In searching the model space, AutoML already tests simple baselines to ensure they do not out-perform the selected model. 	Performance metrics are reported with associated cross-validation scores. The goal of the paper was not the claim that the result is superior to that of others, but rather to show that the result persists even when applying strict rigorous ML approaches. 	Two standard measures are report: AUC-ROC which is a standard measure used in the machine learning community, and Preicision-Recall (PR)-AUC, which is more interpretable within the medical literature as it relates to the sensitivity/specificity measures that are widely used. 	Two evaluation strategies were used. First, 10 times 5-folds cross validation were performed on the train/test data to ensure that model performance was not reliant on any one specific data split. Finally, the model was evaluated on a lock-box, pre-registered held out test set of participant data to show generalizability. 	5.0	0.0	Yes, the source code is available on GitHub (https://github.com/quietscientist/gma_score_prediction_from_video.git) under CC BY-NC 4.0. A container has not yet been released. 	prediction requires approximately 30 seconds on a standard machine. 	The model is interpretable as it is possible to inspect the weights assigned to each of the 38 movement features included in the model. 	The model is a binary classifier. 	4.0	0.0	"
AutoSklearn-2.0 employs a meta-learning approach to automate the selection and tuning of well-established machine learning algorithms like decision trees, random forests, and support vector machines. The final model selected was Stochastic Gradient Descent. "	All hyper parameters are available in the stored model details, which are available on the pre-registration site: 	All data used for training was numerical, and pre-processed to remove missing values. All other parameters of the data pre-processing and encoding were determined by AutoSklearn 	38 features were used as input. No feature selection was performed - features were determined by medical specialists and pre-registered in advance of any data collection or analysis. 	The number of training datapoints (> 600) is much larger than the number of features (38). Additionally, AutoML mitigates the risk of overfitting by abstracting away any feature and model parameter selection. Moreover, the trained model was evaluated on a lock-box holdout set of data after pre-registration to determine generalizability. 	The model does not use data from other ML algorithms as input. The inputs to the classifier are the features derived from the pose-estimate time-series data. 	p was selected by the AutoML optimizer based on the classification algorithm selected. The search space was restricted to choosing the best model as opposed to an ensemble, in this case SGD, so p corresponds to the weights given to each of the 38 features. 	AutoML was the primary approach used to prevent the risk of overfitting. 	8.0	0.0	6786955bdd85bd59b3220ba6				Melanie Segado, Laura Prosser, Andrea F. Duncan, Michelle J. Johnson, Konrad P. Kording	medRxiv	2024	0.0	6.0		0.0	2025-01-14T16:48:27.379Z	2025-01-14T16:48:27.379Z	c6de596a-e3ca-43e5-bb12-30bed5c8f63a	undenfined	d0zszar7uh
TITLE_MATCH DOI_MATCH 	44		Assessing infant risk of cerebral palsy with video-based motion tracking	https://doi.org/10.1101/2024.11.06.24316844	67892c3e69b4c4f26423638e	Pre-processed data has been made available on the OSF pre-registration site: https://osf.io/gztmd/	"Data were collected as part of standard clinical care by team members of the CHOP site of the US CP Early Detection and Intervention Network in a REDcap database between May 2019 and December 2023. This included the secure uploading of iPad- or iPhone-recorded videos, General Movement Assessments (GMA) scores and demographic information.  Exclusions were applied to intubated patients, those under the influence of sedation medications, within a week post-operative, on ECMO support, or diagnosed with myelomeningocele. For all infants who were between 10-20 weeks post-term age (corrected for preterm birth, if applicable) at the time of a clinic visit, and whose parents or legal guardians agreed to video recording for clinical care, clinicians captured a 1-2 minute video of the infant lying in supine.  

Data classes were defined according to the GMA scores: 
1 - Fidgety (Nneg = 820) 
2 - Absent Fidgety (Npos = 108) 
3 - Atypical Fidgety (excluded = 3) 

The full dataset comprised 1060 infants. 129 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (â€œatypical fidgetyâ€) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of â€œabsent fidgetyâ€ infants. 

"	"The true holdout set (n = 187) was randomly sampled from the dataset, stratified by GMA score, prior to any algorithm development, and pre-registered on OSF. 

"	The full dataset comprised 1060 infants. 129 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (â€œatypical fidgetyâ€) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of â€œabsent fidgetyâ€ infants. The remaining 931 videos were split into an analysis set (744) and a lock box holdout set (187) (Figure 1.). The analysis set was further split into train/val/test sets (558, 93, 93), each of which had a 12% representation of the â€œabsent fidgetyâ€ movement type. The splits were stratified to preserve the ratios of boy/girl infants, as well as age, and race/ethnicity. There was a total recording duration of 60-120 s per infant.	4.0	0.0	All model outputs and CV scores are available. 	No other publicly available methods or datasets are available. In searching the model space, AutoML already tests simple baselines to ensure they do not out-perform the selected model. 	Performance metrics are reported with associated cross-validation scores. The goal of the paper was not the claim that the result is superior to that of others, but rather to show that the result persists even when applying strict rigorous ML approaches. 	Two standard measures are report: AUC-ROC which is a standard measure used in the machine learning community, and Preicision-Recall (PR)-AUC, which is more interpretable within the medical literature as it relates to the sensitivity/specificity measures that are widely used. 	Two evaluation strategies were used. First, 10 times 5-folds cross validation were performed on the train/test data to ensure that model performance was not reliant on any one specific data split. Finally, the model was evaluated on a lock-box, pre-registered held out test set of participant data to show generalizability. 	5.0	0.0	Yes, the source code is available on GitHub (https://github.com/quietscientist/gma_score_prediction_from_video.git) under CC BY-NC 4.0. A container has not yet been released. 	prediction requires approximately 30 seconds on a standard machine. 	The model is interpretable as it is possible to inspect the weights assigned to each of the 38 movement features included in the model. 	The model is a binary classifier. 	4.0	0.0	"
AutoSklearn-2.0 employs a meta-learning approach to automate the selection and tuning of well-established machine learning algorithms like decision trees, random forests, and support vector machines. The final model selected was Stochastic Gradient Descent. "	All hyper parameters are available in the stored model details, which are available on the pre-registration site: 	All data used for training was numerical, and pre-processed to remove missing values. All other parameters of the data pre-processing and encoding were determined by AutoSklearn 	38 features were used as input. No feature selection was performed - features were determined by medical specialists and pre-registered in advance of any data collection or analysis. 	The number of training datapoints (> 600) is much larger than the number of features (38). Additionally, AutoML mitigates the risk of overfitting by abstracting away any feature and model parameter selection. Moreover, the trained model was evaluated on a lock-box holdout set of data after pre-registration to determine generalizability. 	The model does not use data from other ML algorithms as input. The inputs to the classifier are the features derived from the pose-estimate time-series data. 	p was selected by the AutoML optimizer based on the classification algorithm selected. The search space was restricted to choosing the best model as opposed to an ensemble, in this case SGD, so p corresponds to the weights given to each of the 38 features. 	AutoML was the primary approach used to prevent the risk of overfitting. 	8.0	0.0	6786955bdd85bd59b3220ba6				Melanie Segado, Laura Prosser, Andrea F. Duncan, Michelle J. Johnson, Konrad P. Kording	GigaScience 	2025	0.0	6.0		0.0	2025-01-16T15:56:46.734Z	2025-01-16T15:56:46.734Z	94004a0d-5be8-4c6d-894d-fbe9884187b9	undenfined	b9zlfytluw
TITLE_MATCH DOI_MATCH 	44		Assessing infant risk of cerebral palsy with video-based motion tracking	https://doi.org/10.1101/2024.11.06.24316844	67892cf369b4c4f264236392	Pre-processed data has been made available on the OSF pre-registration site: https://osf.io/gztmd/	"Data were collected as part of standard clinical care by team members of the CHOP site of the US CP Early Detection and Intervention Network in a REDcap database between May 2019 and December 2023. This included the secure uploading of iPad- or iPhone-recorded videos, General Movement Assessments (GMA) scores and demographic information.  Exclusions were applied to intubated patients, those under the influence of sedation medications, within a week post-operative, on ECMO support, or diagnosed with myelomeningocele. For all infants who were between 10-20 weeks post-term age (corrected for preterm birth, if applicable) at the time of a clinic visit, and whose parents or legal guardians agreed to video recording for clinical care, clinicians captured a 1-2 minute video of the infant lying in supine.  

Data classes were defined according to the GMA scores: 
1 - Fidgety (Nneg = 820) 
2 - Absent Fidgety (Npos = 108) 
3 - Atypical Fidgety (excluded = 3) 

The full dataset comprised 1060 infants. 129 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (â€œatypical fidgetyâ€) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of â€œabsent fidgetyâ€ infants. 

"	"The true holdout set (n = 187) was randomly sampled from the dataset, stratified by GMA score, prior to any algorithm development, and pre-registered on OSF. 

"	The full dataset comprised 1060 infants. 129 were then excluded for meeting one or more exclusion criteria listed above. Six infants with a GMA score of 3 (â€œatypical fidgetyâ€) were also excluded, since there were not enough infants in this group for model training/testing, and movement patterns differ from those of â€œabsent fidgetyâ€ infants. The remaining 931 videos were split into an analysis set (744) and a lock box holdout set (187) (Figure 1.). The analysis set was further split into train/val/test sets (558, 93, 93), each of which had a 12% representation of the â€œabsent fidgetyâ€ movement type. The splits were stratified to preserve the ratios of boy/girl infants, as well as age, and race/ethnicity. There was a total recording duration of 60-120 s per infant.	4.0	0.0	All model outputs and CV scores are available. 	No other publicly available methods or datasets are available. In searching the model space, AutoML already tests simple baselines to ensure they do not out-perform the selected model. 	Performance metrics are reported with associated cross-validation scores. The goal of the paper was not the claim that the result is superior to that of others, but rather to show that the result persists even when applying strict rigorous ML approaches. 	Two standard measures are report: AUC-ROC which is a standard measure used in the machine learning community, and Preicision-Recall (PR)-AUC, which is more interpretable within the medical literature as it relates to the sensitivity/specificity measures that are widely used. 	Two evaluation strategies were used. First, 10 times 5-folds cross validation were performed on the train/test data to ensure that model performance was not reliant on any one specific data split. Finally, the model was evaluated on a lock-box, pre-registered held out test set of participant data to show generalizability. 	5.0	0.0	Yes, the source code is available on GitHub (https://github.com/quietscientist/gma_score_prediction_from_video.git) under an MIT License. A container has not yet been released. 	prediction requires approximately 30 seconds on a standard machine. 	The model is interpretable as it is possible to inspect the weights assigned to each of the 38 movement features included in the model. 	The model is a binary classifier. 	4.0	0.0	"
AutoSklearn-2.0 employs a meta-learning approach to automate the selection and tuning of well-established machine learning algorithms like decision trees, random forests, and support vector machines. The final model selected was Stochastic Gradient Descent. "	All hyper parameters are available in the stored model details, which are available on the pre-registration site: 	All data used for training was numerical, and pre-processed to remove missing values. All other parameters of the data pre-processing and encoding were determined by AutoSklearn 	38 features were used as input. No feature selection was performed - features were determined by medical specialists and pre-registered in advance of any data collection or analysis. 	The number of training datapoints (> 600) is much larger than the number of features (38). Additionally, AutoML mitigates the risk of overfitting by abstracting away any feature and model parameter selection. Moreover, the trained model was evaluated on a lock-box holdout set of data after pre-registration to determine generalizability. 	The model does not use data from other ML algorithms as input. The inputs to the classifier are the features derived from the pose-estimate time-series data. 	p was selected by the AutoML optimizer based on the classification algorithm selected. The search space was restricted to choosing the best model as opposed to an ensemble, in this case SGD, so p corresponds to the weights given to each of the 38 features. 	AutoML was the primary approach used to prevent the risk of overfitting. 	8.0	0.0	6786955bdd85bd59b3220ba6				Melanie Segado, Laura Prosser, Andrea F. Duncan, Michelle J. Johnson, Konrad P. Kording	GigaScience 	2025	0.0	6.0		0.0	2025-01-16T15:59:47.832Z	2025-01-16T15:59:47.832Z	b4c5b4b5-99b6-4d85-a7af-c64b035a9afe	undenfined	k6ngu5cy7a
TITLE_MATCH DOI_MATCH 	24	3/8/2022 15:42:05	BioConceptVec: Creating and evaluating literature-based biomedical concept embeddings on a large scale.	10.1371/journal.pcbi.1007617		Yes. Dataset available at URL: https://github.com/ncbi/BioConceptVec	Only one dataset employed, created using all PubMed abastracts 					No	"Compared with other methods (BioAgvWord and ""Yu et al."")"	No confidence interval	Precision, Recall, F1-score, Area Under Curve (AUC)	Intrinsic and Extrinsic evaluation from 9 independent datasets			Yes, available at URL: https://github.com/ncbi/BioConceptVec	Not stated	Black box	Embeddings			Novel approach called BioConceptVec. The paper explains why they used a novel approach	Yes, available at URL: https://github.com/ncbi/BioConceptVec				No.	Not stated				6312169df3794236aa987a01	32324731	PMC7237030		Chen Q, Lee K, Yan S, Kim S, Wei CH, Lu Z.	PLoS computational biology	2020									
TITLE_MATCH DOI_MATCH 	9	3/8/2022 15:46:06	BioConceptVec: Creating and evaluating literature-based biomedical concept embeddings on a large scale.	10.1371/journal.pcbi.1007617																															6312169df3794236aa987a01	32324731	PMC7237030		Chen Q, Lee K, Yan S, Kim S, Wei CH, Lu Z.	PLoS computational biology	2020									
DOI_MATCH 	44		CellBinDB: A Large-Scale Multimodal Annotated Dataset for Cell Segmentation with Benchmarking of Universal Models	not yet published	676a178069b4c4f264235f77	CellBinDB has been uploaded to Zenodo: https://zenodo.org/records/14312044. 	The data comes from the dataset CellBinDB published in this article. The images in CellBinDB come from two sources: 844 mouse images come from internal experiments based on Stereo-seq technology, and 200 human images come from the open access platform 10x Genomics. The dataset contains four staining types: DAPI, ssDNA, H&E, and mIF, covering more than 30 histologically different tissue types, including disease-related tissues. And some internal experimental data that will not be made public for the time being are used.	Independent	we manually annotated a small initial dataset, which was then used to train the model. We did not use a separate test set; instead, we employed mini-batch cross-validation to evaluate the model's performance. As part of the training process, we iteratively added newly generated annotations back into the training set, allowing for continuous training and improvement of the model.	4.0	0.0	No	Because our annotation process consists of manual modification steps, we do not need to use the model with the best segmentation performance, as long as the segmentation results are generally satisfactory. Therefore, we did not compare with other models.	No	F1 score on the test set.	independent dataset	3.0	2.0	This model is from a published work: https://github.com/STOmics/CellBin. MIT license.	Each image takes a few seconds on a consumer laptop.	The model is interpretable, but interpretation goes beyond the scope of this work.	classification	4.0	0.0	The algorithm is not new and is an open source model from published work. This model was chosen because it is suitable for the characteristics of our data.	https://github.com/STOmics/CellBin.  MIT license	The input data consists of microscope images.	We use microscope images as input. Instead of using feature selection, we use a deep learning framework for abstract feature extraction.	By comparing train and test accuracies.	No, the model does not use data from other machine learning algorithms as input.	Default hyperparameters from the scikit-learn implementations, as in the original study.	No.	7.0	1.0	676a1780dd85bd59b3ab6ad7				Can Shi, Jinghong Fan, Zhonghan Deng, Huanlin Liu, Qiang Kang, Yumei Li, Jing Guo, Jingwen Wang, Jinjiang Gong, Sha Liao, Ao Chen, Ying Zhang, Mei Li1	GigaScience	not yet published	0.0	6.0		0.0	2024-12-24T02:08:00.672Z	2024-12-24T02:08:00.672Z	5c1093bd-bbfc-4ac4-a8a1-b0d7f1d8095e	undenfined	c0idbhv7ex
TITLE_MATCH DOI_MATCH 	30	1/31/2022 8:45:48	Classification and analysis of a large collection of in vivo bioassay descriptions.	10.1371/journal.pcbi.1005641		Yes	Yes, CheMBL	Yes	Yes			Yes	No	No	precision, recall, F1-score	Cross-validation			Soft available separately.	No	Black box	Multiclass Multilabel Classification			Word2Vec + Random Forest	No	Doc2Vec	Word2Vec	No	No	No	No			6312169df3794236aa987a36	28678787	PMC5517062		Zwierzyna M, Overington JP.	PLoS computational biology	2017									
TITLE_MATCH DOI_MATCH 	10	5/20/2022 18:00:21	Classification and analysis of a large collection of in vivo bioassay descriptions.	10.1371/journal.pcbi.1005641			COMMENT  --  Not Applicable (ML in text-mining).																												65faa4f792c76639b82bab29	28678787	PMC5517062		Zwierzyna M, Overington JP.	PLoS computational biology	2017									
TITLE_MATCH DOI_MATCH 	30	1/24/2022 17:28:42	Classification of pallidal oscillations with increasing parkinsonian severity.	10.1152/jn.00840.2014		no.	Data collection was conducted through samples aggregated from mango plants. Data is composed of images.  They perform 3 experimental scenarios: Dataset size (510, 46.500, 62.047).	Not stated.	The overall original dataset has been divided into three subsets namely (i) training; (ii) validation and (iii) testing with 60%, 20% and 20% respectively.			no.	Not stated.	Not stated.	Accuracy.	Not stated.			no.	The cloud-based deployment of the CPU-only Tensorflow with Keras library has resulted in the computational time between 2 s to 2.99 s for the classification of the input image and provide a response to the mobile application.	Black box.	Classification.			The VGG-16 network architecture (Simonyan & Zisserman, 2014) has been updated to replace the last block containing the softmax classification with a fully-connected layer (FCL).	no.	Data augmentation: blur (+ affine transform), contrast (+ affine transform), noise (+ affine transform). In total, for every image in the original dataset, additional 150 images have been generated through the presented augmentation process.	The extracted features are flattened: input feature vector [7*7*512].	They report overfitting and state that they will evaluate the performance of the network training against the overfitting in future research activity.	no.	The weights of the VGG-16 network have been preserved from the pre-trained model. FCL layer network which consists of 2-layers. The first layer is activated by the ReLU function and consists of 256 nodes, followed by the second layer consisting of 16 nodes activated by softmax.	no.			6312169df3794236aa987a03	25878156	PMC4507953		Connolly AT, Jensen AL, Baker KB, Vitek JL, Johnson MD.	Journal of neurophysiology	2015									
TITLE_MATCH DOI_MATCH 	30	1/21/2022 17:00:14	Classification of pallidal oscillations with increasing parkinsonian severity.	10.1152/jn.00840.2014		no.	669 paired recordings were made across 3 subjects. Data are divided in 4 classes of (276, 149, 157, 87 points each). Data source: experiment.	Not stated.	90% of the data was used for training and the remaining 10% was used for testing.			no.	no.	Not stated.	Sensitivity/specificity measures for each subject/class.	10-fold cross-validation			no.	Not stated.	black box.	Classification.			support vector machine (SVM)	no.	Each 30- to 120-s paired recording was converted into a single bipolar LFP. The multitaper method was used to calculate a spectrogram of the LFP. using a 2-s window with 10% window steps, resulting in a 1-Hz frequency resolution. Time windows containing movement artifacts were visually identified and removed. The spectrogram was averaged over the remaining windows to produce a single power spectral density (PSD) for each paired recording. An additional feature set was created with phase-amplitude coupling (PAC).	The entire feature set including both spectral and PAC components contained 206 features.  Feature selection was performed. The lasso regularization technique with 10-fold cross-vali- dation was used to perform feature selection through least-squares regression with a penalty on the size of the estimated coefficients. This procedure finds the combination of features that produces the minimum mean-square error (MSE). Lasso regularization was performed separately for the spectral and PAC feature sets.	Large number of features. Feature selection was performed to reduce the number of features used for the classificationto avoid overfitting.	no.	Not stated.	yes: The lasso regularization technique with 10-fold cross-validation was used to perform feature selection through least-squares regression with a penalty on the size of the estimated coefficients			6312169df3794236aa987a03	25878156	PMC4507953		Connolly AT, Jensen AL, Baker KB, Vitek JL, Johnson MD.	Journal of neurophysiology	2015									
DOI_MATCH 	45		Deep neural networks with knockoff features identify nonlinear causal relations and estimate effect sizes in complex biological systems	https://doi.org/10.1101/2021.07.17.452800	64541a9dbcc9ba89a8f2ca22	described above	We used a simulation data set, two public data, and one access-controlled data. Our simulation data are downloadable from our project website (https://github.com/ZhenjiangFan/DAG-deepVASE). TCGA breast invasive carcinoma (BRCA) data were downloaded from https://tcga.xenahubs.net, available under BRCA cohort, under gene expression RNAseq section, on IlluminaHiSeq (n=1,218) TCGA Hub. It consists of the gene expression RNAseq dataset (dataset ID: TCGA.BRCA.sampleMap/HiSeqV2) and the clinical phenotype dataset (dataset ID: TCGA.BRCA.sampleMap/BRCA_clinicalMatrix). To investigate the dietary effect of the human gut microbiome, we downloaded a cross-sectional data of 98 healthy volunteers from https://noble.gs.washington.edu/proj/DeepPINK/. 	The datasets were split by 10 fold cross-validation	10 fold cross-validation	4.0	0.0	https://github.com/ZhenjiangFan/DAG-deepVASE	We compared our method to threeexisting methods, causalMGM, DAG-GNN, and NOTEAR.	The performance metrics have confidence intervals.	Sensitivity, specificity, and AUC	cross-validation and novel experiments	5.0	0.0	"Project name: DAG-deepVASE
Project home page: https://github.com/ZhenjiangFan/DAG-deepVASE
Operating system(s): Platform independent
Programming language: Python, Java, C, and R
Other requirements: e.g., Java 1.3.1 or higher, Tomcat 4.0 or higher
License: MIT license
"	Within 6 hours for mid-sized samples.	we set out to estimate the effect size on the individual variable relationships in our DNN model. Although estimating the effect size is important to design further clinical trials and/or experimental validations with strong drivers, it is not straightforward to summarize the edge weights across multiple layers for effect size estimation in a regular DNN approach . DAG-deepVASE successfully estimates the effect size of the nonlinear associations by embedding the knockoff variables in the DNN model.	Classification.	4.0	0.0	To train this model, mean of squares of errors (MSE) is used to calculate the loss in comparison with the response on the output layer. To train the modelâ€™s parameters with respect to the loss function, we used a stochastic gradient descent method called â€œAdam optimizationâ€. 	"	Parameters	Value
DNN	Activation function	Rectified linear unit (ReLU)
	Initial weight values	Glorot normal intializer
	Regularization	ğ¿1âˆ’ğ‘Ÿğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
	Optimization	Adam optimization
	Loss function	Mean of squares of errors (MSE)
FDR	FDR control rate	0.05
"	Not encoded.	For the case of p input variables, each hidden layer has p neurons, further transformed using the rectified linear unit activation (ReLU) function[103]. The initial weights for the hidden layer are generated using the Glorot normal initializer	p is not much larger than N. 	No. 	Users can set different number of input parameters.	"DAG-deepVASE generates model-X knockoff[106]. For input variables x_i and x_j, the exchangeability property ensures that (x_i,x_j,(x_j ) Ìƒ ) =^d (x_i,(x_j ) Ìƒ,x_j ), where ""=^d "" denotes equality in distribution. This exchangeability properties help prioritize causal relations with x_i over simple correlations. For example, suppose (x_i,x_k ) is a correlation without causal relation. Then, the feature exchangeability (x_i,x_k,(x_k ) Ìƒ ) =^d (x_i,(x_k ) Ìƒ,x_k ) will hold and make their relationship measure |ã€–RIã€—_ik | and |(ã€–RIã€—_ik ) Ìƒ | exchangeable, which will make S_ik= |ã€–RIã€—_ik |-|(ã€–RIã€—_ik ) Ìƒ | to follow a distribution symmetric around 0. "	8.0	0.0	6454174d92c76639b843f17b	Not yet assigned			Zhenjiang Fan [0000-0002-5889-5340]1, Kate F. Kernan [0000-0002-6337-841X]2, Aditya Sriram [0000-0001-7303-9529]3, Panayiotis V. Benos [0000-0003-3172-3132]4, Scott W. Canna [0000-0003-3837-5337]5, Joseph A. Carcillo2, Soyeon Kim [0000-0003-1573-2733]6,7,*, and Hyun Jung Park [0000-0002-8324-2624]3*	GigaScience	2023	0.0	0.0		0.0	2023-05-04T20:50:37.524Z	2023-05-04T20:53:13.741Z	db461736-5566-46a2-82d1-7170b13bb55f	undenfined	g6bge3l9vz
DOI_MATCH 	45		Deep neural networks with knockoff features identify nonlinear causal relations and estimate effect sizes in complex biological systems.	https://doi.org/10.1101/2021.07.17.452800	645aa34160bf612a3caabc1b	https://github.com/ZhenjiangFan/DAG-deepVASE	We used a simulation data set, two public data, and one access-controlled data. Our simulation data are downloadable from our project website (https://github.com/ZhenjiangFan/DAG-deepVASE). TCGA breast invasive carcinoma (BRCA) data were downloaded from https://tcga.xenahubs.net, available under BRCA cohort, under gene expression RNAseq section, on IlluminaHiSeq (n=1,218) TCGA Hub. It consists of the gene expression RNAseq dataset (dataset ID: TCGA.BRCA.sampleMap/HiSeqV2) and the clinical phenotype dataset (dataset ID: TCGA.BRCA.sampleMap/BRCA_clinicalMatrix). To investigate the dietary effect of the human gut microbiome, we downloaded a cross-sectional data of 98 healthy volunteers from https://noble.gs.washington.edu/proj/DeepPINK/ that preprocessed the data set.	all datasets are without overlap, kept coincident for each trial of the algorithms.	We used 10 fold cross validation in all the data sets. 	4.0	0.0	No	We compared our method to threeexisting methods, causalMGM, DAG-GNN, and NOTEAR.	better timing and accuracy	Sensivitity and specificity	10-fold cross validation	4.0	1.0	https://github.com/ZhenjiangFan/DAG-deepVASE	Within 6 hours for mid-sized samples.	we developed the first computational method that explicitly learns nonlinear causal relations and estimates the effect size using a deep-neural network approach coupled with the knockoff framework.	Model classification.	4.0	0.0	The initial weights for the hidden layer are generated using the Glorot normal initializer, which uses ğ¿1âˆ’ğ‘Ÿğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘› with the regularization parameter set to O(âˆš(2logp/n)). To train this model, mean of squares of errors (MSE) is used to calculate the loss in comparison with the response on the output layer. To train the modelâ€™s parameters with respect to the loss function, we used a stochastic gradient descent method called â€œAdam optimizationâ€. 	No	All the inputs have been normalized into the range [-1, 1] for fairness.	We used a simulation data set, two public data, and one access-controlled data. Our simulation data are downloadable from our project website (https://github.com/ZhenjiangFan/DAG-deepVASE). TCGA breast invasive carcinoma (BRCA) data were downloaded from https://tcga.xenahubs.net, available under BRCA cohort, under gene expression RNAseq section, on IlluminaHiSeq (n=1,218) TCGA Hub. It consists of the gene expression RNAseq dataset (dataset ID: TCGA.BRCA.sampleMap/HiSeqV2) and the clinical phenotype dataset (dataset ID: TCGA.BRCA.sampleMap/BRCA_clinicalMatrix). To investigate the dietary effect of the human gut microbiome, we downloaded a cross-sectional data of 98 healthy volunteers from https://noble.gs.washington.edu/proj/DeepPINK/ that preprocessed the data set.	We used linear fit.	No	"Parameters	Value
Activation function	Rectified linear unit (ReLU)
Initial weight values	Glorot normal intializer
Regularization	ğ¿1âˆ’ğ‘Ÿğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
Optimization	Adam optimization
Loss function	Mean of squares of errors (MSE)
FDR control rate	0.05
"	We used ğ¿1âˆ’ğ‘Ÿğ‘’ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘› with the regularization parameter set to O(âˆš(2logp/n)).	6.0	2.0	6454174d92c76639b843f17b	Not yet assigned			Zhenjiang Fan, Kate F. Kernan, Aditya Sriram, Panayiotis V. Benos, Scott W. Canna, Joseph A. Carcillo, Soyeon Kim, and Hyun Jung Park	GigaScience	2023	0.0	0.0		1.0	2023-05-09T19:47:13.177Z	2023-05-09T19:48:29.691Z	9b42e80b-19bd-49d8-b06e-3b8302ceb152	undenfined	ngjci1cyqx
DOI_MATCH 	44		DeepAnnotation: A novel interpretable deep learning-based genomic selection model that integrates comprehensive functional annotations	xxx	67a74b8075e2c69534578f4a	The datasets we used in our study are all public avaliable. All the relevant details can be found in the manuscript.	We include genome, epigenome, and gene function data in our study. All datasets are public avaliable. Totally, i) the genotype data containing 11,633,164 SNPs from 1,940 samples; ii) the phenotype data containing three pork production traits from 1940 samples; iii) The processed ATAC-seq driven includes five pig tissues (muscle, liver, fat, spleen, and heart) from four breeds including Duroc, Enshi Black (ES), Large White (LW), and Meishan (MS), as well as eight Yorkshire tissues (adipose, cerebellum, cortex, hypothalamus, liver, lung, muscle, spleen); iv) 31,908 genes from 177 samples with high-quality normalized transcripts per million expression levels were prepared; v) pig reference genome Sscrofa11.1 was download from NCBI; vi) 186,152 records together covering 20,037 genes and 15,017 terms from GO and KEGG databases; vii) the functionally conserved annotation (including gene body, intergenic, and promoter coordinates) of pig was downloaded from Kernâ€™s study (https://www.nature.com/articles/s41467-021-22100-8).	"We split total 1900 samples into two parts, including 1700 samples for training the model and 240 independent test samples for robustness evaluation. 5-fold cross validation experiment is used to training the model. The independent test set never appeared in the training process.
"	1700 samples were used for 5-fold cross validation experiment. In each fold, 80% samples are used for training the model and the rest 20% samples are used for testing the model performance. We also used additional 240 independent test set to evaluate the prediction performance, which never appeared during the training process.	4.0	0.0	Yes, we provide all the raw data to reproduce the results reported in manuscript at Zenodo (https://doi.org/10.5281/zenodo.14586911).	Yes, we compared with seven classical GS models, including rrBLUP, LightGBM, KAML, BLUP, BayesR, MBLUP, and BayesRC.	Yes, we report the significant level as P-values calculated through Student test.	Yes, we report the prediction performance of our model compared with seven well established models in our manuscript.	We use 5-fold cross validation to evaluate the prediction performance, and utilize an independent data set to evaluate the robustness of our method.	5.0	0.0	We release the source code and example data at GitHub (https://github.com/mawenlong2016/DeepAnnotation), and all data used for reproducing our results at Zenodo (https://doi.org/10.5281/zenodo.14586911). Moreover, we provide a docker image with configured environment at DockerHub (https://hub.docker.com/r/wenlong2023/deepannotation).	The training elasped time usually takes serval hours for one 5-fold cross validation experiment. We report the running time in our manuscript.	DeepAnnotation is interpretable. We clearly describe the interpretability in our manuscript.	DeepAnnotation is a regression model.	4.0	0.0	DeepAnnotation is a novel genomic selection approach which consists of 7 layers, each dedicated to processing distinct types of omics data. These layers encompass genotype data, epigenomic features, RNA secondary structure data, transcriptomic profiles, gene function annotations, regulatory module metaterms, and additional high-order features, represented by two hidden layers.	Yes, we report the results of the hyperparameter optimization in the main manuscript and could be found in our supplementary data. 	The genotype and SNP functional impacts are saved in three dimensional matrix with 'h5' format, the gene annotations and metaterm regulatory modules are saved in two dimensional matrix with 'h5' format, the phenotypes are saved in 'txt' format.	The input features including: i) 656048 genotype matrix; ii) 590342 non-coding SNPs with predicted DeepSEA scores and 65706 coding SNPs with MFE scores; iii) 4111 genes' predicted scores in 227 funtional terms; iv) 14996 functional terms' scores in 31 metaterms. These features driven from four distinct components including genotype data, SNP functional impacts, gene annotations and metaterm regulatory modules. 	The number of parameters is much larger than the number of features. We reduce the overfitting by employing ReLU activate function and early stop the training process based on the performance on the validation set.	Specifically, the input data for training the DeepAnnotation model was organized into four distinct components: 1) Genotype data: 590,342 potential non-coding cis-regulatory SNPs within open chromatin regions and 65,706 coding SNPs. 2) SNP functional impacts: Predicted by DeepSEA for non-coding SNPs and RNAfold for coding SNPs to assess their potential biological effects. 3) Gene annotations: Derived from 4,111 annotated genes located in conserved regions, encompassing 227 GO and KEGG terms, all with AUC value exceeding 0.9, as determined by easyMF. 4) Metaterm regulatory modules: 31 metaterms extracted from a pool of 27,797 genes, excluding the 4,111 genes from (3), spread across 14,996 terms using easyMF.	Totally, 1667805154 parameters are estimated in the DeepAnnotation model when utilizing all functional annotations. These parameters are corresponding to the weights and biases in each layer.	Yes, we employ the ReLU activation function and early stop the training process based on the performance on the validation set. The validation set is splited as the 20% training set in each round of 5-fold cross validation.	8.0	0.0	67a74b80edf7b650392eec2d				Wenlong Maâ€ , Weigang Zhengâ€ , Shenghua Qin, Chao Wang, Bowen Lei, and Yuwen Liu*	GigaScience	xxx	0.0	6.0		0.0	2025-02-08T12:18:08.756Z	2025-02-08T12:18:08.756Z	0935a381-a6f4-4ff1-93db-9e41262b2223	undenfined	2hfmqmrwm0
TITLE_MATCH DOI_MATCH 	43		External validation of machine learning models - registered models and adaptive sample splitting	https://doi.org/10.1101/2023.12.01.569626	67a5eb0e75e2c69534578efb	"1. ABIDE: Available, URL: https://osf.io/hc4md Creative Common CC BY-SA 3.0 License
2. HCP: Available under request. Data and custom license available at https://www.humanconnectome.org/ 
3. IXI: Available, URL: https://zenodo.org/records/11635168 Creative Common CC BY-SA 3.0 License
4. BCW: Available, URL: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data Creative Common CC BY-NC-SA 4.0 License"	"Four openly available datasets have been used in our study:

1. Autism Brain Imaging Data Exchange (ABIDE). 866 participants (Autism spectrum disorder = 402; Neurotypical controls = 464). Preprocessed data published on October 30, 2018.

2. Human Connectome Project (HCP). 999 healthy participants. HCP1200 released on March 01, 2017

3. Information Extraction from Images (IXI). 600 structural MRI images of healthy individuals.

4. Breast Cancer Wisconsin (BCW). 569 samples for 30 diagnostic features computed from digitized images of fine needle aspirates (FNA) of breast masses (malignant = 357, benign = 212). Released on October 31, 1995."		"Important: We use our proposed method (AdaptiveSplit) to find the optimal sample splitting, compared to more common, fixed, splitting strategies. 

1. for the ABIDE dataset:

Sample size budget: 400, 442, 489, 542, 599

AdaptiveSplit: 
41% training, 59 % test (with 400 total samples)
49% training, 51 % test (with 442 total samples) 
59% training, 41 % test (with 489 total samples)
71% training, 29 % test (with 542 total samples)
82% training, 18 % test (with 599 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

2. for the HCP dataset:

Sample size budget: 242, 272, 305, 343, 384

AdaptiveSplit: 
44% training, 56 % test (with 242 total samples)
53% training, 47 % test (with 272 total samples) 
64% training, 36 % test (with 305 total samples)
76% training, 24 % test (with 343 total samples)
89% training, 11 % test (with 384 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

3. for the IXI dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

4. for the BCW dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)"	3.0	1.0	A dedicated repository with analysis code is present at https://github.com/gallg/AdaptiveSplitAnalysis	We compare Ridge Regression with AdaptiveSplit against Ridge with fixed splitting methods: Pareto Split, Half Split and 90/10 Split	we run each model on 100 different permutation of the data for each sample size budget. Final results are shown with using SEM as confidence intervals (See Fig.3 of the manuscript)	"1. negative mean squared error for regression tasks
2. accuracy for classification tasks
3. p-values to evaluate the statistical significance"	cross-validation, novel experiments	5.0	0.0	The maintained version of the software, for general use, is available in the following GitHub repository: https://github.com/pni-lab/adaptivesplit	Depends on the adaptivesplit package configuration file. The program allows a fast mode to speed up computation but lowering precision.	The models are simple enough to allow intepretation. However, interpretation of the models is not important for our study	We fit a Ridge model for each dataset. We have two regression tasks (for the HCP and IXI datasets) and two classification tasks (for the ABIDE and BCW datasets)	4.0	0.0	"RidgeCV, as implemented in Scikit-Learn
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html"	configuration files and analysis scripts with used parameters are present in the following GitHub repository: https://github.com/pni-lab/AdaptiveSplitAnalysis	All features are standard scaled.	All the features present in each dataset.	"We use only the default ""alphas"" parameter from RidgeCV. Overfitting is ruled out by cross validation."	No	"On each dataset we fit a simple Ridge Regressor (HCP, IXI) / Classifier (ABIDE, BCW) with a default alphas parameter:

RidgeCV scoring = â€œneg_mean_absolute_errorâ€ (for regression datasets: HCP, IXI); â€œaccuracyâ€ (for classification datasets: ABIDE, BCW)
RidgeCV alphas = â€œ(0.1, 1.0, 10.0)â€

The Adaptivesplit package includes a configuration file with parameters that define the ""stopping rule"".
Depending on the ""stopping rule"", the adaptivesplit algorithm finds the optimal splitting strategy to define the boundaries between what can be used as training data and what can be used as external validation. This is extremely useful during prospective data acquisition. 
When the algorithm finds the optimal splitting strategy it stops (this is why we talk about ""stopping rule"").

AdaptiveSplit parameters:

min_training_sample_size = 0
max_training_sample_size = inf
target_power = 0.8
alpha = 0.05
min_score = -inf
min_relevant_score = 0
min_validation_sample_size = 12

window_size= 6
step = 1
cv = 5

bootstrap_samples = 100
power_bootstrap_samples = 1
n_jobs = -1
scoring = â€œneg_mean_squared errorâ€ (regression); â€œaccuracyâ€ (classification)
total_sample_size = highest sample size budget (depends on each dataset)"	L2 regularization, as implemented in scikit-learn's Ridge Regression	7.0	1.0	67a5eb0eedf7b6503929386c				Giuseppe Gallitto, Robert Englert, Balint Kincses, Raviteja Kotikalapudi, Jialin Li, Kevin Hoffschlag, Ulrike Bingel, Tamas Spisak	bioRxiv	2023	0.0	6.0		0.0	2025-02-07T11:14:22.071Z	2025-02-07T11:14:22.071Z	5334d626-f6b1-4b77-a194-15a7257b0897	undenfined	aw9rfhabrd
TITLE_MATCH DOI_MATCH 	43		External validation of machine learning models - registered models and adaptive sample splitting	https://doi.org/10.1101/2023.12.01.569626	67af12fd1f0965481b0c6ce9	"1. ABIDE: Available, URL: https://osf.io/hc4md Creative Common CC BY-SA 3.0 License
2. HCP: Available under request. Data and custom license available at https://www.humanconnectome.org/ 
3. IXI: Available, URL: https://zenodo.org/records/11635168 Creative Common CC BY-SA 3.0 License
4. BCW: Available, URL: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data Creative Common CC BY-NC-SA 4.0 License"	"Four openly available datasets have been used in our study:

1. Autism Brain Imaging Data Exchange (ABIDE). 866 participants (Autism spectrum disorder = 402; Neurotypical controls = 464). Preprocessed data published on October 30, 2018.

2. Human Connectome Project (HCP). 999 healthy participants. HCP1200 released on March 01, 2017

3. Information Extraction from Images (IXI). 600 structural MRI images of healthy individuals.

4. Breast Cancer Wisconsin (BCW). 569 samples for 30 diagnostic features computed from digitized images of fine needle aspirates (FNA) of breast masses (malignant = 357, benign = 212). Released on October 31, 1995."		"Important: We use our proposed method (AdaptiveSplit) to find the optimal sample splitting, compared to more common, fixed, splitting strategies. 

1. for the ABIDE dataset:

Sample size budget: 400, 442, 489, 542, 599

AdaptiveSplit: 
41% training, 59 % test (with 400 total samples)
49% training, 51 % test (with 442 total samples) 
59% training, 41 % test (with 489 total samples)
71% training, 29 % test (with 542 total samples)
82% training, 18 % test (with 599 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

2. for the HCP dataset:

Sample size budget: 242, 272, 305, 343, 384

AdaptiveSplit: 
44% training, 56 % test (with 242 total samples)
53% training, 47 % test (with 272 total samples) 
64% training, 36 % test (with 305 total samples)
76% training, 24 % test (with 343 total samples)
89% training, 11 % test (with 384 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

3. for the IXI dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

4. for the BCW dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)"	3.0	1.0	A dedicated repository with analysis code is present at https://github.com/gallg/AdaptiveSplitAnalysis	We compare Ridge Regression with AdaptiveSplit against Ridge with fixed splitting methods: Pareto Split, Half Split and 90/10 Split	we run each model on 100 different permutation of the data for each sample size budget. Final results are shown with using SEM as confidence intervals (See Fig.3 of the manuscript)	"1. negative mean squared error for regression tasks
2. accuracy for classification tasks
3. p-values to evaluate the statistical significance"	cross-validation, novel experiments	5.0	0.0	The maintained version of the software, for general use, is available in the following GitHub repository: https://github.com/pni-lab/adaptivesplit	Depends on the adaptivesplit package configuration file. The program allows a fast mode to speed up computation but lowering precision.	The models are simple enough to allow intepretation. However, interpretation of the models is not important for our study	We fit a Ridge model for each dataset. We have two regression tasks (for the HCP and IXI datasets) and two classification tasks (for the ABIDE and BCW datasets)	4.0	0.0	"RidgeCV, as implemented in Scikit-Learn
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html"	configuration files and analysis scripts with used parameters are present in the following GitHub repository: https://github.com/pni-lab/AdaptiveSplitAnalysis	All features are standard scaled.	All the features present in each dataset.	"We use only the default ""alphas"" parameter from RidgeCV. Overfitting is ruled out by cross validation."	No	"On each dataset we fit a simple Ridge Regressor (HCP, IXI) / Classifier (ABIDE, BCW) with a default alphas parameter:

RidgeCV scoring = â€œneg_mean_absolute_errorâ€ (for regression datasets: HCP, IXI); â€œaccuracyâ€ (for classification datasets: ABIDE, BCW)
RidgeCV alphas = â€œ(0.1, 1.0, 10.0)â€

The Adaptivesplit package includes a configuration file with parameters that define the ""stopping rule"".
Depending on the ""stopping rule"", the adaptivesplit algorithm finds the optimal splitting strategy to define the boundaries between what can be used as training data and what can be used as external validation. This is extremely useful during prospective data acquisition. 
When the algorithm finds the optimal splitting strategy it stops (this is why we talk about ""stopping rule"").

AdaptiveSplit parameters:

min_training_sample_size = 0
max_training_sample_size = inf
target_power = 0.8
alpha = 0.05
min_score = -inf
min_relevant_score = 0
min_validation_sample_size = 12

window_size= 6
step = 1
cv = 5

bootstrap_samples = 100
power_bootstrap_samples = 1
n_jobs = -1
scoring = â€œneg_mean_squared errorâ€ (regression); â€œaccuracyâ€ (classification)
total_sample_size = highest sample size budget (depends on each dataset)"	L2 regularization, as implemented in scikit-learn's Ridge Regression	7.0	1.0	67a5eb0eedf7b6503929386c				Giuseppe Gallitto, Robert Englert, Balint Kincses, Raviteja Kotikalapudi, Jialin Li, Kevin Hoffschlag, Ulrike Bingel, Tamas Spisak	bioRxiv	2023	0.0	6.0		0.0	2025-02-14T09:55:09.487Z	2025-02-14T09:55:09.487Z	00a01652-3501-4f52-82d1-cd4cc678fb54	undenfined	6qtmjr0bjn
TITLE_MATCH 	44		External validation of machine learning models - registered models and adaptive sample splitting	pending	67af37c51f0965481b0c6de1	"1. ABIDE: Available, URL: https://osf.io/hc4md Creative Common CC BY-SA 3.0 License
2. HCP: Available under request. Data and custom license available at https://www.humanconnectome.org/ 
3. IXI: Available, URL: https://zenodo.org/records/11635168 Creative Common CC BY-SA 3.0 License
4. BCW: Available, URL: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data Creative Common CC BY-NC-SA 4.0 License"	"Four openly available datasets have been used in our study:

1. Autism Brain Imaging Data Exchange (ABIDE). 866 participants (Autism spectrum disorder = 402; Neurotypical controls = 464). Preprocessed data published on October 30, 2018.

2. Human Connectome Project (HCP). 999 healthy participants. HCP1200 released on March 01, 2017

3. Information Extraction from Images (IXI). 600 structural MRI images of healthy individuals.

4. Breast Cancer Wisconsin (BCW). 569 samples for 30 diagnostic features computed from digitized images of fine needle aspirates (FNA) of breast masses (malignant = 357, benign = 212). Released on October 31, 1995."	The splits are determined based on the chosen splitting strategy, since our focus is not on the prediction outcome itself but rather on comparing the performance of the fixed splitting strategies with the adaptive one. Redundancy is naturally reduced by training a different model on each sample size.	"Important: We use our proposed method (AdaptiveSplit) to find the optimal sample splitting, compared to more common, fixed, splitting strategies. 

1. for the ABIDE dataset:

Sample size budget: 400, 442, 489, 542, 599

AdaptiveSplit: 
41% training, 59 % test (with 400 total samples)
49% training, 51 % test (with 442 total samples) 
59% training, 41 % test (with 489 total samples)
71% training, 29 % test (with 542 total samples)
82% training, 18 % test (with 599 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

2. for the HCP dataset:

Sample size budget: 242, 272, 305, 343, 384

AdaptiveSplit: 
44% training, 56 % test (with 242 total samples)
53% training, 47 % test (with 272 total samples) 
64% training, 36 % test (with 305 total samples)
76% training, 24 % test (with 343 total samples)
89% training, 11 % test (with 384 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

3. for the IXI dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)

4. for the BCW dataset:

Sample size budget: 49, 65, 86, 113, 150

AdaptiveSplit: 
21% training, 79 % test (with 49 total samples)
25% training, 75 % test (with 65 total samples) 
40% training, 60 % test (with 86 total samples)
61% training, 39 % test (with 113 total samples)
89% training, 11 % test (with 150 total samples)

Pareto Split: 80% train, 20% test (on all sample size budgets)
90/10 Split: 90% train, 10% test (on all sample size budgets)
Half Split: 50% train, 50% test (on all sample size budgets)"	4.0	0.0	A dedicated repository with analysis code is present at https://github.com/gallg/AdaptiveSplitAnalysis	We compare Ridge Regression with AdaptiveSplit against Ridge with fixed splitting methods: Pareto Split, Half Split and 90/10 Split	we run each model on 100 different permutation of the data for each sample size budget. Final results are shown with using SEM as confidence intervals (See Fig.3 of the manuscript)	"1. negative mean squared error for regression tasks
2. accuracy for classification tasks
3. p-values to evaluate the statistical significance"	cross-validation, novel experiments	5.0	0.0	The maintained version of the software, for general use, is available in the following GitHub repository: https://github.com/pni-lab/adaptivesplit	Depends on the adaptivesplit package configuration file. The program allows a fast mode to speed up computation but lowering precision.	The models are simple enough to allow intepretation. However, interpretation of the models is not important for our study	We fit a Ridge model for each dataset. We have two regression tasks (for the HCP and IXI datasets) and two classification tasks (for the ABIDE and BCW datasets)	4.0	0.0	"RidgeCV, as implemented in Scikit-Learn
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html"	configuration files and analysis scripts with used parameters are present in the following GitHub repository: https://github.com/pni-lab/AdaptiveSplitAnalysis	All features are standard scaled.	All the features present in each dataset.	"We use only the default ""alphas"" parameter from RidgeCV. Overfitting is ruled out by cross validation."	No	"On each dataset we fit a simple Ridge Regressor (HCP, IXI) / Classifier (ABIDE, BCW) with a default alphas parameter:

RidgeCV scoring = â€œneg_mean_absolute_errorâ€ (for regression datasets: HCP, IXI); â€œaccuracyâ€ (for classification datasets: ABIDE, BCW)
RidgeCV alphas = â€œ(0.1, 1.0, 10.0)â€

The Adaptivesplit package includes a configuration file with parameters that define the ""stopping rule"".
Depending on the ""stopping rule"", the adaptivesplit algorithm finds the optimal splitting strategy to define the boundaries between what can be used as training data and what can be used as external validation. This is extremely useful during prospective data acquisition. 
When the algorithm finds the optimal splitting strategy it stops (this is why we talk about ""stopping rule"").

AdaptiveSplit parameters:

min_training_sample_size = 0
max_training_sample_size = inf
target_power = 0.8
alpha = 0.05
min_score = -inf
min_relevant_score = 0
min_validation_sample_size = 12

window_size= 6
step = 1
cv = 5

bootstrap_samples = 100
power_bootstrap_samples = 1
n_jobs = -1
scoring = â€œneg_mean_squared errorâ€ (regression); â€œaccuracyâ€ (classification)
total_sample_size = highest sample size budget (depends on each dataset)"	L2 regularization, as implemented in scikit-learn's Ridge Regression	7.0	1.0	67a5eb0eedf7b6503929386c				Giuseppe Gallitto, Robert Englert, Balint Kincses, Raviteja Kotikalapudi, Jialin Li, Kevin Hoffschlag, Ulrike Bingel, Tamas Spisak	GigaScience	2025	0.0	6.0		0.0	2025-02-14T12:32:05.015Z	2025-02-14T12:32:05.015Z	d4ff70aa-eecf-4aa3-b8d9-80c1cc82b56b	undenfined	0p6q20kd4b
TITLE_MATCH DOI_MATCH 	30	2/2/2022 10:08:47	From flamingo dance to (desirable) drug discovery: a nature-inspired approach.	10.1016/j.drudis.2017.05.008		Yes, in supporting information	Yes (ChEMBL-NTD Novartis dataset and dataset from previous publication)	Yes, independent training, test and external test sets generated using sphere exclusion algorithms	Yes: N_pos and N_neg for training, test and external test sets			No	No	No	Accuracy (Acc), Sensitivity (Se), Specificity (Sp) and Balanced Classification Rate (BCR) Area Under the Accumulation Curve (AUAC); Area under the Receiver Operating Characteristic Curve (ROC); Enrichment factor (EF) and Boltzmann-enhanced discrimination of ROC (BEDROC)	Independent dataset			No	No	Black box	Classification and score based on LSSVM scores			LSSVM	No	Fragments of atoms and bonds (ISIDA Fragmentor software) scaled to the interval [0-1]	Yes: 5-25 features per model, randomly selected among 250 most informative on training dataset (removal of features returning nearly constant values (about for 99%) and selection of top-250 according to Mutual Information Quotient score (Minimal Redundancy Maximal Relevance mRMR algorithm))	No	Yes: model aggregation (ensemble modeling) and multi-criteria decision making Trained on the same dataset	2 LSSVM parameters (RBF kernel (Ïƒ2) and regularization (Î³))  Minimization of the misclassification rate of the 10-fold cross-validated training dataset	Yes: Î³ parameter (LSSVM) 			6312169df3794236aa9879f4	28624633	PMC5650527		SÃ¡nchez-RodrÃ­guez A, PÃ©rez-Castillo Y, SchÃ¼rer SC, Nicolotti O, Mangiatordi GF, Borges F, Cordeiro MNDS, Tejera E, Medina-Franco JL, Cruz-Monteagudo M.	Drug discovery today	2017									
TITLE_MATCH DOI_MATCH 	29	3/8/2022 10:24:58	From flamingo dance to (desirable) drug discovery: a nature-inspired approach.	10.1016/j.drudis.2017.05.008		Yes	Yes	Yes	Yes			Yes	Local baseline	Yes	AUR	Cross validation			Yes	No	Black box but voting schema is applied	Classification			SVM with RBF kernel	Yes	QSAR	Yes, SMILES Chem structures with 250 fragments selected by MI		No	No	Yes - intrinsic to SVM			6312169df3794236aa987a36	28624633	PMC5650527		SÃ¡nchez-RodrÃ­guez A, PÃ©rez-Castillo Y, SchÃ¼rer SC, Nicolotti O, Mangiatordi GF, Borges F, Cordeiro MNDS, Tejera E, Medina-Franco JL, Cruz-Monteagudo M.	Drug discovery today	2017									
DOI_MATCH 	45		HVSeeker: A Deep Learning-based Method for Identification of Host and Viral DNA sequences	not yet published	6760e79869b4c4f264235d59	Yes, the data and the splits are part of the gigascience database.	The data was taken from the National Center for Biotechnology Information (NCBI) and the Integrated Microbial Genomes & Microbiomes - Viruses (IMGVR) databases. The corresponding GigaScience database contains the original data, including accession IDs	To ensure the independence of the training and test sets, we first delete duplicates in the dataset. Then, we use BLAST to remove strongly homologous sequences and create different test subsets to evaluate our models	The exact training/test splits are recorded and are part of the GigaScience database. Originally, the test data comprised 20% of the total data (565,760 original sequences) and was later downsampled for different homology conditions. To ensure a fair comparison, we created multiple test sets using different sequence homologies and report the performance on each one of them. Additionally, we use a nested 5-fold cross-validation (CV) to properly assess the quality of our optimization in the HVSeeker-protein setting (98,720 phage sequences and 122,366 bacteria sequences). Hence, during optimization, HVSeeker-protein uses another 20% of the data for validation. The different data splits used for optimization are also part of the GigaScience database.	4.0	0.0	All evaluation files, including previous test-splits and models used to achieve the performance are supplied as part of the gigascience dataset. 	We compare our method against other SOTA methods (Seeker, Rnn-VirSeeker, DeepVirFinder, PPR-Meta ) and an additional HMM baseline. The scripts used for this evaluation are part of the gigascience database. 	HVSeeker is shown to outperform other methods on various comprehensive datasets and on different performance metrics. 	We use precision, recall, accuracy and F1-score, ROC-AUC  to fairly compare different methods. 	For optimization runs we used a nested 5-fold CV, ensuring fair assessment of our results due to independent test datasets. When testing HVSeeker-DNA we additionally used subsets of our original independent test-set of different homology conditions to challenge our models. In the last step we test HVSeeker against a novel dataset based on the infant gut metagenome, showing it's utility in a practical setting.	5.0	0.0	The source code is released on github and can be found in the gigascience database aswell (MIT license included). To make it easier to run the scripts, we also supply a conda environment with instructions for easy installation. 	Executing a simple forward pass with one sequence only takes around 0.0366 seconds on a standard machine (cpu only). Prediction time is therfore neglegtible. 	As it is the case with most deep-learning methods, interpretaing the classification results is unfortunately not directly possible. 	The model performs classification. 	4.0	0.0	HVSeeker is based on established machine learning (ML) algorithms, such as LSTM and Transformer, which are known to work well with sequences and have been adapted to our specific problem	The model files, optimization schedule and paramter and hyperparameter configurations are reported and can be found in the gigascience database. 	HVSeeker relies on one-hot encoding but merges data in different ways, such as padding (cycling through the sequence until it reaches the required length), contig assembly (combining multiple shorter sequences to generate a longer sequence, then splitting it into subsequences of 1000 bp), and sliding window (using a window of 1000 bp to slide over the input DNA sequence in 100 bp steps)	Both versions of HVSeeker rely only on the sequence, the input features are therfore the nucleotides/amino acids of the input sequence. No further feature selection was performed.	We evaluate our models during training using a validation set to achieve an optimal fit of the paramters.	HVSeeker-Protein uses input from a pre-trained version of ProtBert. Since ProtBert was previously trained on a masked language task, it had not encountered a classification task prior to this study.	HVSeeker contains 1026152 trainable parameter. The number of parameter is a result of adapting the model to the training process. 	We use early stopping using a validation set and dropout to prevent overfitting to the training data. You can find details in the provided scripts. 	8.0	0.0	6760e76ddd85bd59b3841ff8	not yet published			Abdullatif Al-Najim, Sven Hauns, Van Dinh Tran, Rolf Backofen and Omer S. Alkhnbashi	GigaScience	not yet published	0.0	6.0		0.0	2024-12-17T02:53:12.396Z	2024-12-17T02:53:12.396Z	40ca4b51-f205-4b23-93ed-a95042d470e7	undenfined	igr5x3a1vs
TITLE_MATCH DOI_MATCH 	30	3/29/2022 21:41:26	KIR Genes and Patterns Given by the A Priori Algorithm: Immunity for Haematological Malignancies.	10.1155/2015/141363		no	300 healthy individuals (Mexican Reference Genomic DNA Collection (MGDC-REF),) 43 patients with haematological malignancies (Haematology Department of Hospital Central â€œDr. Ignacio Morones Prietoâ€)	no	no			no	no	no	ğ‘ value, ğœ’2	statisctical evaluation			no	"no, the authors only described the machine, ""Weka software using a personal computer with a processor IntelCore i7with 2.3Ghz speed and 3 Gb memory."""	transparent, the authors provided biological explanation some of scenarios	classification			J48 Algorithm, A Priori Algorithm	no	no	no	no	no	no	no			6312169df3794236aa9879e5	26495028	PMC4606520		RodrÃ­guez-Escobedo JG, GarcÃ­a-SepÃºlveda CA, Cuevas-Tello JC.	Computational and mathematical methods in medicine	2015									
TITLE_MATCH DOI_MATCH 	30	2/8/2022 15:10:18	KIR Genes and Patterns Given by the A Priori Algorithm: Immunity for Haematological Malignancies.	10.1155/2015/141363		No	Yes (Mexican Reference Genomic DNA Collection (MGDC-REF) ). Source of data and number of positives and negatives given, negative dataset was used by a previous paper.	No, only training set	Yes: number of N_pos and N_neg No data splits. Only training set			Yes: confusion matrix in publication	Univariate statistical analysis (Fishersâ€™ exact test) and decision tree classifier (J48 ID3)	Chi-squared test p-value comparison	chi-squared test	Evaluation on the training dataset			Yes (URL http://www.cs.waikato.ac.nz/~ml/weka/index.html)	No	Transparent: 24 most frequent rules generated by the apriori algorithm are detailed	Binary prediction (statistical classifier)			â€œA prioriâ€ algorithm	No	Yes: Presence/absence of genes	Yes (13 features)	No exclusion	No	2 parameters (minimum support threshold and confidence levels)	No			6312169df3794236aa9879f4	26495028	PMC4606520		RodrÃ­guez-Escobedo JG, GarcÃ­a-SepÃºlveda CA, Cuevas-Tello JC.	Computational and mathematical methods in medicine	2015									
TITLE_MATCH DOI_MATCH 	26	3/8/2022 10:53:29	KIR Genes and Patterns Given by the A Priori Algorithm: Immunity for Haematological Malignancies.	10.1155/2015/141363		No	Yes	Yes	Control & clinical sets (300 healthy individuals and 43 patients)			No	No	Yes	P-value of association				Yes	Partially	Black box (~large decision-tree)	Statistical analysis: association between features and disease			Decision tree (C4.5)	Yes	Unclear	Unclear		No					6312169df3794236aa987a36	26495028	PMC4606520		RodrÃ­guez-Escobedo JG, GarcÃ­a-SepÃºlveda CA, Cuevas-Tello JC.	Computational and mathematical methods in medicine	2015									
TITLE_MATCH 	41		Large-scale Genomic Survey with Deep Learning-based Method Reveals Strain-Level Phage Specificity Determinants		65256a4f5cb89e9826b25659	The data will be made available upon request, subject to the approval of the authors, yet the deep-learning model trained is provided at https://figshare.com/articles/online_resource/SpikeHunter_trained_model_pth_file/23577051.	The source of the dataset is INfrastructure for a PHAge REference Database (INPHARED) provided by https://github.com/RyanCook94/inphared on Aug 1st, 2022. There are 3,659 bacteriophage genomes in this dataset. We curated 1912 tailspike protein sequences and 200732 non-tailspike protein sequences from this dataset. This data has not been used in previous papers.	To train and validate the SpikeHunter the manually curated set of tailspike proteins was first clustered into 20,274 clusters at 30% identity using CD-HIT. The dataset was divided into training, validation, and testing datasets in a ratio of 3:1:1 using the StratifiedGroupKFold function in the Scikit-learn python package, with two key objectives in mind: 1) ensuring the absence of overlaps among protein clusters, and 2) maintaining consistent ratios of positive to negative samples across the splits. As a result of this process, the training, validation and test sets are independent.	The dataset was divided into training, validation, and testing datasets in a ratio of 3:1:1. The training set includes 122,506 proteins (comprising 1,023 positive samples and 121,483 negative samples belonging to 12,170 clusters), a validation set 40,838 proteins (comprising 343 positive samples and 40,495 negative samples belonging to 4,054 clusters), and a test set 39,300 proteins (comprising 546 positive samples and 38,754 negative samples belonging to 4,050 clusters).	4.0	0.0	No. The data will be made available upon request, subject to the approval of the authors.	No 	No 	Evaluation of the model on the testing dataset demonstrated an F1-score of 0.994, precision of 0.991, recall of 0.996, specificity of 1.000, a Mathewâ€™s correlation coefficient (MCC) of 0.994, and the area under the precision-recall curve of 0.994.	It was evaluated by an independent test dataset.	5.0	0.0	Yes, the source code is released at https://github.com/nlm-irp-jianglab/SpikeHunter.	3 days with two NVIDIA v100x GPUs.	It is a black box.	The model is a classification that outputs probabilities to indicate whether each phage protein sequence corresponds to a tailspike protein or not.	4.0	0.0	This is a deep-learning model that utilizes the pre-trained transformer protein language model ESM-2 (esm2_t33_650M_UR50D) as an embedding layer. This embedding layer is then linked to a neural network comprising three fully connected layers with 1280, 568, and 2 nodes, respectively. The output layer employs a softmax activation function to produce probabilities, indicating whether each sequence corresponds to a tailspike protein or not.	Yes. The deep-learning model trained in this study is provided at https://figshare.com/articles/online_resource/SpikeHunter_trained_model_pth_file/23577051.	Phage protein sequences were tokenized and transformed into numerical vectors using the batch_converter function in the ESM Python package (https://github.com/facebookresearch/esm).	Since the sequences are embedded as 1280-length representations using a pre-trained transformer protein language model ESM-2 during training, the number of input features before embedding varies depending on the lengths of protein sequences, while the number of features after the embedding is 1280. No feature selection strategy was performed.	The model's parameter count is approximately eight times the number of training data points. Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	No. 	800698	Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	8.0	0.0	652556ed92c76639b86e085d				Yiyan Yang, Keith Dufault-Thompson, Wei Yan, Tian Cai, Lei Xie, Xiaofang Jiang			0.0	6.0		0.0	2023-10-10T15:14:23.770Z	2023-10-10T18:39:07.700Z	eab9d7f1-54e0-4a31-9070-f65416e06ed2	undenfined	ggaeng30i9
TITLE_MATCH 	45		Large-scale Genomic Survey with Deep Learning-based Method Reveals Strain-Level Phage Specificity Determinants	10.1093/gigascience/giae017	670e6a0961d57eb8bca6960a	The data is accessible via GigaDB and the supplemental material.	The source of the dataset is INfrastructure for a PHAge REference Database (INPHARED) provided by https://github.com/RyanCook94/inphared on Aug 1st, 2022. There are 3,659 bacteriophage genomes in this dataset. We curated 1912 tailspike protein sequences and 200732 non-tailspike protein sequences from this dataset. This data has not been used in previous papers.	To train and validate the SpikeHunter the manually curated set of tailspike proteins was first clustered into 20,274 clusters at 30% identity using CD-HIT. The dataset was divided into training, validation, and testing datasets in a ratio of 3:1:1 using the StratifiedGroupKFold function in the Scikit-learn python package, with two key objectives in mind: 1) ensuring the absence of overlaps among protein clusters, and 2) maintaining consistent ratios of positive to negative samples across the splits. As a result of this process, the training, validation and test sets are independent.	The dataset was divided into training, validation, and testing datasets in a ratio of 3:1:1. The training set includes 122,506 proteins (comprising 1,023 positive samples and 121,483 negative samples belonging to 12,170 clusters), a validation set 40,838 proteins (comprising 343 positive samples and 40,495 negative samples belonging to 4,054 clusters), and a test set 39,300 proteins (comprising 546 positive samples and 38,754 negative samples belonging to 4,050 clusters).	4.0	0.0	Yes. The data is accessible via GigaDB and the supplemental material.	No.	No.	Evaluation of the model on the testing dataset demonstrated an F1-score of 0.994, precision of 0.991, recall of 0.996, specificity of 1.000, a Mathewâ€™s correlation coefficient (MCC) of 0.994, and the area under the precision-recall curve of 0.994.	It was evaluated by an independent test dataset.	3.0	2.0	Yes, the source code is released at https://github.com/nlm-irp-jianglab/SpikeHunter.	3 days with two NVIDIA v100x GPUs on a high-performance computing cluster.	It is a black box.	The model is a classification that outputs probabilities to indicate whether each sequence corresponds to a tailspike protein or not.	4.0	0.0	This is a deep-learning model that utilizes the pre-trained transformer protein language model ESM-2 (esm2_t33_650M_UR50D) as an embedding layer. This embedding layer is then linked to a neural network comprising three fully connected layers with 1280, 568, and 2 nodes, respectively. The output layer employs a softmax activation function to produce probabilities, indicating whether each sequence corresponds to a tailspike protein or not.	Yes. The deep-learning model trained in this study is provided at https://figshare.com/articles/online_resource/SpikeHunter_trained_model_pth_file/23577051.	Phage protein sequences were tokenized and transformed into numerical vectors using the batch_converter function in the ESM Python package (https://github.com/facebookresearch/esm).	Since the sequences are embedded as 1280-length representations using a pre-trained transformer protein language model ESM-2 during training, the number of input features before embedding varies depending on the lengths of protein sequences, while the number of features after the embedding is 1280. No feature selection strategy was performed.	The model's parameter count is approximately eight times the number of training data points. Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	This model is not a meta-predictor.	800698	Early stopping was adopted to stop training once the model performance was no longer improved on the validation dataset for three consecutive epochs to avoid overfitting.	8.0	0.0	652556ed92c76639b86e085d	38649301			Yiyan Yang, Keith Dufault-Thompson, Wei Yan, Tian Cai, Lei Xie, Xiaofang Jiang	GigaScience	2024	0.0	6.0		1.0	2024-10-15T13:11:37.812Z	2024-10-15T13:11:37.812Z	1261ec25-2b6d-4a8a-aca6-a50c235a4737	undenfined	dfyn1yvtz3
TITLE_MATCH 	41		Learning A Generalized Graph Transformer for Protein Function Prediction in Dissimilar Sequences		673749794040aef5ee81efdc	Data is re-splited from other publication and also available via GitHub link.	The dataset is collected from the previous publication DeepFRI (https://github.com/flatironinstitute/DeepFRI) and HEAL (https://github.com/ZhonghuiGu/HEAL), and then we split them according to our methodology.	Both datasets meet the criterion that the sequence identity between samples from different sets is below 30%.	"1) PDBch training set: 29304, validation set: 3660, test set 3665;
2) AFch training set: 34135, validation set : 3881, test set 3981."	4.0	0.0	might be part of the GitHub repo.	We compared it with several baselines.	compare performance between methods.	AUPR and Fmax.	Independent dataset and several experiments.	5.0	0.0	Yes via GitHub.	About two hours for traning.	Hard to interpretable.	Multi-label classification.	4.0	0.0	GCN and Graph Transformer were employed for representation learning, while adversarial learning was utilized for domain alignment.	No	The paper proposed a novel model based on Graph Transformer to acquire graph-level embeddings that capture spatial semantics.	The protein features including protein sequence, distance map between residues, residue embeddings extracted from protein language model are used as input.	The number of parameters is reasonable.	No, the model used raw data as input.	About 8 million1 parameters are used.	 Yes, we implemented early stopping using a validation set. If the validation loss increased consistently for several epochs, training would be halted.	7.0	1.0	661e294a92c76639b8a6ac73				Yiwei Fu, Zhonghui Gu, Xiao Luo, Qirui Guo, Luhua Lai and Minghua Deng			0.0	6.0		0.0	2024-11-15T13:15:37.441Z	2024-11-15T13:15:37.441Z	1da2ba55-9295-4be9-8f97-44ad49894b9e	undenfined	o3fs056lry
TITLE_MATCH 	41		Learning A Generalized Graph Transformer for Protein Function Prediction in Dissimilar Sequences		673749cd4040aef5ee81efe0	Data is re-splited from other publication and also available via GitHub link.	The dataset is collected from the previous publication DeepFRI (https://github.com/flatironinstitute/DeepFRI) and HEAL (https://github.com/ZhonghuiGu/HEAL), and then we split them according to our methodology.	Both datasets meet the criterion that the sequence identity between samples from different sets is below 30%.	"1) PDBch training set: 29304, validation set: 3660, test set 3665;
2) AFch training set: 34135, validation set : 3881, test set 3981."	4.0	0.0	might be part of the GitHub repo.	We compared it with several baselines.	compare performance between methods.	AUPR and Fmax.	Independent dataset and several experiments.	5.0	0.0	Yes via GitHub.	About two hours for traning.	Hard to interpretable.	Multi-label classification.	4.0	0.0	GCN and Graph Transformer were employed for representation learning, while adversarial learning was utilized for domain alignment.	No	The paper proposed a novel model based on Graph Transformer to acquire graph-level embeddings that capture spatial semantics.	The protein features including protein sequence, distance map between residues, residue embeddings extracted from protein language model are used as input.	The number of parameters is reasonable.	No, the model used raw data as input.	About 8 million1 parameters are used.	 Yes, we implemented early stopping using a validation set. If the validation loss increased consistently for several epochs, training would be halted.	7.0	1.0	661e294a92c76639b8a6ac73				Yiwei Fu, Zhonghui Gu, Xiao Luo, Qirui Guo, Luhua Lai and Minghua Deng			0.0	6.0		0.0	2024-11-15T13:17:01.570Z	2024-11-15T13:17:01.570Z	080f66a7-3cbe-4f77-aec5-c5174c0a2377	undenfined	z4txajqyy3
TITLE_MATCH 	41		Learning A Generalized Graph Transformer for Protein Function Prediction in Dissimilar Sequences		67504b4469b4c4f264235690	Data is re-splited from other publication and also available via GitHub link.	The dataset is collected from the previous publication DeepFRI (https://github.com/flatironinstitute/DeepFRI) and HEAL (https://github.com/ZhonghuiGu/HEAL), and then we split them according to our methodology.	Both datasets meet the criterion that the sequence identity between samples from different sets is below 30%.	"1) PDBch training set: 29304, validation set: 3660, test set 3665;
2) AFch training set: 34135, validation set : 3881, test set 3981."	4.0	0.0	might be part of the GitHub repo.	We compared it with several baselines.	compare performance between methods.	AUPR and Fmax.	Independent dataset and several experiments.	5.0	0.0	Yes via GitHub.	About two hours for traning.	Hard to interpretable.	Multi-label classification.	4.0	0.0	GCN and Graph Transformer were employed for representation learning, while adversarial learning was utilized for domain alignment.	No	The paper proposed a novel model based on Graph Transformer to acquire graph-level embeddings that capture spatial semantics.	The protein features including protein sequence, distance map between residues, residue embeddings extracted from protein language model are used as input.	The number of parameters is reasonable.	No, the model used raw data as input.	About 8 million1 parameters are used.	 Yes, we implemented early stopping using a validation set. If the validation loss increased consistently for several epochs, training would be halted.	7.0	1.0	661e294a92c76639b8a6ac73				Yiwei Fu, Zhonghui Gu, Xiao Luo, Qirui Guo, Luhua Lai and Minghua Deng			0.0	6.0		0.0	2024-12-04T12:29:56.458Z	2024-12-04T12:29:56.458Z	66385402-93fb-421f-b9c6-0fb154d994b7	undenfined	8v069jkmlj
TITLE_MATCH 	44		Learning A Generalized Graph Transformer for Protein Function Prediction in Dissimilar Sequences	10.1093/gigascience/giae093	67504d6469b4c4f264235694	Data is re-splited from other publication and also available via GitHub link.	The dataset is collected from the previous publication DeepFRI (https://github.com/flatironinstitute/DeepFRI) and HEAL (https://github.com/ZhonghuiGu/HEAL), and then we split them according to our methodology.	Both datasets meet the criterion that the sequence identity between samples from different sets is below 30%.	"1) PDBch training set: 29304, validation set: 3660, test set 3665;
2) AFch training set: 34135, validation set : 3881, test set 3981."	4.0	0.0	might be part of the GitHub repo.	We compared it with several baselines.	compare performance between methods.	AUPR and Fmax.	Independent dataset and several experiments.	5.0	0.0	Yes via GitHub.	About two hours for traning.	Hard to interpretable.	Multi-label classification.	4.0	0.0	GCN and Graph Transformer were employed for representation learning, while adversarial learning was utilized for domain alignment.	No	The paper proposed a novel model based on Graph Transformer to acquire graph-level embeddings that capture spatial semantics.	The protein features including protein sequence, distance map between residues, residue embeddings extracted from protein language model are used as input.	The number of parameters is reasonable.	No, the model used raw data as input.	About 8 million1 parameters are used.	 Yes, we implemented early stopping using a validation set. If the validation loss increased consistently for several epochs, training would be halted.	7.0	1.0	661e294a92c76639b8a6ac73				Yiwei Fu, Zhonghui Gu, Xiao Luo, Qirui Guo, Luhua Lai and Minghua Deng	GigaScience	2024	0.0	6.0		1.0	2024-12-04T12:39:00.931Z	2024-12-12T15:56:56.709Z	5d9e7df0-7107-499a-ae55-79c125fde132	undenfined	ccm469wdz5
TITLE_MATCH DOI_MATCH 	30	3/18/2022 11:23:23	m5CPred-SVM: a novel method for predicting m5C sites of RNA.	10.1186/s12859-020-03828-4		Yes: Supp data of referenced publication,  and GEO query	Yes: used by previous papers. N_pos and N_neg are given.	Yes: random selection with redundancy removal in pos and neg sets (>70% sequence identity CD-HIT)	Yes: Size of N_pos and N_neg for training and test sets.			No	Yes: RNAm5Cfinder, iRNA-m5C, iRNAm5C-PseDNC, RNAm5CPred, PEA-m5C	No. Justification based only on evaluation values difference (acc, sn, sp, pre, mcc, f1, AUROC)	 accuracy, sensitivity, specificity, precision, Matthews correlation coefficient and F1-score, AUROC	independent datasets			No. Link to project (https://zhulab.ahu.edu.cn/m5CPred-SVM/) is broken	No	~Yes: Results for different feature selections	Classification			Yes: SVM (compared to KNN, adaboost, random forest, decision tree, logistic regression and XGBoost)	No	Yes: sequence features (KNF/NC, KSNPF, PSNP, KSPSDP, PseDNC, CPD)	Yes: 6 f. Wrapper method (sequence forward selection). ten-fold cross-validation on training dataset.	No	No	Yes: box constraint and kernel scale, optimised by a grid search	Yes:  box constraint parameter			6312169df3794236aa9879f4	33126851	PMC7602301		Chen X, Xiong Y, Liu Y, Chen Y, Bi S, Zhu X.	BMC bioinformatics	2020									
TITLE_MATCH DOI_MATCH 	30	2/9/2022 14:43:24	m5CPred-SVM: a novel method for predicting m5C sites of RNA.	10.1186/s12859-020-03828-4		Yes, supposedly. Datasets are supposed to be available through URL (https://zhulab.ahu.edu.cn/m5CPred-SVM/) but link does not respond	Yes: source of data. Used by previous papers.	Yes: procedure to remove sequences with similarity >70% is applied in negative and positive datasets	Yes: size of training, validation and test sets as well as distribution of N_pos and N_neg are given.			No	Comparison of SVM with other classifiers (KNN, Adaboost, random forests, decision tree, logistic regression and XGBoost) on the same cross-validation dataset. Comparison of their best method (SVM) with five other existing methods with webserver availability (RNAm5Cfinder, iRNA-m5C, iRNAm5C-PseDNC and RNAm5Cpred, PEA-m5C). Table presenting algorithm class and features used by these methods is presented.	List of so-called â€œsignificantly higherâ€œ performance metric values in favor of their method, but without confidence interval or explicit p-values or even name of test performed. 	Accuracy, sensitivity, specificity, precision, Matthews correlation coefficient and F1-score. Area under ROC and PRC	Independent test set			Website URL (but link is broken)	No	Transparent at the level of the feature selection (evaluation results for different combinations of features), put in connection with mean sequence difference between positive and negative sets	Classification			SVM (FITCSVM)	No. URL given for model availability but link is broken (not sure what exact information was available in the URL)	Yes: specific sequence features (k-nucleotide frequency (KNF), k-spaced nucleotide pair frequency (KSNPF), position-specific nucleotide propensity (PSNP), k-spaced position-specific dinucleotide propensity (KSPSDP), pseudo dinucleotide composition (PseDNC), Chemical property with density (CPD)) 	Yes: number of features and feature selection on the ten-fold cross-validation dataset using sequential forward feature selection (SFS)	No exclusion mentioned	No	Yes: 2 (box constraint and kernel scale) Selected through grid search on ten-fold cross-validation dataset	Yes (sequential forward feature selection (SFS) to reduce redundant features)			6312169df3794236aa9879f4	33126851	PMC7602301		Chen X, Xiong Y, Liu Y, Chen Y, Bi S, Zhu X.	BMC bioinformatics	2020									
TITLE_MATCH 	45		MMV_Im2Im: An Open Source Microscopy Machine Vision Toolbox for Image-to-Image Transformation	10.1093/gigascience/giad120	670e65ee61d57eb8bca695fe	"The data are all publicly available, as they were all released with previous publications. Please refer the ""Data Availability"" section of the manuscript for details."	"The data were all from public repositories released with previous publications. Please refer the ""Data Availability"" section of the manuscript for details."	When we do train/test split, all splits are random.	"There are over ten different experiments in this work. In general, K% of the data were held out for testing, while (100-K)% were used during training. Among this (100-K)%, 85%*(100-K)% were used in the training loops, while 15%*(100-K)% were used in the validation loops (e.g., for the purpose of early stoping determination). In most cases, K = 20. But in some cases, due to limited amount of public data, we can only hold out smaller amount of data; otherwise, the training set would be too small to be effective. There are also a few experiments, we just followed the original split from the published dataset. Please refer the ""Data Availability"" section of the manuscript for details."	4.0	0.0	All models are released https://zenodo.org/records/10034416, which can be used to reproduce raw outputs	No	No	"The objective of this paper is to introduce a geneic machien learning tool, not for a specific scientific study. The evaluation metric varies from application to application. Please refer the ""Results"" section of the manuscript for details."	evaluated on hold-out sets	3.0	2.0	yes, open source python package. https://github.com/MMV-Lab/mmv_im2im (MIT license)	Generally about a few seconds, depending on the size of the image and the specific model and application.	in general, deep neural networks are not fully interpretable	The objective of this paper is to introduce a machine learning tool, not for a specific scientific question. The model can be regression or classification depending on specific image-to-image transformation application. For labelfree, denoising, modality transfer, synthetic image generation, the models are regression; for other segmentation tasks, the models are classification.	4.0	0.0	Neural network. Four types of methods were used, conditional GAN, cycle GAN, FCN, and embedding based instance segmentation network.	yes, https://github.com/MMV-Lab/mmv_im2im/tree/main/paper_configs	Usually, the images need to go through intensity normalization before feeding into the neural network. Different experiments may use different intensity normalization methods.	Inputs are images	We demonstration examples of various regularization methods in different experiments, such as weigh decay and early stopping, etc.. The objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question. Users can configure their own way to control overfitting and under fitting, depending on their own data.	No	"We used different neural networks for different applications. Please refer the ""Results"" section of the manuscript for details. The goal is not to show the best model. Instead, it is to highlight the flexibility of our tool, where users can test differnet models without changing any line of code. The objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question."	yes, we used weight decay in the optimizer and early stopping monitoring the validation loss. (Again, the objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question. One highlight of this tool is that users can easily configure different regularization methods, e.g., different early stopping criteria, different optimizer, adding additional regularization term in the loss, etc.)	7.0	1.0	6435665292c76639b80869bd	38280188			Justin Sonneck, Yu Zhou, Jianxu Chen	GigaScience	2023	0.0	6.0		1.0	2024-10-15T12:54:06.607Z	2024-10-15T12:54:06.607Z	b2f6bf1e-bccb-4250-98a3-0b71335847dc	undenfined	9izph0m2we
TITLE_MATCH 	45		MMV_Im2Im: An Open Source Microscopy Machine Vision Toolbox for Image-to-Image Transformation	10.48550/arXiv.2209.02498	6435763bbcc9ba89a8f2c70d	"The data are all publicly available, as they were all released with previous publications. Please refer the ""Data Availability"" section of the manuscript for details."	"The data were all from public repositories released with previous publications. Please refer the ""Data Availability"" section of the manuscript for details."	When we do train/test split, all splits are random. 	"There are over ten different experiments in this work. In general, K% of the data were held out for testing, while (100-K)% were used during training. Among this (100-K)%, 85%*(100-K)% were used in the training loops, while 15%*(100-K)% were used in the validation loops (e.g., for the purpose of early stoping determination). In most cases, K = 20. But in some cases, due to limited amount of public data, we can only hold out smaller amount of data; otherwise, the training set would be too small to be effective. There are also a few experiments, we just followed the original split from the published dataset. Please refer the ""Data Availability"" section of the manuscript for details."	4.0	0.0	The raw output will be availabe on GigaDB	No	No	"Again, the objective of this paper is to introduce a generic machine learning tool, not for a specific scientific study. The evaluation metric varies from application to application. Please refer the ""Results"" section of the manuscript for details."	evaluated on hold-out sets	3.0	2.0	yes, open source python package. https://github.com/MMV-Lab/mmv_im2im (MIT license)	generally about a few seconds, depending on the size of the image and the specific model and application. 	in general, deep neural networks are not fully interpretable	The objective of this paper is to introduce a machine learning tool, not for a specific scientific question. The model can be regresssion or classification depending on specific image-to-image transformation application. For labelfree, denosing, modality transfer, sythetic image generation, the models are regression; for other segmentation tasks, the models are classification. 	4.0	0.0	Nerual network. Four types of methods were used, conditional GAN, cycle GAN, FCN, and embeding based instance segmentation network. 	yes, https://github.com/MMV-Lab/mmv_im2im/tree/main/paper_configs	Usually, the images need to go through intensity normalization before feeding into the neural network. Different experiments may use different intensity normalization methods.  	Inputs are images	We demonstration examples of various regularization methods in different experiements, such as weigh decay and early stopping, etc.. The objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question. Users can configure their own way to control overfitting and under fitting, depending on their own data.	No	"we used different neural networks for different applications. Please refer the ""Results"" section of the manuscript for details. The goal is not to show the best model. Instead, it is to highlight the flexibility of our tool, where users can test differnet models without changing any line of code. The objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question."	"yes, we used weight decay in the optimizer and early stopping monitoring the validation loss.
(Again, the objective of this paper is to introduce a generic machine learning tool, not for a specific scientific question. One highlight of this tool is that users can easily configure different regularization methods, e.g., different early stopping criteria, different optimizor, adding additional regularization term in the loss, etc.)"	7.0	1.0	6435665292c76639b80869bd	under review			Justin Sonneck, Jianxu Chen	under review	under review	0.0	0.0		0.0	2023-04-11T15:01:15.153Z	2023-04-18T21:44:49.827Z	92c57bf0-690a-41fe-8194-fda03e250336	undenfined	ht79gi32gt
TITLE_MATCH DOI_MATCH 	30	4/18/2022 16:33:16	MRI-based machine learning radiomics can predict HER2 expression level and pathologic response after neoadjuvant therapy in HER2 overexpressing breast cancer.	10.1016/j.ebiom.2020.103042		Not explicitly stated	311 patients chosen out of 445 available records. 	Not explicitly stated	Split 80:20. Random splitting of the data resulted in 249 cases (150 pCR, 99 no pCR) in the training set and 62 cases (38 pCR, 24 no pCR) in the test set.			Not explicitly stated	Comparison of different configurations. Spearman correlation	p-value	Percentile, specificity, sensitivity, positive predict value, negative predict value, accuracy	AUROC			Not explicitly stated	Not explicitly stated	Not explicitly stated	Classification (pCR or no pCR). HER2 expression levels wrt IHC and FISH			Coarse decision trees and five-fold cross-validation.	Not explicitly stated	3D segmentation on MRI exams, enhancement maps calculated. Data reduced to fixed bin number of 16 greys levels. Combat harmonisation to reduce centre effect	Features computed from 2D directional matrix and average over 2D directions and slices. 	Not explicitly stated	Not explicitly stated	102 texture parameters in 6 categories	Not explicitly stated			6312169df3794236aa987a13	33039708	PMC7648120		Bitencourt AGV, Gibbs P, Rossi Saccarelli C, Daimiel I, Lo Gullo R, Fox MJ, Thakur S, Pinker K, Morris EA, Morrow M, Jochelson MS.	EBioMedicine	2020									
TITLE_MATCH DOI_MATCH 	30	3/27/2022 23:10:49	MRI-based machine learning radiomics can predict HER2 expression level and pathologic response after neoadjuvant therapy in HER2 overexpressing breast cancer.	10.1016/j.ebiom.2020.103042		Not available	Breast magnetic resonance imaging (MRI) and clinical data from 311 HER2 overexpressing breast cancer patients.  1)        Patients were classified into two groups based on HER2 expression level. Npos=279 patients with tumours that showed HER2 protein overexpression on immunohistochemistry (IHC 3+; IHC group). Nneg=32 patients with tumours that showed HER2 gene amplification detected by FISH in the absence of protein overexpression on IHC (IHC 2+ or 1+ to 2+; FISH group). 2)        Npos=188 patients with pathologic complete response (pCR), which was defined as no residual invasive carcinoma in the breast or axillary lymph nodes (ypT0/isN0) at surgical resection. Nneg=123 patients that are non-pCR. 	Inclusion criteria for this study were HER2 overexpressing breast cancer patients who underwent NAC and pretreatment state-of-the-art contrast-enhanced breast MRI. 70 were excluded because they did not have pretreatment breast MRI and 64 patients with outside images were excluded because of poor image quality.	For the prediction of pathological complete response, the data was split into training and test sets at a ratio of 4:1 (80% training and 20% test), with feature selection performed purely on the training set. Npos,train=150 patients with pCR. Npos,test=38 patients with pCR. Nneg,train=99 patients with non-pCR. Nneg,test=24 patients with non-pCR.  Due to the low number of cases in the minority class, this was not feasible for the comparison between the IHC and FISH groups.			Not available	Not specified	Not specified	Sensitivity, specificity, diagnostic accuracy	5-fold cross validation			Not available	Not specified	Transparent. Assessing the presence of statistically significant differences, using Mann-Whitney U-test and Chi-squared test, across clinical and MRI features of 1) IHC vs FISH and 2) pCR vs non-pCR groups, before including them to the final model.	1) Binary predictions of IHC (positive) or FISH (negative) samples. 2) Binary predictions of pCR (positive) or non-pCR (negative) samples. 			Coarse decision trees	Not available	Following ROC and correlation analysis, Breast MRIs were assessed according to the American College of Radiology (ACR) Breast Imaging Reporting and Data System (BI-RADS) lexicon.  Manual, expert review on the MRI exams and performed 3D segmentations of the whole tumour in the first post-contrast non-subtracted sequence using ITK-SNAP software.  Susceptibility artefacts related to post-biopsy changes, when present, were excluded from segmentation and only the largest lesion was segmented in multifocal tumours.   Enhancement maps were calculated as the percentage increase in signal from the pre-contrast image to the first post-contrast image. Radiomics and statistical analysis were performed using MATLAB and publicly available CERR (Computational Environment for Radiological Research) software.  Furthermore, data was reduced to a fixed bin number of 16 grey levels and only an interpixel distance of one was considered. CERR analysis resulted in 102 texture parameters sub-divided into six categories - 22 first order statistics, 26 statistics based on grey level cooccurrence matrices, 16 statistics based on run length matrices, 16 statistics based on size zone matrices, 17 statistics based on neighborhood grey level dependence matrices, and finally five statistics based on neighborhood grey tone difference matrices.  Features were computed for each 2D directional matrix and averaged over 2D directions and slices, since data was not isotropic. As patients were scanned at different sites, Combat harmonisation was performed to remove the centre effect (local vs. foreign scans) while retaining the pathophysiologic information (either HER2 expression or pathologic response). The harmonisation employed Bayes estimates to account for both additive and multiplicative scanner effects.  Univariate analysis was performed to identify significant parameters. Continuous variables were described as mean, standard deviation (SD), and range. The two-tailed Mann-Whitney U test for two independent samples was used to determine significant differences between groups. Correlation analysis was then employed to remove redundant parameters from advancement to model development. If a highly positive (> 0.9) or highly negative (< -0.9) correlation was noted, the parameter with the lowest area under the receiver operating curve (AUROC) was removed. 	1) The final model to predict HER2 intratumour expression levels (IHC vs. FISH) utilized three MRI features. Lesion type and multifocality (clinical features) and large zone emphasis (radiomic feature). 2) The model to predict pCR status included six MRI parameters. Lesion type and size (clinical parameters) and variance, first order entropy, 90th percentile and zone length variance (radiomic parameters). 	Not specified	No	Coarse decision tree modelling was implemented in MATLAB, with the maximum number of splits set at four and utilizing Giniâ€™s diversity index as the splitting criterion.	Not specified			6312169df3794236aa9879ed	33039708	PMC7648120		Bitencourt AGV, Gibbs P, Rossi Saccarelli C, Daimiel I, Lo Gullo R, Fox MJ, Thakur S, Pinker K, Morris EA, Morrow M, Jochelson MS.	EBioMedicine	2020									
TITLE_MATCH 	42		Multi-omics Analysis of Umbilical Cord Hematopoietic Stem Cells from a Multi-ethnic Cohort of Hawaii Reveals the Intergenerational Effect of Maternal Pre-Pregnancy Obesity and Risk Prediction for Cancers		6780005469b4c4f26423614a	GEO (public upon acceptance)	The DNA methylation data of the CD34+/CD38âˆ’/Linâˆ’ cord blood hematopoietic stem cells  (uHSCs) was directly obtained from a multi-ethnic cohort of 72 pregnant women (Npos = 34 obese; Nneg = 38 non-obese) from Kapiolani Medical Center for Women and Children in Honolulu, Hawaii (collected between 2016 and 2018). 	The data was randomly split into 8:2 train/test ratio. Training and test sets were independent.	A total of 72 samples were split into 8:2 train/test ratio, with 59 samples in the training set, 13 samples in the testing set. No separate validation set was used. The distribution of samples in train and test are balanced. 	4.0	0.0	Codes are available in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license	No	No, only one-time prediction performance metric was reported.	Balanced accuracy, accuracy, F1 score	5-fold cross-validation on obesity data. Prediction on external independent TCGA datatsets.	4.0	1.0	Code released in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license.	Run time 0.005 second on HPC.	This random forest obesity model offers some interpretability features based on feature importance scores. For example, cgxxxxxxxx has the highest feature importance in obesity classification.	Classification	4.0	0.0	Random forest model was used for normal vs obesity classification.	All codes are available in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license	Methylation beta values were directly used used after preprocessing (batch removal, normalization, etc.).	61 predictors were used in the model. They were selected from top obesity-related pathways.	No. P is smaller than sample size.	No. The model directly use methylation data as input.	Based on default tuning strategy, p was implicitly selected based on the heuristic sqrt(predictors) = 7.81025.		8.0	0.0	677eef3ddd85bd59b303c812				Yuheng Du, Paula A. Benny, Yuchen Shao, Ryan J. Schlueter, Alexandra Gurary, Annette Lum-Jones, Cameron B Lassiter, Fadhl M. AlAkwaa, Maarit Tiirikainen, Dena Towner, W. Steven Ward, Lana X Garmire	GigaScience	2024	0.0	6.0		0.0	2025-01-09T16:59:00.197Z	2025-01-09T16:59:00.197Z	0c113f48-b8de-4364-9535-6a6d64345ca7	undenfined	0hgj0n9qo2
TITLE_MATCH 	41		Multi-omics Analysis of Umbilical Cord Hematopoietic Stem Cells from a Multi-ethnic Cohort of Hawaii Reveals the Intergenerational Effect of Maternal Pre-Pregnancy Obesity and Risk Prediction for Cancers		677eef3d69b4c4f2642360f8	GEO (public upon acceptance)	The DNA methylation data of the CD34+/CD38âˆ’/Linâˆ’ cord blood hematopoietic stem cells  (uHSCs) was directly obtained from a multi-ethnic cohort of 72 pregnant women (Npos = 34 obese; Nneg = 38 non-obese) from Kapiolani Medical Center for Women and Children in Honolulu, Hawaii (collected between 2016 and 2018). 	The data was randomly split into 8:2 train/test ratio. Training and test sets were independent.	A total of 72 samples were split into 8:2 train/test ratio, with 59 samples in the training set, 13 samples in the testing set. No separate validation set was used. The distribution of samples in train and test are balanced. 	4.0	0.0	Codes are available in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license	No	No, only one-time prediction performance metric was reported.	Balanced accuracy, accuracy, F1 score	5-fold cross-validation on obesity data. Prediction on external independent TCGA datatsets.	4.0	1.0	Code released in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license.	Run time 0.005 second on HPC.	This random forest obesity model offers some interpretability features based on feature importance scores. For example, cgxxxxxxxx has the highest feature importance in obesity classification.	Classification	4.0	0.0	Random forest model was used for normal vs obesity classification.	All codes are available in https://github.com/lanagarmire/COBRE_methyl with GPL-3.0 license	Methylation beta values were directly used used after preprocessing (batch removal, normalization, etc.).	61 predictors were used in the model. They were selected from top obesity-related pathways.	No. P is smaller than sample size.	No. The model directly use methylation data as input.	Based on default tuning strategy, p was implicitly selected based on the heuristic sqrt(predictors) = 7.81025.		8.0	0.0	677eef3ddd85bd59b303c812				Yuheng Du, Paula A. Benny, Yuchen Shao, Ryan J. Schlueter, Alexandra Gurary, Annette Lum-Jones, Cameron B Lassiter, Fadhl M. AlAkwaa, Maarit Tiirikainen, Dena Towner, W. Steven Ward, Lana X Garmire		2024	0.0	6.0		0.0	2025-01-08T21:33:49.196Z	2025-01-08T21:33:49.196Z	2993636d-6c3c-4a36-9659-b046faf0accb	undenfined	ko71gt58ln
TITLE_MATCH 	45		Opaque Ontology: Neuroimaging classification of ICD-10 Diagnostic Groups in the UK Biobank	https://doi.org/10.1093/gigascience/giae119	66213f37ded6e7820f749fde	No, the data is not released.	The source of data is UK Biobank, it has been used in various papers and recognized by the community. We have selected 5861 cases(see criteria details in the manuscripts) and 5861 carefully matched health controls.	A shuffle-split resampling scheme was used to subdivide the data.	The data is split into 100 stratified training (80%) and validation (20%) splits. The number of data points varies based on different diagnostic groups (see more details in the supplemental file). The distributions of the data types have not been plotted.	4.0	0.0	No	No	No	The performance metrics are reported by comparing the resulting 100 classification accuracies for each shuffle split against chance level (0.5).	We use cross-validation for method evaluation.	2.0	3.0	The source code is provided in https://github.com/tyo8/WAPIAW3 with MIT license.	The execution time for a single prediction is around 1 hour when using a high-performance computing cluster, this can vary depending on the feature set and diagnostic groups used.	The model is black box as it's using random forest.	The model is classification.	4.0	0.0	The main algorithm we used is random forest classification. The support vector classification and K-nearest neighbor classification are used for comparison between different machine learning algorithms.	The hyperparameter configurations are available in https://github.com/tyo8/WAPIAW3/classification_model/model_specification.py .	The categorical demographic and behavioral data is encoded by UK Biobank. Data were preprocessed by a standard scaler (i.e., z-scoring) before entry into the dimension reduction and predictive estimator pipelines.	We have 2 structural MRI feature sets, 16 resting state functional MRI feature sets, and 1 sociodemographic feature set (see more details in the supplements file).Feature selection was not performed.	To control overfitting, we (a) selected model hyperparameters in held-out sets and (b) trained 100 realizations of the model on shuffle-split data. The total number of parameters and hyperparameters across all investigated models is 365; no single model's number of parameters or hyperparameters (100-140) exceeded the number of training points in the smallest group (250).	No.	"For random forest classification, there are 5 parameters including number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), impurity criterion (fixed as ""Gini""), maximum tree depth(5, 10, 20, 40, full depth), fraction of features(1, 5, log2, sqrt, complete) for split and number of trees. The parameters are selected using grid-search, except the number of trees are fixed to 250 based on previous publications. This search was performed over a grid of 125 total hyperparameters.
For support vector classification, there are 3 parameters including number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), Margin regularization coefficient C(10^(-3),10^(-2),10^(-1),1,10^1,10^2,10^3), Kernel function(linear, rbf, sigmoid, poly). The parameters are selected using grid-search, which was performed over a grid of 140 total hyperparameters.
For K-nearest neighbor classification, there are 4 parameters including Number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), Number of neighbors(1, 5, 11, 18, 27), Neighbor weights(â€˜uniformâ€™, â€˜distanceâ€™), Distance metric(â€˜L1â€™, â€˜L2â€™, â€˜cosineâ€™). The parameters are selected using grid-search, which was performed over a grid of 150 total hyperparameters."	We did not employ regulariazation via pre-convergence early stopping.	7.0	1.0	66213f37b2aaa82f502f52d5	Not applicable			Ty Easley, Xiaoke Luo, Kayla Hannon, Petra Lenzini, Janine Bijsterbosch	GigaScience	2025	0.0	6.0		1.0	2024-04-18T15:41:43.382Z	2025-01-31T11:33:36.319Z	2c0f9bb8-4a98-4182-97e4-992de146b7ac	undenfined	e0eaxjld0h
TITLE_MATCH 	45		Opaque Ontology: Neuroimaging classification of ICD-10 Diagnostic Groups in the UK Biobank	Not applicable	6734fe97a96b586fdb2723fb	UK Biobank data are available following an access application process, for more information please see: https://www.ukbiobank.ac.uk/enable-your-research/apply-for-access. This research was performed under UK Biobank application number 47267.	The source of data is UK Biobank, it has been used in various papers and recognized by the community. We have selected 5861 cases(see criteria details in the manuscripts) and 5861 carefully matched health controls.	A shuffle-split resampling scheme was used to subdivide the data.	The data is split into 100 stratified training (80%) and validation (20%) splits. The number of data points varies based on different diagnostic groups (see more details in the supplemental file). The distributions of the data types have not been plotted.	4.0	0.0	No	No	No	The performance metrics are reported by comparing the resulting 100 classification accuracies for each shuffle split against chance level (0.5).	We use cross-validation for method evaluation.	2.0	3.0	The source code is provided in https://github.com/tyo8/WAPIAW3 with MIT license.	The execution time for a single prediction is around 1 hour when using a high-performance computing cluster, this can vary depending on the feature set and diagnostic groups used.	The model is black box as it's using random forest.	The model is classification.	4.0	0.0	The main algorithm we used is random forest classification. The support vector classification and K-nearest neighbor classification are used for comparison between different machine learning algorithms.	The hyperparameter configurations are available in https://github.com/tyo8/WAPIAW3/classification_model/model_specification.py .	The categorical demographic and behavioral data is encoded by UK Biobank. Data were preprocessed by a standard scaler (i.e., z-scoring) before entry into the dimension reduction and predictive estimator pipelines.	We have 2 structural MRI feature sets, 16 resting state functional MRI feature sets, and 1 sociodemographic feature set (see more details in the supplements file).Feature selection was not performed.	To control overfitting, we (a) selected model hyperparameters in held-out sets and (b) trained 100 realizations of the model on shuffle-split data. The total number of parameters and hyperparameters across all investigated models is 365; no single model's number of parameters or hyperparameters (100-140) exceeded the number of training points in the smallest group (250).	No.	"For random forest classification, there are 5 parameters including number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), impurity criterion (fixed as ""Gini""), maximum tree depth(5, 10, 20, 40, full depth), fraction of features(1, 5, log2, sqrt, complete) for split and number of trees. The parameters are selected using grid-search, except the number of trees are fixed to 250 based on previous publications. This search was performed over a grid of 125 total hyperparameters.
For support vector classification, there are 3 parameters including number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), Margin regularization coefficient C(10^(-3),10^(-2),10^(-1),1,10^1,10^2,10^3), Kernel function(linear, rbf, sigmoid, poly). The parameters are selected using grid-search, which was performed over a grid of 140 total hyperparameters.
For K-nearest neighbor classification, there are 4 parameters including Number of PCs(rank/100, rank/32, rank/10, rank/3.2, full rank), Number of neighbors(1, 5, 11, 18, 27), Neighbor weights(â€˜uniformâ€™, â€˜distanceâ€™), Distance metric(â€˜L1â€™, â€˜L2â€™, â€˜cosineâ€™). The parameters are selected using grid-search, which was performed over a grid of 150 total hyperparameters."	We did not employ regulariazation via pre-convergence early stopping.	7.0	1.0	66213f37b2aaa82f502f52d5	Not applicable			Ty Easley, Xiaoke Luo, Kayla Hannon, Petra Lenzini, Janine Bijsterbosch	Gigascience	Not applicable	0.0	6.0		0.0	2024-11-13T19:31:35.768Z	2024-11-13T19:31:35.768Z	1e45a9d1-0130-4240-8ef5-4d4593490be6	undenfined	m686e5vx1l
TITLE_MATCH DOI_MATCH 	45		P2Rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure	10.1186/s13321-018-0285-8	65e843f41502715bfe53cd1e	"All the mentioned datasets are selected from previous studies and are publicly available.
Datasets are both mentioned through referencing to other studies and accessible through the URL: https://github.com/rdk/p2rank-datasets"	"Training: CHEN11â€”a dataset of 251 proteins harboring 476 ligands introduced in LBS prediction benchmarking study.

Optimization and validation: JOINEDâ€”consists of structures from several smaller datasets used in previous studies (B48/U48, B210, DT198, ASTEX) joined into one larger dataset. you can find the details below:
B48/U48â€”Datasets that contain a set of 48 proteins in a bound and unbound state.
B210â€”a benchmarking dataset of 210 proteins in bound state.
DT198â€”a dataset of 198 drug-target complexes.
ASTEXâ€”Astex Diverse set is a collection of 85 proteins that was introduced as a benchmarking dataset for molecular docking methods."	"Datasets for training and validation purposes are selected arbitrarily.
Testing dataset is not mentioned.
Only training set is mentioned to be non-redundant without explicit mention on the cutout identity percentage."	"2 data splits were utilized:
Training: CHEN11 dataset composed of 251 data points.
Optimization and validation: JOINED dataset composed of an overall  541 data points.
All the data points are positive examples of protein-ligand complexes."	4.0	0.0	No	The method was compared to other publicly available methods in the 3 mentioned metrics.	"The confidence intervals are only reported for ""Average prediction time per protein"".
While the method is reported to perform best in the first 2 mentioned metrics, which is not the case in terms of prediction time, the difference is not substantial."	"Instead of the conventional performance metrics, 3 reported metrics are as follows:
1. Identification success rate [%] measured by DCCcriterion (distance from pocket center to closest ligand atom) with 4 Ã… threshold.
2. Average total number of binding sites predicted per protein by on a given dataset.
3. Average time required for prediction on a single protein."	The model was evaluated on two independent datasets. Disjunct with data points present in training and validation set.	4.0	1.0	"The software and the source code is publicly available through the provided link to the GitHub repository of the software.
It is a stand-alone executable software, they currently have a web server, but at the time of publication the web server was under development.
URL: https://github.com/rdk/p2rank
Licence: MIT "	It is mentioned that it requires under 1 s for prediction on one protein.	"The model has 200 trees, each grown with no depth limit using 6 features, this makes it challenging to interpret due to the sheer number of trees and their depth.
However, it is still possible to obtain feature importance scores."	It is a regression model outputting numbers between 0 and 1.	4.0	0.0	"The used algorithm is a Random Forest Regression model.
The model is not novel."	The model is accessible through the URL of the GitHub repository.	The model takes vectors of 35 numerical features as input.	"Every vector is composed of 35 numerical features.
No mention on feature selection."	"The clear number of parameters is not mentioned.
No assessment on over/under fitting was mentioned."	The algorithm is not a meta-predictor.	"Beside the hyper-parameters of Random Forest, the algorithm uses other parameters and ""cut-of's"", ""thresholds"", ""protrusion radius"", are explicitly mentioned.
It is mentioned that only hyper-parameters are optimized on a separate dataset from training set.
The method of optimization is not mentioned."	A validation set was used to optimize the parameters before training on the training set.	8.0	0.0	65e73fdb92c76639b8e309f3	30109435			Radoslav KrivÃ¡k, David Hoksza	Journal of cheminformatics	2018	6.0	0.0		1.0	2024-03-06T10:22:44.950Z	2024-03-06T10:35:57.616Z	9bd31f10-4479-4940-810c-3e6a4bcfb1e3	undenfined	iblymem5cf
TITLE_MATCH DOI_MATCH 	38		P2Rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure	10.1186/s13321-018-0285-8	6638a2f7b30933003cc215b9	"All the mentioned datasets are selected from previous studies and are publicly available.
Datasets are both mentioned through referencing to other studies and accessible through the URL: https://github.com/rdk/p2rank-datasets"	"Training:  CHEN11â€”a dataset of 251 proteins harboring 476 ligands introduced in LBS prediction benchmarking study.
Optimization and validation: JOINEDâ€”consists of structures from several smaller datasets used in previous studies (B48/U48, B210, DT198, ASTEX) joined into one larger dataset. you can find the details below:
B48/U48â€”Datasets that contain a set of 48 proteins in a bound and unbound state.
B210â€”a benchmarking dataset of 210 proteins in bound state.
DT198â€”a dataset of 198 drug-target complexes.
ASTEXâ€”Astex Diverse set is a collection of 85 proteins that was introduced as a benchmarking dataset for molecular docking methods."	"Datasets for training and validation purposes are selected arbitrarily.
Testing dataset is not mentioned.
Only training set is mentioned to be non-redundant without explicit mention on the cutout identity percentage."	"2 data splits were utilized:
Training: CHEN11 dataset composed of 251 data points.
Optimization and validation: JOINED dataset composed of an overall  541 data points.
All the data points are positive examples of protein-ligand complexes."	4.0	0.0						0.0	5.0	The software and the source code is publicly available through the provided link to the GitHub repository of the software. It is a stand-alone executable software, they currently have a web server, but at the time of publication the web server was under development. URL: https://github.com/rdk/p2rank. Licence: MIT	It is mentioned that it requires under 1 s for prediction on one protein.	The model has 200 trees, each grown with no depth limit using 6 features, this makes it challenging to interpret due to the sheer number of trees and their depth. However, it is still possible to obtain feature importance scores.	It is a regression model outputting numbers between 0 and 1.	4.0	0.0	"The used algorithm is a Random Forest Regression model.
The model is not novel."	URL: https://github.com/rdk/p2rank. Licence: MIT	The model takes vectors of 35 numerical features as input.	"Every vector is composed of 35 numerical features.
No mention on feature selection."	The clear number of parameters is not mentioned. No assessment on over/under fitting was mentioned.	The algorithm is not a meta-predictor.	"Beside the hyper-parameters of Random Forest, the algorithm uses other parameters and ""cut-of's"", thresholds"", ""protrusion radius"", are explicitly mentioned.
It is mentioned that only hyper-parameters are optimized on a separate dataset from training set.
The method of optimization is not mentioned."	A validation set was used to optimize the parameters before training on the training set.	8.0	0.0	65e73fdb92c76639b8e309f3	30109435			Radoslav KrivÃ¡k, David Hoksza	Journal of cheminformatics	2018				1.0	2024-05-06T09:29:27.218Z	2024-05-06T09:29:27.218Z	71898e53-9df0-464d-9e78-bb95274221ec	undenfined	toe45v1h7a
TITLE_MATCH 	45		PlasGO: enhancing GO-based function prediction for plasmid-encoded proteins based on genetic structure	10.1093/gigascience/giae104	67333632a96b586fdb27231e	You can access to the curated RefSeq dataset with the link http://zenodo.org/records/14005015 (dataset.tar.gz), which is licensed under the CC0 license.	The data is all curated from the NCBI RefSeq database. There are 173,666, 99,945, 28,081 data points for the three GO categories, MF, BP, and CC, repectively (three sub-tasks). The minimum positive points of each label is set to 50. Because we formulate the plasmid protein function prediction as a multi-class, multi-label classification on 377 labels (172 MF labels + 174 BP labels + 31 CC labels), we can't show all the detailed number of positive/negative points for the 377 binary classifications here. However, you can check more detailed information from the Supplementary Figure S3 in the manuscript. Because we manually curated the dataset based on the RefSeq database, the protein sequences and their GO annotations are very reliable.	As described above, the generated test set can serve as a novel protein set compared to the training set. Specficially, the minimum normalized edit distances between the training set and the test set for most of the labels are larger than 0.7 (as shown in Supplementary Figure S4 in the manuscript).	There are three sub-tasks for our tool, namely the GO term classifications on the MF, BP, and CC categories, respectively. The detailed training/validation/test ratios for the three sub-tasks are 99,806/56,491/17,369, 60,143/29,768/10,034, and 21,228/4,045/2,808. You might find that the splitting ratio is not fixed, because we curated this dataset by simulating the novel protein function prediction scenario. Specifically, for each GO category, we allocated 10% of the most recently released proteins with GO annotations as the test set. Additionally, we ensure that the novel test set significantly differs from the training set in terms of protein sequences. Therefore, among the remaining 90% annotated proteins, those lacking significant alignments (E-value>1e-3) to the test set were assigned to the training set, while others were assigned to the validation set. This splitting strategy poses significant challenges for both PlasGO. Because of the large number of the binary classifications (377), we can't plot all the distributions. However, the data type distributions are roughly consistent for the training, validation, and test sets.	4.0	0.0	Yes, you can check the dataset used for benchmark experiments via the Zenodo repository [14005015] at http://zenodo.org/records /14005015.	Because the RefSeq test set we curated is to simulate the novel protein function prediction scenario, we didn't include any alignment-based tools for benchmarking, such as Diamond and BLASTP. Additionally, we conducted the benchmarking experiments with six learning-based state-of-the-art tools, including DeepGOPlus, PFresGO, TALE, DeepSeq, TM-Vec, and CaLM. For a fair comparison, we retrained all of them using our curated RefSeq dataset (training/validation/test). The results showed that PlasGO performed the best on all three GO categories (Figure 6 in the manuscript).	No. The two evaluation metrics do not relate to confidence intervals. For more details, please refer to the CAFA challanges.	"We used the two commonly used metrics in the CAFA challenge, namely the protein-centric Fmax and the term-centric AUPR. For more details of the two metrics, please refer to the ""Experimental setup"" section in the manuscript."	We conducted a series of rigorous experiments to evaluate the performance of PlasGO. All of them can be checked in the manuscript's Results part. Specifically, they include 1) the performance on the novel RefSeq test set, 2) two ablation studies for validating PlasGOâ€™s design rationale, 3) visualization of the PlasGO embeddings for interpreting the BERT module, 4) identification of elusive GO term labels to improve the precision of PlasGO, 5) labels of different frequencies and confidence scores which show the performance stability on different labels and the effectiveness of the learned confidence scores, 6) application: automatic GO prediction for unannotated plasmid-encoded proteins in RefSeq, which pre-annotates high-confidence GO terms for all available plasmid proteins (678,197), 7) case study: annotations for two well-studied conjugative plasmids associated with AMR.	5.0	0.0	Yes. You can refer to the GitHub repository https://github.com/Orin-beep/PlasGO for most recent updates. Licensed under the MIT license.	The running speed of PlasGO is very fast. For the 365 plasmid proteins in the example data, it only took 1m48s for the GO term prediction on one NVIDIA GeForce RTX 2080 Ti. 	The model is interpretable. We mainly interpreted the global BERT module of PlasGO by comparing the raw embeddings generated from the ProtTrans model and the contextualized embeddings from the global BERT module. You can check the results from Figure 7 in the manuscript. The results showed thats suggests that PlasGO effectively captures the latent features associated with plasmid-specific functions.	Classification.	4.0	0.0	We employed deep learning in our tool. PlasGO consists of three sub-modules, a pre-trained protein language model, a global BERT model, and a classifier module incorporating a self-attention confidence weighting mechanism. The core part of PlasGO is the BERT model to learn the global context within plasmid sentences. We used the BERT model because we formuated plasmids as a language defined on the protein token set, and BERT is one of the state-of-the-art language models.	Yes, you can check PlasGO's default models (models.tar.gz) via the Zenodo repository [14005015] at http://zenodo.org/records/14005015, which is licensed under the CC0 license.	We employed the powerful foundation protein languange model, ProtTrans, to encode our input protein sequences. It can generate biologically meaningful embedding for each plasmid-encoded protein. Then, the raw input embeddings can greatly improve the performance of GO term prediction through transfer learning. 	"As described in V.3, the only input is the plasmid protein amino acid (AA) sequences. Then, ProtTrans is employed to generate the raw embeddings for each protein (preprocessing). By capturing the
semantic meaning of individual AA tokens and their contextual relationships within the protein sequence, ProtTrans will generate per-protein embeddings (1024 dimensions) as the input to the subsequent BERT module."	We have 89,835 plasmid sentences in total for training the simplified global BERT model with token classification task. As shown above, the simplest BERT model (CC) have hyperparameter of hidden_size=512, head_num=8, and layer_num=2. The corpus size (89,835) is sufficient to fit the BERT module. In addition, there are several overfitting strategies in the PlasGO models, which will be listed in V.7.	No. There is no data from other ML algorithms as input to PlasGO.	The parameter number is 13340504 for MF model, 13698908 for BP model, and 6891070 for CC model. Actually, the three models are all simple BERT models with hidden_size = 512, head_num = 8. Besides, we used 4 Transformer layers for MF and BP, and 2 Transformer layers for CC, considering the relatively smaller dataset size and label size for CC.	Yes. 1) Dropout layers adopted on both the classifier FC layer and also the attention blocks in the BERT model. 2) Validation set is employed to prevent overfitting. 3) Layer Normalization is employed for each Transformer block and also the token embedding layer. 4) A rank regularization method is utilized in the classifier module to promote the ability of the model to distinguish low-confidence and high-confidence predictions. For more details, please check the Supplementary Section S1.  	8.0	0.0	668fc3487089c469b454be8b	Pending			Yongxin Ji, Jiayu Shang, Jiaojiao Guan, Wei Zou, Herui Liao, Xubo Tang and Yanni Sun	GigaScience	2024	0.0	6.0		1.0	2024-11-12T11:04:18.047Z	2025-01-31T11:36:01.915Z	07fc5bdf-9ef7-423d-9bf5-acb545419ac3	undenfined	pugj4ycv1x
TITLE_MATCH 	22		PlasGO: enhancing GO-based function prediction for plasmid-encoded proteins based on genetic structure	Not published yet	668fc60a37ea6fa797a6ca74					0.0	4.0						0.0	5.0					0.0	4.0									0.0	8.0	668fc3487089c469b454be8b	Not published yet			Yongxin Ji, Jiayu Shang, Jiaojiao Guan, Wei Zou, Herui Liao, Xubo Tang and Yanni Sun	Not published yet	2024				0.0	2024-07-11T11:46:18.121Z	2024-07-11T11:46:18.121Z	7fe699c2-4bae-4576-b1f5-5e081868177a	undenfined	sahwo4cflw
TITLE_MATCH DOI_MATCH 	29	3/22/2022 13:13:16	Prediction of plant lncRNA by ensemble machine learning classifiers.	10.1186/s12864-018-4665-2		(http://lncrnadb.org), (http://www.cuilab.cn/lncrnadisease), (http://www.ensembl.org), (https://araport-dev.tacc.utexas.edu)	positive: lncRNAdb v2.0, lncRNAdisease , total of 436 unique, validated lncRNA sequences // negative: Ensembl, Araportv11 	variety of training datasets was used to maximize model diversity and samples were equally and randomly selected to get a balanced training	 8 different combinations of negative data from multiple species			no	compared to GreeNC (uses a transcript filtering method, rather than a machine learning approaches).	(qualitative explanation) An important consideration of this tool is that it is not constrained by preconceived rules that may or may not appropriately classify lncRNA properties and the stacking generalizer model based on gradient boosting models will facilitate lncRNA identification without imposing arbitrary rules for lncRNA detection.	 accuracy, sensitivity, specificity and AUC values	10-fold cross-validation			https://github.com/gbgolding/crema	measured in minutes		classification  binary if a transcript was or was not predicted as a lncRNA and stacking with logistic regression for ensemble method			 random forest and gradient boosting. Also ensemble method from different classifiers, Two separate values were used for the creation of each ensemble model â€“ scores sij and predictions pij where i represents model number and j transcript.  The four ensemble approaches included both algebraic combiners and voting methods as non-trainable methods, and a stacking generalizer as a meta-learner.	no	Diamond alignment in SwissProt database	9 features were extracted using a combination of custom Python scripts and known software CPAT, Diamond, RepeatMasker.	not applicable	no	 gradient boosting parameters :  learning_rate, max_depth, subsample, n_estimators.  Random forest parameters: only change from default parameters being n_estimators and min_samples_leaf.	not applicable			6312169df3794236aa987a1b	29720103	PMC5930664		Simopoulos CMA, Weretilnyk EA, Golding GB.	BMC genomics	2018									
TITLE_MATCH DOI_MATCH 	28	3/1/2022 19:41:49	Prediction of plant lncRNA by ensemble machine learning classifiers.	10.1186/s12864-018-4665-2		Yes, supplementary material.	Positive data from lncRNAdb v2, lncRNAdisease. Negative data from Ensembl, Araport v11. Npos=436 lncRNA sequences. Nneg=? (total number of negative not known). Not previously used.	Not available	Ten different indipendent 10-fold cross-validation performed. N_pos and N_neg for training and testing unknown. 			Not available	GreeNC method (transcript filtering, no machine-learning), CPAT	No confidence reported	Sensitivity, Specificity, Accuracy, ROC-AUC	Cross-validation. No indipendent datasets			Code not available		Transparent. Random forests provide feature importance	Classification			Stochastic gradient boosting and random forests	Configuration and hyper-parameter configuration available in the main text (Table 2)	Eleven features for describing complete lncRNA sequence: mRNA length, ORF length, GC%, Fickett score, hexamer score, alignment identity in SwissProt database, length of alignment in SwissProt database, proportion of alignment length and mRNA length (alignment length:mRNA length), proportion of alignment length and ORF length (alignment length:ORF), presence of transposable element, and sequence percent divergence from transposable element.	f=11. Feature selection by Recursive feature elimination		No	Not known	No			6603042f92c76639b849e69f	29720103	PMC5930664		Simopoulos CMA, Weretilnyk EA, Golding GB.	BMC genomics	2018									
TITLE_MATCH DOI_MATCH 	43		Protein-protein and protein-nucleic acid binding site prediction via interpretable hierarchical geometric deep learning	xxx	6696385237ea6fa797a6cbdf	"All the binding proteins are from public databases, Protein-Protein Docking Benchmark 5.5 (DBD 5.5), Dockground and BioLiP2 database. But the data splits were our  random sets.
"	"The protein-protein interaction datasets were obtained from Protein-Protein Docking Benchmark 5.5 (DBD 5.5)[1] and Dockground[2] database. These datasets consist of non-redundant, high-resolution structures, including both bound and unbound structures for each protein complex. Considering that protein conformation tends to change to varying degrees when interacting with other molecules in the cell, it makes more sense to use the unbound structures prior to the interaction to train and test the model. Finally, we obtained 1,251 protein-binding proteins as a dataset.
Then we constructed a baseline dataset of nucleic acid binding proteins from the BioLiP2[3] database, which collects biologically relevant ligand-protein interactions that are structurally resolved in complexes. DNA/RNA-binding proteins were collected from the dataset as of October 7, 2023, and contain a total of 42,457 DNA-protein complex entries and 146,306 RNA-protein complex entries. First, we selected high-resolution structures with a resolution of less than 8.0 Ã…. The resolution can reflect the accuracy of protein structure analysis; higher resolution indicates finer protein structure, while lower resolution indicates that the atomic structure in the crystal is more difficult to be clearly resolved and localized. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. Finally, we kept 1,871 RNA-binding proteins and 1,101 DNA-binding proteins.

[1]Vreven, T. et al. Updates to the Integrated Protein-Protein Interaction Benchmarks: Docking Benchmark Version 5 and Affinity Benchmark Version 2. J Mol Biol 427, 3031-3041, doi:10.1016/j.jmb.2015.07.016 (2015).
[2]Kundrotas, P. J. et al. Dockground: A comprehensive data resource for modeling of protein complexes. Protein Sci 27, 172-181, doi:10.1002/pro.3295 (2018).
[3]Zhang, C., Zhang, X., Freddolino, P. L. & Zhang, Y. BioLiP2: an updated structure database for biologically relevant ligand-protein interactions. Nucleic Acids Res 52, D404-D412, doi:10.1093/nar/gkad630 (2024)."	"The training and test sets are independent. We randomly divided the data we selected from the datasets into a training dataset and a test dataset at a ratio of 0.2. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. 
"	"We obtained 1,251 protein-binding proteins, 1,871 RNA-binding proteins and 1,101 DNA-binding proteins as three binding datasets, and then randomly divided the data into a training dataset and a test dataset at a ratio of 0.2. In addition, we further divided the training data into a training dataset and a validation dataset. Eventually we obtained 1001 protein-binding proteins, 881 DNA-binding proteins and 1497 RNA-binding proteins as the training set; 250 protein-binding proteins, 220 DNA-binding proteins and 374 RNA-binding proteins as the test set. The details of the dataset are here:
Type	Dataset	       Nproteina	     Nposb	Nnegc	PNratiod
Protein	P-1001_Train	   1001	     25172	        264564	0.095
	        P-250_Test	   250	     6386	        73391	0.087
DNA	DNA-881_Train   881	     9962	        251759	0.04
	        DNA-220_Test	   220	     2537	        68401	0.037
RNA	        RNA-1497_Train 1497	     33444	        384358	0.087
	        RNA-374_Test	   374	     9331	        110259	0.085
aNumber of proteins.
bNumber of binding residues.
cNumber of non-binding residues.
dPNratio= Npos/Nneg."	4.0	0.0	We set the evaluation method and trained model in https://github.com/Wssduer/GraphRBF.	"GraphRBF model was compared with several state-of-the-art methods on benchmark datasets. This comparison included structure-based methods such as SPPIDER, GraphPPIS, MaSIF-site, and ScanNet, as well as sequence-based methods like TargetDNA, hybridRNAbind, DeepDISOBind, and DRNApred. The performance was evaluated using various metrics, including Recall, Precision, F1-Score, Matthews Correlation Coefficient, AUC, and AUPRC.

Additionally, the paper mentioned comparisons with simpler baseline models, such as sequence-based methods that use only sequence information and structure-based methods that rely solely on the 3D structure of proteins. Through these comparisons, the GraphRBF model demonstrated superior performance in predicting protein binding sites.On the protein-protein binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.028, 0.108, 0.055, and 0.066 higher than the second highest value, with relative improvements achieving 3.5%, 41.9%, 16.2%, and 24% respectively. On the protein-DNA (RNA) binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.017 (0.076), 0.052 (0.106), 0.061 (0.087), and 0.049 (0.095) higher than the second highest value, with relative improvements reaching 1.9% (9.2%), 18.0% (26.1%), 17.4% (20.0%), and 13.8% (24.7%), respectively. "	We did not set confidence intervals but instead directly compared the various metrics of our model with other models (using the same computational methods), and obtained the best results, which sufficiently demonstrates the superiority of our model.	"A variety of performance metrics are reported to evaluate the GraphRBF model, including:
Recall
Precision
F1-Score
Matthews Correlation Coefficient (MCC)
Area Under the Receiver Operating Characteristic Curve (AUC)
Area Under the Precision-Recall Curve (AUPRC)
These metrics are standard in the fields of bioinformatics and machine learning and provide a comprehensive assessment of the predictive performance of the model. Recall and precision measure the model's ability to identify positive and negative instances, the F1-Score is the harmonic mean of these two metrics, MCC provides a correlation measure between -1 and 1, and AUC and AUPRC offer an overall measure of the model's performance, especially useful when dealing with imbalanced datasets.

Regarding the representativeness of this set of performance metrics, they are consistent with those used in the literature for similar tasks and can be considered representative. These metrics allow researchers to make comparisons with existing methods and demonstrate the performance of the GraphRBF model in predicting protein binding sites."	GraphRBF method was evaluated using an independent test dataset. This approach involved training and testing the model on datasets for protein-protein and protein-nucleic acid interaction sites separately, and comparing it with other leading prediction methods. A variety of evaluation metrics were used in the assessment, such as Recall, Precision, F1-Score, Matthews Correlation Coefficient (MCC), Area Under the Receiver Operating Characteristic Curve (AUC), and Area Under the Precision-Recall Curve (AUPRC). Moreover, the GraphRBF model was applied to a novel protein structure, the spike protein of the SARS-CoV-2 Omicron variant, and the predictions were analyzed and visualized, offering additional validation of the method.	5.0	0.0	Source codes are in https://github.com/Wssduer/GraphRBF. We also have a web server http://www.liulab.top/GraphRBF.	Prediction time is about 3-5 minutes for small proteins and 10-30 minutes for very huge proteins. For training, the model needs 0.5-1 hour for one epoch.	GraphRBF model is interpretable rather than a black box. It provides not only predictive results but also insights into its learned representations and decision-making processes.	The GraphRBF model is a classification model. It is designed to classify whether each residue in a protein is a binding site or not, which is a categorical outcomeâ€”a fundamental aspect of classification tasks. The model uses learned features from protein structures to make these classifications, predicting the likelihood of a residue being part of a binding site, which is then typically binary or multiclass labeled (e.g., binding site or non-binding site).	4.0	0.0	"The ML algorithm class used is Deep Learning and supervised learning, specifically an interpretable hierarchical geometric deep learning model known as GraphRBF. This is a novel algorithm because it integrates both Graph Neural Networks (GNNs) and Radial Basis Function Neural Networks (RBFNNs), with an enhanced graph neural network designed to describe physicochemical information interactions between amino acid residues and the introduction of a prioritized radial basis function to characterize the spatial distribution of residues.

The novel algorithm was chosen over better-known alternatives because it can more comprehensively represent the binding patterns of protein residues, including their spatial distribution and physicochemical information interactions, which could not be achieved by previous methods. GraphRBF offers enhanced interpretability and has demonstrated superior performance in predicting protein binding sites compared to existing state-of-the-art methods."	We reported these information in our paper(under review) and https://github.com/Wssduer/GraphRBF.	Multiple types of amino acid features are extracted that include atomic features of residues, secondary structure encoding, evolutionary information, and residue type encoding. Seven atomic features are extracted for each heavy atom belonging to the target residue based on the PDB file: atom mass, B-factor, whether it is a residue side-chain atom, electronic charge, the number of hydrogen atoms bonded to it, whether it is in a ring, and the van der Waals radius of the atom. The DSSP tool is applied to generate secondary structure features of residues including a residue water-exposed surface, five bond and torsion angles, and eight one-hot encoded secondary structure with eight states. For the evolutionary information, PSI-BLAST is used to search the Uniref 90 database for homologous sequences with three iterations and e-value<10^âˆ’3 to generate the position-specific scoring matrix (PSSM) with 20 dimensions. HHblits is implemented to search the uniclust30 database to generate an HMM matrix of the query sequence with 30 dimensions. For the residue type encoding, the 20 types of amino acids are ordered as ACDEFGHIKLMNPQRSTVWY, and an 20-dimension one-hot coding of each residue is generated accordingly. In conclusion, a node feature matrix of size LÃ—91 is constructed for each protein, in which L is the length of the protein. In addition, the original features are min-max normalized before training.	Multiple types of amino acid features are extracted that include atomic features of residues, secondary structure encoding, evolutionary information, and residue type encoding. Seven atomic features are extracted for each heavy atom belonging to the target residue based on the PDB file: atom mass, B-factor, whether it is a residue side-chain atom, electronic charge, the number of hydrogen atoms bonded to it, whether it is in a ring, and the van der Waals radius of the atom. The DSSP tool is applied to generate secondary structure features of residues including a residue water-exposed surface, five bond and torsion angles, and eight one-hot encoded secondary structure with eight states. For the evolutionary information, PSI-BLAST is used to search the Uniref 90 database for homologous sequences with three iterations and e-value<10^âˆ’3 to generate the position-specific scoring matrix (PSSM) with 20 dimensions. HHblits is implemented to search the uniclust30 database to generate an HMM matrix of the query sequence with 30 dimensions. For the residue type encoding, the 20 types of amino acids are ordered as ACDEFGHIKLMNPQRSTVWY, and an 20-dimension one-hot coding of each residue is generated accordingly. In conclusion, a node feature matrix of size LÃ—91 is constructed for each protein, in which L is the length of the protein. In addition, the original features are min-max normalized before training.	p is much larger. We uesd a smaller number of hidden later parameters(128) and network layers(1 or 2), as well as using dropout regularization, are effective methods to prevent overfitting.	The data and features are selected by ourselves from the public datasets and not from other algorithms.	"We conduct hyperparameter tuning experiments for GraphRBF and select the best hyperparameters on the validation set. 
Here are the hyperparameters:
Radius of the structural neighborhood: it defines the point cloud within the neighborhood of the target residue, and its unit is Ã….(20)
The distance threshold to get the adjacency matrix: it defines the threshold for concatenating edges between residues, and its unit is Ã….(10)
The number of GDG kernels in each filter.(32)
The number of filters used in PRBFNN.(64)
The number of representation layer. Layer 0 stands for the encoder layer.(1 for protein-DNA/RNA and 2 for protein-protein)
The dimension  of encoded position information vector using local coordinates.(64)
The dimension of encoded node, edge and graph feature vector. (128)
Learning rate for training the deep model.(0.0001)
Batch size for training deep model.(256)
Training epochs.(60)"	Dropout and early stopping.	8.0	0.0	66921d947089c469b45e7e98	xxx			xxx	xxx	xxx				0.0	2024-07-16T09:07:30.666Z	2024-07-16T09:07:30.666Z	ee1f7ca8-6fe6-443f-bc98-2dd5f6312e10	undenfined	m3ysuvjg7v
TITLE_MATCH DOI_MATCH 	43		Protein-protein and protein-nucleic acid binding site prediction via interpretable hierarchical geometric deep learning	xxx	669638a137ea6fa797a6cbe3	"All the binding proteins are from public databases, Protein-Protein Docking Benchmark 5.5 (DBD 5.5), Dockground and BioLiP2 database. But the data splits were our  random sets.
"	"The protein-protein interaction datasets were obtained from Protein-Protein Docking Benchmark 5.5 (DBD 5.5)[1] and Dockground[2] database. These datasets consist of non-redundant, high-resolution structures, including both bound and unbound structures for each protein complex. Considering that protein conformation tends to change to varying degrees when interacting with other molecules in the cell, it makes more sense to use the unbound structures prior to the interaction to train and test the model. Finally, we obtained 1,251 protein-binding proteins as a dataset.
Then we constructed a baseline dataset of nucleic acid binding proteins from the BioLiP2[3] database, which collects biologically relevant ligand-protein interactions that are structurally resolved in complexes. DNA/RNA-binding proteins were collected from the dataset as of October 7, 2023, and contain a total of 42,457 DNA-protein complex entries and 146,306 RNA-protein complex entries. First, we selected high-resolution structures with a resolution of less than 8.0 Ã…. The resolution can reflect the accuracy of protein structure analysis; higher resolution indicates finer protein structure, while lower resolution indicates that the atomic structure in the crystal is more difficult to be clearly resolved and localized. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. Finally, we kept 1,871 RNA-binding proteins and 1,101 DNA-binding proteins.

[1]Vreven, T. et al. Updates to the Integrated Protein-Protein Interaction Benchmarks: Docking Benchmark Version 5 and Affinity Benchmark Version 2. J Mol Biol 427, 3031-3041, doi:10.1016/j.jmb.2015.07.016 (2015).
[2]Kundrotas, P. J. et al. Dockground: A comprehensive data resource for modeling of protein complexes. Protein Sci 27, 172-181, doi:10.1002/pro.3295 (2018).
[3]Zhang, C., Zhang, X., Freddolino, P. L. & Zhang, Y. BioLiP2: an updated structure database for biologically relevant ligand-protein interactions. Nucleic Acids Res 52, D404-D412, doi:10.1093/nar/gkad630 (2024)."	"The training and test sets are independent. We randomly divided the data we selected from the datasets into a training dataset and a test dataset at a ratio of 0.2. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. 
"	"We obtained 1,251 protein-binding proteins, 1,871 RNA-binding proteins and 1,101 DNA-binding proteins as three binding datasets, and then randomly divided the data into a training dataset and a test dataset at a ratio of 0.2. In addition, we further divided the training data into a training dataset and a validation dataset. Eventually we obtained 1001 protein-binding proteins, 881 DNA-binding proteins and 1497 RNA-binding proteins as the training set; 250 protein-binding proteins, 220 DNA-binding proteins and 374 RNA-binding proteins as the test set. The details of the dataset are here:
Type	Dataset	       Nproteina	     Nposb	Nnegc	PNratiod
Protein	P-1001_Train	   1001	     25172	        264564	0.095
	        P-250_Test	   250	     6386	        73391	0.087
DNA	DNA-881_Train   881	     9962	        251759	0.04
	        DNA-220_Test	   220	     2537	        68401	0.037
RNA	        RNA-1497_Train 1497	     33444	        384358	0.087
	        RNA-374_Test	   374	     9331	        110259	0.085
aNumber of proteins.
bNumber of binding residues.
cNumber of non-binding residues.
dPNratio= Npos/Nneg."	4.0	0.0	We set the evaluation method and trained model in https://github.com/Wssduer/GraphRBF.	"GraphRBF model was compared with several state-of-the-art methods on benchmark datasets. This comparison included structure-based methods such as SPPIDER, GraphPPIS, MaSIF-site, and ScanNet, as well as sequence-based methods like TargetDNA, hybridRNAbind, DeepDISOBind, and DRNApred. The performance was evaluated using various metrics, including Recall, Precision, F1-Score, Matthews Correlation Coefficient, AUC, and AUPRC.

Additionally, the paper mentioned comparisons with simpler baseline models, such as sequence-based methods that use only sequence information and structure-based methods that rely solely on the 3D structure of proteins. Through these comparisons, the GraphRBF model demonstrated superior performance in predicting protein binding sites.On the protein-protein binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.028, 0.108, 0.055, and 0.066 higher than the second highest value, with relative improvements achieving 3.5%, 41.9%, 16.2%, and 24% respectively. On the protein-DNA (RNA) binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.017 (0.076), 0.052 (0.106), 0.061 (0.087), and 0.049 (0.095) higher than the second highest value, with relative improvements reaching 1.9% (9.2%), 18.0% (26.1%), 17.4% (20.0%), and 13.8% (24.7%), respectively. "	We did not set confidence intervals but instead directly compared the various metrics of our model with other models (using the same computational methods), and obtained the best results, which sufficiently demonstrates the superiority of our model.	"A variety of performance metrics are reported to evaluate the GraphRBF model, including:
Recall
Precision
F1-Score
Matthews Correlation Coefficient (MCC)
Area Under the Receiver Operating Characteristic Curve (AUC)
Area Under the Precision-Recall Curve (AUPRC)
These metrics are standard in the fields of bioinformatics and machine learning and provide a comprehensive assessment of the predictive performance of the model. Recall and precision measure the model's ability to identify positive and negative instances, the F1-Score is the harmonic mean of these two metrics, MCC provides a correlation measure between -1 and 1, and AUC and AUPRC offer an overall measure of the model's performance, especially useful when dealing with imbalanced datasets.

Regarding the representativeness of this set of performance metrics, they are consistent with those used in the literature for similar tasks and can be considered representative. These metrics allow researchers to make comparisons with existing methods and demonstrate the performance of the GraphRBF model in predicting protein binding sites."	GraphRBF method was evaluated using an independent test dataset. This approach involved training and testing the model on datasets for protein-protein and protein-nucleic acid interaction sites separately, and comparing it with other leading prediction methods. A variety of evaluation metrics were used in the assessment, such as Recall, Precision, F1-Score, Matthews Correlation Coefficient (MCC), Area Under the Receiver Operating Characteristic Curve (AUC), and Area Under the Precision-Recall Curve (AUPRC). Moreover, the GraphRBF model was applied to a novel protein structure, the spike protein of the SARS-CoV-2 Omicron variant, and the predictions were analyzed and visualized, offering additional validation of the method.	5.0	0.0	Source codes are in https://github.com/Wssduer/GraphRBF. We also have a web server http://www.liulab.top/GraphRBF.	Prediction time is about 3-5 minutes for small proteins and 10-30 minutes for very huge proteins. For training, the model needs 0.5-1 hour for one epoch.	GraphRBF model is interpretable rather than a black box. It provides not only predictive results but also insights into its learned representations and decision-making processes.	The GraphRBF model is a classification model. It is designed to classify whether each residue in a protein is a binding site or not, which is a categorical outcomeâ€”a fundamental aspect of classification tasks. The model uses learned features from protein structures to make these classifications, predicting the likelihood of a residue being part of a binding site, which is then typically binary or multiclass labeled (e.g., binding site or non-binding site).	4.0	0.0	"The ML algorithm class used is Deep Learning and supervised learning, specifically an interpretable hierarchical geometric deep learning model known as GraphRBF. This is a novel algorithm because it integrates both Graph Neural Networks (GNNs) and Radial Basis Function Neural Networks (RBFNNs), with an enhanced graph neural network designed to describe physicochemical information interactions between amino acid residues and the introduction of a prioritized radial basis function to characterize the spatial distribution of residues.

The novel algorithm was chosen over better-known alternatives because it can more comprehensively represent the binding patterns of protein residues, including their spatial distribution and physicochemical information interactions, which could not be achieved by previous methods. GraphRBF offers enhanced interpretability and has demonstrated superior performance in predicting protein binding sites compared to existing state-of-the-art methods."	We reported these information in our paper(under review) and https://github.com/Wssduer/GraphRBF.	Multiple types of amino acid features are extracted that include atomic features of residues, secondary structure encoding, evolutionary information, and residue type encoding. Seven atomic features are extracted for each heavy atom belonging to the target residue based on the PDB file: atom mass, B-factor, whether it is a residue side-chain atom, electronic charge, the number of hydrogen atoms bonded to it, whether it is in a ring, and the van der Waals radius of the atom. The DSSP tool is applied to generate secondary structure features of residues including a residue water-exposed surface, five bond and torsion angles, and eight one-hot encoded secondary structure with eight states. For the evolutionary information, PSI-BLAST is used to search the Uniref 90 database for homologous sequences with three iterations and e-value<10^âˆ’3 to generate the position-specific scoring matrix (PSSM) with 20 dimensions. HHblits is implemented to search the uniclust30 database to generate an HMM matrix of the query sequence with 30 dimensions. For the residue type encoding, the 20 types of amino acids are ordered as ACDEFGHIKLMNPQRSTVWY, and an 20-dimension one-hot coding of each residue is generated accordingly. In conclusion, a node feature matrix of size LÃ—91 is constructed for each protein, in which L is the length of the protein. In addition, the original features are min-max normalized before training.	Multiple types of amino acid features are extracted that include atomic features of residues, secondary structure encoding, evolutionary information, and residue type encoding. Seven atomic features are extracted for each heavy atom belonging to the target residue based on the PDB file: atom mass, B-factor, whether it is a residue side-chain atom, electronic charge, the number of hydrogen atoms bonded to it, whether it is in a ring, and the van der Waals radius of the atom. The DSSP tool is applied to generate secondary structure features of residues including a residue water-exposed surface, five bond and torsion angles, and eight one-hot encoded secondary structure with eight states. For the evolutionary information, PSI-BLAST is used to search the Uniref 90 database for homologous sequences with three iterations and e-value<10^âˆ’3 to generate the position-specific scoring matrix (PSSM) with 20 dimensions. HHblits is implemented to search the uniclust30 database to generate an HMM matrix of the query sequence with 30 dimensions. For the residue type encoding, the 20 types of amino acids are ordered as ACDEFGHIKLMNPQRSTVWY, and an 20-dimension one-hot coding of each residue is generated accordingly. In conclusion, a node feature matrix of size LÃ—91 is constructed for each protein, in which L is the length of the protein. In addition, the original features are min-max normalized before training.	p is much larger. We uesd a smaller number of hidden later parameters(128) and network layers(1 or 2), as well as using dropout regularization, are effective methods to prevent overfitting.	The data and features are selected by ourselves from the public datasets and not from other algorithms.	"We conduct hyperparameter tuning experiments for GraphRBF and select the best hyperparameters on the validation set. 
Here are the hyperparameters:
Radius of the structural neighborhood: it defines the point cloud within the neighborhood of the target residue, and its unit is Ã….(20)
The distance threshold to get the adjacency matrix: it defines the threshold for concatenating edges between residues, and its unit is Ã….(10)
The number of GDG kernels in each filter.(32)
The number of filters used in PRBFNN.(64)
The number of representation layer. Layer 0 stands for the encoder layer.(1 for protein-DNA/RNA and 2 for protein-protein)
The dimension  of encoded position information vector using local coordinates.(64)
The dimension of encoded node, edge and graph feature vector. (128)
Learning rate for training the deep model.(0.0001)
Batch size for training deep model.(256)
Training epochs.(60)"	Dropout and early stopping.	8.0	0.0	66921d947089c469b45e7e98	xxx			xxx	xxx	xxx				0.0	2024-07-16T09:08:49.427Z	2024-07-16T09:08:49.427Z	cccf2563-8547-418c-b802-171f59a959a2	undenfined	pso59qmwle
TITLE_MATCH DOI_MATCH 	43		Protein-protein and protein-nucleic acid binding site prediction via interpretable hierarchical geometric deep learning	xxx	669640d037ea6fa797a6cbe7	All the binding proteins are from public databases, Protein-Protein Docking Benchmark 5.5 (DBD 5.5), Dockground and BioLiP2 database. But the data splits were our  random sets. And the protein datasets we established are inhttps://github.com/Wssduer/GraphRBF	"The protein-protein interaction datasets were obtained from Protein-Protein Docking Benchmark 5.5 (DBD 5.5)[1] and Dockground[2] database. These datasets consist of non-redundant, high-resolution structures, including both bound and unbound structures for each protein complex. Considering that protein conformation tends to change to varying degrees when interacting with other molecules in the cell, it makes more sense to use the unbound structures prior to the interaction to train and test the model. Finally, we obtained 1,251 protein-binding proteins as a dataset.
Then we constructed a baseline dataset of nucleic acid binding proteins from the BioLiP2[3] database, which collects biologically relevant ligand-protein interactions that are structurally resolved in complexes. DNA/RNA-binding proteins were collected from the dataset as of October 7, 2023, and contain a total of 42,457 DNA-protein complex entries and 146,306 RNA-protein complex entries. First, we selected high-resolution structures with a resolution of less than 8.0 Ã…. The resolution can reflect the accuracy of protein structure analysis; higher resolution indicates finer protein structure, while lower resolution indicates that the atomic structure in the crystal is more difficult to be clearly resolved and localized. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. Finally, we kept 1,871 RNA-binding proteins and 1,101 DNA-binding proteins.

[1]Vreven, T. et al. Updates to the Integrated Protein-Protein Interaction Benchmarks: Docking Benchmark Version 5 and Affinity Benchmark Version 2. J Mol Biol 427, 3031-3041, doi:10.1016/j.jmb.2015.07.016 (2015).
[2]Kundrotas, P. J. et al. Dockground: A comprehensive data resource for modeling of protein complexes. Protein Sci 27, 172-181, doi:10.1002/pro.3295 (2018).
[3]Zhang, C., Zhang, X., Freddolino, P. L. & Zhang, Y. BioLiP2: an updated structure database for biologically relevant ligand-protein interactions. Nucleic Acids Res 52, D404-D412, doi:10.1093/nar/gkad630 (2024)."	"The training and test sets are independent. We randomly divided the data we selected from the datasets into a training dataset and a test dataset at a ratio of 0.2. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. 
"	"We obtained 1,251 protein-binding proteins, 1,871 RNA-binding proteins and 1,101 DNA-binding proteins as three binding datasets, and then randomly divided the data into a training dataset and a test dataset at a ratio of 0.2. In addition, we further divided the training data into a training dataset and a validation dataset. Eventually we obtained 1001 protein-binding proteins, 881 DNA-binding proteins and 1497 RNA-binding proteins as the training set; 250 protein-binding proteins, 220 DNA-binding proteins and 374 RNA-binding proteins as the test set. The details of the dataset are here:
Type	Dataset	       Nproteina	     Nposb	Nnegc	PNratiod
Protein	P-1001_Train	   1001	     25172	        264564	0.095
	        P-250_Test	   250	     6386	        73391	0.087
DNA	DNA-881_Train   881	     9962	        251759	0.04
	        DNA-220_Test	   220	     2537	        68401	0.037
RNA	        RNA-1497_Train 1497	     33444	        384358	0.087
	        RNA-374_Test	   374	     9331	        110259	0.085
aNumber of proteins.
bNumber of binding residues.
cNumber of non-binding residues.
dPNratio= Npos/Nneg."	4.0	0.0	We set the evaluation method and trained model in https://github.com/Wssduer/GraphRBF.	"GraphRBF model was compared with several state-of-the-art methods on benchmark datasets. This comparison included structure-based methods such as SPPIDER, GraphPPIS, MaSIF-site, and ScanNet, as well as sequence-based methods like TargetDNA, hybridRNAbind, DeepDISOBind, and DRNApred. The performance was evaluated using various metrics, including Recall, Precision, F1-Score, Matthews Correlation Coefficient, AUC, and AUPRC.

Additionally, the paper mentioned comparisons with simpler baseline models, such as sequence-based methods that use only sequence information and structure-based methods that rely solely on the 3D structure of proteins. Through these comparisons, the GraphRBF model demonstrated superior performance in predicting protein binding sites.On the protein-protein binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.028, 0.108, 0.055, and 0.066 higher than the second highest value, with relative improvements achieving 3.5%, 41.9%, 16.2%, and 24% respectively. On the protein-DNA (RNA) binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.017 (0.076), 0.052 (0.106), 0.061 (0.087), and 0.049 (0.095) higher than the second highest value, with relative improvements reaching 1.9% (9.2%), 18.0% (26.1%), 17.4% (20.0%), and 13.8% (24.7%), respectively. "	We did not set confidence intervals but instead directly compared the various metrics of our model with other models (using the same computational methods), and obtained the best results, which sufficiently demonstrates the superiority of our model.	"A variety of performance metrics are reported to evaluate the GraphRBF model, including:
Recall
Precision
F1-Score
Matthews Correlation Coefficient (MCC)
Area Under the Receiver Operating Characteristic Curve (AUC)
Area Under the Precision-Recall Curve (AUPRC)
These metrics are standard in the fields of bioinformatics and machine learning and provide a comprehensive assessment of the predictive performance of the model. Recall and precision measure the model's ability to identify positive and negative instances, the F1-Score is the harmonic mean of these two metrics, MCC provides a correlation measure between -1 and 1, and AUC and AUPRC offer an overall measure of the model's performance, especially useful when dealing with imbalanced datasets.

Regarding the representativeness of this set of performance metrics, they are consistent with those used in the literature for similar tasks and can be considered representative. These metrics allow researchers to make comparisons with existing methods and demonstrate the performance of the GraphRBF model in predicting protein binding sites."	GraphRBF method was evaluated using an independent test dataset. This approach involved training and testing the model on datasets for protein-protein and protein-nucleic acid interaction sites separately, and comparing it with other leading prediction methods. A variety of evaluation metrics were used in the assessment, such as Recall, Precision, F1-Score, Matthews Correlation Coefficient (MCC), Area Under the Receiver Operating Characteristic Curve (AUC), and Area Under the Precision-Recall Curve (AUPRC). Moreover, the GraphRBF model was applied to a novel protein structure, the spike protein of the SARS-CoV-2 Omicron variant, and the predictions were analyzed and visualized, offering additional validation of the method.	5.0	0.0	Source codes are in https://github.com/Wssduer/GraphRBF. We also have a web server http://www.liulab.top/GraphRBF.	Prediction time is about 3-5 minutes for small proteins and 10-30 minutes for very huge proteins. For training, the model needs 0.5-1 hour for one epoch.	GraphRBF model is interpretable rather than a black box. It provides not only predictive results but also insights into its learned representations and decision-making processes.	The GraphRBF model is a classification model. It is designed to classify whether each residue in a protein is a binding site or not, which is a categorical outcomeâ€”a fundamental aspect of classification tasks. The model uses learned features from protein structures to make these classifications, predicting the likelihood of a residue being part of a binding site, which is then typically binary or multiclass labeled (e.g., binding site or non-binding site).	4.0	0.0	"The ML algorithm class used is Deep Learning and supervised learning, specifically an interpretable hierarchical geometric deep learning model known as GraphRBF. This is a novel algorithm because it integrates both Graph Neural Networks (GNNs) and Radial Basis Function Neural Networks (RBFNNs), with an enhanced graph neural network designed to describe physicochemical information interactions between amino acid residues and the introduction of a prioritized radial basis function to characterize the spatial distribution of residues.

The novel algorithm was chosen over better-known alternatives because it can more comprehensively represent the binding patterns of protein residues, including their spatial distribution and physicochemical information interactions, which could not be achieved by previous methods. GraphRBF offers enhanced interpretability and has demonstrated superior performance in predicting protein binding sites compared to existing state-of-the-art methods."	We reported these information in our paper(under review) and https://github.com/Wssduer/GraphRBF.	The node features are extracted based on the protein sequence and structure for each amino acid, while the residue coordinates are defined by the 3D coordinates of its Î±-carbon. Towards learning the binding patterns of a residue from its local neighborhood, we first extract its spatial neighbors and construct a local coordinate system that ensures rotation and translation invariance and then we build a local graph with nodes representing residues and edges denoting their close distances.	Multiple types of amino acid features are extracted that include atomic features of residues, secondary structure encoding, evolutionary information, and residue type encoding. Seven atomic features are extracted for each heavy atom belonging to the target residue based on the PDB file: atom mass, B-factor, whether it is a residue side-chain atom, electronic charge, the number of hydrogen atoms bonded to it, whether it is in a ring, and the van der Waals radius of the atom. The DSSP tool is applied to generate secondary structure features of residues including a residue water-exposed surface, five bond and torsion angles, and eight one-hot encoded secondary structure with eight states. For the evolutionary information, PSI-BLAST is used to search the Uniref 90 database for homologous sequences with three iterations and e-value<10^âˆ’3 to generate the position-specific scoring matrix (PSSM) with 20 dimensions. HHblits is implemented to search the uniclust30 database to generate an HMM matrix of the query sequence with 30 dimensions. For the residue type encoding, the 20 types of amino acids are ordered as ACDEFGHIKLMNPQRSTVWY, and an 20-dimension one-hot coding of each residue is generated accordingly. In conclusion, a node feature matrix of size LÃ—91 is constructed for each protein, in which L is the length of the protein. In addition, the original features are min-max normalized before training.	p is much larger. We uesd a smaller number of hidden later parameters(128) and network layers(1 or 2), as well as using dropout regularization, are effective methods to prevent overfitting.	The data and features are selected by ourselves from the public datasets and not from other algorithms.	"We conduct hyperparameter tuning experiments for GraphRBF and select the best hyperparameters on the validation set. 
Here are the hyperparameters:
Radius of the structural neighborhood: it defines the point cloud within the neighborhood of the target residue, and its unit is Ã….(20)
The distance threshold to get the adjacency matrix: it defines the threshold for concatenating edges between residues, and its unit is Ã….(10)
The number of GDG kernels in each filter.(32)
The number of filters used in PRBFNN.(64)
The number of representation layer. Layer 0 stands for the encoder layer.(1 for protein-DNA/RNA and 2 for protein-protein)
The dimension  of encoded position information vector using local coordinates.(64)
The dimension of encoded node, edge and graph feature vector. (128)
Learning rate for training the deep model.(0.0001)
Batch size for training deep model.(256)
Training epochs.(60)"	Dropout and early stopping.	8.0	0.0	66921d947089c469b45e7e98	xxx			xxx	xxx	xxx				0.0	2024-07-16T09:43:44.122Z	2024-07-16T09:43:44.122Z	4d6dadd1-8bc8-412b-9e6b-1af821e3749e	undenfined	ybvq2ayfnu
TITLE_MATCH DOI_MATCH 	43		Protein-protein and protein-nucleic acid binding site prediction via interpretable hierarchical geometric deep learning	xxx	669646ec37ea6fa797a6cbf7	All data resources used are freely available from the following databases: BioLiP (https://zhanggroup.org/BioLiP), DBD5.5 (https://zlab.umassmed.edu/benchmark), Dockground (http://dockground.compbio.ku.edu/). The processed data used for training and testing the models in this study can be available at https://zenodo.org/records/10826801.	"The protein-protein interaction datasets were obtained from Protein-Protein Docking Benchmark 5.5 (DBD 5.5)[1] and Dockground[2] database. These datasets consist of non-redundant, high-resolution structures, including both bound and unbound structures for each protein complex. Considering that protein conformation tends to change to varying degrees when interacting with other molecules in the cell, it makes more sense to use the unbound structures prior to the interaction to train and test the model. Finally, we obtained 1,251 protein-binding proteins as a dataset.
Then we constructed a baseline dataset of nucleic acid binding proteins from the BioLiP2[3] database, which collects biologically relevant ligand-protein interactions that are structurally resolved in complexes. DNA/RNA-binding proteins were collected from the dataset as of October 7, 2023, and contain a total of 42,457 DNA-protein complex entries and 146,306 RNA-protein complex entries. First, we selected high-resolution structures with a resolution of less than 8.0 Ã…. The resolution can reflect the accuracy of protein structure analysis; higher resolution indicates finer protein structure, while lower resolution indicates that the atomic structure in the crystal is more difficult to be clearly resolved and localized. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. Finally, we kept 1,871 RNA-binding proteins and 1,101 DNA-binding proteins.

[1]Vreven, T. et al. Updates to the Integrated Protein-Protein Interaction Benchmarks: Docking Benchmark Version 5 and Affinity Benchmark Version 2. J Mol Biol 427, 3031-3041, doi:10.1016/j.jmb.2015.07.016 (2015).
[2]Kundrotas, P. J. et al. Dockground: A comprehensive data resource for modeling of protein complexes. Protein Sci 27, 172-181, doi:10.1002/pro.3295 (2018).
[3]Zhang, C., Zhang, X., Freddolino, P. L. & Zhang, Y. BioLiP2: an updated structure database for biologically relevant ligand-protein interactions. Nucleic Acids Res 52, D404-D412, doi:10.1093/nar/gkad630 (2024)."	"The training and test sets are independent. We randomly divided the data we selected from the datasets into a training dataset and a test dataset at a ratio of 0.2. Next, sequence similarity clustering was performed on the redundant data using psi-cd-hit, and the structures with the highest resolution were selected as representative entries for each cluster. 
"	"We obtained 1,251 protein-binding proteins, 1,871 RNA-binding proteins and 1,101 DNA-binding proteins as three binding datasets, and then randomly divided the data into a training dataset and a test dataset at a ratio of 0.2. In addition, we further divided the training data into a training dataset and a validation dataset. Eventually we obtained 1001 protein-binding proteins, 881 DNA-binding proteins and 1497 RNA-binding proteins as the training set; 250 protein-binding proteins, 220 DNA-binding proteins and 374 RNA-binding proteins as the test set. The details of the dataset are here:
Type	Dataset	       Nproteina	     Nposb	Nnegc	PNratiod
Protein	P-1001_Train	   1001	     25172	        264564	0.095
	        P-250_Test	   250	     6386	        73391	0.087
DNA	DNA-881_Train   881	     9962	        251759	0.04
	        DNA-220_Test	   220	     2537	        68401	0.037
RNA	        RNA-1497_Train 1497	     33444	        384358	0.087
	        RNA-374_Test	   374	     9331	        110259	0.085
aNumber of proteins.
bNumber of binding residues.
cNumber of non-binding residues.
dPNratio= Npos/Nneg."	4.0	0.0	We set the evaluation method and trained model in https://github.com/Wssduer/GraphRBF.	"GraphRBF model was compared with several state-of-the-art methods on benchmark datasets. This comparison included structure-based methods such as SPPIDER, GraphPPIS, MaSIF-site, and ScanNet, as well as sequence-based methods like TargetDNA, hybridRNAbind, DeepDISOBind, and DRNApred. The performance was evaluated using various metrics, including Recall, Precision, F1-Score, Matthews Correlation Coefficient, AUC, and AUPRC.

Additionally, the paper mentioned comparisons with simpler baseline models, such as sequence-based methods that use only sequence information and structure-based methods that rely solely on the 3D structure of proteins. Through these comparisons, the GraphRBF model demonstrated superior performance in predicting protein binding sites.On the protein-protein binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.028, 0.108, 0.055, and 0.066 higher than the second highest value, with relative improvements achieving 3.5%, 41.9%, 16.2%, and 24% respectively. On the protein-DNA (RNA) binding test set, the AUC, PRC, F1-score, and MCC of GraphRBF are 0.017 (0.076), 0.052 (0.106), 0.061 (0.087), and 0.049 (0.095) higher than the second highest value, with relative improvements reaching 1.9% (9.2%), 18.0% (26.1%), 17.4% (20.0%), and 13.8% (24.7%), respectively. "	We did not set confidence intervals but instead directly compared the various metrics of our model with other models (using the same computational methods), and obtained the best results, which sufficiently demonstrates the superiority of our model.	"A variety of performance metrics are reported to evaluate the GraphRBF model, including:
Recall
Precision
F1-Score
Matthews Correlation Coefficient (MCC)
Area Under the Receiver Operating Characteristic Curve (AUC)
Area Under the Precision-Recall Curve (AUPRC)
These metrics are standard in the fields of bioinformatics and machine learning and provide a comprehensive assessment of the predictive performance of the model. Recall and precision measure the model's ability to identify positive and negative instances, the F1-Score is the harmonic mean of these two metrics, MCC provides a correlation measure between -1 and 1, and AUC and AUPRC offer an overall measure of the model's performance, especially useful when dealing with imbalanced datasets.

Regarding the representativeness of this set of performance metrics, they are consistent with those used in the literature for similar tasks and can be considered representative. These metrics allow researchers to make comparisons with existing methods and demonstrate the performance of the GraphRBF model in predicting protein binding sites."	GraphRBF method was evaluated using an independent test dataset. This approach involved training and testing the model on datasets for protein-protein and protein-nucleic acid interaction sites separately, and comparing it with other leading prediction methods. A variety of evaluation metrics were used in the assessment, such as Recall, Precision, F1-Score, Matthews Correlation Coefficient (MCC), Area Under the Receiver Operating Characteristic Curve (AUC), and Area Under the Precision-Recall Curve (AUPRC). Moreover, the GraphRBF model was applied to a novel protein structure, the spike protein of the SARS-CoV-2 Omicron variant, and the predictions were analyzed and visualized, offering additional validation of the method.	5.0	0.0	Source codes are in https://github.com/Wssduer/GraphRBF. We also have a web server http://www.liulab.top/GraphRBF.	Prediction time is about 3-5 minutes for small proteins and 10-30 minutes for very huge proteins. For training, the model needs 0.5-1 hour for one epoch.	GraphRBF model is interpretable rather than a black box. It provides not only predictive results but also insights into its learned representations and decision-making processes.	The GraphRBF model is a classification model. It is designed to classify whether each residue in a protein is a binding site or not, which is a categorical outcomeâ€”a fundamental aspect of classification tasks. The model uses learned features from protein structures to make these classifications, predicting the likelihood of a residue being part of a binding site, which is then typically binary or multiclass labeled (e.g., binding site or non-binding site).	4.0	0.0	"The ML algorithm class used is Deep Learning and supervised learning, specifically an interpretable hierarchical geometric deep learning model known as GraphRBF. This is a novel algorithm because it integrates both Graph Neural Networks (GNNs) and Radial Basis Function Neural Networks (RBFNNs), with an enhanced graph neural network designed to describe physicochemical information interactions between amino acid residues and the introduction of a prioritized radial basis function to characterize the spatial distribution of residues.

The novel algorithm was chosen over better-known alternatives because it can more comprehensively represent the binding patterns of protein residues, including their spatial distribution and physicochemical information interactions, which could not be achieved by previous methods. GraphRBF offers enhanced interpretability and has demonstrated superior performance in predicting protein binding sites compared to existing state-of-the-art methods."	We reported these information in our paper(under review) and https://github.com/Wssduer/GraphRBF.	The node features are extracted based on the protein sequence and structure for each amino acid, while the residue coordinates are defined by the 3D coordinates of its Î±-carbon. Towards learning the binding patterns of a residue from its local neighborhood, we first extract its spatial neighbors and construct a local coordinate system that ensures rotation and translation invariance and then we build a local graph with nodes representing residues and edges denoting their close distances.	Multiple types of amino acid features are extracted that include atomic features of residues, secondary structure encoding, evolutionary information, and residue type encoding. Seven atomic features are extracted for each heavy atom belonging to the target residue based on the PDB file: atom mass, B-factor, whether it is a residue side-chain atom, electronic charge, the number of hydrogen atoms bonded to it, whether it is in a ring, and the van der Waals radius of the atom. The DSSP tool is applied to generate secondary structure features of residues including a residue water-exposed surface, five bond and torsion angles, and eight one-hot encoded secondary structure with eight states. For the evolutionary information, PSI-BLAST is used to search the Uniref 90 database for homologous sequences with three iterations and e-value<10^âˆ’3 to generate the position-specific scoring matrix (PSSM) with 20 dimensions. HHblits is implemented to search the uniclust30 database to generate an HMM matrix of the query sequence with 30 dimensions. For the residue type encoding, the 20 types of amino acids are ordered as ACDEFGHIKLMNPQRSTVWY, and an 20-dimension one-hot coding of each residue is generated accordingly. In conclusion, a node feature matrix of size LÃ—91 is constructed for each protein, in which L is the length of the protein. In addition, the original features are min-max normalized before training.	p is much larger. We uesd a smaller number of hidden later parameters(128) and network layers(1 or 2), as well as using dropout regularization, are effective methods to prevent overfitting.	The data and features are selected by ourselves from the public datasets and not from other algorithms.	"We conduct hyperparameter tuning experiments for GraphRBF and select the best hyperparameters on the validation set. 
Here are the hyperparameters:
Radius of the structural neighborhood: it defines the point cloud within the neighborhood of the target residue, and its unit is Ã….(20)
The distance threshold to get the adjacency matrix: it defines the threshold for concatenating edges between residues, and its unit is Ã….(10)
The number of GDG kernels in each filter.(32)
The number of filters used in PRBFNN.(64)
The number of representation layer. Layer 0 stands for the encoder layer.(1 for protein-DNA/RNA and 2 for protein-protein)
The dimension  of encoded position information vector using local coordinates.(64)
The dimension of encoded node, edge and graph feature vector. (128)
Learning rate for training the deep model.(0.0001)
Batch size for training deep model.(256)
Training epochs.(60)"	Dropout and early stopping.	8.0	0.0	66921d947089c469b45e7e98	xxx			xxx	xxx	xxx				0.0	2024-07-16T10:09:48.846Z	2024-07-16T10:09:48.846Z	e7caeb66-0532-4f26-89ec-e1693c440f9d	undenfined	5l2lduic3p
TITLE_MATCH 	41		spatiAlign: An Unsupervised Contrastive Learning Model for Data Integration of Spatially Resolved Transcriptomics		64ffdc4298825f6c133aeb8b	"1. Mouse olfactory bulb: 
a. The 10x Geomics Visium dataset can be download from: https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1
b. The Stereo-seq datasets can be download from: https://db.cngb.org/stomics/mosta/download/
2. Human dorsolateral prefrontal cortex (DLPFC): the 10x Geomics Visium dataset and annotation file can be download from: https://zenodo.org/record/6925603#.YuM5WXZBwuU
3. Mouse hippocampal dataset: the Slide-seq datasets can be download from: https://singlecell.broadinstitute.org/single_cell/study/SCP815/highly-sensitive-spatial-transcriptomics-at-near-cellular-resolution-with-slide-seqv2#study-summary, https://singlecell.broadinstitute.org/single_cell/study/SCP354/slide-seq-study#study-summary, and https://singlecell.broadinstitute.org/single_cell/study/SCP948/robust-decomposition-of-cell-type-mixtures-in-spatial-transcriptomics#study-summary, respectively.
4. Mouse embryonic brain: the Stereo-seq datasets can be download from: https://db.cngb.org/stomics/mosta/download/"	"1. Mouse olfactory bulb: collected from two different sequencing platforms, 10x Geomics Visium and Stereo-seq, respectively, and which consists of 3 data sections.
2. Human dorsolateral prefrontal cortex (DLPFC): collected from different donors and measured by 10x Geomics Visium. We selected the dataset comprised four sections that were manually annotated into six tissue layers.
3. Mouse hippocampal dataset: collected from different region in the mouse brain, and measured by Slide-seq platform, and which consists of 3 data sections.
4. Mouse embryonic brain: collected from different time-series in the mouse embyro, and measured by Stereo-seq platform, and which consists of 6 data sections."	We did not split dataset.	We did not split the data and used all datasets for model training and testing.	4.0	0.0	"No
No"	benchmarking method: PRECAST, GraphST, SCALEX, Harmony, Combat, BBKNN, Scanorama, MNN	"No
"	"F1 score of local inverse Simpson's index, adjusted rand index, Moran's I index
Downstream bioinformation analysis, differential expression analysis, GO enrichment analysis, trajectory inference analysis, et al."	We use different dataset to evaluate our method, which include measured by different sequencing platforms, different time-series, different regions, et al.	5.0	0.0	"github1: https://github.com/zhangchao162/Spatialign.git
github2: https://github.com/STOmics/Spatialign.git
pypi: https://pypi.org/project/spatialign/
tutorial: https://spatialign-tutorials.readthedocs.io/en/latest/index.html"	base on the input dataset	blackbox	No, our model output a latent embedding and reconstructed representation, respectively.	4.0	0.0	We first implement a self-supervised contrastive learning architecture (Deep graph infomax framework) for dimensional reduction while simultaneously propagating neighbouring spatil context between spots/cells. And we employ an across-domain adaptation technique to align joint embeddings. 	"No
No"	"The collected datasets were be saved as '*.h5ad' format, and also includes two-dimensional spatial coordinates for each spot/cell. The dataformat can reference: https://anndata.readthedocs.io/en/latest/

In the preprocessing step, the raw gene expression matrices were first filtered according to criteria 'min_gene' smaller than 20 and 'min_cell' smaller than 20  for each data using SCANPY (version: 1.9.1), and followed by normalization and log transformation of individual spots.

In our algorithm, spatiAlign, we just set the 'is_norm_log' to True."	base on input data list, we choose common genes as input	"No
No"	"No
No"	"There are 17 instantiated model parameters. Details as follows,
:param data_path: List of input dataset path.
:param min_genes: Minimum number of genes expressed required for a cell to pass filtering, default 20.
:param min_cells: Minimum number of cells expressed required for a gene to pass filtering, default 20.
:param batch_key: The batch annotation to :attr:`obs` using this key, default, 'batch'.
:param is_norm_log: Whether to perform 'sc.pp.normalize_total' and 'sc.pp.log1p' processing, default, True.
:param is_scale: Whether to perform 'sc.pp.scale' processing, default, False.
:param is_hvg: Whether to perform 'sc.pp.highly_variable_genes' processing, default, False.
:param is_reduce: Whether to perform PCA reduce dimensional processing, default, False.
:param n_pcs: PCA dimension reduction parameter, valid when 'is_reduce' is True, default, 100.
:param n_hvg: 'sc.pp.highly_variable_genes' parameter, valid when 'is_reduce' is True, default, 2000.
:param n_neigh: The number of neighbors selected when constructing a spatial neighbor graph. default, 15.
:param is_undirected: Whether the constructed spatial neighbor graph is undirected graph, default, True.
:param latent_dims: The number of embedding dimensions, default, 100.
:param is_verbose: Whether the detail information is print, default, True.
:param seed: Random seed.
:param gpu: Whether the GPU device is using to train spatialign.
:param save_path: The path of alignment dataset and saved spatialign.

There are 7 training model parameters. Details as follows,
:param lr: Learning rate, default, 1e-3.
:param max_epoch: The number of maximum epochs, default, 500.
:param alpha: The momentum parameter, default, 0.5
:param patient: Early stop parameter, default, 15.
:param tau1: Instance level and pseudo prototypical cluster level contrastive learning parameters, default, 0.2
param tau2: Pseudo prototypical cluster entropy parameter, default, 1.
:param tau3: Cross-batch instance self-supervised learning parameter, default, 0.5
"	early stopping	8.0	0.0	64ffbbb492c76639b801efb4				Chao Zhang, Lin Liu, Ying Zhang, Mei Li, Shuangsang Fang, Qiang Kang, Ao Chen, Xun Xu, Yong Zhang, Yuxiang Li			0.0	4.0		0.0	2023-09-12T03:34:26.225Z	2023-09-13T01:42:30.186Z	3e7243cf-d4ff-488e-af83-e79a5854c4d4	undenfined	zwuvq25z2v
TITLE_MATCH 	45		spatiAlign: an unsupervised contrastive learning model for data integration of spatially resolved transcriptomics	10.1093/gigascience/giae042	670e6eef61d57eb8bca6960e	"Yes
1. Mouse olfactory bulb: 
a. The 10x Geomics Visium dataset can be download from: https://www.10xgenomics.com/resources/datasets/adult-mouse-olfactory-bulb-1-standard-1
b. The Stereo-seq datasets can be download from: https://db.cngb.org/stomics/mosta/download/
2. Human dorsolateral prefrontal cortex (DLPFC): the 10x Geomics Visium dataset and annotation file can be download from: https://zenodo.org/record/6925603#.YuM5WXZBwuU
3. Mouse hippocampal dataset: the Slide-seq datasets can be download from: https://singlecell.broadinstitute.org/single_cell/study/SCP815/highly-sensitive-spatial-transcriptomics-at-near-cellular-resolution-with-slide-seqv2#study-summary, https://singlecell.broadinstitute.org/single_cell/study/SCP354/slide-seq-study#study-summary, and https://singlecell.broadinstitute.org/single_cell/study/SCP948/robust-decomposition-of-cell-type-mixtures-in-spatial-transcriptomics#study-summary, respectively.
4. Mouse embryonic brain: the Stereo-seq datasets can be download from: https://db.cngb.org/stomics/mosta/download/"	publication	No	We did not split the data and used all datasets for model training and testing.	3.0	1.0	No	benchmarking method: PRECAST, GraphST, SCALEX, Harmony, Combat, BBKNN, Scanorama, MNN	No	"F1 score of local inverse Simpson's index, adjusted rand index, Moran's I index
Downstream bioinformation analysis, differential expression analysis, GO enrichment analysis, trajectory inference analysis, et al."	We use different dataset to evaluate our method, which include measured by different sequencing platforms, different time-series, different regions, et al.	3.0	2.0	github1: https://github.com/zhangchao162/Spatialign.git github2: https://github.com/STOmics/Spatialign.git pypi: https://pypi.org/project/spatialign/ tutorial: https://spatialign-tutorials.readthedocs.io/en/latest/index.html	base on the input dataset	early stopping	No, our model output a latent embedding and reconstructed representation, respectively.	4.0	0.0	We first implement a self-supervised contrastive learning architecture (Deep graph infomax framework) for dimensional reduction while simultaneously propagating neighbouring spatil context between spots/cells. And we employ an across-domain adaptation technique to align joint embeddings. 	 No	The collected datasets were be saved as '*.h5ad' format, and also includes two-dimensional spatial coordinates for each spot/cell. The dataformat can reference: https://anndata.readthedocs.io/en/latest/  In the preprocessing step, the raw gene expression matrices were first filtered according to criteria 'min_gene' smaller than 20 and 'min_cell' smaller than 20  for each data using SCANPY (version: 1.9.1), and followed by normalization and log transformation of individual spots.  In our algorithm, spatiAlign, we just set the 'is_norm_log' to True.	base on input data list, we choose common genes as input	No	No	"There are 17 instantiated model parameters. Details as follows,
:param data_path: List of input dataset path.
:param min_genes: Minimum number of genes expressed required for a cell to pass filtering, default 20.
:param min_cells: Minimum number of cells expressed required for a gene to pass filtering, default 20.
:param batch_key: The batch annotation to :attr:`obs` using this key, default, 'batch'.
:param is_norm_log: Whether to perform 'sc.pp.normalize_total' and 'sc.pp.log1p' processing, default, True.
:param is_scale: Whether to perform 'sc.pp.scale' processing, default, False.
:param is_hvg: Whether to perform 'sc.pp.highly_variable_genes' processing, default, False.
:param is_reduce: Whether to perform PCA reduce dimensional processing, default, False.
:param n_pcs: PCA dimension reduction parameter, valid when 'is_reduce' is True, default, 100.
:param n_hvg: 'sc.pp.highly_variable_genes' parameter, valid when 'is_reduce' is True, default, 2000.
:param n_neigh: The number of neighbors selected when constructing a spatial neighbor graph. default, 15.
:param is_undirected: Whether the constructed spatial neighbor graph is undirected graph, default, True.
:param latent_dims: The number of embedding dimensions, default, 100.
:param is_verbose: Whether the detail information is print, default, True.
:param seed: Random seed.
:param gpu: Whether the GPU device is using to train spatialign.
:param save_path: The path of alignment dataset and saved spatialign.

There are 7 training model parameters. Details as follows,
:param lr: Learning rate, default, 1e-3.
:param max_epoch: The number of maximum epochs, default, 500.
:param alpha: The momentum parameter, default, 0.5
:param patient: Early stop parameter, default, 15.
:param tau1: Instance level and pseudo prototypical cluster level contrastive learning parameters, default, 0.2
param tau2: Pseudo prototypical cluster entropy parameter, default, 1.
:param tau3: Cross-batch instance self-supervised learning parameter, default, 0.5
"	early stopping	6.0	2.0	64ffbbb492c76639b801efb4	39028588			Chao Zhang, Lin Liu, Ying Zhang, Mei Li, Shuangsang Fang, Qiang Kang, Ao Chen, Xun Xu, Yong Zhang, Yuxiang Li	GigaScience	2024	0.0	6.0		1.0	2024-10-15T13:32:31.707Z	2024-10-15T13:32:31.707Z	dfddd412-f46b-447b-8ab6-a45bfeaf37c2	undenfined	345ml05xtn
TITLE_MATCH DOI_MATCH 	45		Unveiling Patterns in Spatial Transcriptomics Data: A Novel Approach Utilizing Graph Attention Autoencoder and Multi-Scale Deep Subspace Clustering Network	000	6747ebe369b4c4f264235422	"Yes.
Human dorsolateral pre-frontal cortex (DLPFC) can be downloaded from: http://research.libd.org/spatialLIBD/.
The 10x Geomics Visium dataset can be download from: https://www.10xgenomics.com/resources/datasets.
The mouse visual cortex STARmap data is accessible on https://www.dropbox.com/sh/f7ebheru1lbz91s/AADm6D54GSEFXB1feRy6OSASa/visual_1020/20180505_BY3_1kgenes?dl=0&subfolder_nav_tracking=1."	publication	We did not split the data and used all datasets for model training and testing.	"We did not split the data and used all datasets for model training and testing.
"	4.0	0.0	No	benchmarking method:  SCANPY, SEDR, CCST, DeepST, and GraphST.	No	For three datasets with labels (Human Breast Cancer (Block A Section 1), DLPFC, and mouse visual cortex STARmap), we employed adjusted rand index (ARI) to evaluate the performance of different spatial clustering algorithms. For two datasets whose spatial domain annotations are unavailable (Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS)), we evaluated the performance of spatial clustering algorithms based on three clustering metrics, that is, Davies-Bouldin (DB) score, Calinski-Harabasz (CH) score, and S_Dbw score.	"We use different dataset to evaluate our method, which include measured by different sequencing platforms, different tissue.
"	3.0	2.0	https://github.com/plhhnu/STMSGAL	The training time on one DLPFC section is about 1 hour.	black box	"No, our model is clustering.
"	4.0	0.0	We introduce STMSGAL, a novel framework for analyzing ST data by incorporating graph attention autoencoder and multi-scale deep subspace clustering.	No	The collected datasets were be saved as '*.h5ad' format, also includes two-dimensional spatial coordinates for each spot/cell and H&E staining figures. We use SCANPY package to preprocess ST data.	"We choose the top 3000 highly variable genes as input.
"	No	No	"
:param rad_cutoff: When model=='Radius', the spot is connected to spots whose distance is less than rad_cutoff, default 150.
:param cost_ssc:  The tradeoff parameter of the multi-scale self-expression loss, default 1.
:param refinement: Whether to refine the human brain DLPFC, default, True.
:param pre_resolution: The resolution value based on pre-clustering, default 0.2.
:param n_top_genes: 'sc.pp.highly_variable_genes' parameter, default, 3000.

:param lr: Learning rate, default,1e-4.
:param n_epochs: The number of maximum epochs, default, 200.
:param alpha: The weight of cell type-aware spatial neighbor network, default 0.5.

"	weight decay	5.0	3.0	651bcf4792c76639b8537ad6	000			Zhou L; Peng X; Chen M; He X; Tian G; Yang J; Peng L	GigaScience	2024	0.0	6.0		0.0	2024-11-28T04:04:51.931Z	2024-11-28T04:04:51.931Z	5d09346b-eeec-4afd-a5a3-55427bd1e8f2	undenfined	h94ymrtm4j
TITLE_MATCH DOI_MATCH 	45		Unveiling Patterns in Spatial Transcriptomics Data: A Novel Approach Utilizing Graph Attention Autoencoder and Multi-Scale Deep Subspace Clustering Network	000	6747ebfc69b4c4f264235426	"Yes.
Human dorsolateral pre-frontal cortex (DLPFC) can be downloaded from: http://research.libd.org/spatialLIBD/.
The 10x Geomics Visium dataset can be download from: https://www.10xgenomics.com/resources/datasets.
The mouse visual cortex STARmap data is accessible on https://www.dropbox.com/sh/f7ebheru1lbz91s/AADm6D54GSEFXB1feRy6OSASa/visual_1020/20180505_BY3_1kgenes?dl=0&subfolder_nav_tracking=1."	publication	We did not split the data and used all datasets for model training and testing.	"We did not split the data and used all datasets for model training and testing.
"	4.0	0.0	No	benchmarking method:  SCANPY, SEDR, CCST, DeepST, and GraphST.	No	For three datasets with labels (Human Breast Cancer (Block A Section 1), DLPFC, and mouse visual cortex STARmap), we employed adjusted rand index (ARI) to evaluate the performance of different spatial clustering algorithms. For two datasets whose spatial domain annotations are unavailable (Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS)), we evaluated the performance of spatial clustering algorithms based on three clustering metrics, that is, Davies-Bouldin (DB) score, Calinski-Harabasz (CH) score, and S_Dbw score.	"We use different dataset to evaluate our method, which include measured by different sequencing platforms, different tissue.
"	3.0	2.0	https://github.com/plhhnu/STMSGAL	The training time on one DLPFC section is about 1 hour.	black box	"No, our model is clustering.
"	4.0	0.0	We introduce STMSGAL, a novel framework for analyzing ST data by incorporating graph attention autoencoder and multi-scale deep subspace clustering.	No	The collected datasets were be saved as '*.h5ad' format, also includes two-dimensional spatial coordinates for each spot/cell and H&E staining figures. We use SCANPY package to preprocess ST data.	"We choose the top 3000 highly variable genes as input.
"	No	No	"
:param rad_cutoff: When model=='Radius', the spot is connected to spots whose distance is less than rad_cutoff, default 150.
:param cost_ssc:  The tradeoff parameter of the multi-scale self-expression loss, default 1.
:param refinement: Whether to refine the human brain DLPFC, default, True.
:param pre_resolution: The resolution value based on pre-clustering, default 0.2.
:param n_top_genes: 'sc.pp.highly_variable_genes' parameter, default, 3000.

:param lr: Learning rate, default,1e-4.
:param n_epochs: The number of maximum epochs, default, 200.
:param alpha: The weight of cell type-aware spatial neighbor network, default 0.5.

"	weight decay	5.0	3.0	651bcf4792c76639b8537ad6	000			Zhou L; Peng X; Chen M; He X; Tian G; Yang J; Peng L	GigaScience	2024	0.0	6.0		0.0	2024-11-28T04:05:16.337Z	2024-11-28T04:05:16.337Z	26867f01-55b2-4962-b2bd-9dc5ace36623	undenfined	z34d6rg20c
TITLE_MATCH DOI_MATCH 	45		Unveiling Patterns in Spatial Transcriptomics Data: A Novel Approach Utilizing Graph Attention Autoencoder and Multi-Scale Deep Subspace Clustering Network	000	6747ec6169b4c4f26423542a	"Yes.
Human dorsolateral pre-frontal cortex (DLPFC) can be downloaded from: http://research.libd.org/spatialLIBD/.
The 10x Geomics Visium dataset can be download from: https://www.10xgenomics.com/resources/datasets.
The mouse visual cortex STARmap data is accessible on https://www.dropbox.com/sh/f7ebheru1lbz91s/AADm6D54GSEFXB1feRy6OSASa/visual_1020/20180505_BY3_1kgenes?dl=0&subfolder_nav_tracking=1."	publication	We did not split the data and used all datasets for model training and testing.	"We did not split the data and used all datasets for model training and testing.
"	4.0	0.0	No	benchmarking method:  SCANPY, SEDR, CCST, DeepST, and GraphST.	No	For three datasets with labels (Human Breast Cancer (Block A Section 1), DLPFC, and mouse visual cortex STARmap), we employed adjusted rand index (ARI) to evaluate the performance of different spatial clustering algorithms. For two datasets whose spatial domain annotations are unavailable (Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS)), we evaluated the performance of spatial clustering algorithms based on three clustering metrics, that is, Davies-Bouldin (DB) score, Calinski-Harabasz (CH) score, and S_Dbw score.	"We use different dataset to evaluate our method, which include measured by different sequencing platforms, different tissue.
"	3.0	2.0	https://github.com/plhhnu/STMSGAL	The training time on one DLPFC section is about 1 hour.	black box	"No, our model is clustering.
"	4.0	0.0	We introduce STMSGAL, a novel framework for analyzing ST data by incorporating graph attention autoencoder and multi-scale deep subspace clustering.	No	The collected datasets were be saved as '*.h5ad' format, also includes two-dimensional spatial coordinates for each spot/cell and H&E staining figures. We use SCANPY package to preprocess ST data.	"We choose the top 3000 highly variable genes as input.
"	No	No	"
:param rad_cutoff: When model=='Radius', the spot is connected to spots whose distance is less than rad_cutoff, default 150.
:param cost_ssc:  The tradeoff parameter of the multi-scale self-expression loss, default 1.
:param refinement: Whether to refine the human brain DLPFC, default, True.
:param pre_resolution: The resolution value based on pre-clustering, default 0.2.
:param n_top_genes: 'sc.pp.highly_variable_genes' parameter, default, 3000.

:param lr: Learning rate, default,1e-4.
:param n_epochs: The number of maximum epochs, default, 200.
:param alpha: The weight of cell type-aware spatial neighbor network, default 0.5.

"	weight decay	5.0	3.0	651bcf4792c76639b8537ad6	000			Zhou L; Peng X; Chen M; He X; Tian G; Yang J; Peng L	GigaScience	2024	0.0	6.0		0.0	2024-11-28T04:06:57.210Z	2024-11-28T04:06:57.210Z	4bf8e27c-a35f-4e52-ad2b-8f12c20ba165	undenfined	hcqighjdrv
TITLE_MATCH 	45		Unveiling Patterns in Spatial Transcriptomics Data: A Novel Approach Utilizing Graph Attention Autoencoder and Multi-Scale Deep Subspace Clustering Network	https://doi.org/10.1093/gigascience/giae103	6728dc731cd60917c59ba5a9	"Yes.
Human dorsolateral pre-frontal cortex (DLPFC) can be downloaded from: http://research.libd.org/spatialLIBD/.
The 10x Geomics Visium dataset can be download from: https://www.10xgenomics.com/resources/datasets.
The mouse visual cortex STARmap data is accessible on https://www.dropbox.com/sh/f7ebheru1lbz91s/AADm6D54GSEFXB1feRy6OSASa/visual_1020/20180505_BY3_1kgenes?dl=0&subfolder_nav_tracking=1."	publication	We did not split the data and used all datasets for model training and testing.	"We did not split the data and used all datasets for model training and testing.
"	4.0	0.0	No	benchmarking method:  SCANPY, SEDR, CCST, DeepST, and GraphST.	No	For three datasets with labels (Human Breast Cancer (Block A Section 1), DLPFC, and mouse visual cortex STARmap), we employed adjusted rand index (ARI) to evaluate the performance of different spatial clustering algorithms. For two datasets whose spatial domain annotations are unavailable (Adult Mouse Brain (FFPE) and Human Breast Cancer (DCIS)), we evaluated the performance of spatial clustering algorithms based on three clustering metrics, that is, Davies-Bouldin (DB) score, Calinski-Harabasz (CH) score, and S_Dbw score.	"We use different dataset to evaluate our method, which include measured by different sequencing platforms, different tissue.
"	3.0	2.0	https://github.com/plhhnu/STMSGAL	The training time on one DLPFC section is about 1 hour.	black box	"No, our model is clustering.
"	4.0	0.0	We introduce STMSGAL, a novel framework for analyzing ST data by incorporating graph attention autoencoder and multi-scale deep subspace clustering.	No	The collected datasets were be saved as '*.h5ad' format, also includes two-dimensional spatial coordinates for each spot/cell and H&E staining figures. We use SCANPY package to preprocess ST data.	"We choose the top 3000 highly variable genes as input.
"	No	No	"
:param rad_cutoff: When model=='Radius', the spot is connected to spots whose distance is less than rad_cutoff, default 150.
:param cost_ssc:  The tradeoff parameter of the multi-scale self-expression loss, default 1.
:param refinement: Whether to refine the human brain DLPFC, default, True.
:param pre_resolution: The resolution value based on pre-clustering, default 0.2.
:param n_top_genes: 'sc.pp.highly_variable_genes' parameter, default, 3000.

:param lr: Learning rate, default,1e-4.
:param n_epochs: The number of maximum epochs, default, 200.
:param alpha: The weight of cell type-aware spatial neighbor network, default 0.5.

"	weight decay	5.0	3.0	651bcf4792c76639b8537ad6	000			Zhou L; Peng X; Chen M; He X; Tian G; Yang J; Peng L	GigaScience	2025	0.0	6.0		1.0	2024-11-04T14:38:43.805Z	2025-01-31T10:12:12.140Z	1b825b5a-8d52-4f38-8391-d9850b3203f4	undenfined	cji1mirt7b
TITLE_MATCH DOI_MATCH 	30	2/21/2022 11:17:51	Using a machine learning approach to identify key prognostic molecules for esophageal squamous cell carcinoma.	10.1186/s12885-021-08647-1		GEO and TCGA data is available. The 86 clinical cases are not provided, neither is the code provided for the ML part of the paper	Pubmed for relevant molecules (48 mol based on 38 articles), GEO (GSE53625 179 data points), TCGA (TCGA-ESCC 82 data points, 37 used for validation) and own clinical samples (86 samples)	No mention made. Separation appears to be done randomly for the GEO set.  All the rest was used as validation.  No mention about the overlap between the sets.	179 split into 134 training and 45 testing ESCC cases (randomly), 17 out of 48 moulecues were used as features for the classification.  ESCC cases with survival times more than 3 years were labelled 1 and the others were labelled 0, no information about the number in each set.			not provided	No comparison is made, as this is not the main point of this paper. 	not applicable	Only AUC was reported for the classifier, which is not the most suitable measure.	They claim to have used crossvalidation but it is not explained.  They use independent sets to test their findings and to relate the survival of patients to these findings.			Not available	not given	both, they used LR as well as ANN. 	Classification that is used to determine the most relevant set of features out of all 2^17-1 combinations. 			5 algorithms used, LR, SVM, ANN, RF and XGBoost 	no 	mRNA data for 17 out 48 relevant molecules was used. The values expressed the difference in expression between the cancer and corresponding adjacent tissue.	17 features corresponding to the most relevant molecules, which were obtained from literature and an additional network analysis based on String. They tested with each ML algorithm all 2^17-1 (131071) feature combinations to see which ones lead to the best predictions. Selection was done on training set (I assume)	No information about parameters of the model and the model implementation.	No data is used from other predictors, but the 5 predictors are used together as an ensemble to identify the most predictive features, i.e. the molecules useful for separating cases with good prognosis from bas ones	No details provided about the ML algorithm parameters	not clear.  They claim to have uses cross-validation but it is not explained in detail.			6312169df3794236aa9879f2	34372798	PMC8351329		Li MX, Sun XM, Cheng WG, Ruan HJ, Liu K, Chen P, Xu HJ, Gao SG, Feng XS, Qi YJ.	BMC cancer	2021									
TITLE_MATCH DOI_MATCH 	30	2/2/2022 10:53:38	Using a machine learning approach to identify key prognostic molecules for esophageal squamous cell carcinoma.	10.1186/s12885-021-08647-1		No. However, raw data can be downloaded  as data sets can be of GSE53625 from GEO (https://www.ncbi.nlm.nih.gov/geo/) and 37 ESCC cases with Asian ancestry from TCGA (UCSC Xena, https://xena.ucsc.edu/).	Downloaded from public sources	No similarity check	Train set of 134 samples, test of 45 samples, and a validation sets consisting of 86 samples. No mention to N_pos or N_neg.			No	No comparison	Unpaired or paired Student t-test and log-rank tests for Kaplan-Meier	ROC-AUC	one training set, one test set and one validation set.			No	No description	Linear models and ensemble methods	both regression and classification			Generalized Linear models, SVM, NN, Random Forests and XGBoost,	No	Normalization according to an external reference Gut. 2014;63(11):1700â€“10. https://doi.org/10.1136/gutjnl-2013-305806.	No description	No description	No	No description	No			6312169df3794236aa987a0a	34372798	PMC8351329		Li MX, Sun XM, Cheng WG, Ruan HJ, Liu K, Chen P, Xu HJ, Gao SG, Feng XS, Qi YJ.	BMC cancer	2021									
TITLE_MATCH DOI_MATCH 	44		WaveSeekerNet: Accurate Prediction of Influenza A Virus Subtypes and Host Source Using Attention-Based Deep Learning	https://doi.org/10.1101/2025.02.25.639900	67c5f5201f0965481b0c7628	"https://github.com/nhhaidee/WaveSeekerNet

License: Creative Commons CC0"	"Sequences can be downloaded at EpiFlu GISAID

Host Prediction: 3 classes (0-Humans, 1-Avian, 2-Nonhuman Mammals)
HA Subtype: 18 classes (0-17)
NA Subtype: 11 classes (0-10)

For data description and distribution, please refer to paper and metadata (available in github page: https://github.com/nhhaidee/WaveSeekerNet)"	Data is split temporally (based on collection date): sequences collected before 2020 are used to train model, while  sequences collected since 2020 are used to evaluate model	Please refer to paper for details of data distribution (Method Section)	4.0	0.0	The metadata of sequences used for this study is available at  https://github.com/nhhaidee/WaveSeekerNet. Creative Commons CC0 licence	Compare WaveSeekerNet with tradition approaches such as VADR, Blastp and other deep learning approaches such as self-attention mechanism of Transformer and FNET.	Boxen, bar plots are used to compare the performance of methods. Baycomp (a library for Bayesian comparison of classifiers) are also used to compare the generalization performance of WaveSeekerNet with various hyperparameters.	F1-Score (Macro Average), Balanced Accuracy and MCC	10-fold Cross Validation and independent dataset for testing	5.0	0.0	Yes - https://github.com/nhhaidee/WaveSeekerNet	"The training time and inference time are depended on data, hyper-parameters. Please refer to training log on GitHub pages  (https://github.com/nhhaidee/WaveSeekerNet)

For example: Baseline WaveSeekerNet model trained with HA RNA Subtype dataset, the model is trained on high-performance computing cluster using GPU A100, the training time per epoch is 18s, Inference time is 0.6s (For cross validation)."	The model is still a black box.	Classification	4.0	0.0	Attention-based deep learning, an ensemble approach for attention mechanism with newly designed algorithms using the Fast Fourier Transform, the Wavelet Transform 	Yes, please refer to paper and code (available on Github page: https://github.com/nhhaidee/WaveSeekerNet)	The RNA and protein sequences of Influenza A virus are encoded using Frequency Chaos Game Representation and One-hot encoding, respectively.	WaveSeekerNet is a deep learning model, thus no feature selection performed	Dropout, normalisation are applied throughout network of WaveSeekerNet to prevent over-fitting. 	No	"The models was with diverse dataset, classification task (host or subtpe prediction) and various parameters, thus the number of parameters is various, please see refer to training log of GitHub page for more information (https://github.com/nhhaidee/WaveSeekerNet)

For example: Baseline WaveSeekerNet model trained with HA RNA Subtype dataset, Total Trainable Parameters:  1,286,033"	Dropout, normalisation are applied throughout network of WaveSeekerNet to prevent over-fitting. 	7.0	1.0	67c5dc98edf7b65039ab7e9d				"Hoang-Hai Nguyen, Josip Rudar, Nathaniel Lesperance, Oksana Vernygora, Graham W. Taylor, Chad Laing, David Lapen, Carson K. Leung, Oliver Lung
"	GigaScience	2025	0.0	6.0		0.0	2025-03-03T18:29:52.090Z	2025-03-03T18:29:52.090Z	968a0a36-7364-458b-9371-7271725d6186	undenfined	5p6et54szn
TITLE_MATCH DOI_MATCH 	44		WaveSeekerNet: Accurate Prediction of Influenza A Virus Subtypes and Host Source Using Attention-Based Deep Learning	https://doi.org/10.1101/2025.02.25.639900	67c5fc4b1f0965481b0c762c	"Yes.
https://github.com/nhhaidee/WaveSeekerNet
License: Creative Commons CC0"	"Sequences can be downloaded at EpiFlu GISAID

Host Prediction: 3 classes (0-Humans, 1-Avian, 2-Nonhuman Mammals)
HA Subtype: 18 classes (0-17)
NA Subtype: 11 classes (0-10)

For data description and distribution, please refer to paper and metadata (available in github page: https://github.com/nhhaidee/WaveSeekerNet)"	"Data is split temporally (based on collection date): sequences collected before 2020 are used to train model, while  sequences collected since 2020 are used to evaluate model
Preprint verion: https://www.biorxiv.org/content/10.1101/2025.02.25.639900v1"	"Please refer to paper for details of data distribution (Method Section and Supplementary Tables) 
Preprint verion: https://www.biorxiv.org/content/10.1101/2025.02.25.639900v1"	4.0	0.0	The metadata of sequences used for this study is available at  https://github.com/nhhaidee/WaveSeekerNet. Creative Commons CC0 licence.	Compare WaveSeekerNet with tradition approaches such as VADR, Blastp and other deep learning approaches such as self-attention mechanism of Transformer and FNET.	Boxen, bar plots are used to compare the performance of methods. Baycomp (a library for Bayesian comparison of classifiers) are also used to compare the generalization performance of WaveSeekerNet with various hyperparameters.	F1-Score (Macro Average), Balanced Accuracy and MCC	10-fold Cross Validation and independent dataset for testing	5.0	0.0	Yes - https://github.com/nhhaidee/WaveSeekerNet	"The training time and inference time are depended on data, hyper-parameters. Please refer to training log on GitHub pages  (https://github.com/nhhaidee/WaveSeekerNet)

For example: Baseline WaveSeekerNet model trained with HA RNA Subtype dataset, the model is trained on high-performance computing cluster using GPU A100, the training time per epoch is 18s, Inference time is 0.6s (For cross validation)."	The model is still a black box.	Classification	4.0	0.0	"Attention-based deep learning, an ensemble approach for attention mechanism with newly designed algorithms using the Fast Fourier Transform, the Wavelet Transform.
Sequences are transformed to embedding matrix which can be considered as discrete signal, this powerful technique like the Fast Fourier Transform, the Wavelet Transform can be used to analysis sequences."	Yes, please refer to paper and code (available on Github page: https://github.com/nhhaidee/WaveSeekerNet). Preprint verion: https://www.biorxiv.org/content/10.1101/2025.02.25.639900v1	The RNA and protein sequences of Influenza A virus are encoded using Frequency Chaos Game Representation and One-hot encoding, respectively.	WaveSeekerNet is a deep learning model, thus no feature selection performed	There are no over-fitting and under-fitting observed. Training, testing and evaluation errors are very low and comparable.	No	"The models was trained with diverse datasets, classification tasks (host or subtpe prediction) and various hyper-parameters, thus the number of parameters is various, please see refer to training log of GitHub page for more information (https://github.com/nhhaidee/WaveSeekerNet)

For example: Baseline WaveSeekerNet model trained with HA RNA Subtype dataset, Total Trainable Parameters:  1,286,033"	Dropout, normalisation are applied throughout network of WaveSeekerNet to prevent over-fitting. 	7.0	1.0	67c5dc98edf7b65039ab7e9d				"Hoang-Hai Nguyen, Josip Rudar, Nathaniel Lesperance, Oksana Vernygora, Graham W. Taylor, Chad Laing, David Lapen, Carson K. Leung, Oliver Lung
"	GigaScience	2025	0.0	6.0		0.0	2025-03-03T19:00:27.758Z	2025-03-03T19:00:27.758Z	5c703271-7f71-468f-877c-029644a92fb6	undenfined	3q2pdir3wt
TITLE_MATCH DOI_MATCH 	23		WaveSeekerNet: Accurate Prediction of Influenza A Virus Subtypes and Host Source Using Attention-Based Deep Learning	https://doi.org/10.1101/2025.02.25.639900	67c5dc981f0965481b0c7607					0.0	4.0						0.0	5.0					0.0	4.0									0.0	8.0	67c5dc98edf7b65039ab7e9d				"Hoang-Hai Nguyen(1,2#), Josip Rudar(1,4#), Nathaniel Lesperance(4,6), Oksana Vernygora(1), Graham W. Taylor(5,6), Chad Laing(1), David Lapen(7), Carson K. Leung(2), Oliver Lung(1,3)


1 National Centre for Foreign Animal Disease, Canadian Food Inspection Agency, Winnipeg, Manitoba, Canada
2 Department of Computer Science, University of Manitoba, Winnipeg, Manitoba, Canada
3 Department of Biological Sciences, University of Manitoba, Winnipeg, Manitoba, Canada
4 Department of Integrative Biology & Centre for Biodiversity Genomics, University of Guelph, Guelph, Ontario, Canada
5 School of Engineering, University of Guelph, Guelph, Ontario, Canada
6 Vector Institute, Toronto, Ontario, Canada
7 Agriculture and Agri-Food Canada, Ottawa, Canada

#Address correspondence to Hoang-Hai Nguyen (nguyen92@myumanitoba.ca) and Josip Rudar (joe.rudar@inspection.gc.ca)"	GigaScience	2025	0.0	6.0		0.0	2025-03-03T16:45:12.238Z	2025-03-03T16:45:12.238Z	d0b643f7-c9ba-4048-8196-d56d9098fa81	undenfined	pruvs12u49
