{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Python script to download latest DOME Registry contents, related full text papers & provide DOME Registry entries metadata read out (20241220)**\n",
    "1. DOME Registry contents will be downloaded by API call providing the json file of DOME Registry data\n",
    "2. DOME Registry data json will be flattened and converted into TSV for working with entries data (row based data)\n",
    "3. DOME Registry TSV will be checked and used to produce a metadata readout file (+ graphs)\n",
    "4. DOME Registry DOIs of articles will be converted to PMCIDs and Europe PMC IDs for full text retrieval \n",
    "5. DOME Registry entries will be downloaded as full text PDF files using EPMC API\n",
    "6. DOME Registry supplementary files will be downloaded using EPMC API\n",
    "7. DOME Registry title and abstracts enriched in TSV from EPMC to support data analysis\n",
    "8. Metadata and graphs produced on available DOME Registry articles retrieval\n",
    "\n",
    "#### To do: dockerise & put into simple run script vs jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latest DOME Registry contents will be downloaded by DOME Registry API call providing the .json file of DOME Registry data for the given day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists for storing DOME Registry JSON files, files will be stored here\n",
      "File already exists for today's date: DOME_Registry_JSON_Files/DOME_Registry_Contents_2025-11-20.json\n",
      "Skipping download. Delete the file manually if you want to re-download.\n",
      "Continuing with existing file...\n",
      "Using file: DOME_Registry_JSON_Files/DOME_Registry_Contents_2025-11-20.json\n",
      "Block 1 complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Use the DOME API to download all entries of the DOME Registry and store them in a json file \n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "# Define the URL for the call\n",
    "url = \"https://registry.dome-ml.org/api/review?skip=0&limit=250&text=%20&public=true&sort=publication.year&asc=true\"\n",
    "\n",
    "# Make an API request to the URL to check the response\n",
    "response = requests.get(url, headers={'accept': '*/*'})\n",
    "\n",
    "# Create folder to store all JSON files\n",
    "if not os.path.exists('DOME_Registry_JSON_Files'):\n",
    "    os.makedirs('DOME_Registry_JSON_Files')\n",
    "    print('Created folder for storing DOME Registry JSON files')\n",
    "else:\n",
    "    print('Folder already exists for storing DOME Registry JSON files, files will be stored here')\n",
    "\n",
    "# Specify the desired folder path for JSON files\n",
    "json_folder_path = \"DOME_Registry_JSON_Files\"\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the current date in ISO format for file naming\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Create the output file name \n",
    "    file_name = f\"DOME_Registry_Contents_{current_date}.json\"\n",
    "    json_file_path = os.path.join(json_folder_path, file_name)\n",
    "\n",
    "    # Check if the file pathway already exists\n",
    "    if os.path.exists(json_file_path):\n",
    "        print(f\"File already exists for today's date: {json_file_path}\")\n",
    "        print('Skipping download. Delete the file manually if you want to re-download.')\n",
    "        print('Continuing with existing file...')\n",
    "    else:\n",
    "        print('Downloading new file...')\n",
    "        # Save the content to a file\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"DOME Registry data downloaded and saved to '{json_file_path}'\")\n",
    "    \n",
    "    print(f\"Using file: {json_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the data. Status code: {response.status_code}\")\n",
    "    # Set json_file_path to None to prevent errors in subsequent cells\n",
    "    json_file_path = None\n",
    "\n",
    "print(\"Block 1 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DOME Registry data .json file will be flattened and converted into TSV for easier working with entries data (row and column based data format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read in JSON file.\n",
      "Flattened JSON data saved to 'DOME_Registry_JSON_Files/flattened_DOME_Registry_Contents_2025-11-20.json'\n",
      "Folder already exists for storing DOME Registry TSV files\n",
      "JSON data written to 'DOME_Registry_TSV_Files/flattened_DOME_Registry_Contents_2025-11-20.tsv'\n"
     ]
    }
   ],
   "source": [
    "# 2. Produce DOME Registry contents metadata .tsv file and data visualisation\n",
    "import json\n",
    "\n",
    "# 2.1 Pretty print DOME Registry contents JSON file for inspection to ensure all looks as expected \n",
    "# remove comment to activate print and debug where needed\n",
    "\n",
    "# Function to read in and pretty-print the JSON DOME Registry file entry\n",
    "def pretty_print_json(file_name):\n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Pretty-print the JSON data\n",
    "        print('Successfully read in JSON file.')\n",
    "        #print(json.dumps(data, indent=4))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the JSON file: {e}\")\n",
    "\n",
    "# Call the function to pretty-print the JSON file\n",
    "pretty_print_json(json_file_path)\n",
    "\n",
    "\n",
    "# 2.2 Flatten the JSON for easier data processing and write to a new .json file \n",
    "# Function to read JSON data\n",
    "def read_json(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to flatten JSON\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "# Function to save flattened JSON to a file\n",
    "def save_flattened_json(flattened_data, output_file_name):\n",
    "    try:\n",
    "        with open(output_file_name, 'w', encoding='utf-8') as file:\n",
    "            json.dump(flattened_data, file, indent=4)\n",
    "        print(f\"Flattened JSON data saved to '{output_file_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the flattened JSON file: {e}\")\n",
    "\n",
    "# Read JSON data\n",
    "data = read_json(json_file_path)\n",
    "\n",
    "# Flatten JSON data and save to a new JSON file\n",
    "if data:\n",
    "    flattened_data = [flatten_json(entry) for entry in data]\n",
    "    flattened_file_name = (\"flattened_\"+file_name)\n",
    "    # Make file path to save flattened JSON file\n",
    "    json_folder_path = \"DOME_Registry_JSON_Files\"\n",
    "    json_file_path = os.path.join(json_folder_path, flattened_file_name)\n",
    "    save_flattened_json(flattened_data, json_file_path)\n",
    "    # Print the flattened JSON data to view it\n",
    "\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "\n",
    "#2.3 Convert flattened json to tsv \n",
    "# Function to read flattened JSON data\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Function to read flattened JSON data\n",
    "def read_flattened_json(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the flattened JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# TSV folders created to store tsv\n",
    "if not os.path.exists('DOME_Registry_TSV_Files'):\n",
    "    print('Creating folder to store DOME Registry TSV files')\n",
    "    os.makedirs('DOME_Registry_TSV_Files')\n",
    "else:\n",
    "    print('Folder already exists for storing DOME Registry TSV files')\n",
    "\n",
    "# Function to write JSON data to a TSV file\n",
    "def write_json_to_tsv(json_data, tsv_file_name):\n",
    "    try:\n",
    "        # Determine all possible headers from the entire dataset\n",
    "        headers = set()\n",
    "        for entry in json_data:\n",
    "            headers.update(entry.keys())\n",
    "        headers = list(headers)\n",
    "        \n",
    "        # Write data to TSV file\n",
    "        with open(tsv_file_name, 'w', newline='', encoding='utf-8') as tsvfile:\n",
    "            writer = csv.DictWriter(tsvfile, fieldnames=headers, delimiter='\\t')\n",
    "            writer.writeheader()\n",
    "            for entry in json_data:\n",
    "                writer.writerow(entry)\n",
    "        \n",
    "        print(f\"JSON data written to '{tsv_file_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to the TSV file: {e}\")\n",
    "\n",
    "# Read flattened JSON data\n",
    "flattened_data = read_flattened_json(json_file_path)\n",
    "\n",
    "# Create TSV file name and file pathway\n",
    "tsv_file_name = flattened_file_name[:-5]+'.tsv'\n",
    "tsv_file_path = os.path.join('DOME_Registry_TSV_Files', tsv_file_name)\n",
    "\n",
    "# Process JSON data into TSV\n",
    "if flattened_data:\n",
    "    write_json_to_tsv(flattened_data, tsv_file_path)\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "\n",
    "tsv_file_name = flattened_file_name[:-5]+'.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DOME Registry TSV data file will be formatted with shortid as the row index and other fields cleaned (publication data) and ordered by D O M E fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered TSV data saved to 'DOME_Registry_TSV_Files/flattened_DOME_Registry_Contents_2025-11-20.tsv'\n"
     ]
    }
   ],
   "source": [
    "#3 Reorder TSV using pandas data frame \n",
    "import pandas as pd\n",
    "\n",
    "# Read the TSV file as a DataFrame using pandas\n",
    "df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "\n",
    "# Define the prefixes to match and group tsv data\n",
    "prefix_publications_cols = 'publication_'\n",
    "prefix_tags_cols = 'publication_tags_'\n",
    "prefix_data_cols = 'matches_data'\n",
    "prefix_optimization_cols = 'matches_optimization'\n",
    "prefix_model_cols = 'matches_model'\n",
    "prefix_evaluation_cols = 'matches_evaluation'\n",
    "\n",
    "# Separate columns based on whether they start with the prefix\n",
    "publication_columns = [col for col in df.columns if col.startswith(prefix_publications_cols) and not col.startswith(prefix_tags_cols)]\n",
    "publication_tags_columns = [col for col in df.columns if col.startswith(prefix_tags_cols)]\n",
    "# Sort tags columns numerically (e.g., publication_tags_0, publication_tags_1, ...)\n",
    "publication_tags_columns = sorted(publication_tags_columns, key=lambda x: int(x.split('_')[-1]) if x.split('_')[-1].isdigit() else 0)\n",
    "matches_data_columns = [col for col in df.columns if col.startswith(prefix_data_cols)]\n",
    "matches_optimization_columns = [col for col in df.columns if col.startswith(prefix_optimization_cols)]\n",
    "matches_model_columns = [col for col in df.columns if col.startswith(prefix_model_cols)]\n",
    "matches_evaluation_columns = [col for col in df.columns if col.startswith(prefix_evaluation_cols)]\n",
    "other_columns = [col for col in df.columns if not col.startswith('matches_') and not col.startswith('publication_')]\n",
    "\n",
    "# Reorder columns\n",
    "reordered_columns = (other_columns + publication_columns + publication_tags_columns + matches_data_columns +\n",
    "                     matches_optimization_columns + matches_model_columns + matches_evaluation_columns)\n",
    "df = df[reordered_columns]\n",
    "\n",
    "# Print the reordered DataFrame\n",
    "#print(df.head())\n",
    "\n",
    "df = pd.DataFrame(df).set_index('shortid')\n",
    "df.to_csv(tsv_file_path, sep='\\t', index=True, encoding='utf-8') \n",
    "\n",
    "print(f\"Reordered TSV data saved to '{tsv_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DOME Registry data tsv will have columns added with PMCIDs and Europe PMC IDs returned from DOI search using NCBI E-Utilities API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping columns not found. Proceeding with API calls...\n",
      "Warning: Potentially invalid DOI format: doi.org/10.1093/nar/gkae385\n",
      "Warning: Potentially invalid DOI format: doi.org/10.1093/nar/gkae385\n",
      "Cleaned 239 DOIs for processing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m id_mapping\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Map DOIs to PMCIDs and Europe PMC IDs\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m doi_to_id_mapping \u001b[38;5;241m=\u001b[39m map_dois_to_ids(dois)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Add the mapped IDs to the DataFrame\u001b[39;00m\n\u001b[1;32m    108\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmapped_pmcid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_doi\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: doi_to_id_mapping\u001b[38;5;241m.\u001b[39mget(x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://doi.org/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpmcid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[11], line 83\u001b[0m, in \u001b[0;36mmap_dois_to_ids\u001b[0;34m(dois, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m doi_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(batch)\n\u001b[1;32m     82\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoi_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&format=json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 83\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     85\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    266\u001b[0m         req,\n\u001b[1;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[1;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[1;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[1;32m    789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 741\u001b[0m     sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[1;32m    742\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    743\u001b[0m         cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[1;32m    744\u001b[0m         ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[1;32m    745\u001b[0m         ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[1;32m    746\u001b[0m         ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[1;32m    747\u001b[0m         ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[1;32m    748\u001b[0m         ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[1;32m    749\u001b[0m         ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[1;32m    750\u001b[0m         cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[1;32m    751\u001b[0m         key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[1;32m    752\u001b[0m         key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[1;32m    753\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname_rm_dot,\n\u001b[1;32m    754\u001b[0m         ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[1;32m    755\u001b[0m         tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    756\u001b[0m         assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[1;32m    757\u001b[0m         assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[1;32m    758\u001b[0m     )\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    761\u001b[0m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    921\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    922\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[1;32m    923\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[1;32m    924\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[1;32m    925\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[1;32m    926\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[1;32m    927\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[1;32m    928\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    929\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    930\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    931\u001b[0m )\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    456\u001b[0m         context\u001b[38;5;241m.\u001b[39mload_cert_chain(certfile, keyfile, key_password)\n\u001b[1;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[0;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    501\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[1;32m    456\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    457\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[1;32m    458\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[1;32m    459\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[1;32m    460\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m    463\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1039\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1319\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4. From DOIs get PMCIDs and Europe PMC IDs for full text search\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Read in DOME Entries TSV as dataframe via pandas library functions\n",
    "df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "\n",
    "# Check if mapping has already been completed\n",
    "if all(col in df.columns for col in ['mapped_pmcid', 'mapped_europepmc_id', 'mapped_pmid']):\n",
    "    existing_mappings = df['mapped_pmcid'].notna().sum()\n",
    "    if existing_mappings > 0:\n",
    "        print(f\"DOI-to-PMCID mapping already exists for {existing_mappings} entries.\")\n",
    "        print(\"Skipping API calls. Using existing mappings...\")\n",
    "        \n",
    "        # Ensure output file exists for next block\n",
    "        output_tsv_file_name = f'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_{current_date}.tsv'\n",
    "        if not os.path.exists(output_tsv_file_name):\n",
    "            df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "            print(f\"Saved existing mappings to '{output_tsv_file_name}'\")\n",
    "        else:\n",
    "            print(f\"Output file already exists: '{output_tsv_file_name}'\")\n",
    "        \n",
    "        print(\"\\nBlock 4 complete (skipped API calls).\")\n",
    "    else:\n",
    "        print(\"Mapping columns exist but are empty. Proceeding with API calls...\")\n",
    "        need_mapping = True\n",
    "else:\n",
    "    print(\"Mapping columns not found. Proceeding with API calls...\")\n",
    "    need_mapping = True\n",
    "\n",
    "# Only proceed with mapping if needed\n",
    "if 'need_mapping' in locals() and need_mapping:\n",
    "    # Extract DOIs from the DataFrame\n",
    "    dois = df['publication_doi'].dropna().unique()\n",
    "    print(f\"Found {len(dois)} unique DOIs to process\")\n",
    "\n",
    "    # Function to clean and normalize DOI strings\n",
    "    def clean_doi(doi_string):\n",
    "        \"\"\"\n",
    "        Clean DOI string by removing common prefixes and URLs.\n",
    "        Handles formats like:\n",
    "        - https://doi.org/10.1038/nature123\n",
    "        - http://dx.doi.org/10.1016/j.cell.2020\n",
    "        - doi:10.1126/science.abc456\n",
    "        - 10.1002/anie.202100001\n",
    "        \n",
    "        Returns clean DOI like: 10.1038/nature123\n",
    "        \"\"\"\n",
    "        if pd.isna(doi_string):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and strip whitespace\n",
    "        doi_string = str(doi_string).strip()\n",
    "        \n",
    "        # Remove common URL prefixes\n",
    "        doi_string = re.sub(r'^https?://doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        doi_string = re.sub(r'^https?://dx\\.doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        doi_string = re.sub(r'^https?://www\\.doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove 'doi:' prefix\n",
    "        doi_string = re.sub(r'^doi:\\s*', '', doi_string, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Strip any remaining whitespace\n",
    "        doi_string = doi_string.strip()\n",
    "        \n",
    "        # Validate that it starts with '10.' (all DOIs start with 10.)\n",
    "        if not doi_string.startswith('10.'):\n",
    "            print(f\"Warning: Potentially invalid DOI format: {doi_string}\")\n",
    "        \n",
    "        return doi_string\n",
    "\n",
    "    # Clean the DOIs\n",
    "    print(\"Cleaning DOI strings...\")\n",
    "    dois = [clean_doi(doi) for doi in dois if clean_doi(doi) is not None]\n",
    "    print(f\"Cleaned {len(dois)} DOIs for processing\\n\")\n",
    "\n",
    "    # Map DOIs to PMCIDs and Europe PMC IDs using NCBI E-utilities API\n",
    "    def map_dois_to_ids(dois, batch_size=1):\n",
    "        id_mapping = {}\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        \n",
    "        print(f\"Starting DOI-to-PMCID mapping for {len(dois)} DOIs...\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for i in range(0, len(dois), batch_size):\n",
    "            batch = dois[i:i + batch_size]\n",
    "            doi_str = ','.join(batch)\n",
    "            \n",
    "            # Progress indicator\n",
    "            progress = i + len(batch)\n",
    "            print(f\"[{progress}/{len(dois)}] Processing: {batch[0][:50]}...\")\n",
    "            \n",
    "            url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids={doi_str}&format=json\"\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    records = data.get('records', [])\n",
    "                    if records:\n",
    "                        for record in records:\n",
    "                            doi = record.get('doi')\n",
    "                            pmcid = record.get('pmcid')\n",
    "                            pmid = record.get('pmid')\n",
    "                            # Europe PMC ID is typically the PMCID without the 'PMC' prefix, or the PMID\n",
    "                            europepmc_id = pmcid if pmcid else (f\"MED/{pmid}\" if pmid else None)\n",
    "                            id_mapping[doi] = {\n",
    "                                'pmcid': pmcid,\n",
    "                                'europepmc_id': europepmc_id,\n",
    "                                'pmid': pmid\n",
    "                            }\n",
    "                            \n",
    "                            if pmcid:\n",
    "                                print(f\"  ✓ Mapped to PMCID: {pmcid}\")\n",
    "                                success_count += 1\n",
    "                            elif pmid:\n",
    "                                print(f\"  ✓ Mapped to PMID: {pmid} (no PMCID)\")\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                print(f\"  ✗ No PMCID/PMID found\")\n",
    "                                fail_count += 1\n",
    "                    else:\n",
    "                        print(f\"  ✗ No mapping found\")\n",
    "                        for doi in batch:\n",
    "                            id_mapping[doi] = {'pmcid': None, 'europepmc_id': None, 'pmid': None}\n",
    "                        fail_count += 1\n",
    "                else:\n",
    "                    print(f\"  ✗ API Error (status {response.status_code})\")\n",
    "                    for doi in batch:\n",
    "                        id_mapping[doi] = {'pmcid': None, 'europepmc_id': None, 'pmid': None}\n",
    "                    fail_count += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {str(e)}\")\n",
    "                for doi in batch:\n",
    "                    id_mapping[doi] = {'pmcid': None, 'europepmc_id': None, 'pmid': None}\n",
    "                fail_count += 1\n",
    "            \n",
    "            # Rate limiting - be respectful to the API\n",
    "            if i + batch_size < len(dois):\n",
    "                import time\n",
    "                time.sleep(0.3)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DOI-TO-PMCID MAPPING SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Successfully mapped: {success_count}\")\n",
    "        print(f\"Failed/not available: {fail_count}\")\n",
    "        print(f\"Success rate: {success_count/len(dois)*100:.1f}%\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return id_mapping\n",
    "\n",
    "    # Map DOIs to PMCIDs and Europe PMC IDs\n",
    "    doi_to_id_mapping = map_dois_to_ids(dois)\n",
    "\n",
    "    # Add the mapped IDs to the DataFrame\n",
    "    print(\"Adding mapped IDs to DataFrame...\")\n",
    "    df['mapped_pmcid'] = df['publication_doi'].apply(lambda x: doi_to_id_mapping.get(x.replace('https://doi.org/', '') if pd.notna(x) else None, {}).get('pmcid'))\n",
    "    df['mapped_europepmc_id'] = df['publication_doi'].apply(lambda x: doi_to_id_mapping.get(x.replace('https://doi.org/', '') if pd.notna(x) else None, {}).get('europepmc_id'))\n",
    "    df['mapped_pmid'] = df['publication_doi'].apply(lambda x: doi_to_id_mapping.get(x.replace('https://doi.org/', '') if pd.notna(x) else None, {}).get('pmid'))\n",
    "\n",
    "    # Save the updated DataFrame to a new TSV file\n",
    "    output_tsv_file_name = f'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_{current_date}.tsv'\n",
    "    df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "    print(f\"✓ Updated DataFrame with mapped PMCIDs and Europe PMC IDs saved to '{output_tsv_file_name}'\")\n",
    "    print(\"\\nBlock 4 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use EPMC API to download full text PDFs of all DOME Registry entries and store in folder named DOME_Registry_PMC_PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_tsv_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Read in DOME Entries TSV as dataframe via pandas library functions\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(output_tsv_file_name, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract PMCIDs from the DataFrame\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pmcids \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmapped_pmcid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_tsv_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. Download full text PDFs using PMCIDs from Europe PMC\n",
    "# Note: Europe PMC does not directly provide PDFs through REST API - we need to use alternative methods\n",
    " \n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Read in DOME Entries TSV as dataframe via pandas library functions\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Extract PMCIDs from the DataFrame\n",
    "pmcids = df['mapped_pmcid'].dropna().unique()\n",
    "\n",
    "# Define the output folder for PDF files\n",
    "output_folder = 'DOME_Registry_PMC_PDFs'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Track which PMCIDs need downloading (skip already downloaded)\n",
    "to_download_pmcid = []\n",
    "for pmcid in pmcids:\n",
    "    if os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "        print(f\"PDF for PMCID {pmcid} already downloaded.\")\n",
    "    else:\n",
    "        print(f\"PDF for PMCID {pmcid} not yet downloaded.\")\n",
    "        to_download_pmcid.append(pmcid)\n",
    "\n",
    "print(f\"\\nNeed to download {len(to_download_pmcid)} PDFs out of {len(pmcids)} total entries.\\n\")\n",
    "\n",
    "# Function to download full text PDF and supplementary materials\n",
    "def download_pdfs(pmcids):\n",
    "    \"\"\"\n",
    "    Download PDFs from Europe PMC. \n",
    "    Note: Direct PDF downloads are not always available through Europe PMC REST API.\n",
    "    We'll try multiple approaches:\n",
    "    1. Try to get PDF link from article metadata\n",
    "    2. Download supplementary files if available\n",
    "    3. Construct publisher URLs where possible\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    supp_count = 0\n",
    "    \n",
    "    for idx, pmcid in enumerate(pmcids, 1):\n",
    "        print(f\"[{idx}/{len(pmcids)}] Processing {pmcid}...\")\n",
    "        \n",
    "        # Clean PMCID (remove 'PMC' prefix for some API calls)\n",
    "        clean_pmcid = pmcid.replace('PMC', '') if pmcid.startswith('PMC') else pmcid\n",
    "        \n",
    "        # Try Method 1: Get article metadata to find PDF link\n",
    "        try:\n",
    "            metadata_url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=PMCID:{pmcid}&resultType=core&format=json\"\n",
    "            metadata_response = requests.get(metadata_url, timeout=30)\n",
    "            \n",
    "            if metadata_response.status_code == 200:\n",
    "                metadata = metadata_response.json()\n",
    "                \n",
    "                if metadata.get('hitCount', 0) > 0:\n",
    "                    result = metadata['resultList']['result'][0]\n",
    "                    \n",
    "                    # Try to get PDF link from fullTextUrlList\n",
    "                    if 'fullTextUrlList' in result and result['fullTextUrlList']:\n",
    "                        for url_info in result['fullTextUrlList']['fullTextUrl']:\n",
    "                            if url_info.get('documentStyle') == 'pdf' or url_info.get('availabilityCode') == 'OA':\n",
    "                                pdf_url = url_info.get('url')\n",
    "                                \n",
    "                                if pdf_url and '.pdf' in pdf_url.lower():\n",
    "                                    # Try to download the PDF\n",
    "                                    pdf_response = requests.get(pdf_url, timeout=30, allow_redirects=True)\n",
    "                                    \n",
    "                                    if pdf_response.status_code == 200 and pdf_response.headers.get('Content-Type', '').startswith('application/pdf'):\n",
    "                                        output_file = os.path.join(output_folder, f\"{pmcid}_main.pdf\")\n",
    "                                        with open(output_file, 'wb') as file:\n",
    "                                            file.write(pdf_response.content)\n",
    "                                        print(f\"  ✓ Downloaded main PDF from publisher\")\n",
    "                                        success_count += 1\n",
    "                                        break\n",
    "                    \n",
    "                    # If no PDF found yet, try PMC OA service\n",
    "                    if not os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "                        # Try Europe PMC OA PDF service (different endpoint)\n",
    "                        pmc_oa_url = f\"https://europepmc.org/articles/{pmcid}?pdf=render\"\n",
    "                        pmc_response = requests.get(pmc_oa_url, timeout=30, allow_redirects=True)\n",
    "                        \n",
    "                        if pmc_response.status_code == 200 and len(pmc_response.content) > 1000:\n",
    "                            # Check if it's actually a PDF\n",
    "                            if pmc_response.content[:4] == b'%PDF':\n",
    "                                output_file = os.path.join(output_folder, f\"{pmcid}_main.pdf\")\n",
    "                                with open(output_file, 'wb') as file:\n",
    "                                    file.write(pmc_response.content)\n",
    "                                print(f\"  ✓ Downloaded main PDF from PMC OA service\")\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                print(f\"  ✗ Could not retrieve PDF (not openly available)\")\n",
    "                                fail_count += 1\n",
    "                        else:\n",
    "                            print(f\"  ✗ Could not retrieve PDF (status: {pmc_response.status_code})\")\n",
    "                            fail_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error downloading main PDF: {str(e)}\")\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Try to download supplementary files\n",
    "        try:\n",
    "            supp_url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/supplementaryFiles\"\n",
    "            supp_response = requests.get(supp_url, timeout=30)\n",
    "            \n",
    "            if supp_response.status_code == 200:\n",
    "                try:\n",
    "                    supp_data = supp_response.json()\n",
    "                    \n",
    "                    if 'supplementaryFiles' in supp_data and supp_data['supplementaryFiles']:\n",
    "                        for idx_supp, supp_file in enumerate(supp_data['supplementaryFiles'], 1):\n",
    "                            file_url = supp_file.get('url')\n",
    "                            \n",
    "                            if file_url:\n",
    "                                # Download all supplementary files (not just PDFs)\n",
    "                                try:\n",
    "                                    file_response = requests.get(file_url, timeout=30, allow_redirects=True)\n",
    "                                    \n",
    "                                    if file_response.status_code == 200:\n",
    "                                        # Determine file extension from URL or content-type\n",
    "                                        file_ext = ''\n",
    "                                        if '.pdf' in file_url.lower():\n",
    "                                            file_ext = '.pdf'\n",
    "                                        elif '.xlsx' in file_url.lower() or '.xls' in file_url.lower():\n",
    "                                            file_ext = '.xlsx'\n",
    "                                        elif '.docx' in file_url.lower() or '.doc' in file_url.lower():\n",
    "                                            file_ext = '.docx'\n",
    "                                        elif '.zip' in file_url.lower():\n",
    "                                            file_ext = '.zip'\n",
    "                                        else:\n",
    "                                            # Try to get from content-type\n",
    "                                            content_type = file_response.headers.get('Content-Type', '')\n",
    "                                            if 'pdf' in content_type:\n",
    "                                                file_ext = '.pdf'\n",
    "                                            elif 'excel' in content_type or 'spreadsheet' in content_type:\n",
    "                                                file_ext = '.xlsx'\n",
    "                                            else:\n",
    "                                                file_ext = '.dat'  # Default extension\n",
    "                                        \n",
    "                                        supp_output_file = os.path.join(output_folder, f\"{pmcid}_supp_{idx_supp}{file_ext}\")\n",
    "                                        with open(supp_output_file, 'wb') as file:\n",
    "                                            file.write(file_response.content)\n",
    "                                        print(f\"  ✓ Downloaded supplementary file {idx_supp}{file_ext}\")\n",
    "                                        supp_count += 1\n",
    "                                \n",
    "                                except Exception as e_supp:\n",
    "                                    print(f\"  ⚠ Could not download supplementary file {idx_supp}: {str(e_supp)}\")\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    pass  # No supplementary files available\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass  # Supplementary files not critical, continue\n",
    "        \n",
    "        # Rate limiting - be respectful to the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Main PDFs successfully downloaded: {success_count}\")\n",
    "    print(f\"Main PDFs failed/not available: {fail_count}\")\n",
    "    print(f\"Supplementary files downloaded: {supp_count}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Download PDFs for each PMCID that hasn't been downloaded yet\n",
    "if to_download_pmcid:\n",
    "    download_pdfs(to_download_pmcid)\n",
    "else:\n",
    "    print(\"All PDFs already downloaded. Skipping download step.\")\n",
    "\n",
    "# Update the TSV with download status\n",
    "print(\"Updating TSV with PDF download status...\")\n",
    "pdf_downloadable = []\n",
    "\n",
    "for pmcid in df['mapped_pmcid']:\n",
    "    if pd.notna(pmcid) and os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "        pdf_downloadable.append('yes')\n",
    "    else:\n",
    "        pdf_downloadable.append('no')\n",
    "\n",
    "# Add the new column of download status to the DataFrame and save\n",
    "df['pdf_downloadable'] = pdf_downloadable\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Updated TSV with PDF download status saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "print(\"\\nBlock 5 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download supplementary files (PDFs and DOC files) using Europe PMC supplementary files API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_tsv_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Read in DOME Entries TSV as dataframe\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(output_tsv_file_name, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract PMCIDs from the DataFrame\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmapped_pmcid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_tsv_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "# 6. Download supplementary files using NCBI OA API and organize by PMCID\n",
    " \n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "# Read in DOME Entries TSV as dataframe\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Extract PMCIDs from the DataFrame\n",
    "if 'mapped_pmcid' in df.columns:\n",
    "    pmcids = df['mapped_pmcid'].dropna().unique()\n",
    "    print(f\"Found {len(pmcids)} PMCIDs to process for supplementary files\")\n",
    "else:\n",
    "    print(\"Error: 'mapped_pmcid' column not found in TSV.\")\n",
    "    pmcids = []\n",
    "\n",
    "# Define the output folder for supplementary files\n",
    "supp_output_folder = 'DOME_Registry_PMC_Supplementary'\n",
    "os.makedirs(supp_output_folder, exist_ok=True)\n",
    "\n",
    "# Human-readable file extensions to keep\n",
    "KEEP_EXTENSIONS = {'.pdf', '.doc', '.docx', '.txt', '.xml', '.csv', '.tsv', \n",
    "                   '.xls', '.xlsx', '.html', '.htm', '.rtf', '.md'}\n",
    "\n",
    "# Track which PMCIDs need downloading\n",
    "to_download_pmcids = []\n",
    "already_downloaded = 0\n",
    "\n",
    "for pmcid in pmcids:\n",
    "    pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "    # Check if folder exists and has files\n",
    "    if os.path.exists(pmcid_folder):\n",
    "        files = [f for f in os.listdir(pmcid_folder) if os.path.isfile(os.path.join(pmcid_folder, f))]\n",
    "        if files:\n",
    "            print(f\"Supplementary files for {pmcid} already downloaded ({len(files)} files).\")\n",
    "            already_downloaded += 1\n",
    "        else:\n",
    "            # Folder exists but empty - re-download\n",
    "            to_download_pmcids.append(pmcid)\n",
    "    else:\n",
    "        to_download_pmcids.append(pmcid)\n",
    "\n",
    "print(f\"\\nAlready downloaded: {already_downloaded} PMCIDs\")\n",
    "print(f\"Need to download: {len(to_download_pmcids)} PMCIDs\\n\")\n",
    "\n",
    "# Function to download and extract supplementary files from NCBI OA\n",
    "def download_supplementary_files(pmcids):\n",
    "    \"\"\"\n",
    "    Download supplementary files from NCBI PMC Open Access FTP.\n",
    "    Uses NCBI OA API to find FTP location for each PMCID.\n",
    "    Extracts only human-readable files (PDF, DOC, TXT, etc.).\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    total_files_extracted = 0\n",
    "    \n",
    "    for idx, pmcid in enumerate(pmcids, 1):\n",
    "        print(f\"[{idx}/{len(pmcids)}] Processing {pmcid}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create folder for this PMCID\n",
    "            pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "            os.makedirs(pmcid_folder, exist_ok=True)\n",
    "            \n",
    "            # Use NCBI OA API to find the exact FTP path\n",
    "            api_url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmcid}\"\n",
    "            \n",
    "            with urllib.request.urlopen(api_url, timeout=30) as response:\n",
    "                xml_data = response.read()\n",
    "            \n",
    "            # Parse the XML to find the FTP link (format=\"tgz\" contains all files)\n",
    "            root = ET.fromstring(xml_data)\n",
    "            link_element = root.find(\".//link[@format='tgz']\")\n",
    "            \n",
    "            if link_element is not None:\n",
    "                ftp_url = link_element.get(\"href\")\n",
    "                tar_filename = os.path.join(pmcid_folder, f\"{pmcid}.tar.gz\")\n",
    "                \n",
    "                print(f\"  Found FTP location: {ftp_url}\")\n",
    "                print(f\"  Downloading...\")\n",
    "                \n",
    "                # Download the .tar.gz file\n",
    "                urllib.request.urlretrieve(ftp_url, tar_filename)\n",
    "                \n",
    "                print(f\"  Extracting files...\")\n",
    "                \n",
    "                # Extract and filter files\n",
    "                files_kept = 0\n",
    "                with tarfile.open(tar_filename, \"r:gz\") as tar:\n",
    "                    for member in tar.getmembers():\n",
    "                        if member.isfile():\n",
    "                            # Get file extension\n",
    "                            _, ext = os.path.splitext(member.name.lower())\n",
    "                            \n",
    "                            # Keep only human-readable files\n",
    "                            if ext in KEEP_EXTENSIONS:\n",
    "                                # Extract with sanitized filename (remove directory structure)\n",
    "                                member.name = os.path.basename(member.name)\n",
    "                                tar.extract(member, path=pmcid_folder)\n",
    "                                files_kept += 1\n",
    "                                print(f\"    ✓ Extracted: {member.name}\")\n",
    "                            else:\n",
    "                                print(f\"    ✗ Skipped (not human-readable): {member.name}\")\n",
    "                \n",
    "                # Remove the .tar.gz file after extraction\n",
    "                os.remove(tar_filename)\n",
    "                \n",
    "                if files_kept > 0:\n",
    "                    print(f\"  ✓ Successfully extracted {files_kept} human-readable file(s)\")\n",
    "                    success_count += 1\n",
    "                    total_files_extracted += files_kept\n",
    "                else:\n",
    "                    print(f\"  ⚠ No human-readable files found in package\")\n",
    "                    # Remove empty folder\n",
    "                    try:\n",
    "                        os.rmdir(pmcid_folder)\n",
    "                    except:\n",
    "                        pass\n",
    "                    fail_count += 1\n",
    "                    \n",
    "            else:\n",
    "                print(f\"  ✗ No open access FTP link found (may be under copyright)\")\n",
    "                # Remove empty folder\n",
    "                try:\n",
    "                    os.rmdir(pmcid_folder)\n",
    "                except:\n",
    "                    pass\n",
    "                fail_count += 1\n",
    "                \n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"  ✗ HTTP Error: {e.code} - {e.reason}\")\n",
    "            fail_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {str(e)}\")\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Rate limiting - be respectful to the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUPPLEMENTARY FILES DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"PMCIDs with supplementary files downloaded: {success_count}\")\n",
    "    print(f\"PMCIDs without supplementary files/failed: {fail_count}\")\n",
    "    print(f\"Total human-readable files extracted: {total_files_extracted}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Download supplementary files only for those that need it\n",
    "if to_download_pmcids:\n",
    "    download_supplementary_files(to_download_pmcids)\n",
    "else:\n",
    "    print(\"All supplementary files already downloaded. Skipping download step.\\n\")\n",
    "\n",
    "# Update TSV with supplementary files information\n",
    "print(\"Updating TSV with supplementary files information...\")\n",
    "\n",
    "supp_download_status = []\n",
    "supp_file_count = []\n",
    "supp_file_list = []\n",
    "\n",
    "for pmcid in df['mapped_pmcid']:\n",
    "    if pd.notna(pmcid):\n",
    "        pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "        \n",
    "        if os.path.exists(pmcid_folder):\n",
    "            # Count files in the folder\n",
    "            files = [f for f in os.listdir(pmcid_folder) if os.path.isfile(os.path.join(pmcid_folder, f))]\n",
    "            count = len(files)\n",
    "            \n",
    "            if count > 0:\n",
    "                supp_download_status.append('yes')\n",
    "                supp_file_count.append(count)\n",
    "                supp_file_list.append('; '.join(files))\n",
    "            else:\n",
    "                supp_download_status.append('no')\n",
    "                supp_file_count.append(0)\n",
    "                supp_file_list.append(None)\n",
    "        else:\n",
    "            supp_download_status.append('no')\n",
    "            supp_file_count.append(0)\n",
    "            supp_file_list.append(None)\n",
    "    else:\n",
    "        supp_download_status.append('no')\n",
    "        supp_file_count.append(0)\n",
    "        supp_file_list.append(None)\n",
    "\n",
    "# Add columns to DataFrame\n",
    "df['supplementary_downloadable'] = supp_download_status\n",
    "df['supplementary_file_count'] = supp_file_count\n",
    "df['supplementary_file_list'] = supp_file_list\n",
    "\n",
    "# Save updated TSV\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Updated TSV saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "# Print statistics\n",
    "total_supp_files = sum(supp_file_count)\n",
    "entries_with_supp = sum(1 for status in supp_download_status if status == 'yes')\n",
    "\n",
    "print(f\"\\nSupplementary Files Statistics:\")\n",
    "print(f\"  Total supplementary files: {total_supp_files}\")\n",
    "print(f\"  Entries with supplementary files: {entries_with_supp}/{len(df)}\")\n",
    "if len(df) > 0:\n",
    "    print(f\"  Success rate: {entries_with_supp/len(df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nBlock 6 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enrich TSV with title and abstract data from Europe PMC for all DOME Registry entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_tsv_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read in DOME Entries TSV as dataframe via pandas library functions\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(output_tsv_file_name, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Check if title and abstract columns already exist\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_title\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_abstract\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_tsv_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. Enrich the TSV file with title and abstract columns from Europe PMC\n",
    " \n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Read in DOME Entries TSV as dataframe via pandas library functions\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Check if title and abstract columns already exist\n",
    "if 'article_title' in df.columns and 'article_abstract' in df.columns:\n",
    "    print(\"Title and abstract columns already exist in TSV.\")\n",
    "    print(\"Checking for entries that need to be enriched...\")\n",
    "    # Count how many entries already have data\n",
    "    existing_count = df['article_title'].notna().sum()\n",
    "    print(f\"{existing_count} out of {len(df)} entries already have title/abstract data.\")\n",
    "else:\n",
    "    print(\"Adding new columns for title and abstract...\")\n",
    "    df['article_title'] = None\n",
    "    df['article_abstract'] = None\n",
    "\n",
    "# Function to fetch article details from Europe PMC\n",
    "def fetch_article_details(pmcid):\n",
    "    \"\"\"\n",
    "    Fetch title and abstract for a given PMCID from Europe PMC.\n",
    "    \n",
    "    Args:\n",
    "        pmcid (str): PubMed Central ID\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (title, abstract) or (None, None) if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=PMCID:{pmcid}&resultType=core&format=json\"\n",
    "        response = requests.get(url, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('hitCount', 0) > 0:\n",
    "                article = data['resultList']['result'][0]\n",
    "                title = article.get('title', None)\n",
    "                abstract = article.get('abstractText', None)\n",
    "                return title, abstract\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error fetching details: {str(e)}\")\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Process each row and enrich with title/abstract if needed\n",
    "print(\"\\nEnriching TSV with title and abstract data...\")\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "skip_count = 0\n",
    "total_to_process = 0\n",
    "\n",
    "# First, count how many need processing\n",
    "for idx, row in df.iterrows():\n",
    "    pmcid = row.get('mapped_pmcid')\n",
    "    if pd.notna(pmcid):\n",
    "        # Check if this entry already has title/abstract\n",
    "        if pd.isna(row.get('article_title')) or pd.isna(row.get('article_abstract')):\n",
    "            total_to_process += 1\n",
    "\n",
    "print(f\"Need to enrich {total_to_process} entries with title/abstract data.\\n\")\n",
    "\n",
    "# Now process the entries\n",
    "processed = 0\n",
    "for idx, row in df.iterrows():\n",
    "    pmcid = row.get('mapped_pmcid')\n",
    "    \n",
    "    if pd.notna(pmcid):\n",
    "        # Check if this entry already has title/abstract\n",
    "        if pd.isna(row.get('article_title')) or pd.isna(row.get('article_abstract')):\n",
    "            processed += 1\n",
    "            print(f\"[{processed}/{total_to_process}] Processing {pmcid}...\")\n",
    "            \n",
    "            title, abstract = fetch_article_details(pmcid)\n",
    "            \n",
    "            if title and abstract:\n",
    "                df.at[idx, 'article_title'] = title\n",
    "                df.at[idx, 'article_abstract'] = abstract\n",
    "                print(f\"  ✓ Added title and abstract\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"  ✗ Failed to retrieve article details\")\n",
    "                fail_count += 1\n",
    "            \n",
    "            # Rate limiting - be respectful to the API\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            skip_count += 1\n",
    "    else:\n",
    "        # No PMCID available\n",
    "        skip_count += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TITLE/ABSTRACT ENRICHMENT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully enriched: {success_count}\")\n",
    "print(f\"Failed/not available: {fail_count}\")\n",
    "print(f\"Skipped (already had data or no PMCID): {skip_count}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save the enriched TSV\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Enriched TSV with title and abstract columns saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "# Show a sample of the enriched data\n",
    "print(f\"\\nSample of enriched data (first 3 rows with title/abstract):\")\n",
    "sample_df = df[df['article_title'].notna()][['mapped_pmcid', 'article_title', 'article_abstract']].head(3)\n",
    "if not sample_df.empty:\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nPMCID: {row['mapped_pmcid']}\")\n",
    "        print(f\"Title: {row['article_title'][:100]}...\" if len(str(row['article_title'])) > 100 else f\"Title: {row['article_title']}\")\n",
    "        print(f\"Abstract: {str(row['article_abstract'])[:150]}...\" if len(str(row['article_abstract'])) > 150 else f\"Abstract: {row['article_abstract']}\")\n",
    "else:\n",
    "    print(\"No entries with title/abstract data found.\")\n",
    "\n",
    "print(\"\\nBlock 7 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abstract_title_downloadable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abstract_title_downloadable'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m pdf_yes \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_downloadable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m pdf_no \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_downloadable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m abstract_title_yes \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract_title_downloadable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m abstract_title_no \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract_title_downloadable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Create a metadata DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abstract_title_downloadable'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Create metadata file readout as a TSV, corresponding text file to explain contents and graphs to go with these\n",
    "# Metadata file readout as TSV and text file to explain contents and graph visualisation of data validation \n",
    "\n",
    "#REMOVE LATER - UPDATE TO HANDLE SUPPLEMNATRY FILES AND FULL TEXT ETC ALSO GET RETURNS OF NON DOI GETTING FULLL TEXT PDF AND WHERE POSSIBLE LICENSING INFO \n",
    "\n",
    "#import libraries\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in the TSV file\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Calculate metadata\n",
    "total_entries = len(df)\n",
    "pdf_yes = df['pdf_downloadable'].value_counts().get('yes', 0)\n",
    "pdf_no = df['pdf_downloadable'].value_counts().get('no', 0)\n",
    "\n",
    "# Check for title/abstract data availability\n",
    "title_abstract_available = (df['article_title'].notna() & df['article_abstract'].notna()).sum()\n",
    "title_abstract_missing = total_entries - title_abstract_available\n",
    "\n",
    "# Check for supplementary files\n",
    "entries_with_supp = (df['supplementary_file_count'] > 0).sum()\n",
    "entries_without_supp = total_entries - entries_with_supp\n",
    "total_supp_files = df['supplementary_file_count'].sum()\n",
    "\n",
    "# Create a metadata DataFrame\n",
    "metadata = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Entries', \n",
    "        'PDF Available (Yes)', \n",
    "        'PDF Available (No)', \n",
    "        'Title/Abstract Available', \n",
    "        'Title/Abstract Missing',\n",
    "        'Entries with Supplementary Files',\n",
    "        'Entries without Supplementary Files',\n",
    "        'Total Supplementary Files Count'\n",
    "    ],\n",
    "    'Count': [\n",
    "        total_entries, \n",
    "        pdf_yes, \n",
    "        pdf_no, \n",
    "        title_abstract_available, \n",
    "        title_abstract_missing,\n",
    "        entries_with_supp,\n",
    "        entries_without_supp,\n",
    "        int(total_supp_files)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Save metadata to a new TSV file\n",
    "metadata_tsv_path = 'DOME_Registry_TSV_Files/DOME_Metadata.tsv'\n",
    "metadata.to_csv(metadata_tsv_path, sep='\\t', index=False)\n",
    "print(f\"Metadata saved to '{metadata_tsv_path}'\")\n",
    "\n",
    "# Plot bar charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Bar chart for PDF availability\n",
    "axes[0].bar(['Yes', 'No'], [pdf_yes, pdf_no], color=['green', 'red'])\n",
    "axes[0].set_title('PDF Availability')\n",
    "axes[0].set_xlabel('Availability')\n",
    "axes[0].set_ylabel('Paper count')\n",
    "\n",
    "# Bar chart for title/abstract availability\n",
    "axes[1].bar(['Available', 'Missing'], [title_abstract_available, title_abstract_missing], color=['green', 'red'])\n",
    "axes[1].set_title('Title/Abstract Data Availability')\n",
    "axes[1].set_xlabel('Availability')\n",
    "axes[1].set_ylabel('Paper count')\n",
    "\n",
    "# Bar chart for supplementary files\n",
    "axes[2].bar(['With Supp Files', 'Without Supp Files'], [entries_with_supp, entries_without_supp], color=['blue', 'gray'])\n",
    "axes[2].set_title('Supplementary Files Availability')\n",
    "axes[2].set_xlabel('Availability')\n",
    "axes[2].set_ylabel('Paper count')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image file\n",
    "plot_image_path = 'DOME_Registry_TSV_Files/DOME_Metadata_Bar_Charts.png'\n",
    "plt.savefig(plot_image_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Bar charts saved to '{plot_image_path}'\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print detailed summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DOME REGISTRY METADATA SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total entries: {total_entries}\")\n",
    "print(f\"PDFs available: {pdf_yes} ({pdf_yes/total_entries*100:.1f}%)\")\n",
    "print(f\"Title/Abstract available: {title_abstract_available} ({title_abstract_available/total_entries*100:.1f}%)\")\n",
    "print(f\"Entries with supplementary files: {entries_with_supp} ({entries_with_supp/total_entries*100:.1f}%)\")\n",
    "print(f\"Total supplementary files: {int(total_supp_files)}\")\n",
    "if entries_with_supp > 0:\n",
    "    print(f\"Average supplementary files per entry (with supp): {total_supp_files/entries_with_supp:.2f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# To add\n",
    "# 8.2 Turn TSV data into corresponding text file to verbally explain metrics\n",
    "# 8.3 Turn TSV into corresponding graphed data to visualise the metrics\n",
    "\n",
    "print(\"\\nBlock 8 complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
