{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Python script to download latest DOME Registry contents, related full text papers & provide DOME Registry entries metadata read out (20241220)**\n",
    "1. DOME Registry contents will be downloaded by API call providing the json file of DOME Registry data\n",
    "2. DOME Registry data json will be flattened and converted into TSV for working with entries data (row based data)\n",
    "3. DOME Registry TSV will be checked and used to produce a metadata readout file (+ graphs)\n",
    "4. DOME Registry DOIs of articles will be converted to PMCIDs and Europe PMC IDs for full text retrieval \n",
    "5. DOME Registry entries will be downloaded as full text PDF files using EPMC API\n",
    "6. DOME Registry supplementary files will be downloaded using EPMC API\n",
    "7. DOME Registry title and abstracts enriched in TSV from EPMC to support data analysis\n",
    "8. Metadata and graphs produced on available DOME Registry articles retrieval\n",
    "\n",
    "#### To do: dockerise & put into simple run script vs jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latest DOME Registry contents will be downloaded by DOME Registry API call providing the .json file of DOME Registry data for the given day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists for storing DOME Registry JSON files, files will be stored here\n",
      "Number of DOME Registry entries returned from API: 275\n",
      "File already exists for today's date: DOME_Registry_JSON_Files/DOME_Registry_Contents_2025-11-25.json\n",
      "Skipping download. Delete the file manually if you want to re-download.\n",
      "Continuing with existing file...\n",
      "Using file: DOME_Registry_JSON_Files/DOME_Registry_Contents_2025-11-25.json\n",
      "Block 1 complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Use the DOME API to download all entries of the DOME Registry and store them in a json file \n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "# Define the URL for the call\n",
    "url = \"https://registry.dome-ml.org/api/review?skip=0&limit=1000&text=%20&public=true&sort=publication.year&asc=true\"\n",
    "\n",
    "# Make an API request to the URL to check the response\n",
    "response = requests.get(url, headers={'accept': '*/*'})\n",
    "\n",
    "# Create folder to store all JSON files\n",
    "if not os.path.exists('DOME_Registry_JSON_Files'):\n",
    "    os.makedirs('DOME_Registry_JSON_Files')\n",
    "    print('Created folder for storing DOME Registry JSON files')\n",
    "else:\n",
    "    print('Folder already exists for storing DOME Registry JSON files, files will be stored here')\n",
    "\n",
    "# Specify the desired folder path for JSON files\n",
    "json_folder_path = \"DOME_Registry_JSON_Files\"\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the current date in ISO format for file naming\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Create the output file name \n",
    "    file_name = f\"DOME_Registry_Contents_{current_date}.json\"\n",
    "    json_file_path = os.path.join(json_folder_path, file_name)\n",
    "\n",
    "    # Print number of entries returned by API\n",
    "    try:\n",
    "        entries_count = len(response.json())\n",
    "        print(f\"Number of DOME Registry entries returned from API: {entries_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine number of entries: {e}\")\n",
    "\n",
    "    # Check if the file pathway already exists\n",
    "    if os.path.exists(json_file_path):\n",
    "        print(f\"File already exists for today's date: {json_file_path}\")\n",
    "        print('Skipping download. Delete the file manually if you want to re-download.')\n",
    "        print('Continuing with existing file...')\n",
    "    else:\n",
    "        print('Downloading new file...')\n",
    "        # Save the content to a file\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"DOME Registry data downloaded and saved to '{json_file_path}'\")\n",
    "    \n",
    "    print(f\"Using file: {json_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the data. Status code: {response.status_code}\")\n",
    "    # Set json_file_path to None to prevent errors in subsequent cells\n",
    "    json_file_path = None\n",
    "\n",
    "print(\"Block 1 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DOME Registry data .json file will be flattened and converted into TSV for easier working with entries data (row and column based data format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read in JSON file.\n",
      "Flattened JSON data saved to 'DOME_Registry_JSON_Files/flattened_DOME_Registry_Contents_2025-11-25.json'\n",
      "Folder already exists for storing DOME Registry TSV files\n",
      "JSON data written to 'DOME_Registry_TSV_Files/flattened_DOME_Registry_Contents_2025-11-25.tsv'\n"
     ]
    }
   ],
   "source": [
    "# 2. Produce DOME Registry contents metadata .tsv file and data visualisation\n",
    "import json\n",
    "\n",
    "# 2.1 Pretty print DOME Registry contents JSON file for inspection to ensure all looks as expected \n",
    "# remove comment to activate print and debug where needed\n",
    "\n",
    "# Function to read in and pretty-print the JSON DOME Registry file entry\n",
    "def pretty_print_json(file_name):\n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Pretty-print the JSON data\n",
    "        print('Successfully read in JSON file.')\n",
    "        #print(json.dumps(data, indent=4))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the JSON file: {e}\")\n",
    "\n",
    "# Call the function to pretty-print the JSON file\n",
    "pretty_print_json(json_file_path)\n",
    "\n",
    "\n",
    "# 2.2 Flatten the JSON for easier data processing and write to a new .json file \n",
    "# Function to read JSON data\n",
    "def read_json(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to flatten JSON\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "# Function to save flattened JSON to a file\n",
    "def save_flattened_json(flattened_data, output_file_name):\n",
    "    try:\n",
    "        with open(output_file_name, 'w', encoding='utf-8') as file:\n",
    "            json.dump(flattened_data, file, indent=4)\n",
    "        print(f\"Flattened JSON data saved to '{output_file_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the flattened JSON file: {e}\")\n",
    "\n",
    "# Read JSON data\n",
    "data = read_json(json_file_path)\n",
    "\n",
    "# Flatten JSON data and save to a new JSON file\n",
    "if data:\n",
    "    flattened_data = [flatten_json(entry) for entry in data]\n",
    "    flattened_file_name = (\"flattened_\"+file_name)\n",
    "    # Make file path to save flattened JSON file\n",
    "    json_folder_path = \"DOME_Registry_JSON_Files\"\n",
    "    json_file_path = os.path.join(json_folder_path, flattened_file_name)\n",
    "    save_flattened_json(flattened_data, json_file_path)\n",
    "    # Print the flattened JSON data to view it\n",
    "\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "\n",
    "#2.3 Convert flattened json to tsv \n",
    "# Function to read flattened JSON data\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Function to read flattened JSON data\n",
    "def read_flattened_json(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the flattened JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# TSV folders created to store tsv\n",
    "if not os.path.exists('DOME_Registry_TSV_Files'):\n",
    "    print('Creating folder to store DOME Registry TSV files')\n",
    "    os.makedirs('DOME_Registry_TSV_Files')\n",
    "else:\n",
    "    print('Folder already exists for storing DOME Registry TSV files')\n",
    "\n",
    "# Function to write JSON data to a TSV file\n",
    "def write_json_to_tsv(json_data, tsv_file_name):\n",
    "    try:\n",
    "        # Determine all possible headers from the entire dataset\n",
    "        headers = set()\n",
    "        for entry in json_data:\n",
    "            headers.update(entry.keys())\n",
    "        headers = list(headers)\n",
    "        \n",
    "        # Write data to TSV file\n",
    "        with open(tsv_file_name, 'w', newline='', encoding='utf-8') as tsvfile:\n",
    "            writer = csv.DictWriter(tsvfile, fieldnames=headers, delimiter='\\t')\n",
    "            writer.writeheader()\n",
    "            for entry in json_data:\n",
    "                writer.writerow(entry)\n",
    "        \n",
    "        print(f\"JSON data written to '{tsv_file_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to the TSV file: {e}\")\n",
    "\n",
    "# Read flattened JSON data\n",
    "flattened_data = read_flattened_json(json_file_path)\n",
    "\n",
    "# Create TSV file name and file pathway\n",
    "tsv_file_name = flattened_file_name[:-5]+'.tsv'\n",
    "tsv_file_path = os.path.join('DOME_Registry_TSV_Files', tsv_file_name)\n",
    "\n",
    "# Process JSON data into TSV\n",
    "if flattened_data:\n",
    "    write_json_to_tsv(flattened_data, tsv_file_path)\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "\n",
    "tsv_file_name = flattened_file_name[:-5]+'.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DOME Registry TSV data file will be formatted with shortid as the row index and other fields cleaned (publication data) and ordered by D O M E fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered TSV data saved to 'DOME_Registry_TSV_Files/flattened_DOME_Registry_Contents_2025-11-25.tsv'\n"
     ]
    }
   ],
   "source": [
    "#3 Reorder TSV using pandas data frame \n",
    "import pandas as pd\n",
    "\n",
    "# Read the TSV file as a DataFrame using pandas\n",
    "df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "\n",
    "# Define the prefixes to match and group tsv data\n",
    "prefix_publications_cols = 'publication_'\n",
    "prefix_tags_cols = 'publication_tags_'\n",
    "prefix_data_cols = 'matches_data'\n",
    "prefix_optimization_cols = 'matches_optimization'\n",
    "prefix_model_cols = 'matches_model'\n",
    "prefix_evaluation_cols = 'matches_evaluation'\n",
    "\n",
    "# Separate columns based on whether they start with the prefix\n",
    "publication_columns = [col for col in df.columns if col.startswith(prefix_publications_cols) and not col.startswith(prefix_tags_cols)]\n",
    "publication_tags_columns = [col for col in df.columns if col.startswith(prefix_tags_cols)]\n",
    "# Sort tags columns numerically (e.g., publication_tags_0, publication_tags_1, ...)\n",
    "publication_tags_columns = sorted(publication_tags_columns, key=lambda x: int(x.split('_')[-1]) if x.split('_')[-1].isdigit() else 0)\n",
    "matches_data_columns = [col for col in df.columns if col.startswith(prefix_data_cols)]\n",
    "matches_optimization_columns = [col for col in df.columns if col.startswith(prefix_optimization_cols)]\n",
    "matches_model_columns = [col for col in df.columns if col.startswith(prefix_model_cols)]\n",
    "matches_evaluation_columns = [col for col in df.columns if col.startswith(prefix_evaluation_cols)]\n",
    "other_columns = [col for col in df.columns if not col.startswith('matches_') and not col.startswith('publication_')]\n",
    "\n",
    "# Reorder columns\n",
    "reordered_columns = (other_columns + publication_columns + publication_tags_columns + matches_data_columns +\n",
    "                     matches_optimization_columns + matches_model_columns + matches_evaluation_columns)\n",
    "df = df[reordered_columns]\n",
    "\n",
    "# Print the reordered DataFrame\n",
    "#print(df.head())\n",
    "\n",
    "df = pd.DataFrame(df).set_index('shortid')\n",
    "df.to_csv(tsv_file_path, sep='\\t', index=True, encoding='utf-8') \n",
    "\n",
    "print(f\"Reordered TSV data saved to '{tsv_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DOME Registry data tsv will have columns added with PMCIDs and Europe PMC IDs returned from DOI search using NCBI E-Utilities API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing output file: DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2025-11-25.tsv\n",
      "Mapping columns already exist in the output file.\n",
      "Skipping DOI mapping API calls to avoid duplication.\n",
      "\n",
      "Block 4 complete.\n"
     ]
    }
   ],
   "source": [
    "# 4. From DOIs get PMCIDs and Europe PMC IDs for full text search\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Define output file name based on current date\n",
    "output_tsv_file_name = f'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_{current_date}.tsv'\n",
    "\n",
    "# Check if output file already exists to avoid re-running time intensive API calls\n",
    "if os.path.exists(output_tsv_file_name):\n",
    "    print(f\"Found existing output file: {output_tsv_file_name}\")\n",
    "    df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "    \n",
    "    # Check if mapping columns exist\n",
    "    if all(col in df.columns for col in ['mapped_pmcid', 'mapped_europepmc_id', 'mapped_pmid']):\n",
    "        print(\"Mapping columns already exist in the output file.\")\n",
    "        print(\"Skipping DOI mapping API calls to avoid duplication.\")\n",
    "        need_mapping = False\n",
    "    else:\n",
    "        print(\"Output file exists but mapping columns are missing. Proceeding with mapping...\")\n",
    "        need_mapping = True\n",
    "else:\n",
    "    print(f\"Output file not found. Reading input file: {tsv_file_path}\")\n",
    "    # Read in DOME Entries TSV as dataframe via pandas library functions\n",
    "    df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "    need_mapping = True\n",
    "\n",
    "# Only proceed with mapping if needed\n",
    "if need_mapping:\n",
    "    # Extract DOIs from the DataFrame\n",
    "    raw_dois = df['publication_doi'].dropna().unique()\n",
    "    print(f\"Found {len(raw_dois)} unique raw DOIs to process\")\n",
    "\n",
    "    # Function to clean and normalize DOI strings\n",
    "    def clean_doi(doi_string):\n",
    "        \"\"\"\n",
    "        Clean DOI string by removing common prefixes and URLs.\n",
    "        Handles formats like:\n",
    "        - https://doi.org/10.1038/nature123\n",
    "        - http://dx.doi.org/10.1016/j.cell.2020\n",
    "        - doi:10.1126/science.abc456\n",
    "        - 10.1002/anie.202100001\n",
    "        \n",
    "        Returns clean DOI like: 10.1038/nature123\n",
    "        \"\"\"\n",
    "        if pd.isna(doi_string):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and strip whitespace\n",
    "        doi_string = str(doi_string).strip()\n",
    "        \n",
    "        # Remove common URL prefixes\n",
    "        doi_string = re.sub(r'^https?://doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        doi_string = re.sub(r'^https?://dx\\.doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        doi_string = re.sub(r'^https?://www\\.doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove 'doi:' prefix\n",
    "        doi_string = re.sub(r'^doi:\\s*', '', doi_string, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Strip any remaining whitespace\n",
    "        doi_string = doi_string.strip()\n",
    "        \n",
    "        # Validate that it starts with '10.' (all DOIs start with 10.)\n",
    "        if not doi_string.startswith('10.'):\n",
    "            return None\n",
    "        \n",
    "        return doi_string\n",
    "\n",
    "    # Clean the DOIs and report issues\n",
    "    print(\"Cleaning DOI strings...\")\n",
    "    dois = []\n",
    "    for raw_doi in raw_dois:\n",
    "        cleaned = clean_doi(raw_doi)\n",
    "        if cleaned:\n",
    "            dois.append(cleaned)\n",
    "        else:\n",
    "            print(f\"  ⚠ Issue extracting/validating DOI: '{raw_doi}'\")\n",
    "            \n",
    "    print(f\"Cleaned {len(dois)} DOIs for processing\\n\")\n",
    "\n",
    "    # Map DOIs to PMCIDs and Europe PMC IDs using NCBI E-utilities API\n",
    "    def map_dois_to_ids(dois, batch_size=1):\n",
    "        id_mapping = {}\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        \n",
    "        print(f\"Starting DOI-to-PMCID mapping for {len(dois)} DOIs...\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for i in range(0, len(dois), batch_size):\n",
    "            batch = dois[i:i + batch_size]\n",
    "            doi_str = ','.join(batch)\n",
    "            \n",
    "            # Progress indicator\n",
    "            progress = i + len(batch)\n",
    "            print(f\"[{progress}/{len(dois)}] Processing: {batch[0][:50]}...\")\n",
    "            \n",
    "            url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids={doi_str}&format=json\"\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    records = data.get('records', [])\n",
    "                    if records:\n",
    "                        for record in records:\n",
    "                            doi = record.get('doi')\n",
    "                            pmcid = record.get('pmcid')\n",
    "                            pmid = record.get('pmid')\n",
    "                            # Europe PMC ID is typically the PMCID without the 'PMC' prefix, or the PMID\n",
    "                            europepmc_id = pmcid if pmcid else (f\"MED/{pmid}\" if pmid else None)\n",
    "                            id_mapping[doi] = {\n",
    "                                'pmcid': pmcid,\n",
    "                                'europepmc_id': europepmc_id,\n",
    "                                'pmid': pmid\n",
    "                            }\n",
    "                            \n",
    "                            if pmcid:\n",
    "                                print(f\"  ✓ Mapped to PMCID: {pmcid}\")\n",
    "                                success_count += 1\n",
    "                            elif pmid:\n",
    "                                print(f\"  ✓ Mapped to PMID: {pmid} (no PMCID)\")\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                print(f\"  ✗ No PMCID/PMID found\")\n",
    "                                fail_count += 1\n",
    "                    else:\n",
    "                        print(f\"  ✗ No mapping found\")\n",
    "                        for doi in batch:\n",
    "                            id_mapping[doi] = {'pmcid': None, 'europepmc_id': None, 'pmid': None}\n",
    "                        fail_count += 1\n",
    "                else:\n",
    "                    print(f\"  ✗ API Error (status {response.status_code})\")\n",
    "                    for doi in batch:\n",
    "                        id_mapping[doi] = {'pmcid': None, 'europepmc_id': None, 'pmid': None}\n",
    "                    fail_count += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {str(e)}\")\n",
    "                for doi in batch:\n",
    "                    id_mapping[doi] = {'pmcid': None, 'europepmc_id': None, 'pmid': None}\n",
    "                fail_count += 1\n",
    "            \n",
    "            # Rate limiting - be respectful to the API\n",
    "            if i + batch_size < len(dois):\n",
    "                import time\n",
    "                time.sleep(0.3)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DOI-TO-PMCID MAPPING SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Successfully mapped: {success_count}\")\n",
    "        print(f\"Failed/not available: {fail_count}\")\n",
    "        print(f\"Success rate: {success_count/len(dois)*100:.1f}%\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return id_mapping\n",
    "\n",
    "    # Map DOIs to PMCIDs and Europe PMC IDs\n",
    "    doi_to_id_mapping = map_dois_to_ids(dois)\n",
    "\n",
    "    # Add the mapped IDs to the DataFrame\n",
    "    print(\"Adding mapped IDs to DataFrame...\")\n",
    "    # Use clean_doi helper for lookup to ensure keys match\n",
    "    df['mapped_pmcid'] = df['publication_doi'].apply(lambda x: doi_to_id_mapping.get(clean_doi(x) if pd.notna(x) else None, {}).get('pmcid'))\n",
    "    df['mapped_europepmc_id'] = df['publication_doi'].apply(lambda x: doi_to_id_mapping.get(clean_doi(x) if pd.notna(x) else None, {}).get('europepmc_id'))\n",
    "    df['mapped_pmid'] = df['publication_doi'].apply(lambda x: doi_to_id_mapping.get(clean_doi(x) if pd.notna(x) else None, {}).get('pmid'))\n",
    "\n",
    "    # Save the updated DataFrame to a new TSV file\n",
    "    df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "    print(f\"✓ Updated DataFrame with mapped PMCIDs and Europe PMC IDs saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "print(\"\\nBlock 4 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Integrate manually remediated PMID/PMCID values from remediation file if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FOR REMEDIATION FILE\n",
      "============================================================\n",
      "\n",
      "✓ Found 2 remediation file(s):\n",
      "    - remediated_Failed_DOI_Mappings_2025-11-20.tsv\n",
      "    - remediated_Failed_DOI_Mappings_2025-11-25.tsv\n",
      "\n",
      "============================================================\n",
      "Processing: remediated_Failed_DOI_Mappings_2025-11-20.tsv\n",
      "============================================================\n",
      "Remediation file contains 33 entries\n",
      "Found 29 manually remediated entries\n",
      "Integrating remediated values...\n",
      "  ✓ Updated vqpw9fqmuu: PMID = 19692556.0\n",
      "  ✓ Updated 9hqbg4dzys: PMID = 23102953.0\n",
      "  ✓ Updated jqi03r1d0i: PMID = 26213851.0\n",
      "  ✓ Updated 0mlbkqclbr: PMID = 25175491.0\n",
      "  ✓ Updated cvu6djmvfq: PMID = 27898976.0\n",
      "  ✓ Updated o94lxlja8t: PMID = 27491922.0\n",
      "  ✓ Updated wd9oesbckf: PMID = 26746583.0\n",
      "  ✓ Updated lrlwou3dt7: PMID = 28696170.0\n",
      "  ✓ Updated ie0f3buyr4: PMID = 26957000.0\n",
      "  ✓ Updated ek1c4fvz6e: PMID = 30590545.0\n",
      "  ✓ Updated dnrghw65xy: PMID = 31466478.0\n",
      "  ✓ Updated iugmhjifoe: PMID = 30769139.0\n",
      "  ✓ Updated rggiypqqgz: PMID = 31137222.0\n",
      "  ✓ Updated l9ylxpirie: PMID = 32344344.0\n",
      "  ✓ Updated fozqqtqf2d: PMID = 32344344.0\n",
      "  ✓ Updated apf7wp9txp: PMID = 34905768.0\n",
      "  ✓ Updated lmmde595pw: PMID = 32991297.0\n",
      "  ✓ Updated dmb4u51v5o: PMID = 33169030.0\n",
      "  ✓ Updated joud1a8rk2: PMID = 36458437.0\n",
      "  ✓ Updated gd9a2ccysk: PMID = 37204193.0\n",
      "  ✓ Updated ngjci1cyqx: PMCID = PMC10316696\n",
      "  ✓ Updated ngjci1cyqx: PMID = 37395630.0\n",
      "  ✓ Updated lgfn7xigc7: PMID = 38359660.0\n",
      "  ✓ Updated 1cvlnr4o3g: PMID = 39237195.0\n",
      "  ✓ Updated goldx6bwjb: PMCID = PMC11223784\n",
      "  ✓ Updated goldx6bwjb: PMID = 38747347.0\n",
      "  ✓ Updated 88n4lxv68p: PMCID = PMC11512451\n",
      "  ✓ Updated 88n4lxv68p: PMID = 39460934.0\n",
      "\n",
      "File summary: 28 updates (3 PMCIDs, 25 PMIDs)\n",
      "\n",
      "============================================================\n",
      "Processing: remediated_Failed_DOI_Mappings_2025-11-25.tsv\n",
      "============================================================\n",
      "Remediation file contains 5 entries\n",
      "Found 4 manually remediated entries\n",
      "Integrating remediated values...\n",
      "  ✓ Updated kkkb6kyxxz: PMID = 40054813.0\n",
      "  ✓ Updated stp0kvhixm: PMID = 39571839.0\n",
      "  ✓ Updated znnn9k8683: PMID = 39708500.0\n",
      "  ✓ Updated vsx18unv9t: PMCID = PMC12077394\n",
      "  ✓ Updated vsx18unv9t: PMID = 40366868.0\n",
      "\n",
      "File summary: 5 updates (1 PMCIDs, 4 PMIDs)\n",
      "\n",
      "============================================================\n",
      "REMEDIATION INTEGRATION SUMMARY (ALL FILES)\n",
      "============================================================\n",
      "Files processed: 2\n",
      "Total updates applied: 33\n",
      "  - PMCID updates: 4\n",
      "  - PMID updates: 29\n",
      "\n",
      "✓ Updated TSV saved: DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2025-11-25.tsv\n",
      "============================================================\n",
      "\n",
      "\n",
      "Block 4.5 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_325784/2920528336.py:108: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '19692556.0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_main.loc[mask, 'mapped_pmid'] = str(manual_pmid).strip()\n"
     ]
    }
   ],
   "source": [
    "# 4.5 Check for remediation file and integrate manually remediated PMID/PMCID values\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING FOR REMEDIATION FILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Look for remediation files in the TSV folder (where user manually saves remediated files)\n",
    "tsv_folder = 'DOME_Registry_TSV_Files'\n",
    "remediation_pattern = os.path.join(tsv_folder, 'remediated_Failed_DOI_Mappings_*.tsv')\n",
    "\n",
    "remediation_files = glob.glob(remediation_pattern)\n",
    "\n",
    "if not remediation_files:\n",
    "    print(\"\\nNo remediation file found.\")\n",
    "    print(\"Skipping remediation integration.\")\n",
    "    print(\"If you have manually remediated DOIs, ensure the file is saved as:\")\n",
    "    print(f\"  {tsv_folder}/remediated_Failed_DOI_Mappings_YYYY-MM-DD.tsv\")\n",
    "    print(\"\\nProceeding to next step...\")\n",
    "else:\n",
    "    # Sort files by modification time (oldest first) to process in order\n",
    "    remediation_files = sorted(remediation_files, key=os.path.getmtime)\n",
    "    print(f\"\\n✓ Found {len(remediation_files)} remediation file(s):\")\n",
    "    for f in remediation_files:\n",
    "        print(f\"    - {os.path.basename(f)}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the main TSV file\n",
    "        df_main = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "        \n",
    "        total_updates = 0\n",
    "        total_pmcid_updates = 0\n",
    "        total_pmid_updates = 0\n",
    "        \n",
    "        # Process each remediation file\n",
    "        for remediation_file in remediation_files:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing: {os.path.basename(remediation_file)}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Read the remediation file\n",
    "            df_remediation = pd.read_csv(remediation_file, sep='\\t')\n",
    "            \n",
    "            print(f\"Remediation file contains {len(df_remediation)} entries\")\n",
    "            \n",
    "            # Filter for entries that have been manually remediated\n",
    "            df_remediated = df_remediation[\n",
    "                (df_remediation['Remediation_Status'] == 'RESOLVED') &\n",
    "                ((df_remediation['Manual_PMCID'].notna()) | (df_remediation['Manual_PMID'].notna()))\n",
    "            ]\n",
    "            \n",
    "            remediated_count = len(df_remediated)\n",
    "            \n",
    "            if remediated_count == 0:\n",
    "                print(\"⚠ No manually remediated entries found in this file.\")\n",
    "                print(\"   (No entries with Remediation_Status='RESOLVED' and Manual PMID/PMCID)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Found {remediated_count} manually remediated entries\")\n",
    "            print(\"Integrating remediated values...\")\n",
    "            \n",
    "            file_updates = 0\n",
    "            file_pmcid_updates = 0\n",
    "            file_pmid_updates = 0\n",
    "            \n",
    "            # Determine the identifier column (shortid or _id)\n",
    "            id_col = 'shortid' if 'shortid' in df_remediated.columns else '_id'\n",
    "            \n",
    "            if id_col not in df_main.columns:\n",
    "                print(f\"✗ Error: Identifier column '{id_col}' not found in main dataset\")\n",
    "                print(\"   Cannot integrate remediation data from this file\")\n",
    "                continue\n",
    "            \n",
    "            # Iterate through remediated entries\n",
    "            for idx, remediated_row in df_remediated.iterrows():\n",
    "                entry_id = remediated_row.get(id_col)\n",
    "                manual_pmcid = remediated_row.get('Manual_PMCID')\n",
    "                manual_pmid = remediated_row.get('Manual_PMID')\n",
    "                \n",
    "                if pd.isna(entry_id):\n",
    "                    continue\n",
    "                \n",
    "                # Find matching row in main dataframe\n",
    "                mask = df_main[id_col] == entry_id\n",
    "                matching_rows = df_main[mask]\n",
    "                \n",
    "                if len(matching_rows) == 0:\n",
    "                    print(f\"  ⚠ Warning: Entry {entry_id} not found in main dataset\")\n",
    "                    continue\n",
    "                \n",
    "                # Update PMCID if provided and currently empty\n",
    "                if pd.notna(manual_pmcid) and str(manual_pmcid).strip() != '':\n",
    "                    if pd.isna(df_main.loc[mask, 'mapped_pmcid'].iloc[0]):\n",
    "                        df_main.loc[mask, 'mapped_pmcid'] = str(manual_pmcid).strip()\n",
    "                        \n",
    "                        # Also update Europe PMC ID based on PMCID\n",
    "                        df_main.loc[mask, 'mapped_europepmc_id'] = str(manual_pmcid).strip()\n",
    "                        \n",
    "                        file_pmcid_updates += 1\n",
    "                        file_updates += 1\n",
    "                        print(f\"  ✓ Updated {entry_id}: PMCID = {manual_pmcid}\")\n",
    "                \n",
    "                # Update PMID if provided and currently empty\n",
    "                if pd.notna(manual_pmid) and str(manual_pmid).strip() != '':\n",
    "                    if pd.isna(df_main.loc[mask, 'mapped_pmid'].iloc[0]):\n",
    "                        df_main.loc[mask, 'mapped_pmid'] = str(manual_pmid).strip()\n",
    "                        \n",
    "                        # If no PMCID but PMID exists, create Europe PMC ID\n",
    "                        if pd.isna(df_main.loc[mask, 'mapped_europepmc_id'].iloc[0]):\n",
    "                            df_main.loc[mask, 'mapped_europepmc_id'] = f\"MED/{str(manual_pmid).strip()}\"\n",
    "                        \n",
    "                        file_pmid_updates += 1\n",
    "                        file_updates += 1\n",
    "                        print(f\"  ✓ Updated {entry_id}: PMID = {manual_pmid}\")\n",
    "            \n",
    "            # Update totals\n",
    "            total_updates += file_updates\n",
    "            total_pmcid_updates += file_pmcid_updates\n",
    "            total_pmid_updates += file_pmid_updates\n",
    "            \n",
    "            if file_updates > 0:\n",
    "                print(f\"\\nFile summary: {file_updates} updates ({file_pmcid_updates} PMCIDs, {file_pmid_updates} PMIDs)\")\n",
    "            else:\n",
    "                print(\"\\n⚠ No updates applied from this file (entries may already have values)\")\n",
    "        \n",
    "        # Save the updated main TSV file if any updates were made\n",
    "        if total_updates > 0:\n",
    "            df_main.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"REMEDIATION INTEGRATION SUMMARY (ALL FILES)\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Files processed: {len(remediation_files)}\")\n",
    "            print(f\"Total updates applied: {total_updates}\")\n",
    "            print(f\"  - PMCID updates: {total_pmcid_updates}\")\n",
    "            print(f\"  - PMID updates: {total_pmid_updates}\")\n",
    "            print(f\"\\n✓ Updated TSV saved: {output_tsv_file_name}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        else:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"No updates were applied across all remediation files\")\n",
    "            print(\"(all remediated entries may already have values)\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error processing remediation file(s): {str(e)}\")\n",
    "        print(\"Continuing with existing data...\")\n",
    "\n",
    "print(\"\\nBlock 4.5 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF for PMCID PMC1421439 already downloaded.\n",
      "PDF for PMCID PMC1892091 already downloaded.\n",
      "PDF for PMCID PMC2213690 already downloaded.\n",
      "PDF for PMCID PMC1847686 already downloaded.\n",
      "PDF for PMCID PMC2561051 already downloaded.\n",
      "PDF for PMCID PMC2275242 already downloaded.\n",
      "PDF for PMCID PMC2665034 already downloaded.\n",
      "PDF for PMCID PMC2638158 already downloaded.\n",
      "PDF for PMCID PMC2752621 already downloaded.\n",
      "PDF for PMCID PMC2660303 already downloaded.\n",
      "PDF for PMCID PMC3169429 already downloaded.\n",
      "PDF for PMCID PMC3009519 already downloaded.\n",
      "PDF for PMCID PMC3340366 already downloaded.\n",
      "PDF for PMCID PMC3292016 already downloaded.\n",
      "PDF for PMCID PMC3396452 already downloaded.\n",
      "PDF for PMCID PMC3542245 already downloaded.\n",
      "PDF for PMCID PMC4380029 already downloaded.\n",
      "PDF for PMCID PMC3912131 already downloaded.\n",
      "PDF for PMCID PMC4058174 already downloaded.\n",
      "PDF for PMCID PMC3967921 already downloaded.\n",
      "PDF for PMCID PMC4289375 already downloaded.\n",
      "PDF for PMCID PMC4507953 not yet downloaded.\n",
      "PDF for PMCID PMC4315436 already downloaded.\n",
      "PDF for PMCID PMC4315323 already downloaded.\n",
      "PDF for PMCID PMC4606520 already downloaded.\n",
      "PDF for PMCID PMC4589233 already downloaded.\n",
      "PDF for PMCID PMC4466774 already downloaded.\n",
      "PDF for PMCID PMC4388847 already downloaded.\n",
      "PDF for PMCID PMC4706063 already downloaded.\n",
      "PDF for PMCID PMC4908364 already downloaded.\n",
      "PDF for PMCID PMC5120500 already downloaded.\n",
      "PDF for PMCID PMC4894951 already downloaded.\n",
      "PDF for PMCID PMC5113897 already downloaded.\n",
      "PDF for PMCID PMC5104375 already downloaded.\n",
      "PDF for PMCID PMC5034704 already downloaded.\n",
      "PDF for PMCID PMC5079830 already downloaded.\n",
      "PDF for PMCID PMC4931851 already downloaded.\n",
      "PDF for PMCID PMC5042084 already downloaded.\n",
      "PDF for PMCID PMC4773135 already downloaded.\n",
      "PDF for PMCID PMC4834164 already downloaded.\n",
      "PDF for PMCID PMC5688026 already downloaded.\n",
      "PDF for PMCID PMC5860114 already downloaded.\n",
      "PDF for PMCID PMC5860171 already downloaded.\n",
      "PDF for PMCID PMC5870574 already downloaded.\n",
      "PDF for PMCID PMC5550971 already downloaded.\n",
      "PDF for PMCID PMC5610945 already downloaded.\n",
      "PDF for PMCID PMC5650527 already downloaded.\n",
      "PDF for PMCID PMC5773889 already downloaded.\n",
      "PDF for PMCID PMC5821114 already downloaded.\n",
      "PDF for PMCID PMC5656045 already downloaded.\n",
      "PDF for PMCID PMC5517062 already downloaded.\n",
      "PDF for PMCID PMC5738356 already downloaded.\n",
      "PDF for PMCID PMC5775817 already downloaded.\n",
      "PDF for PMCID PMC5487762 already downloaded.\n",
      "PDF for PMCID PMC6091426 already downloaded.\n",
      "PDF for PMCID PMC6277570 already downloaded.\n",
      "PDF for PMCID PMC5910428 already downloaded.\n",
      "PDF for PMCID PMC5821274 already downloaded.\n",
      "PDF for PMCID PMC6242780 already downloaded.\n",
      "PDF for PMCID PMC6036478 already downloaded.\n",
      "PDF for PMCID PMC5930664 already downloaded.\n",
      "PDF for PMCID PMC6214495 already downloaded.\n",
      "PDF for PMCID PMC6214550 already downloaded.\n",
      "PDF for PMCID PMC5976622 already downloaded.\n",
      "PDF for PMCID PMC6237755 already downloaded.\n",
      "PDF for PMCID PMC6036855 already downloaded.\n",
      "PDF for PMCID PMC6172579 not yet downloaded.\n",
      "PDF for PMCID PMC7212484 already downloaded.\n",
      "PDF for PMCID PMC6594643 already downloaded.\n",
      "PDF for PMCID PMC6982787 already downloaded.\n",
      "PDF for PMCID PMC6548586 already downloaded.\n",
      "PDF for PMCID PMC6737184 already downloaded.\n",
      "PDF for PMCID PMC6386402 already downloaded.\n",
      "PDF for PMCID PMC6510637 already downloaded.\n",
      "PDF for PMCID PMC6817842 already downloaded.\n",
      "PDF for PMCID PMC6732622 already downloaded.\n",
      "PDF for PMCID PMC6679781 already downloaded.\n",
      "PDF for PMCID PMC6629660 already downloaded.\n",
      "PDF for PMCID PMC6923882 already downloaded.\n",
      "PDF for PMCID PMC6929456 already downloaded.\n",
      "PDF for PMCID PMC6743778 already downloaded.\n",
      "PDF for PMCID PMC6436896 already downloaded.\n",
      "PDF for PMCID PMC6457539 already downloaded.\n",
      "PDF for PMCID PMC6664791 already downloaded.\n",
      "PDF for PMCID PMC6690680 already downloaded.\n",
      "PDF for PMCID PMC6459551 already downloaded.\n",
      "PDF for PMCID PMC6478501 already downloaded.\n",
      "PDF for PMCID PMC6908647 already downloaded.\n",
      "PDF for PMCID PMC6657583 already downloaded.\n",
      "PDF for PMCID PMC6715517 already downloaded.\n",
      "PDF for PMCID PMC6902683 not yet downloaded.\n",
      "PDF for PMCID PMC6851483 already downloaded.\n",
      "PDF for PMCID PMC6708480 already downloaded.\n",
      "PDF for PMCID PMC6495231 already downloaded.\n",
      "PDF for PMCID PMC6394031 already downloaded.\n",
      "PDF for PMCID PMC9328381 already downloaded.\n",
      "PDF for PMCID PMC7692026 already downloaded.\n",
      "PDF for PMCID PMC7773485 already downloaded.\n",
      "PDF for PMCID PMC7035778 already downloaded.\n",
      "PDF for PMCID PMC7682087 already downloaded.\n",
      "PDF for PMCID PMC7602301 already downloaded.\n",
      "PDF for PMCID PMC7446623 already downloaded.\n",
      "PDF for PMCID PMC7794018 already downloaded.\n",
      "PDF for PMCID PMC7156652 already downloaded.\n",
      "PDF for PMCID PMC7751093 already downloaded.\n",
      "PDF for PMCID PMC7933593 already downloaded.\n",
      "PDF for PMCID PMC7352871 already downloaded.\n",
      "PDF for PMCID PMC7735824 already downloaded.\n",
      "PDF for PMCID PMC7718328 already downloaded.\n",
      "PDF for PMCID PMC7596958 already downloaded.\n",
      "PDF for PMCID PMC7648120 already downloaded.\n",
      "PDF for PMCID PMC7406221 already downloaded.\n",
      "PDF for PMCID PMC7068237 already downloaded.\n",
      "PDF for PMCID PMC7680913 already downloaded.\n",
      "PDF for PMCID PMC8545175 already downloaded.\n",
      "PDF for PMCID PMC7734183 already downloaded.\n",
      "PDF for PMCID PMC7237030 already downloaded.\n",
      "PDF for PMCID PMC7591939 already downloaded.\n",
      "PDF for PMCID PMC7725002 already downloaded.\n",
      "PDF for PMCID PMC7721480 already downloaded.\n",
      "PDF for PMCID PMC7493359 already downloaded.\n",
      "PDF for PMCID PMC7569858 already downloaded.\n",
      "PDF for PMCID PMC7162491 already downloaded.\n",
      "PDF for PMCID PMC7142336 already downloaded.\n",
      "PDF for PMCID PMC7442807 already downloaded.\n",
      "PDF for PMCID PMC7054390 already downloaded.\n",
      "PDF for PMCID PMC7520974 already downloaded.\n",
      "PDF for PMCID PMC7333383 already downloaded.\n",
      "PDF for PMCID PMC7073919 already downloaded.\n",
      "PDF for PMCID PMC7297119 already downloaded.\n",
      "PDF for PMCID PMC8668950 already downloaded.\n",
      "PDF for PMCID PMC8553642 already downloaded.\n",
      "PDF for PMCID PMC8295265 already downloaded.\n",
      "PDF for PMCID PMC8371605 already downloaded.\n",
      "PDF for PMCID PMC8162250 already downloaded.\n",
      "PDF for PMCID PMC8553134 already downloaded.\n",
      "PDF for PMCID PMC7876317 already downloaded.\n",
      "PDF for PMCID PMC8554859 already downloaded.\n",
      "PDF for PMCID PMC8352508 already downloaded.\n",
      "PDF for PMCID PMC8261512 already downloaded.\n",
      "PDF for PMCID PMC8175075 already downloaded.\n",
      "PDF for PMCID PMC8093828 already downloaded.\n",
      "PDF for PMCID PMC8351329 already downloaded.\n",
      "PDF for PMCID PMC8192578 already downloaded.\n",
      "PDF for PMCID PMC8019900 already downloaded.\n",
      "PDF for PMCID PMC8042551 already downloaded.\n",
      "PDF for PMCID PMC8100172 already downloaded.\n",
      "PDF for PMCID PMC8125376 already downloaded.\n",
      "PDF for PMCID PMC8328792 already downloaded.\n",
      "PDF for PMCID PMC8385175 already downloaded.\n",
      "PDF for PMCID PMC7845959 already downloaded.\n",
      "PDF for PMCID PMC8469072 already downloaded.\n",
      "PDF for PMCID PMC8204819 already downloaded.\n",
      "PDF for PMCID PMC7860026 already downloaded.\n",
      "PDF for PMCID PMC8485143 already downloaded.\n",
      "PDF for PMCID PMC8182908 already downloaded.\n",
      "PDF for PMCID PMC8259419 already downloaded.\n",
      "PDF for PMCID PMC8230313 already downloaded.\n",
      "PDF for PMCID PMC8067080 already downloaded.\n",
      "PDF for PMCID PMC8185002 already downloaded.\n",
      "PDF for PMCID PMC8100175 already downloaded.\n",
      "PDF for PMCID PMC7816647 already downloaded.\n",
      "PDF for PMCID PMC8843059 already downloaded.\n",
      "PDF for PMCID PMC8225676 already downloaded.\n",
      "PDF for PMCID PMC8336795 already downloaded.\n",
      "PDF for PMCID PMC9580958 already downloaded.\n",
      "PDF for PMCID PMC9757591 already downloaded.\n",
      "PDF for PMCID PMC9329459 already downloaded.\n",
      "PDF for PMCID PMC9252801 already downloaded.\n",
      "PDF for PMCID PMC9681730 already downloaded.\n",
      "PDF for PMCID PMC10483879 already downloaded.\n",
      "PDF for PMCID PMC10821710 already downloaded.\n",
      "PDF for PMCID PMC10716825 already downloaded.\n",
      "PDF for PMCID PMC10646871 already downloaded.\n",
      "PDF for PMCID PMC10673642 already downloaded.\n",
      "PDF for PMCID PMC10600917 already downloaded.\n",
      "PDF for PMCID PMC10603766 already downloaded.\n",
      "PDF for PMCID PMC10541796 already downloaded.\n",
      "PDF for PMCID PMC10441000 already downloaded.\n",
      "PDF for PMCID PMC10367125 already downloaded.\n",
      "PDF for PMCID PMC10653424 already downloaded.\n",
      "PDF for PMCID PMC10174551 already downloaded.\n",
      "PDF for PMCID PMC10684096 already downloaded.\n",
      "PDF for PMCID PMC9805592 already downloaded.\n",
      "PDF for PMCID PMC10730818 already downloaded.\n",
      "PDF for PMCID PMC10776382 already downloaded.\n",
      "PDF for PMCID PMC10316696 already downloaded.\n",
      "PDF for PMCID PMC10659119 already downloaded.\n",
      "PDF for PMCID PMC10087011 already downloaded.\n",
      "PDF for PMCID PMC12532322 already downloaded.\n",
      "PDF for PMCID PMC10762911 already downloaded.\n",
      "PDF for PMCID PMC11223784 already downloaded.\n",
      "PDF for PMCID PMC11837757 already downloaded.\n",
      "PDF for PMCID PMC11816797 already downloaded.\n",
      "PDF for PMCID PMC11878767 already downloaded.\n",
      "PDF for PMCID PMC11734293 already downloaded.\n",
      "PDF for PMCID PMC11653894 already downloaded.\n",
      "PDF for PMCID PMC11659981 already downloaded.\n",
      "PDF for PMCID PMC11659980 already downloaded.\n",
      "PDF for PMCID PMC11423353 already downloaded.\n",
      "PDF for PMCID PMC11345537 already downloaded.\n",
      "PDF for PMCID PMC11959188 already downloaded.\n",
      "PDF for PMCID PMC11299106 already downloaded.\n",
      "PDF for PMCID PMC11238428 already downloaded.\n",
      "PDF for PMCID PMC11258913 already downloaded.\n",
      "PDF for PMCID PMC11034027 already downloaded.\n",
      "PDF for PMCID PMC10940896 already downloaded.\n",
      "PDF for PMCID PMC10783149 already downloaded.\n",
      "PDF for PMCID PMC11629979 already downloaded.\n",
      "PDF for PMCID PMC11340644 already downloaded.\n",
      "PDF for PMCID PMC11512451 already downloaded.\n",
      "PDF for PMCID PMC12366053 already downloaded.\n",
      "PDF for PMCID PMC12330522 not yet downloaded.\n",
      "PDF for PMCID PMC12596181 already downloaded.\n",
      "PDF for PMCID PMC12532321 already downloaded.\n",
      "PDF for PMCID PMC12530094 already downloaded.\n",
      "PDF for PMCID PMC12398281 already downloaded.\n",
      "PDF for PMCID PMC12206155 already downloaded.\n",
      "PDF for PMCID PMC12099614 already downloaded.\n",
      "PDF for PMCID PMC11940444 already downloaded.\n",
      "PDF for PMCID PMC12365963 already downloaded.\n",
      "PDF for PMCID PMC12153353 already downloaded.\n",
      "PDF for PMCID PMC12077394 already downloaded.\n",
      "PDF for PMCID PMC12077397 already downloaded.\n",
      "PDF for PMCID PMC11945317 already downloaded.\n",
      "PDF for PMCID PMC11851125 already downloaded.\n",
      "PDF for PMCID PMC12569769 already downloaded.\n",
      "PDF for PMCID PMC11912559 already downloaded.\n",
      "PDF for PMCID PMC11927397 already downloaded.\n",
      "PDF for PMCID PMC11899596 already downloaded.\n",
      "PDF for PMCID PMC12087453 already downloaded.\n",
      "PDF for PMCID PMC12056507 already downloaded.\n",
      "PDF for PMCID PMC12080225 already downloaded.\n",
      "PDF for PMCID PMC11727722 already downloaded.\n",
      "PDF for PMCID PMC11811528 already downloaded.\n",
      "\n",
      "Need to download 4 PDFs out of 235 total entries.\n",
      "\n",
      "[1/4] Processing PMC4507953...\n",
      "  ✗ Could not retrieve PDF (status: 404)\n",
      "  ✗ Could not retrieve PDF (status: 404)\n",
      "[2/4] Processing PMC6172579...\n",
      "[2/4] Processing PMC6172579...\n",
      "  ✗ Could not retrieve PDF (status: 404)\n",
      "  ✗ Could not retrieve PDF (status: 404)\n"
     ]
    }
   ],
   "source": [
    "# 5. Download full text PDFs using PMCIDs from Europe PMC\n",
    "# Note: Europe PMC does not directly provide PDFs through REST API - we need to use alternative methods\n",
    " \n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Read in DOME Entries TSV as dataframe via pandas library functions\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Extract PMCIDs from the DataFrame\n",
    "pmcids = df['mapped_pmcid'].dropna().unique()\n",
    "\n",
    "# Define the output folder for PDF files\n",
    "output_folder = 'DOME_Registry_PMC_PDFs'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Track which PMCIDs need downloading (skip already downloaded)\n",
    "to_download_pmcid = []\n",
    "for pmcid in pmcids:\n",
    "    if os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "        print(f\"PDF for PMCID {pmcid} already downloaded.\")\n",
    "    else:\n",
    "        print(f\"PDF for PMCID {pmcid} not yet downloaded.\")\n",
    "        to_download_pmcid.append(pmcid)\n",
    "\n",
    "print(f\"\\nNeed to download {len(to_download_pmcid)} PDFs out of {len(pmcids)} total entries.\\n\")\n",
    "\n",
    "# Function to download full text PDF and supplementary materials\n",
    "def download_pdfs(pmcids):\n",
    "    \"\"\"\n",
    "    Download PDFs from Europe PMC. \n",
    "    Note: Direct PDF downloads are not always available through Europe PMC REST API.\n",
    "    We'll try multiple approaches:\n",
    "    1. Try to get PDF link from article metadata\n",
    "    2. Download supplementary files if available\n",
    "    3. Construct publisher URLs where possible\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    supp_count = 0\n",
    "    \n",
    "    for idx, pmcid in enumerate(pmcids, 1):\n",
    "        print(f\"[{idx}/{len(pmcids)}] Processing {pmcid}...\")\n",
    "        \n",
    "        # Clean PMCID (remove 'PMC' prefix for some API calls)\n",
    "        clean_pmcid = pmcid.replace('PMC', '') if pmcid.startswith('PMC') else pmcid\n",
    "        \n",
    "        # Try Method 1: Get article metadata to find PDF link\n",
    "        try:\n",
    "            metadata_url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=PMCID:{pmcid}&resultType=core&format=json\"\n",
    "            metadata_response = requests.get(metadata_url, timeout=30)\n",
    "            \n",
    "            if metadata_response.status_code == 200:\n",
    "                metadata = metadata_response.json()\n",
    "                \n",
    "                if metadata.get('hitCount', 0) > 0:\n",
    "                    result = metadata['resultList']['result'][0]\n",
    "                    \n",
    "                    # Try to get PDF link from fullTextUrlList\n",
    "                    if 'fullTextUrlList' in result and result['fullTextUrlList']:\n",
    "                        for url_info in result['fullTextUrlList']['fullTextUrl']:\n",
    "                            if url_info.get('documentStyle') == 'pdf' or url_info.get('availabilityCode') == 'OA':\n",
    "                                pdf_url = url_info.get('url')\n",
    "                                \n",
    "                                if pdf_url and '.pdf' in pdf_url.lower():\n",
    "                                    # Try to download the PDF\n",
    "                                    pdf_response = requests.get(pdf_url, timeout=30, allow_redirects=True)\n",
    "                                    \n",
    "                                    if pdf_response.status_code == 200 and pdf_response.headers.get('Content-Type', '').startswith('application/pdf'):\n",
    "                                        output_file = os.path.join(output_folder, f\"{pmcid}_main.pdf\")\n",
    "                                        with open(output_file, 'wb') as file:\n",
    "                                            file.write(pdf_response.content)\n",
    "                                        print(f\"  ✓ Downloaded main PDF from publisher\")\n",
    "                                        success_count += 1\n",
    "                                        break\n",
    "                    \n",
    "                    # If no PDF found yet, try PMC OA service\n",
    "                    if not os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "                        # Try Europe PMC OA PDF service (different endpoint)\n",
    "                        pmc_oa_url = f\"https://europepmc.org/articles/{pmcid}?pdf=render\"\n",
    "                        pmc_response = requests.get(pmc_oa_url, timeout=30, allow_redirects=True)\n",
    "                        \n",
    "                        if pmc_response.status_code == 200 and len(pmc_response.content) > 1000:\n",
    "                            # Check if it's actually a PDF\n",
    "                            if pmc_response.content[:4] == b'%PDF':\n",
    "                                output_file = os.path.join(output_folder, f\"{pmcid}_main.pdf\")\n",
    "                                with open(output_file, 'wb') as file:\n",
    "                                    file.write(pmc_response.content)\n",
    "                                print(f\"  ✓ Downloaded main PDF from PMC OA service\")\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                print(f\"  ✗ Could not retrieve PDF (not openly available)\")\n",
    "                                fail_count += 1\n",
    "                        else:\n",
    "                            print(f\"  ✗ Could not retrieve PDF (status: {pmc_response.status_code})\")\n",
    "                            fail_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error downloading main PDF: {str(e)}\")\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Try to download supplementary files\n",
    "        try:\n",
    "            supp_url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/supplementaryFiles\"\n",
    "            supp_response = requests.get(supp_url, timeout=30)\n",
    "            \n",
    "            if supp_response.status_code == 200:\n",
    "                try:\n",
    "                    supp_data = supp_response.json()\n",
    "                    \n",
    "                    if 'supplementaryFiles' in supp_data and supp_data['supplementaryFiles']:\n",
    "                        for idx_supp, supp_file in enumerate(supp_data['supplementaryFiles'], 1):\n",
    "                            file_url = supp_file.get('url')\n",
    "                            \n",
    "                            if file_url:\n",
    "                                # Download all supplementary files (not just PDFs)\n",
    "                                try:\n",
    "                                    file_response = requests.get(file_url, timeout=30, allow_redirects=True)\n",
    "                                    \n",
    "                                    if file_response.status_code == 200:\n",
    "                                        # Determine file extension from URL or content-type\n",
    "                                        file_ext = ''\n",
    "                                        if '.pdf' in file_url.lower():\n",
    "                                            file_ext = '.pdf'\n",
    "                                        elif '.xlsx' in file_url.lower() or '.xls' in file_url.lower():\n",
    "                                            file_ext = '.xlsx'\n",
    "                                        elif '.docx' in file_url.lower() or '.doc' in file_url.lower():\n",
    "                                            file_ext = '.docx'\n",
    "                                        elif '.zip' in file_url.lower():\n",
    "                                            file_ext = '.zip'\n",
    "                                        else:\n",
    "                                            # Try to get from content-type\n",
    "                                            content_type = file_response.headers.get('Content-Type', '')\n",
    "                                            if 'pdf' in content_type:\n",
    "                                                file_ext = '.pdf'\n",
    "                                            elif 'excel' in content_type or 'spreadsheet' in content_type:\n",
    "                                                file_ext = '.xlsx'\n",
    "                                            else:\n",
    "                                                file_ext = '.dat'  # Default extension\n",
    "                                        \n",
    "                                        supp_output_file = os.path.join(output_folder, f\"{pmcid}_supp_{idx_supp}{file_ext}\")\n",
    "                                        with open(supp_output_file, 'wb') as file:\n",
    "                                            file.write(file_response.content)\n",
    "                                        print(f\"  ✓ Downloaded supplementary file {idx_supp}{file_ext}\")\n",
    "                                        supp_count += 1\n",
    "                                \n",
    "                                except Exception as e_supp:\n",
    "                                    print(f\"  ⚠ Could not download supplementary file {idx_supp}: {str(e_supp)}\")\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    pass  # No supplementary files available\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass  # Supplementary files not critical, continue\n",
    "        \n",
    "        # Rate limiting - be respectful to the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Main PDFs successfully downloaded: {success_count}\")\n",
    "    print(f\"Main PDFs failed/not available: {fail_count}\")\n",
    "    print(f\"Supplementary files downloaded: {supp_count}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Download PDFs for each PMCID that hasn't been downloaded yet\n",
    "if to_download_pmcid:\n",
    "    download_pdfs(to_download_pmcid)\n",
    "else:\n",
    "    print(\"All PDFs already downloaded. Skipping download step.\")\n",
    "\n",
    "# Update the TSV with download status\n",
    "print(\"Updating TSV with PDF download status...\")\n",
    "pdf_downloadable = []\n",
    "\n",
    "for pmcid in df['mapped_pmcid']:\n",
    "    if pd.notna(pmcid) and os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "        pdf_downloadable.append('yes')\n",
    "    else:\n",
    "        pdf_downloadable.append('no')\n",
    "\n",
    "# Add the new column of download status to the DataFrame and save\n",
    "df['pdf_downloadable'] = pdf_downloadable\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Updated TSV with PDF download status saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "print(\"\\nBlock 5 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download supplementary files (PDFs files) using Europe PMC supplementary files API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 235 PMCIDs to process for supplementary files\n",
      "Supplementary files for PMC1421439 already downloaded (2 files).\n",
      "Supplementary files for PMC1892091 already downloaded (1 files).\n",
      "Supplementary files for PMC2213690 already downloaded (1 files).\n",
      "Supplementary files for PMC1847686 already downloaded (2 files).\n",
      "Supplementary files for PMC2561051 already downloaded (2 files).\n",
      "Supplementary files for PMC2275242 already downloaded (2 files).\n",
      "Supplementary files for PMC2665034 already downloaded (1 files).\n",
      "Supplementary files for PMC2638158 already downloaded (2 files).\n",
      "Supplementary files for PMC2752621 already downloaded (1 files).\n",
      "Supplementary files for PMC2660303 already downloaded (6 files).\n",
      "Supplementary files for PMC3169429 already downloaded (1 files).\n",
      "Supplementary files for PMC3009519 already downloaded (1 files).\n",
      "Supplementary files for PMC3340366 already downloaded (1 files).\n",
      "Supplementary files for PMC3292016 already downloaded (2 files).\n",
      "Supplementary files for PMC3396452 already downloaded (2 files).\n",
      "Supplementary files for PMC3542245 already downloaded (1 files).\n",
      "Supplementary files for PMC4380029 already downloaded (1 files).\n",
      "Supplementary files for PMC3912131 already downloaded (1 files).\n",
      "Supplementary files for PMC4058174 already downloaded (1 files).\n",
      "Supplementary files for PMC3967921 already downloaded (1 files).\n",
      "Supplementary files for PMC4289375 already downloaded (2 files).\n",
      "Supplementary files for PMC4315436 already downloaded (1 files).\n",
      "Supplementary files for PMC4315323 already downloaded (2 files).\n",
      "Supplementary files for PMC4606520 already downloaded (1 files).\n",
      "Supplementary files for PMC4589233 already downloaded (1 files).\n",
      "Supplementary files for PMC4466774 already downloaded (1 files).\n",
      "Supplementary files for PMC4388847 already downloaded (2 files).\n",
      "Supplementary files for PMC4706063 already downloaded (1 files).\n",
      "Supplementary files for PMC4908364 already downloaded (2 files).\n",
      "Supplementary files for PMC5120500 already downloaded (2 files).\n",
      "Supplementary files for PMC4894951 already downloaded (1 files).\n",
      "Supplementary files for PMC5113897 already downloaded (1 files).\n",
      "Supplementary files for PMC5104375 already downloaded (1 files).\n",
      "Supplementary files for PMC5034704 already downloaded (1 files).\n",
      "Supplementary files for PMC5079830 already downloaded (1 files).\n",
      "Supplementary files for PMC4931851 already downloaded (2 files).\n",
      "Supplementary files for PMC5042084 already downloaded (1 files).\n",
      "Supplementary files for PMC4773135 already downloaded (1 files).\n",
      "Supplementary files for PMC4834164 already downloaded (1 files).\n",
      "Supplementary files for PMC5688026 already downloaded (1 files).\n",
      "Supplementary files for PMC5860114 already downloaded (1 files).\n",
      "Supplementary files for PMC5860171 already downloaded (1 files).\n",
      "Supplementary files for PMC5870574 already downloaded (1 files).\n",
      "Supplementary files for PMC5550971 already downloaded (2 files).\n",
      "Supplementary files for PMC5610945 already downloaded (1 files).\n",
      "Supplementary files for PMC5650527 already downloaded (1 files).\n",
      "Supplementary files for PMC5773889 already downloaded (1 files).\n",
      "Supplementary files for PMC5821114 already downloaded (1 files).\n",
      "Supplementary files for PMC5656045 already downloaded (1 files).\n",
      "Supplementary files for PMC5517062 already downloaded (2 files).\n",
      "Supplementary files for PMC5738356 already downloaded (4 files).\n",
      "Supplementary files for PMC5775817 already downloaded (1 files).\n",
      "Supplementary files for PMC5487762 already downloaded (1 files).\n",
      "Supplementary files for PMC6091426 already downloaded (2 files).\n",
      "Supplementary files for PMC6277570 already downloaded (1 files).\n",
      "Supplementary files for PMC5910428 already downloaded (1 files).\n",
      "Supplementary files for PMC5821274 already downloaded (1 files).\n",
      "Supplementary files for PMC6242780 already downloaded (1 files).\n",
      "Supplementary files for PMC6036478 already downloaded (1 files).\n",
      "Supplementary files for PMC5930664 already downloaded (3 files).\n",
      "Supplementary files for PMC6214495 already downloaded (1 files).\n",
      "Supplementary files for PMC6214550 already downloaded (1 files).\n",
      "Supplementary files for PMC5976622 already downloaded (3 files).\n",
      "Supplementary files for PMC6237755 already downloaded (1 files).\n",
      "Supplementary files for PMC6036855 already downloaded (1 files).\n",
      "Supplementary files for PMC7212484 already downloaded (1 files).\n",
      "Supplementary files for PMC6594643 already downloaded (1 files).\n",
      "Supplementary files for PMC6982787 already downloaded (2 files).\n",
      "Supplementary files for PMC6548586 already downloaded (1 files).\n",
      "Supplementary files for PMC6737184 already downloaded (1 files).\n",
      "Supplementary files for PMC6386402 already downloaded (1 files).\n",
      "Supplementary files for PMC6510637 already downloaded (1 files).\n",
      "Supplementary files for PMC6817842 already downloaded (5 files).\n",
      "Supplementary files for PMC6732622 already downloaded (1 files).\n",
      "Supplementary files for PMC6679781 already downloaded (1 files).\n",
      "Supplementary files for PMC6629660 already downloaded (6 files).\n",
      "Supplementary files for PMC6923882 already downloaded (2 files).\n",
      "Supplementary files for PMC6929456 already downloaded (1 files).\n",
      "Supplementary files for PMC6743778 already downloaded (2 files).\n",
      "Supplementary files for PMC6436896 already downloaded (1 files).\n",
      "Supplementary files for PMC6457539 already downloaded (2 files).\n",
      "Supplementary files for PMC6664791 already downloaded (1 files).\n",
      "Supplementary files for PMC6690680 already downloaded (3 files).\n",
      "Supplementary files for PMC6459551 already downloaded (1 files).\n",
      "Supplementary files for PMC6478501 already downloaded (1 files).\n",
      "Supplementary files for PMC6908647 already downloaded (1 files).\n",
      "Supplementary files for PMC6657583 already downloaded (1 files).\n",
      "Supplementary files for PMC6715517 already downloaded (1 files).\n",
      "Supplementary files for PMC6851483 already downloaded (1 files).\n",
      "Supplementary files for PMC6708480 already downloaded (1 files).\n",
      "Supplementary files for PMC6495231 already downloaded (1 files).\n",
      "Supplementary files for PMC6394031 already downloaded (3 files).\n",
      "Supplementary files for PMC9328381 already downloaded (3 files).\n",
      "Supplementary files for PMC7692026 already downloaded (2 files).\n",
      "Supplementary files for PMC7773485 already downloaded (1 files).\n",
      "Supplementary files for PMC7035778 already downloaded (1 files).\n",
      "Supplementary files for PMC7682087 already downloaded (1 files).\n",
      "Supplementary files for PMC7602301 already downloaded (1 files).\n",
      "Supplementary files for PMC7446623 already downloaded (1 files).\n",
      "Supplementary files for PMC7794018 already downloaded (2 files).\n",
      "Supplementary files for PMC7156652 already downloaded (1 files).\n",
      "Supplementary files for PMC7751093 already downloaded (1 files).\n",
      "Supplementary files for PMC7933593 already downloaded (1 files).\n",
      "Supplementary files for PMC7352871 already downloaded (1 files).\n",
      "Supplementary files for PMC7735824 already downloaded (1 files).\n",
      "Supplementary files for PMC7718328 already downloaded (1 files).\n",
      "Supplementary files for PMC7596958 already downloaded (1 files).\n",
      "Supplementary files for PMC7648120 already downloaded (1 files).\n",
      "Supplementary files for PMC7406221 already downloaded (2 files).\n",
      "Supplementary files for PMC7068237 already downloaded (2 files).\n",
      "Supplementary files for PMC7680913 already downloaded (1 files).\n",
      "Supplementary files for PMC8545175 already downloaded (1 files).\n",
      "Supplementary files for PMC7734183 already downloaded (2 files).\n",
      "Supplementary files for PMC7237030 already downloaded (1 files).\n",
      "Supplementary files for PMC7591939 already downloaded (1 files).\n",
      "Supplementary files for PMC7725002 already downloaded (1 files).\n",
      "Supplementary files for PMC7721480 already downloaded (1 files).\n",
      "Supplementary files for PMC7493359 already downloaded (1 files).\n",
      "Supplementary files for PMC7569858 already downloaded (2 files).\n",
      "Supplementary files for PMC7162491 already downloaded (1 files).\n",
      "Supplementary files for PMC7142336 already downloaded (1 files).\n",
      "Supplementary files for PMC7442807 already downloaded (2 files).\n",
      "Supplementary files for PMC7054390 already downloaded (1 files).\n",
      "Supplementary files for PMC7520974 already downloaded (1 files).\n",
      "Supplementary files for PMC7333383 already downloaded (6 files).\n",
      "Supplementary files for PMC7073919 already downloaded (1 files).\n",
      "Supplementary files for PMC7297119 already downloaded (10 files).\n",
      "Supplementary files for PMC8668950 already downloaded (2 files).\n",
      "Supplementary files for PMC8553642 already downloaded (1 files).\n",
      "Supplementary files for PMC8295265 already downloaded (4 files).\n",
      "Supplementary files for PMC8371605 already downloaded (3 files).\n",
      "Supplementary files for PMC8162250 already downloaded (2 files).\n",
      "Supplementary files for PMC8553134 already downloaded (1 files).\n",
      "Supplementary files for PMC7876317 already downloaded (1 files).\n",
      "Supplementary files for PMC8554859 already downloaded (1 files).\n",
      "Supplementary files for PMC8352508 already downloaded (1 files).\n",
      "Supplementary files for PMC8261512 already downloaded (2 files).\n",
      "Supplementary files for PMC8175075 already downloaded (1 files).\n",
      "Supplementary files for PMC8093828 already downloaded (1 files).\n",
      "Supplementary files for PMC8351329 already downloaded (1 files).\n",
      "Supplementary files for PMC8192578 already downloaded (5 files).\n",
      "Supplementary files for PMC8019900 already downloaded (1 files).\n",
      "Supplementary files for PMC8042551 already downloaded (1 files).\n",
      "Supplementary files for PMC8100172 already downloaded (6 files).\n",
      "Supplementary files for PMC8125376 already downloaded (1 files).\n",
      "Supplementary files for PMC8328792 already downloaded (1 files).\n",
      "Supplementary files for PMC8385175 already downloaded (1 files).\n",
      "Supplementary files for PMC7845959 already downloaded (3 files).\n",
      "Supplementary files for PMC8469072 already downloaded (1 files).\n",
      "Supplementary files for PMC8204819 already downloaded (1 files).\n",
      "Supplementary files for PMC7860026 already downloaded (4 files).\n",
      "Supplementary files for PMC8485143 already downloaded (4 files).\n",
      "Supplementary files for PMC8182908 already downloaded (1 files).\n",
      "Supplementary files for PMC8259419 already downloaded (1 files).\n",
      "Supplementary files for PMC8230313 already downloaded (1 files).\n",
      "Supplementary files for PMC8067080 already downloaded (2 files).\n",
      "Supplementary files for PMC8185002 already downloaded (5 files).\n",
      "Supplementary files for PMC8100175 already downloaded (4 files).\n",
      "Supplementary files for PMC7816647 already downloaded (1 files).\n",
      "Supplementary files for PMC8843059 already downloaded (1 files).\n",
      "Supplementary files for PMC8225676 already downloaded (5 files).\n",
      "Supplementary files for PMC8336795 already downloaded (1 files).\n",
      "Supplementary files for PMC9580958 already downloaded (1 files).\n",
      "Supplementary files for PMC9757591 already downloaded (1 files).\n",
      "Supplementary files for PMC9329459 already downloaded (4 files).\n",
      "Supplementary files for PMC9252801 already downloaded (2 files).\n",
      "Supplementary files for PMC9681730 already downloaded (3 files).\n",
      "Supplementary files for PMC10483879 already downloaded (1 files).\n",
      "Supplementary files for PMC10821710 already downloaded (1 files).\n",
      "Supplementary files for PMC10716825 already downloaded (1 files).\n",
      "Supplementary files for PMC10646871 already downloaded (1 files).\n",
      "Supplementary files for PMC10673642 already downloaded (1 files).\n",
      "Supplementary files for PMC10600917 already downloaded (1 files).\n",
      "Supplementary files for PMC10603766 already downloaded (1 files).\n",
      "Supplementary files for PMC10541796 already downloaded (2 files).\n",
      "Supplementary files for PMC10441000 already downloaded (1 files).\n",
      "Supplementary files for PMC10367125 already downloaded (1 files).\n",
      "Supplementary files for PMC10653424 already downloaded (11 files).\n",
      "Supplementary files for PMC10174551 already downloaded (5 files).\n",
      "Supplementary files for PMC10684096 already downloaded (10 files).\n",
      "Supplementary files for PMC9805592 already downloaded (2 files).\n",
      "Supplementary files for PMC10730818 already downloaded (4 files).\n",
      "Supplementary files for PMC10776382 already downloaded (2 files).\n",
      "Supplementary files for PMC10316696 already downloaded (1 files).\n",
      "Supplementary files for PMC10659119 already downloaded (2 files).\n",
      "Supplementary files for PMC10087011 already downloaded (2 files).\n",
      "Supplementary files for PMC12532322 already downloaded (1 files).\n",
      "Supplementary files for PMC10762911 already downloaded (2 files).\n",
      "Supplementary files for PMC11223784 already downloaded (2 files).\n",
      "Supplementary files for PMC11837757 already downloaded (1 files).\n",
      "Supplementary files for PMC11816797 already downloaded (1 files).\n",
      "Supplementary files for PMC11878767 already downloaded (2 files).\n",
      "Supplementary files for PMC11734293 already downloaded (1 files).\n",
      "Supplementary files for PMC11653894 already downloaded (1 files).\n",
      "Supplementary files for PMC11659981 already downloaded (1 files).\n",
      "Supplementary files for PMC11659980 already downloaded (2 files).\n",
      "Supplementary files for PMC11423353 already downloaded (1 files).\n",
      "Supplementary files for PMC11345537 already downloaded (1 files).\n",
      "Supplementary files for PMC11959188 already downloaded (1 files).\n",
      "Supplementary files for PMC11299106 already downloaded (1 files).\n",
      "Supplementary files for PMC11238428 already downloaded (2 files).\n",
      "Supplementary files for PMC11258913 already downloaded (1 files).\n",
      "Supplementary files for PMC11034027 already downloaded (1 files).\n",
      "Supplementary files for PMC10940896 already downloaded (1 files).\n",
      "Supplementary files for PMC10783149 already downloaded (1 files).\n",
      "Supplementary files for PMC11629979 already downloaded (2 files).\n",
      "Supplementary files for PMC11340644 already downloaded (1 files).\n",
      "Supplementary files for PMC11512451 already downloaded (2 files).\n",
      "Supplementary files for PMC12366053 already downloaded (2 files).\n",
      "Supplementary files for PMC12596181 already downloaded (2 files).\n",
      "Supplementary files for PMC12532321 already downloaded (1 files).\n",
      "Supplementary files for PMC12530094 already downloaded (1 files).\n",
      "Supplementary files for PMC12398281 already downloaded (2 files).\n",
      "Supplementary files for PMC12206155 already downloaded (1 files).\n",
      "Supplementary files for PMC12099614 already downloaded (1 files).\n",
      "Supplementary files for PMC11940444 already downloaded (1 files).\n",
      "Supplementary files for PMC12365963 already downloaded (1 files).\n",
      "Supplementary files for PMC12153353 already downloaded (1 files).\n",
      "Supplementary files for PMC12077397 already downloaded (2 files).\n",
      "Supplementary files for PMC11945317 already downloaded (2 files).\n",
      "Supplementary files for PMC11851125 already downloaded (1 files).\n",
      "Supplementary files for PMC12569769 already downloaded (1 files).\n",
      "Supplementary files for PMC11912559 already downloaded (1 files).\n",
      "Supplementary files for PMC11927397 already downloaded (1 files).\n",
      "Supplementary files for PMC11899596 already downloaded (1 files).\n",
      "Supplementary files for PMC12087453 already downloaded (1 files).\n",
      "Supplementary files for PMC12056507 already downloaded (2 files).\n",
      "Supplementary files for PMC12080225 already downloaded (2 files).\n",
      "Supplementary files for PMC11727722 already downloaded (1 files).\n",
      "Supplementary files for PMC11811528 already downloaded (1 files).\n",
      "\n",
      "Already downloaded: 230 PMCIDs\n",
      "Need to download: 5 PMCIDs\n",
      "\n",
      "[1/5] Processing PMC4507953...\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "[2/5] Processing PMC6172579...\n",
      "[2/5] Processing PMC6172579...\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "[3/5] Processing PMC6902683...\n",
      "[3/5] Processing PMC6902683...\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "[4/5] Processing PMC12330522...\n",
      "[4/5] Processing PMC12330522...\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "  ✗ No open access FTP link found (may be under copyright)\n",
      "[5/5] Processing PMC12077394...\n",
      "[5/5] Processing PMC12077394...\n",
      "  Found FTP location: ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/13/0f/PMC12077394.tar.gz\n",
      "  Downloading...\n",
      "  Found FTP location: ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/13/0f/PMC12077394.tar.gz\n",
      "  Downloading...\n",
      "  Extracting files...\n",
      "  Extracting files...\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig1.jpg\n",
      "    ✓ Extracted: giaf035_reviewer_1_original_submission.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig5.gif\n",
      "    ✓ Extracted: giaf035_reviewer_2_original_submission.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035.nxml\n",
      "    ✓ Extracted: giaf035_giga-d-24-00380_revision_2.pdf\n",
      "    ✓ Extracted: giaf035_reviewer_2_revision_1.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig1.jpg\n",
      "    ✓ Extracted: giaf035_reviewer_1_original_submission.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig5.gif\n",
      "    ✓ Extracted: giaf035_reviewer_2_original_submission.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035.nxml\n",
      "    ✓ Extracted: giaf035_giga-d-24-00380_revision_2.pdf\n",
      "    ✓ Extracted: giaf035_reviewer_2_revision_1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_325784/3481204283.py:102: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extract(member, path=pmcid_folder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Extracted: giaf035_giga-d-24-00380_revision_1.pdf\n",
      "    ✓ Extracted: giaf035_authors_response_to_reviewer_comments_original_submission.pdf\n",
      "    ✓ Extracted: giaf035_reviewer_1_revision_1.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig2.gif\n",
      "    ✓ Extracted: giaf035_authors_response_to_reviewer_comments_revision_1.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig4.jpg\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig2.jpg\n",
      "    ✓ Extracted: giaf035.pdf\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig6.gif\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig5.jpg\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig6.jpg\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig4.gif\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig3.jpg\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig1.gif\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035_supplemental_files.zip\n",
      "    ✗ Skipped (not PDF): PMC12077394/giaf035fig3.gif\n",
      "    ✓ Extracted: giaf035_giga-d-24-00380_original_submission.pdf\n",
      "  ✓ Successfully extracted 10 PDF file(s)\n",
      "\n",
      "============================================================\n",
      "SUPPLEMENTARY FILES DOWNLOAD SUMMARY\n",
      "============================================================\n",
      "PMCIDs with supplementary files downloaded: 1\n",
      "PMCIDs without supplementary files/failed: 4\n",
      "Total PDF files extracted: 10\n",
      "============================================================\n",
      "\n",
      "Updating TSV with supplementary files information...\n",
      "✓ Updated TSV saved to 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2025-11-25.tsv'\n",
      "\n",
      "Supplementary Files Statistics:\n",
      "  Total supplementary files: 408\n",
      "  Entries with supplementary files: 240/275\n",
      "  Success rate: 87.3%\n",
      "\n",
      "Block 6 complete.\n",
      "\n",
      "============================================================\n",
      "SUPPLEMENTARY FILES DOWNLOAD SUMMARY\n",
      "============================================================\n",
      "PMCIDs with supplementary files downloaded: 1\n",
      "PMCIDs without supplementary files/failed: 4\n",
      "Total PDF files extracted: 10\n",
      "============================================================\n",
      "\n",
      "Updating TSV with supplementary files information...\n",
      "✓ Updated TSV saved to 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2025-11-25.tsv'\n",
      "\n",
      "Supplementary Files Statistics:\n",
      "  Total supplementary files: 408\n",
      "  Entries with supplementary files: 240/275\n",
      "  Success rate: 87.3%\n",
      "\n",
      "Block 6 complete.\n"
     ]
    }
   ],
   "source": [
    "# 6. Download supplementary files using NCBI OA API and organize by PMCID\n",
    " \n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "# Read in DOME Entries TSV as dataframe\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Extract PMCIDs from the DataFrame\n",
    "if 'mapped_pmcid' in df.columns:\n",
    "    pmcids = df['mapped_pmcid'].dropna().unique()\n",
    "    print(f\"Found {len(pmcids)} PMCIDs to process for supplementary files\")\n",
    "else:\n",
    "    print(\"Error: 'mapped_pmcid' column not found in TSV.\")\n",
    "    pmcids = []\n",
    "\n",
    "# Define the output folder for supplementary files\n",
    "supp_output_folder = 'DOME_Registry_PMC_Supplementary'\n",
    "os.makedirs(supp_output_folder, exist_ok=True)\n",
    "\n",
    "# Human-readable file extensions to keep - ONLY PDF as per request\n",
    "KEEP_EXTENSIONS = {'.pdf'}\n",
    "\n",
    "# Track which PMCIDs need downloading\n",
    "to_download_pmcids = []\n",
    "already_downloaded = 0\n",
    "\n",
    "for pmcid in pmcids:\n",
    "    pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "    # Check if folder exists and has files\n",
    "    if os.path.exists(pmcid_folder):\n",
    "        files = [f for f in os.listdir(pmcid_folder) if os.path.isfile(os.path.join(pmcid_folder, f))]\n",
    "        if files:\n",
    "            print(f\"Supplementary files for {pmcid} already downloaded ({len(files)} files).\")\n",
    "            already_downloaded += 1\n",
    "        else:\n",
    "            # Folder exists but empty - re-download\n",
    "            to_download_pmcids.append(pmcid)\n",
    "    else:\n",
    "        to_download_pmcids.append(pmcid)\n",
    "\n",
    "print(f\"\\nAlready downloaded: {already_downloaded} PMCIDs\")\n",
    "print(f\"Need to download: {len(to_download_pmcids)} PMCIDs\\n\")\n",
    "\n",
    "# Function to download and extract supplementary files from NCBI OA\n",
    "def download_supplementary_files(pmcids):\n",
    "    \"\"\"\n",
    "    Download supplementary files from NCBI PMC Open Access FTP.\n",
    "    Uses NCBI OA API to find FTP location for each PMCID.\n",
    "    Extracts only PDF files.\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    total_files_extracted = 0\n",
    "    \n",
    "    for idx, pmcid in enumerate(pmcids, 1):\n",
    "        print(f\"[{idx}/{len(pmcids)}] Processing {pmcid}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create folder for this PMCID\n",
    "            pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "            os.makedirs(pmcid_folder, exist_ok=True)\n",
    "            \n",
    "            # Use NCBI OA API to find the exact FTP path\n",
    "            api_url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmcid}\"\n",
    "            \n",
    "            with urllib.request.urlopen(api_url, timeout=30) as response:\n",
    "                xml_data = response.read()\n",
    "            \n",
    "            # Parse the XML to find the FTP link (format=\"tgz\" contains all files)\n",
    "            root = ET.fromstring(xml_data)\n",
    "            link_element = root.find(\".//link[@format='tgz']\")\n",
    "            \n",
    "            if link_element is not None:\n",
    "                ftp_url = link_element.get(\"href\")\n",
    "                tar_filename = os.path.join(pmcid_folder, f\"{pmcid}.tar.gz\")\n",
    "                \n",
    "                print(f\"  Found FTP location: {ftp_url}\")\n",
    "                print(f\"  Downloading...\")\n",
    "                \n",
    "                # Download the .tar.gz file\n",
    "                urllib.request.urlretrieve(ftp_url, tar_filename)\n",
    "                \n",
    "                print(f\"  Extracting files...\")\n",
    "                \n",
    "                # Extract and filter files\n",
    "                files_kept = 0\n",
    "                with tarfile.open(tar_filename, \"r:gz\") as tar:\n",
    "                    for member in tar.getmembers():\n",
    "                        if member.isfile():\n",
    "                            # Get file extension\n",
    "                            _, ext = os.path.splitext(member.name.lower())\n",
    "                            \n",
    "                            # Keep only human-readable files\n",
    "                            if ext in KEEP_EXTENSIONS:\n",
    "                                # Extract with sanitized filename (remove directory structure)\n",
    "                                member.name = os.path.basename(member.name)\n",
    "                                tar.extract(member, path=pmcid_folder)\n",
    "                                files_kept += 1\n",
    "                                print(f\"    ✓ Extracted: {member.name}\")\n",
    "                            else:\n",
    "                                print(f\"    ✗ Skipped (not PDF): {member.name}\")\n",
    "                \n",
    "                # Remove the .tar.gz file after extraction\n",
    "                os.remove(tar_filename)\n",
    "                \n",
    "                if files_kept > 0:\n",
    "                    print(f\"  ✓ Successfully extracted {files_kept} PDF file(s)\")\n",
    "                    success_count += 1\n",
    "                    total_files_extracted += files_kept\n",
    "                else:\n",
    "                    print(f\"  ⚠ No PDF files found in package\")\n",
    "                    # Remove empty folder\n",
    "                    try:\n",
    "                        os.rmdir(pmcid_folder)\n",
    "                    except:\n",
    "                        pass\n",
    "                    fail_count += 1\n",
    "                    \n",
    "            else:\n",
    "                print(f\"  ✗ No open access FTP link found (may be under copyright)\")\n",
    "                # Remove empty folder\n",
    "                try:\n",
    "                    os.rmdir(pmcid_folder)\n",
    "                except:\n",
    "                    pass\n",
    "                fail_count += 1\n",
    "                \n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"  ✗ HTTP Error: {e.code} - {e.reason}\")\n",
    "            fail_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {str(e)}\")\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Rate limiting - be respectful to the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUPPLEMENTARY FILES DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"PMCIDs with supplementary files downloaded: {success_count}\")\n",
    "    print(f\"PMCIDs without supplementary files/failed: {fail_count}\")\n",
    "    print(f\"Total PDF files extracted: {total_files_extracted}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Download supplementary files only for those that need it\n",
    "if to_download_pmcids:\n",
    "    download_supplementary_files(to_download_pmcids)\n",
    "else:\n",
    "    print(\"All supplementary files already downloaded. Skipping download step.\\n\")\n",
    "\n",
    "# Update TSV with supplementary files information\n",
    "print(\"Updating TSV with supplementary files information...\")\n",
    "\n",
    "supp_download_status = []\n",
    "supp_file_count = []\n",
    "supp_file_list = []\n",
    "\n",
    "for pmcid in df['mapped_pmcid']:\n",
    "    if pd.notna(pmcid):\n",
    "        pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "        \n",
    "        if os.path.exists(pmcid_folder):\n",
    "            # Count files in the folder\n",
    "            files = [f for f in os.listdir(pmcid_folder) if os.path.isfile(os.path.join(pmcid_folder, f))]\n",
    "            count = len(files)\n",
    "            \n",
    "            if count > 0:\n",
    "                supp_download_status.append('yes')\n",
    "                supp_file_count.append(count)\n",
    "                supp_file_list.append('; '.join(files))\n",
    "            else:\n",
    "                supp_download_status.append('no')\n",
    "                supp_file_count.append(0)\n",
    "                supp_file_list.append(None)\n",
    "        else:\n",
    "            supp_download_status.append('no')\n",
    "            supp_file_count.append(0)\n",
    "            supp_file_list.append(None)\n",
    "    else:\n",
    "        supp_download_status.append('no')\n",
    "        supp_file_count.append(0)\n",
    "        supp_file_list.append(None)\n",
    "\n",
    "# Add columns to DataFrame\n",
    "df['supplementary_downloadable'] = supp_download_status\n",
    "df['supplementary_file_count'] = supp_file_count\n",
    "df['supplementary_file_list'] = supp_file_list\n",
    "\n",
    "# Save updated TSV\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Updated TSV saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "# Print statistics\n",
    "total_supp_files = sum(supp_file_count)\n",
    "entries_with_supp = sum(1 for status in supp_download_status if status == 'yes')\n",
    "\n",
    "print(f\"\\nSupplementary Files Statistics:\")\n",
    "print(f\"  Total supplementary files: {total_supp_files}\")\n",
    "print(f\"  Entries with supplementary files: {entries_with_supp}/{len(df)}\")\n",
    "if len(df) > 0:\n",
    "    print(f\"  Success rate: {entries_with_supp/len(df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nBlock 6 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANING SUPPLEMENTARY FILES FOLDERS\n",
      "============================================================\n",
      "  ✓ Removed redundant main PDF: PMC9757591_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7237030_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC2213690_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11629979_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC9805592_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5821274_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6091426_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5860114_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6036855_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7352871_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6715517_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10441000_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6657583_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12056507_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8545175_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5773889_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8554859_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6478501_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC2638158_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC9681730_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6277570_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10600917_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3340366_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10367125_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6679781_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11034027_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5650527_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC1847686_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8093828_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10087011_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11927397_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12080225_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7068237_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10821710_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5860171_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11345537_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7648120_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8225676_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7680913_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6708480_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC2275242_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8843059_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8553642_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5517062_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6664791_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5104375_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6908647_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3396452_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8469072_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10646871_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7406221_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7692026_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6436896_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8125376_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC9252801_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7816647_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7569858_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10776382_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10673642_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8668950_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7725002_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11238428_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5870574_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4589233_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11945317_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8185002_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4706063_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7602301_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10603766_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7596958_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7162491_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11811528_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6594643_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8351329_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10716825_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6732622_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4773135_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7751093_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8175075_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7735824_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8259419_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC9328381_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3912131_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8230313_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12532322_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12530094_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC2665034_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5930664_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7054390_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6851483_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11653894_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7212484_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8162250_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12398281_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC9580958_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10783149_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10730818_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6459551_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11258913_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6495231_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7442807_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11899596_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4466774_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4606520_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12153353_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7721480_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12532321_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8371605_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7773485_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC2752621_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8553134_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7073919_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5550971_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11816797_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5688026_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5821114_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8204819_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8385175_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10762911_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5042084_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8192578_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7493359_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8485143_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4908364_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5656045_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7446623_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7718328_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11223784_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6214550_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11734293_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10653424_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8100175_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8336795_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5487762_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11299106_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11727722_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5738356_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4315436_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11878767_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7794018_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6237755_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7035778_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11659980_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11851125_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6548586_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7933593_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8182908_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5113897_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12366053_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5034704_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10316696_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6743778_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC2660303_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5079830_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7591939_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12596181_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4931851_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11659981_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6690680_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8019900_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11340644_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7845959_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4380029_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6386402_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4289375_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8261512_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10483879_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC2561051_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6510637_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3542245_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8328792_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7156652_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10174551_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6817842_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11912559_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7142336_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6036478_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4388847_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12077397_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8067080_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10684096_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7297119_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3967921_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC9329459_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6929456_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12099614_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5610945_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7734183_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10940896_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11512451_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7876317_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7333383_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC1421439_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12206155_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7682087_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6242780_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6737184_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7860026_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5120500_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11940444_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10659119_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12087453_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8100172_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8352508_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3169429_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4058174_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8042551_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6629660_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6982787_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5775817_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC10541796_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3292016_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC3009519_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5910428_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC1892091_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6457539_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11837757_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4894951_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4315323_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12365963_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC4834164_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11959188_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6394031_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC7520974_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6214495_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC8295265_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC6923882_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC11423353_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC12569769_main.pdf\n",
      "  ✓ Removed redundant main PDF: PMC5976622_main.pdf\n",
      "\n",
      "Total files removed: 230\n",
      "\n",
      "Block 6.1 complete.\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Clean up irrelevant files from supplementary folders\n",
    "# Removes non-PDF files and main article PDFs that were previously copied (redundant)\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLEANING SUPPLEMENTARY FILES FOLDERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "supp_folder = 'DOME_Registry_PMC_Supplementary'\n",
    "\n",
    "if os.path.exists(supp_folder):\n",
    "    cleaned_count = 0\n",
    "    \n",
    "    # Walk through all directories in the supplementary folder\n",
    "    for root, dirs, files in os.walk(supp_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Condition 1: Remove non-PDF files\n",
    "            if not file.lower().endswith('.pdf'):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"  ✓ Removed non-PDF: {file}\")\n",
    "                    cleaned_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error removing {file}: {e}\")\n",
    "            \n",
    "            # Condition 2: Remove main PDFs (from redundant block 6.5)\n",
    "            # These are typically named {PMCID}_main.pdf\n",
    "            elif file.lower().endswith('_main.pdf'):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"  ✓ Removed redundant main PDF: {file}\")\n",
    "                    cleaned_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error removing {file}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal files removed: {cleaned_count}\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(f\"Supplementary folder not found: {supp_folder}\")\n",
    "\n",
    "print(\"\\nBlock 6.1 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADVANCED CLEANING AND ORGANIZATION\n",
      "============================================================\n",
      "\n",
      "1. Integrating Main PDFs...\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_molecules-25-00044-s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_aging-08-1021-s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1471-2105-7-116-S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_metabolites-10-00357-s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_41592_2023_2086_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1005641.s002.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_btac759_supplementary_data.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae073_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_Supplementary_Data.pdf\n",
      "  ✓ Removed identical file giaf035.pdf (size match)\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_supp_btw280_Kurgan.29.sup.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1755-8794-8-S1-S7-S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae104_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12859_2025_6220_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_mSystems.00224-20-st001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_gkae385_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae039_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_DataSheet_1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf082_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf127_supplement_figures.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_ijms-13-02196-s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1007276.s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_1471-2105-9-389.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1005641.s002.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_btac759_supplementary_data.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae073_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_Supplementary_Data.pdf\n",
      "  ✓ Removed identical file giaf035.pdf (size match)\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_supp_btw280_Kurgan.29.sup.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1755-8794-8-S1-S7-S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae104_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12859_2025_6220_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_mSystems.00224-20-st001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_gkae385_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae039_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_DataSheet_1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf082_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf127_supplement_figures.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_ijms-13-02196-s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1007276.s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_1471-2105-9-389.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_gkac278_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf037_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf022_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_rsif20170203supp1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf027_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1471-2105-9-S12-S18-S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_41398_2020_Article_957.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12915_2023_1803_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1471-2164-8-78-S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12864_2014_6856_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_13321_2018_285_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_ADVS-8-2100104-s009.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12920_2019_630_MOESM2_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_41598_2021_3431_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giad021_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf036_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf008_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1004074.s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giad096_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_biomolecules-11-00500-s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12864_2016_3289_MOESM4_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_1471-2105-9-57.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_gkac278_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf037_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf022_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_rsif20170203supp1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf027_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1471-2105-9-S12-S18-S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_41398_2020_Article_957.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12915_2023_1803_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1471-2164-8-78-S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12864_2014_6856_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_13321_2018_285_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_ADVS-8-2100104-s009.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12920_2019_630_MOESM2_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_41598_2021_3431_MOESM1_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giad021_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf036_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giaf008_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1004074.s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giad096_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_biomolecules-11-00500-s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_12864_2016_3289_MOESM4_ESM.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_1471-2105-9-57.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1006954.s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_annrheumdis-2011-200968.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae095_supplementary_figures_and_tables.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_BSJ-2020026_S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giad071_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1984690.f1.pdf\n",
      "\n",
      "2. Cleaning Irrelevant Files...\n",
      "  ✓ Removed (matched 'review'): giaf035_authors_response_to_reviewer_comments_original_submission.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_2_original_submission.pdf\n",
      "  ✓ Removed (matched 'revision'): giaf035_giga-d-24-00380_revision_2.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_1_original_submission.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_1_revision_1.pdf\n",
      "  ✓ Removed (matched 'revision'): giaf035_giga-d-24-00380_revision_1.pdf\n",
      "  ✓ Removed (matched 'original'): giaf035_giga-d-24-00380_original_submission.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_authors_response_to_reviewer_comments_revision_1.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_2_revision_1.pdf\n",
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY\n",
      "============================================================\n",
      "New folders created for orphans: 0\n",
      "Main PDFs integrated: 231\n",
      "Exact duplicate replacements: 1\n",
      "Different size conflicts handled: 50\n",
      "Irrelevant files removed: 9\n",
      "Supplementary files retained: 156\n",
      "============================================================\n",
      "\n",
      "Block 6.2 complete.\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_pcbi.1006954.s001.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_potential_duplicate_annrheumdis-2011-200968.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giae095_supplementary_figures_and_tables.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_BSJ-2020026_S1.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_giad071_supplemental_file.pdf\n",
      "  ⚠ Renamed different sized file to potential_duplicate_potential_duplicate_1984690.f1.pdf\n",
      "\n",
      "2. Cleaning Irrelevant Files...\n",
      "  ✓ Removed (matched 'review'): giaf035_authors_response_to_reviewer_comments_original_submission.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_2_original_submission.pdf\n",
      "  ✓ Removed (matched 'revision'): giaf035_giga-d-24-00380_revision_2.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_1_original_submission.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_1_revision_1.pdf\n",
      "  ✓ Removed (matched 'revision'): giaf035_giga-d-24-00380_revision_1.pdf\n",
      "  ✓ Removed (matched 'original'): giaf035_giga-d-24-00380_original_submission.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_authors_response_to_reviewer_comments_revision_1.pdf\n",
      "  ✓ Removed (matched 'review'): giaf035_reviewer_2_revision_1.pdf\n",
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY\n",
      "============================================================\n",
      "New folders created for orphans: 0\n",
      "Main PDFs integrated: 231\n",
      "Exact duplicate replacements: 1\n",
      "Different size conflicts handled: 50\n",
      "Irrelevant files removed: 9\n",
      "Supplementary files retained: 156\n",
      "============================================================\n",
      "\n",
      "Block 6.2 complete.\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Advanced cleaning and organization of PDF files\n",
    "# - Integrates main PDFs into supplementary folders with smart replacement\n",
    "# - Cleans irrelevant files based on keyword heuristics\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADVANCED CLEANING AND ORGANIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_source_folder = 'DOME_Registry_PMC_PDFs'\n",
    "supp_root_folder = 'DOME_Registry_PMC_Supplementary'\n",
    "\n",
    "# Statistics tracking\n",
    "stats = {\n",
    "    'folders_created': 0,\n",
    "    'main_pdfs_moved': 0,\n",
    "    'replacements_exact': 0,\n",
    "    'replacements_diff_size': 0,\n",
    "    'irrelevant_removed': 0,\n",
    "    'files_kept': 0\n",
    "}\n",
    "\n",
    "# Keywords for filtering supplementary files\n",
    "KEEP_KEYWORDS = [r's\\d+', r'sup', r'supp', r'supplementary', r'table', r'figure', r'data', r'appendix']\n",
    "REMOVE_KEYWORDS = [r'review', r'comment', r'response', r'revision', r'original', r'author', r'letter', r'correction']\n",
    "\n",
    "def is_irrelevant(filename):\n",
    "    \"\"\"Check if file should be removed based on keywords.\"\"\"\n",
    "    name_lower = filename.lower()\n",
    "    # If it matches remove keywords\n",
    "    for kw in REMOVE_KEYWORDS:\n",
    "        if re.search(kw, name_lower):\n",
    "            return True, kw\n",
    "    return False, None\n",
    "\n",
    "# 1. Process Main PDFs\n",
    "if os.path.exists(pdf_source_folder):\n",
    "    print(\"\\n1. Integrating Main PDFs...\")\n",
    "    for filename in os.listdir(pdf_source_folder):\n",
    "        if filename.endswith('_main.pdf'):\n",
    "            pmcid = filename.replace('_main.pdf', '')\n",
    "            source_path = os.path.join(pdf_source_folder, filename)\n",
    "            dest_folder = os.path.join(supp_root_folder, pmcid)\n",
    "            dest_path = os.path.join(dest_folder, filename)\n",
    "            \n",
    "            # Create folder if it doesn't exist\n",
    "            if not os.path.exists(dest_folder):\n",
    "                os.makedirs(dest_folder, exist_ok=True)\n",
    "                stats['folders_created'] += 1\n",
    "            \n",
    "            # Check for existing files in destination to see if we need to replace/merge\n",
    "            # Exclude the target filename itself to avoid comparing/deleting the file we are about to write\n",
    "            existing_files = [f for f in os.listdir(dest_folder) if f.endswith('.pdf') and f != filename]\n",
    "            \n",
    "            src_size = os.path.getsize(source_path)\n",
    "            \n",
    "            # 1. Check for exact size matches among ANY existing PDFs and remove them\n",
    "            for existing_pdf in existing_files:\n",
    "                existing_path = os.path.join(dest_folder, existing_pdf)\n",
    "                try:\n",
    "                    if os.path.getsize(existing_path) == src_size:\n",
    "                        os.remove(existing_path)\n",
    "                        stats['replacements_exact'] += 1\n",
    "                        print(f\"  ✓ Removed identical file {existing_pdf} (size match)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error checking/removing {existing_pdf}: {e}\")\n",
    "            \n",
    "            # 2. Check remaining files for potential conflicts (single file different size)\n",
    "            # Refresh list after deletions\n",
    "            remaining_files = [f for f in os.listdir(dest_folder) if f.endswith('.pdf') and f != filename]\n",
    "            \n",
    "            if len(remaining_files) == 1:\n",
    "                # Single existing PDF with different size - rename it to preserve it but allow main PDF in\n",
    "                existing_pdf = remaining_files[0]\n",
    "                existing_path = os.path.join(dest_folder, existing_pdf)\n",
    "                \n",
    "                try:\n",
    "                    new_name = f\"potential_duplicate_{existing_pdf}\"\n",
    "                    # Check if renamed file already exists\n",
    "                    if not os.path.exists(os.path.join(dest_folder, new_name)):\n",
    "                        os.rename(existing_path, os.path.join(dest_folder, new_name))\n",
    "                        stats['replacements_diff_size'] += 1\n",
    "                        print(f\"  ⚠ Renamed different sized file to {new_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error renaming {existing_pdf}: {e}\")\n",
    "            \n",
    "            # 3. Copy the main PDF\n",
    "            # Always copy/overwrite to ensure the correctly named main PDF is present\n",
    "            try:\n",
    "                shutil.copy2(source_path, dest_path)\n",
    "                stats['main_pdfs_moved'] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error copying main PDF: {e}\")\n",
    "\n",
    "# 2. Clean Irrelevant Files\n",
    "if os.path.exists(supp_root_folder):\n",
    "    print(\"\\n2. Cleaning Irrelevant Files...\")\n",
    "    for root, dirs, files in os.walk(supp_root_folder):\n",
    "        for file in files:\n",
    "            # Skip the main PDFs we just organized\n",
    "            if file.endswith('_main.pdf'):\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Check for removal keywords\n",
    "            should_remove, reason = is_irrelevant(file)\n",
    "            \n",
    "            if should_remove:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    stats['irrelevant_removed'] += 1\n",
    "                    print(f\"  ✓ Removed (matched '{reason}'): {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error removing {file}: {e}\")\n",
    "            else:\n",
    "                stats['files_kept'] += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"New folders created for orphans: {stats['folders_created']}\")\n",
    "print(f\"Main PDFs integrated: {stats['main_pdfs_moved']}\")\n",
    "print(f\"Exact duplicate replacements: {stats['replacements_exact']}\")\n",
    "print(f\"Different size conflicts handled: {stats['replacements_diff_size']}\")\n",
    "print(f\"Irrelevant files removed: {stats['irrelevant_removed']}\")\n",
    "print(f\"Supplementary files retained: {stats['files_kept']}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"Block 6.2 complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title and abstract columns already exist in TSV.\n",
      "Checking for entries that need to be enriched...\n",
      "242 out of 275 entries already have title/abstract data.\n",
      "\n",
      "Enriching TSV with title and abstract data...\n",
      "Need to enrich 3 entries with title/abstract data.\n",
      "\n",
      "[1/3] Processing PMC8328792...\n",
      "  ✗ Failed to retrieve article details\n",
      "[2/3] Processing PMC10659119...\n",
      "  ✗ Failed to retrieve article details\n",
      "[2/3] Processing PMC10659119...\n",
      "  ✗ Failed to retrieve article details\n",
      "[3/3] Processing PMC12330522...\n",
      "  ✗ Failed to retrieve article details\n",
      "[3/3] Processing PMC12330522...\n",
      "  ✗ Failed to retrieve article details\n",
      "\n",
      "============================================================\n",
      "TITLE/ABSTRACT ENRICHMENT SUMMARY\n",
      "============================================================\n",
      "Successfully enriched: 0\n",
      "Failed/not available: 3\n",
      "Skipped (already had data or no PMCID): 272\n",
      "============================================================\n",
      "\n",
      "✓ Enriched TSV with title and abstract columns saved to 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2025-11-25.tsv'\n",
      "\n",
      "Sample of enriched data (first 3 rows with title/abstract):\n",
      "\n",
      "PMCID: PMC1421439\n",
      "Title: Machine learning approaches to supporting the identification of photoreceptor-enriched genes based o...\n",
      "Abstract: <h4>Background</h4>Retinal photoreceptors are highly specialised cells, which detect light and are central to mammalian vision. Many retinal diseases ...\n",
      "\n",
      "PMCID: PMC1892091\n",
      "Title: Learning biophysically-motivated parameters for alpha helix prediction.\n",
      "Abstract: <h4>Background</h4>Our goal is to develop a state-of-the-art protein secondary structure predictor, with an intuitive and biophysically-motivated ener...\n",
      "\n",
      "PMCID: PMC2213690\n",
      "Title: Combining classifiers to predict gene function in Arabidopsis thaliana using large-scale gene expres...\n",
      "Abstract: <h4>Background</h4>Arabidopsis thaliana is the model species of current plant genomic research with a genome size of 125 Mb and approximately 28,000 g...\n",
      "\n",
      "Block 7 complete.\n",
      "\n",
      "============================================================\n",
      "TITLE/ABSTRACT ENRICHMENT SUMMARY\n",
      "============================================================\n",
      "Successfully enriched: 0\n",
      "Failed/not available: 3\n",
      "Skipped (already had data or no PMCID): 272\n",
      "============================================================\n",
      "\n",
      "✓ Enriched TSV with title and abstract columns saved to 'DOME_Registry_TSV_Files/PMCIDs_DOME_Registry_Contents_2025-11-25.tsv'\n",
      "\n",
      "Sample of enriched data (first 3 rows with title/abstract):\n",
      "\n",
      "PMCID: PMC1421439\n",
      "Title: Machine learning approaches to supporting the identification of photoreceptor-enriched genes based o...\n",
      "Abstract: <h4>Background</h4>Retinal photoreceptors are highly specialised cells, which detect light and are central to mammalian vision. Many retinal diseases ...\n",
      "\n",
      "PMCID: PMC1892091\n",
      "Title: Learning biophysically-motivated parameters for alpha helix prediction.\n",
      "Abstract: <h4>Background</h4>Our goal is to develop a state-of-the-art protein secondary structure predictor, with an intuitive and biophysically-motivated ener...\n",
      "\n",
      "PMCID: PMC2213690\n",
      "Title: Combining classifiers to predict gene function in Arabidopsis thaliana using large-scale gene expres...\n",
      "Abstract: <h4>Background</h4>Arabidopsis thaliana is the model species of current plant genomic research with a genome size of 125 Mb and approximately 28,000 g...\n",
      "\n",
      "Block 7 complete.\n"
     ]
    }
   ],
   "source": [
    "# 7. Enrich the TSV file with title and abstract columns from Europe PMC\n",
    " \n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Read in DOME Entries TSV as dataframe via pandas library functions\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Check if title and abstract columns already exist\n",
    "if 'article_title' in df.columns and 'article_abstract' in df.columns:\n",
    "    print(\"Title and abstract columns already exist in TSV.\")\n",
    "    print(\"Checking for entries that need to be enriched...\")\n",
    "    # Count how many entries already have data\n",
    "    existing_count = df['article_title'].notna().sum()\n",
    "    print(f\"{existing_count} out of {len(df)} entries already have title/abstract data.\")\n",
    "else:\n",
    "    print(\"Adding new columns for title and abstract...\")\n",
    "    df['article_title'] = None\n",
    "    df['article_abstract'] = None\n",
    "\n",
    "# Function to fetch article details from Europe PMC\n",
    "def fetch_article_details(pmcid):\n",
    "    \"\"\"\n",
    "    Fetch title and abstract for a given PMCID from Europe PMC.\n",
    "    \n",
    "    Args:\n",
    "        pmcid (str): PubMed Central ID\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (title, abstract) or (None, None) if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=PMCID:{pmcid}&resultType=core&format=json\"\n",
    "        response = requests.get(url, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('hitCount', 0) > 0:\n",
    "                article = data['resultList']['result'][0]\n",
    "                title = article.get('title', None)\n",
    "                abstract = article.get('abstractText', None)\n",
    "                return title, abstract\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error fetching details: {str(e)}\")\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Process each row and enrich with title/abstract if needed\n",
    "print(\"\\nEnriching TSV with title and abstract data...\")\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "skip_count = 0\n",
    "total_to_process = 0\n",
    "\n",
    "# First, count how many need processing\n",
    "for idx, row in df.iterrows():\n",
    "    pmcid = row.get('mapped_pmcid')\n",
    "    if pd.notna(pmcid):\n",
    "        # Check if this entry already has title/abstract\n",
    "        if pd.isna(row.get('article_title')) or pd.isna(row.get('article_abstract')):\n",
    "            total_to_process += 1\n",
    "\n",
    "print(f\"Need to enrich {total_to_process} entries with title/abstract data.\\n\")\n",
    "\n",
    "# Now process the entries\n",
    "processed = 0\n",
    "for idx, row in df.iterrows():\n",
    "    pmcid = row.get('mapped_pmcid')\n",
    "    \n",
    "    if pd.notna(pmcid):\n",
    "        # Check if this entry already has title/abstract\n",
    "        if pd.isna(row.get('article_title')) or pd.isna(row.get('article_abstract')):\n",
    "            processed += 1\n",
    "            print(f\"[{processed}/{total_to_process}] Processing {pmcid}...\")\n",
    "            \n",
    "            title, abstract = fetch_article_details(pmcid)\n",
    "            \n",
    "            if title and abstract:\n",
    "                df.at[idx, 'article_title'] = title\n",
    "                df.at[idx, 'article_abstract'] = abstract\n",
    "                print(f\"  ✓ Added title and abstract\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"  ✗ Failed to retrieve article details\")\n",
    "                fail_count += 1\n",
    "            \n",
    "            # Rate limiting - be respectful to the API\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            skip_count += 1\n",
    "    else:\n",
    "        # No PMCID available\n",
    "        skip_count += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TITLE/ABSTRACT ENRICHMENT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully enriched: {success_count}\")\n",
    "print(f\"Failed/not available: {fail_count}\")\n",
    "print(f\"Skipped (already had data or no PMCID): {skip_count}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save the enriched TSV\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Enriched TSV with title and abstract columns saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "# Show a sample of the enriched data\n",
    "print(f\"\\nSample of enriched data (first 3 rows with title/abstract):\")\n",
    "sample_df = df[df['article_title'].notna()][['mapped_pmcid', 'article_title', 'article_abstract']].head(3)\n",
    "if not sample_df.empty:\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nPMCID: {row['mapped_pmcid']}\")\n",
    "        print(f\"Title: {row['article_title'][:100]}...\" if len(str(row['article_title'])) > 100 else f\"Title: {row['article_title']}\")\n",
    "        print(f\"Abstract: {str(row['article_abstract'])[:150]}...\" if len(str(row['article_abstract'])) > 150 else f\"Abstract: {row['article_abstract']}\")\n",
    "else:\n",
    "    print(\"No entries with title/abstract data found.\")\n",
    "\n",
    "print(\"\\nBlock 7 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING COMPREHENSIVE METADATA REPORT\n",
      "============================================================\n",
      "\n",
      "1. Analyzing DOME Registry retrieval...\n",
      "  Total DOME Registry entries: 275\n",
      "  Entries with DOI: 275\n",
      "  Entries without DOI: 0\n",
      "\n",
      "2. Analyzing DOI-to-PMCID mapping success...\n",
      "  DOIs processed: 275\n",
      "  Successfully mapped to PMCID: 245 (89.1%)\n",
      "  Mapped to PMID only (no PMCID): 25\n",
      "  Total with Europe PMC ID: 270\n",
      "\n",
      "3. Analyzing full text PDF retrieval...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pdf_downloadable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pdf_downloadable'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 54\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# SECTION 3: FULL TEXT PDF RETRIEVAL METRICS\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. Analyzing full text PDF retrieval...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m pdf_available \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_downloadable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     55\u001b[0m pdf_not_available \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_downloadable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     56\u001b[0m pdf_success_rate \u001b[38;5;241m=\u001b[39m (pdf_available \u001b[38;5;241m/\u001b[39m entries_with_pmcid \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m entries_with_pmcid \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pdf_downloadable'"
     ]
    }
   ],
   "source": [
    "# 8. Create comprehensive metadata analysis and visualizations\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING COMPREHENSIVE METADATA REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read in the final TSV file\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DOME REGISTRY RETRIEVAL METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n1. Analyzing DOME Registry retrieval...\")\n",
    "\n",
    "total_entries = len(df)\n",
    "entries_with_doi = df['publication_doi'].notna().sum()\n",
    "entries_without_doi = total_entries - entries_with_doi\n",
    "\n",
    "print(f\"  Total DOME Registry entries: {total_entries}\")\n",
    "print(f\"  Entries with DOI: {entries_with_doi}\")\n",
    "print(f\"  Entries without DOI: {entries_without_doi}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: DOI-TO-PMCID MAPPING METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n2. Analyzing DOI-to-PMCID mapping success...\")\n",
    "\n",
    "entries_with_pmcid = df['mapped_pmcid'].notna().sum()\n",
    "entries_with_pmid = df['mapped_pmid'].notna().sum()\n",
    "entries_with_pmid_only = (df['mapped_pmid'].notna() & df['mapped_pmcid'].isna()).sum()\n",
    "entries_with_europepmc_id = df['mapped_europepmc_id'].notna().sum()\n",
    "\n",
    "# Calculate mapping success rate\n",
    "mapping_success = entries_with_pmcid\n",
    "mapping_attempted = entries_with_doi\n",
    "mapping_success_rate = (mapping_success / mapping_attempted * 100) if mapping_attempted > 0 else 0\n",
    "\n",
    "print(f\"  DOIs processed: {mapping_attempted}\")\n",
    "print(f\"  Successfully mapped to PMCID: {entries_with_pmcid} ({mapping_success_rate:.1f}%)\")\n",
    "print(f\"  Mapped to PMID only (no PMCID): {entries_with_pmid_only}\")\n",
    "print(f\"  Total with Europe PMC ID: {entries_with_europepmc_id}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: FULL TEXT PDF RETRIEVAL METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n3. Analyzing full text PDF retrieval...\")\n",
    "\n",
    "pdf_available = df['pdf_downloadable'].value_counts().get('yes', 0)\n",
    "pdf_not_available = df['pdf_downloadable'].value_counts().get('no', 0)\n",
    "pdf_success_rate = (pdf_available / entries_with_pmcid * 100) if entries_with_pmcid > 0 else 0\n",
    "\n",
    "print(f\"  PDFs successfully downloaded: {pdf_available} ({pdf_success_rate:.1f}% of PMCIDs)\")\n",
    "print(f\"  PDFs not available: {pdf_not_available}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: SUPPLEMENTARY FILES METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n4. Analyzing supplementary files retrieval...\")\n",
    "\n",
    "entries_with_supp = (df['supplementary_file_count'] > 0).sum()\n",
    "entries_without_supp = total_entries - entries_with_supp\n",
    "total_supp_files = int(df['supplementary_file_count'].sum())\n",
    "\n",
    "# Breakdown by number of supplementary files\n",
    "supp_file_distribution = df[df['supplementary_file_count'] > 0]['supplementary_file_count'].value_counts().sort_index()\n",
    "entries_with_1_file = supp_file_distribution.get(1, 0)\n",
    "entries_with_2_5_files = supp_file_distribution[(supp_file_distribution.index >= 2) & (supp_file_distribution.index <= 5)].sum()\n",
    "entries_with_6plus_files = supp_file_distribution[supp_file_distribution.index > 5].sum()\n",
    "\n",
    "avg_supp_per_entry = (total_supp_files / entries_with_supp) if entries_with_supp > 0 else 0\n",
    "supp_success_rate = (entries_with_supp / entries_with_pmcid * 100) if entries_with_pmcid > 0 else 0\n",
    "\n",
    "print(f\"  Entries with supplementary files: {entries_with_supp} ({supp_success_rate:.1f}% of PMCIDs)\")\n",
    "print(f\"  Total supplementary files downloaded: {total_supp_files}\")\n",
    "print(f\"  Average files per entry (with supp): {avg_supp_per_entry:.2f}\")\n",
    "print(f\"  Entries with 1 file: {entries_with_1_file}\")\n",
    "print(f\"  Entries with 2-5 files: {entries_with_2_5_files}\")\n",
    "print(f\"  Entries with 6+ files: {entries_with_6plus_files}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: TITLE/ABSTRACT ENRICHMENT METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n5. Analyzing title/abstract enrichment...\")\n",
    "\n",
    "title_abstract_available = (df['article_title'].notna() & df['article_abstract'].notna()).sum()\n",
    "title_only = (df['article_title'].notna() & df['article_abstract'].isna()).sum()\n",
    "abstract_only = (df['article_title'].isna() & df['article_abstract'].notna()).sum()\n",
    "title_abstract_missing = total_entries - title_abstract_available - title_only - abstract_only\n",
    "\n",
    "enrichment_success_rate = (title_abstract_available / entries_with_pmcid * 100) if entries_with_pmcid > 0 else 0\n",
    "\n",
    "print(f\"  Complete title & abstract: {title_abstract_available} ({enrichment_success_rate:.1f}% of PMCIDs)\")\n",
    "print(f\"  Title only: {title_only}\")\n",
    "print(f\"  Abstract only: {abstract_only}\")\n",
    "print(f\"  Neither available: {title_abstract_missing}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: OVERALL SUCCESS METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n6. Calculating overall pipeline success...\")\n",
    "\n",
    "# Full data retrieval success (PDF + Supp + Title/Abstract)\n",
    "full_success = ((df['pdf_downloadable'] == 'yes') & \n",
    "                (df['supplementary_file_count'] > 0) & \n",
    "                (df['article_title'].notna()) & \n",
    "                (df['article_abstract'].notna())).sum()\n",
    "\n",
    "# Partial success (PDF + Title/Abstract, no supp required)\n",
    "partial_success = ((df['pdf_downloadable'] == 'yes') & \n",
    "                   (df['article_title'].notna()) & \n",
    "                   (df['article_abstract'].notna())).sum()\n",
    "\n",
    "# At least PDF\n",
    "pdf_only_success = (df['pdf_downloadable'] == 'yes').sum()\n",
    "\n",
    "print(f\"  Full data retrieval (PDF + Supp + Title/Abstract): {full_success} ({full_success/total_entries*100:.1f}%)\")\n",
    "print(f\"  PDF + Title/Abstract (no supp required): {partial_success} ({partial_success/total_entries*100:.1f}%)\")\n",
    "print(f\"  At least PDF available: {pdf_only_success} ({pdf_only_success/total_entries*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: CREATE COMPREHENSIVE METADATA TSV\n",
    "# ============================================================================\n",
    "print(\"\\n7. Creating metadata TSV file...\")\n",
    "\n",
    "metadata = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        # DOME Registry\n",
    "        'Total DOME Registry Entries',\n",
    "        'Entries with DOI',\n",
    "        'Entries without DOI',\n",
    "        '',\n",
    "        # DOI Mapping\n",
    "        'DOI Mapping - Attempted',\n",
    "        'DOI Mapping - Success (PMCID)',\n",
    "        'DOI Mapping - Success Rate (%)',\n",
    "        'DOI Mapping - PMID Only',\n",
    "        'DOI Mapping - Europe PMC ID',\n",
    "        '',\n",
    "        # PDF Retrieval\n",
    "        'PDF - Successfully Downloaded',\n",
    "        'PDF - Not Available',\n",
    "        'PDF - Success Rate (% of PMCIDs)',\n",
    "        '',\n",
    "        # Supplementary Files\n",
    "        'Supplementary - Entries with Files',\n",
    "        'Supplementary - Total Files Count',\n",
    "        'Supplementary - Avg Files per Entry',\n",
    "        'Supplementary - Success Rate (% of PMCIDs)',\n",
    "        'Supplementary - Entries with 1 File',\n",
    "        'Supplementary - Entries with 2-5 Files',\n",
    "        'Supplementary - Entries with 6+ Files',\n",
    "        '',\n",
    "        # Title/Abstract\n",
    "        'Title/Abstract - Complete',\n",
    "        'Title/Abstract - Title Only',\n",
    "        'Title/Abstract - Abstract Only',\n",
    "        'Title/Abstract - Neither',\n",
    "        'Title/Abstract - Success Rate (% of PMCIDs)',\n",
    "        '',\n",
    "        # Overall Success\n",
    "        'Overall - Full Retrieval (PDF+Supp+Title/Abstract)',\n",
    "        'Overall - Partial Retrieval (PDF+Title/Abstract)',\n",
    "        'Overall - Minimum Retrieval (PDF Only)'\n",
    "    ],\n",
    "    'Count': [\n",
    "        # DOME Registry\n",
    "        total_entries,\n",
    "        entries_with_doi,\n",
    "        entries_without_doi,\n",
    "        '',\n",
    "        # DOI Mapping\n",
    "        mapping_attempted,\n",
    "        entries_with_pmcid,\n",
    "        f\"{mapping_success_rate:.1f}\",\n",
    "        entries_with_pmid_only,\n",
    "        entries_with_europepmc_id,\n",
    "        '',\n",
    "        # PDF Retrieval\n",
    "        pdf_available,\n",
    "        pdf_not_available,\n",
    "        f\"{pdf_success_rate:.1f}\",\n",
    "        '',\n",
    "        # Supplementary Files\n",
    "        entries_with_supp,\n",
    "        total_supp_files,\n",
    "        f\"{avg_supp_per_entry:.2f}\",\n",
    "        f\"{supp_success_rate:.1f}\",\n",
    "        entries_with_1_file,\n",
    "        entries_with_2_5_files,\n",
    "        entries_with_6plus_files,\n",
    "        '',\n",
    "        # Title/Abstract\n",
    "        title_abstract_available,\n",
    "        title_only,\n",
    "        abstract_only,\n",
    "        title_abstract_missing,\n",
    "        f\"{enrichment_success_rate:.1f}\",\n",
    "        '',\n",
    "        # Overall Success\n",
    "        full_success,\n",
    "        partial_success,\n",
    "        pdf_only_success\n",
    "    ]\n",
    "})\n",
    "\n",
    "metadata_tsv_path = 'DOME_Registry_TSV_Files/DOME_Metadata_Complete.tsv'\n",
    "metadata.to_csv(metadata_tsv_path, sep='\\t', index=False)\n",
    "print(f\"  ✓ Metadata TSV saved to '{metadata_tsv_path}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: CREATE DETAILED TEXT SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n8. Creating detailed text summary...\")\n",
    "\n",
    "summary_text_path = 'DOME_Registry_TSV_Files/DOME_Metadata_Summary.txt'\n",
    "with open(summary_text_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"DOME REGISTRY DATA RETRIEVAL - COMPREHENSIVE SUMMARY\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. DOME REGISTRY RETRIEVAL\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Total entries retrieved: {total_entries}\\n\")\n",
    "    f.write(f\"Entries with DOI: {entries_with_doi} ({entries_with_doi/total_entries*100:.1f}%)\\n\")\n",
    "    f.write(f\"Entries without DOI: {entries_without_doi} ({entries_without_doi/total_entries*100:.1f}%)\\n\")\n",
    "    f.write(f\"\\nNOTE: DOIs are required for PubMed/PMC mapping and full-text retrieval.\\n\")\n",
    "    f.write(f\"Entries without DOIs cannot be processed further.\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. DOI-TO-PMCID MAPPING (via NCBI E-Utilities)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"DOIs submitted for mapping: {mapping_attempted}\\n\")\n",
    "    f.write(f\"Successfully mapped to PMCID: {entries_with_pmcid} ({mapping_success_rate:.1f}%)\\n\")\n",
    "    f.write(f\"Mapped to PMID only (no PMCID): {entries_with_pmid_only}\\n\")\n",
    "    f.write(f\"Total with Europe PMC identifier: {entries_with_europepmc_id}\\n\")\n",
    "    f.write(f\"\\nNOTE: PMCIDs are required for open-access content retrieval.\\n\")\n",
    "    f.write(f\"Articles without PMCIDs may be behind paywalls or not in PMC.\\n\\n\")\n",
    "    \n",
    "    f.write(\"3. FULL TEXT PDF RETRIEVAL (via Europe PMC)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"PDFs successfully downloaded: {pdf_available}\\n\")\n",
    "    f.write(f\"Success rate: {pdf_success_rate:.1f}% of entries with PMCIDs\\n\")\n",
    "    f.write(f\"PDFs not available: {pdf_not_available}\\n\")\n",
    "    f.write(f\"\\nNOTE: PDF availability depends on open-access status and publisher policies.\\n\")\n",
    "    f.write(f\"Some articles may require institutional access.\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. SUPPLEMENTARY FILES RETRIEVAL (via NCBI PMC OA)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Entries with supplementary files: {entries_with_supp} ({supp_success_rate:.1f}% of PMCIDs)\\n\")\n",
    "    f.write(f\"Total supplementary files downloaded: {total_supp_files}\\n\")\n",
    "    f.write(f\"Average files per entry (with supp): {avg_supp_per_entry:.2f}\\n\\n\")\n",
    "    f.write(\"Distribution by number of files:\\n\")\n",
    "    f.write(f\"  - 1 file: {entries_with_1_file} entries\\n\")\n",
    "    f.write(f\"  - 2-5 files: {entries_with_2_5_files} entries\\n\")\n",
    "    f.write(f\"  - 6+ files: {entries_with_6plus_files} entries\\n\")\n",
    "    f.write(f\"\\nNOTE: Only human-readable files (PDF, DOC, TXT, CSV, etc.) are extracted.\\n\")\n",
    "    f.write(f\"Supplementary files are only available for open-access articles.\\n\\n\")\n",
    "    \n",
    "    f.write(\"5. TITLE & ABSTRACT ENRICHMENT (via Europe PMC)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Complete title & abstract: {title_abstract_available} ({enrichment_success_rate:.1f}% of PMCIDs)\\n\")\n",
    "    f.write(f\"Title only: {title_only}\\n\")\n",
    "    f.write(f\"Abstract only: {abstract_only}\\n\")\n",
    "    f.write(f\"Neither available: {title_abstract_missing}\\n\")\n",
    "    f.write(f\"\\nNOTE: Metadata enrichment enhances searchability and analysis capabilities.\\n\\n\")\n",
    "    \n",
    "    f.write(\"6. OVERALL PIPELINE SUCCESS\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Full data retrieval (PDF + Supp + Title/Abstract): {full_success} ({full_success/total_entries*100:.1f}%)\\n\")\n",
    "    f.write(f\"Partial retrieval (PDF + Title/Abstract): {partial_success} ({partial_success/total_entries*100:.1f}%)\")\n",
    "    f.write(f\"Minimum retrieval (PDF only): {pdf_only_success} ({pdf_only_success/total_entries*100:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"7. DATA LOCATIONS\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Main TSV file: {output_tsv_file_name}\\n\")\n",
    "    f.write(f\"Full text PDFs: DOME_Registry_PMC_PDFs/\\n\")\n",
    "    f.write(f\"Supplementary files: DOME_Registry_PMC_Supplementary/\\n\")\n",
    "    f.write(f\"Metadata files: DOME_Registry_TSV_Files/\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"END OF SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"  ✓ Summary text saved to '{summary_text_path}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: CREATE COMPREHENSIVE VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n9. Creating comprehensive visualizations...\")\n",
    "\n",
    "# Create a larger figure with multiple subplots\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Overall Pipeline Success\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "pipeline_stages = ['Total\\nEntries', 'With\\nDOI', 'PMCID\\nMapped', 'PDF\\nDownloaded', 'Full\\nRetrieval']\n",
    "pipeline_counts = [total_entries, entries_with_doi, entries_with_pmcid, pdf_available, full_success]\n",
    "colors_pipeline = ['#1f77b4', '#2ca02c', '#ff7f0e', '#d62728', '#9467bd']\n",
    "bars1 = ax1.bar(pipeline_stages, pipeline_counts, color=colors_pipeline, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Pipeline Success Funnel', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: DOI Mapping Success\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "mapping_labels = ['PMCID\\nSuccess', 'PMID\\nOnly', 'Failed']\n",
    "mapping_values = [entries_with_pmcid, entries_with_pmid_only, \n",
    "                  mapping_attempted - entries_with_pmcid - entries_with_pmid_only]\n",
    "colors_mapping = ['#2ca02c', '#ff7f0e', '#d62728']\n",
    "wedges2, texts2, autotexts2 = ax2.pie(mapping_values, labels=mapping_labels, autopct='%1.1f%%',\n",
    "                                        colors=colors_mapping, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax2.set_title('DOI-to-PMCID Mapping\\nSuccess Rate', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: PDF Availability\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "pdf_labels = ['Available', 'Not Available']\n",
    "pdf_values = [pdf_available, pdf_not_available]\n",
    "colors_pdf = ['green', 'red']\n",
    "bars3 = ax3.bar(pdf_labels, pdf_values, color=colors_pdf, alpha=0.7, edgecolor='black')\n",
    "ax3.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Full Text PDF Availability', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Supplementary Files Distribution\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "supp_labels = ['1 File', '2-5 Files', '6+ Files', 'No Files']\n",
    "supp_values = [entries_with_1_file, entries_with_2_5_files, \n",
    "               entries_with_6plus_files, entries_without_supp]\n",
    "colors_supp = ['#8c564b', '#e377c2', '#7f7f7f', '#bcbd22']\n",
    "wedges4, texts4, autotexts4 = ax4.pie(supp_values, labels=supp_labels, autopct='%1.1f%%',\n",
    "                                        colors=colors_supp, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax4.set_title('Supplementary Files\\nDistribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 5: Title/Abstract Enrichment\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "enrichment_labels = ['Complete', 'Title Only', 'Abstract Only', 'Missing']\n",
    "enrichment_values = [title_abstract_available, title_only, abstract_only, title_abstract_missing]\n",
    "colors_enrich = ['#2ca02c', '#ff7f0e', '#1f77b4', '#d62728']\n",
    "bars5 = ax5.bar(enrichment_labels, enrichment_values, color=colors_enrich, alpha=0.7, edgecolor='black')\n",
    "ax5.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax5.set_title('Title/Abstract Enrichment', fontsize=14, fontweight='bold')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "for bar in bars5:\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 6: Overall Success Levels\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "success_labels = ['Full\\nRetrieval', 'Partial\\nRetrieval', 'PDF\\nOnly']\n",
    "success_values = [full_success, partial_success - full_success, pdf_only_success - partial_success]\n",
    "colors_success = ['#2ca02c', '#ff7f0e', '#d62728']\n",
    "bars6 = ax6.bar(success_labels, success_values, color=colors_success, alpha=0.7, edgecolor='black')\n",
    "ax6.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax6.set_title('Overall Retrieval Success Levels', fontsize=14, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "for bar in bars6:\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'DOME Registry Data Retrieval - Comprehensive Analysis ({current_date})', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "\n",
    "# Save the comprehensive plot\n",
    "plot_image_path = 'DOME_Registry_TSV_Files/DOME_Metadata_Complete_Analysis.png'\n",
    "plt.savefig(plot_image_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"  ✓ Comprehensive charts saved to '{plot_image_path}'\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY PRINTOUT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOME REGISTRY DATA RETRIEVAL - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total DOME Registry entries: {total_entries}\")\n",
    "print(f\"DOI-to-PMCID mapping success: {mapping_success_rate:.1f}%\")\n",
    "print(f\"PDF retrieval success: {pdf_success_rate:.1f}%\")\n",
    "print(f\"Supplementary files retrieved: {entries_with_supp} entries ({total_supp_files} files)\")\n",
    "print(f\"Title/Abstract enrichment: {enrichment_success_rate:.1f}%\")\n",
    "print(f\"\\nFull data retrieval: {full_success} entries ({full_success/total_entries*100:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAll metadata files saved to: DOME_Registry_TSV_Files/\")\n",
    "print(f\"  - {metadata_tsv_path}\")\n",
    "print(f\"  - {summary_text_path}\")\n",
    "print(f\"  - {plot_image_path}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBlock 8 complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Identify and export failed DOI mappings for manual remediation and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYZING FAILED DOI MAPPINGS FOR REMEDIATION\n",
      "============================================================\n",
      "\n",
      "Created/verified remediation folder: DOME_Registry_Remediation/\n",
      "\n",
      "1. Identifying entries with failed DOI-to-PMCID mappings...\n",
      "  Found 31 entries with DOIs that failed to map to PMCID\n",
      "\n",
      "  Checking 1 existing remediation file(s) for already processed entries...\n",
      "  Found 33 entries already marked as remediated/resolved.\n",
      "  Excluded 26 entries. Remaining to remediate: 5\n",
      "\n",
      "2. Preparing remediation data with cleaned DOIs...\n",
      "\n",
      "3. Exporting remediation file...\n",
      "  ✓ Remediation TSV saved: DOME_Registry_Remediation/Failed_DOI_Mappings_2025-11-25.tsv\n",
      "\n",
      "4. Creating clickable HTML report...\n",
      "  ✓ HTML report saved: DOME_Registry_Remediation/Failed_DOI_Mappings_Report_2025-11-25.html\n",
      "\n",
      "5. Creating remediation instructions...\n",
      "  ✓ Instructions saved: DOME_Registry_Remediation/REMEDIATION_INSTRUCTIONS.txt\n",
      "\n",
      "============================================================\n",
      "REMEDIATION FILES SUMMARY\n",
      "============================================================\n",
      "Failed mappings identified: 5\n",
      "Remediation folder: DOME_Registry_Remediation/\n",
      "\n",
      "Files created:\n",
      "  1. Failed_DOI_Mappings_2025-11-25.tsv\n",
      "  2. Failed_DOI_Mappings_Report_2025-11-25.html\n",
      "  3. REMEDIATION_INSTRUCTIONS.txt\n",
      "\n",
      "Key features:\n",
      "  - DOIs have been cleaned (URLs and prefixes removed)\n",
      "  - Original DOIs shown in HTML for comparison\n",
      "  - Clickable links use cleaned DOI format\n",
      "  - TSV format preserves complex text and special characters\n",
      "\n",
      "Next steps:\n",
      "  - Open the HTML report to review failed DOIs with clickable links\n",
      "  - Follow instructions in REMEDIATION_INSTRUCTIONS.txt\n",
      "  - Update TSV file with manual PMID/PMCID findings\n",
      "============================================================\n",
      "\n",
      "Block 9 complete.\n"
     ]
    }
   ],
   "source": [
    "# 9. Identify DOIs that failed to map to PMID/PMCID and export for remediation\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYZING FAILED DOI MAPPINGS FOR REMEDIATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read in the final TSV file\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Create Remediation folder\n",
    "remediation_folder = 'DOME_Registry_Remediation'\n",
    "os.makedirs(remediation_folder, exist_ok=True)\n",
    "print(f\"\\nCreated/verified remediation folder: {remediation_folder}/\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IDENTIFY FAILED MAPPINGS\n",
    "# ============================================================================\n",
    "print(\"\\n1. Identifying entries with failed DOI-to-PMCID mappings...\")\n",
    "\n",
    "# Filter entries with DOIs but no PMCID mapping\n",
    "failed_mappings = df[\n",
    "    (df['publication_doi'].notna()) & \n",
    "    (df['mapped_pmcid'].isna())\n",
    "].copy()\n",
    "\n",
    "total_failed = len(failed_mappings)\n",
    "print(f\"  Found {total_failed} entries with DOIs that failed to map to PMCID\")\n",
    "\n",
    "# Check for previously remediated entries to exclude\n",
    "tsv_folder = 'DOME_Registry_TSV_Files'\n",
    "remediation_pattern = os.path.join(tsv_folder, 'remediated_Failed_DOI_Mappings_*.tsv')\n",
    "remediation_files = glob.glob(remediation_pattern)\n",
    "\n",
    "if remediation_files:\n",
    "    print(f\"\\n  Checking {len(remediation_files)} existing remediation file(s) for already processed entries...\")\n",
    "    remediated_ids = set()\n",
    "    \n",
    "    for r_file in remediation_files:\n",
    "        try:\n",
    "            r_df = pd.read_csv(r_file, sep='\\t')\n",
    "            # Identify columns\n",
    "            id_col = 'shortid' if 'shortid' in r_df.columns else ('_id' if '_id' in r_df.columns else None)\n",
    "            \n",
    "            if id_col and 'Remediation_Status' in r_df.columns:\n",
    "                # Get IDs that are marked as done (RESOLVED or NOT_IN_PUBMED or DOI_ERROR)\n",
    "                # Basically anything that isn't 'NEEDS_REVIEW'\n",
    "                done_entries = r_df[r_df['Remediation_Status'] != 'NEEDS_REVIEW'][id_col].tolist()\n",
    "                remediated_ids.update(done_entries)\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not read {os.path.basename(r_file)}: {e}\")\n",
    "            \n",
    "    print(f\"  Found {len(remediated_ids)} entries already marked as remediated/resolved.\")\n",
    "    \n",
    "    # Filter out these IDs from failed_mappings\n",
    "    id_col_main = 'shortid' if 'shortid' in failed_mappings.columns else '_id'\n",
    "    if id_col_main in failed_mappings.columns:\n",
    "        initial_count = len(failed_mappings)\n",
    "        failed_mappings = failed_mappings[~failed_mappings[id_col_main].isin(remediated_ids)]\n",
    "        filtered_count = len(failed_mappings)\n",
    "        print(f\"  Excluded {initial_count - filtered_count} entries. Remaining to remediate: {filtered_count}\")\n",
    "        total_failed = filtered_count\n",
    "\n",
    "if total_failed == 0:\n",
    "    print(\"  ✓ All DOIs successfully mapped or already remediated! No new remediation needed.\")\n",
    "    print(\"\\nBlock 9 complete.\")\n",
    "else:\n",
    "    # ============================================================================\n",
    "    # SECTION 2: PREPARE REMEDIATION DATA WITH CLEANED DOIS\n",
    "    # ============================================================================\n",
    "    print(\"\\n2. Preparing remediation data with cleaned DOIs...\")\n",
    "    \n",
    "    # Function to clean and normalize DOI strings (same as Block 4)\n",
    "    def clean_doi(doi_string):\n",
    "        \"\"\"Clean DOI string by removing common prefixes and URLs.\"\"\"\n",
    "        if pd.isna(doi_string):\n",
    "            return None\n",
    "        \n",
    "        doi_string = str(doi_string).strip()\n",
    "        doi_string = re.sub(r'^https?://doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        doi_string = re.sub(r'^https?://dx\\.doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        doi_string = re.sub(r'^https?://www\\.doi\\.org/', '', doi_string, flags=re.IGNORECASE)\n",
    "        doi_string = re.sub(r'^doi:\\s*', '', doi_string, flags=re.IGNORECASE)\n",
    "        \n",
    "        doi_string = doi_string.strip()\n",
    "        return doi_string\n",
    "    \n",
    "    # Select relevant columns for remediation\n",
    "    remediation_columns = []\n",
    "    \n",
    "    # Add key identifier columns if they exist\n",
    "    if 'shortid' in failed_mappings.columns:\n",
    "        remediation_columns.append('shortid')\n",
    "    if '_id' in failed_mappings.columns:\n",
    "        remediation_columns.append('_id')\n",
    "    \n",
    "    # Add publication metadata\n",
    "    publication_cols = ['publication_doi', 'publication_title', 'publication_year', \n",
    "                       'publication_journal', 'publication_authors_0_name']\n",
    "    for col in publication_cols:\n",
    "        if col in failed_mappings.columns:\n",
    "            remediation_columns.append(col)\n",
    "    \n",
    "    # Add mapping attempt results\n",
    "    mapping_cols = ['mapped_pmcid', 'mapped_pmid', 'mapped_europepmc_id']\n",
    "    for col in mapping_cols:\n",
    "        if col in failed_mappings.columns:\n",
    "            remediation_columns.append(col)\n",
    "    \n",
    "    # Create clean remediation dataframe\n",
    "    remediation_df = failed_mappings[remediation_columns].copy()\n",
    "    \n",
    "    # Add cleaned DOI column\n",
    "    remediation_df['DOI_Cleaned'] = remediation_df['publication_doi'].apply(clean_doi)\n",
    "    \n",
    "    # Add clickable DOI links using cleaned DOI\n",
    "    remediation_df['DOI_Link'] = remediation_df['DOI_Cleaned'].apply(\n",
    "        lambda x: f\"https://doi.org/{x}\" if pd.notna(x) else None\n",
    "    )\n",
    "    \n",
    "    # Reorder columns to put cleaned DOI and link first\n",
    "    cols = remediation_df.columns.tolist()\n",
    "    # Move DOI_Cleaned and DOI_Link after publication_doi\n",
    "    doi_idx = cols.index('publication_doi')\n",
    "    cols.remove('DOI_Cleaned')\n",
    "    cols.remove('DOI_Link')\n",
    "    cols.insert(doi_idx + 1, 'DOI_Cleaned')\n",
    "    cols.insert(doi_idx + 2, 'DOI_Link')\n",
    "    remediation_df = remediation_df[cols]\n",
    "    \n",
    "    # Add status column for tracking\n",
    "    remediation_df['Remediation_Status'] = 'NEEDS_REVIEW'\n",
    "    remediation_df['Remediation_Notes'] = ''\n",
    "    remediation_df['Manual_PMCID'] = ''\n",
    "    remediation_df['Manual_PMID'] = ''\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECTION 3: EXPORT REMEDIATION FILE\n",
    "    # ============================================================================\n",
    "    print(\"\\n3. Exporting remediation file...\")\n",
    "    \n",
    "    # Export as TSV (better for text data with special characters)\n",
    "    remediation_tsv_path = os.path.join(remediation_folder, f'Failed_DOI_Mappings_{current_date}.tsv')\n",
    "    remediation_df.to_csv(remediation_tsv_path, sep='\\t', index=False)\n",
    "    print(f\"  ✓ Remediation TSV saved: {remediation_tsv_path}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECTION 4: CREATE CLICKABLE HTML REPORT\n",
    "    # ============================================================================\n",
    "    print(\"\\n4. Creating clickable HTML report...\")\n",
    "    \n",
    "    html_path = os.path.join(remediation_folder, f'Failed_DOI_Mappings_Report_{current_date}.html')\n",
    "    \n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"<!DOCTYPE html>\\n<html>\\n<head>\\n\")\n",
    "        f.write(\"<meta charset='UTF-8'>\\n\")\n",
    "        f.write(\"<title>DOME Registry - Failed DOI Mappings</title>\\n\")\n",
    "        f.write(\"<style>\\n\")\n",
    "        f.write(\"body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\\n\")\n",
    "        f.write(\"h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }\\n\")\n",
    "        f.write(\"h2 { color: #34495e; margin-top: 30px; }\\n\")\n",
    "        f.write(\"table { border-collapse: collapse; width: 100%; background-color: white; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\\n\")\n",
    "        f.write(\"th { background-color: #3498db; color: white; padding: 12px; text-align: left; font-weight: bold; }\\n\")\n",
    "        f.write(\"td { padding: 10px; border-bottom: 1px solid #ddd; }\\n\")\n",
    "        f.write(\"tr:hover { background-color: #f5f5f5; }\\n\")\n",
    "        f.write(\"a { color: #3498db; text-decoration: none; font-weight: bold; }\\n\")\n",
    "        f.write(\"a:hover { color: #2980b9; text-decoration: underline; }\\n\")\n",
    "        f.write(\".stats { background-color: #ecf0f1; padding: 15px; border-radius: 5px; margin: 20px 0; }\\n\")\n",
    "        f.write(\".stats p { margin: 5px 0; }\\n\")\n",
    "        f.write(\".warning { color: #e74c3c; font-weight: bold; }\\n\")\n",
    "        f.write(\".info { color: #16a085; }\\n\")\n",
    "        f.write(\".original-doi { color: #7f8c8d; font-size: 0.85em; font-style: italic; }\\n\")\n",
    "        f.write(\"</style>\\n\")\n",
    "        f.write(\"</head>\\n<body>\\n\")\n",
    "        \n",
    "        # Header\n",
    "        f.write(f\"<h1>DOME Registry - Failed DOI Mappings Report</h1>\\n\")\n",
    "        f.write(f\"<p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\\n\")\n",
    "        \n",
    "        # Statistics\n",
    "        f.write(\"<div class='stats'>\\n\")\n",
    "        f.write(f\"<h2>Summary Statistics</h2>\\n\")\n",
    "        f.write(f\"<p><strong>Total failed mappings:</strong> <span class='warning'>{total_failed}</span></p>\\n\")\n",
    "        f.write(f\"<p><strong>Total DOME Registry entries:</strong> {len(df)}</p>\\n\")\n",
    "        f.write(f\"<p><strong>Entries with DOIs:</strong> {df['publication_doi'].notna().sum()}</p>\\n\")\n",
    "        f.write(f\"<p><strong>Success rate:</strong> <span class='info'>{((df['publication_doi'].notna().sum() - total_failed) / df['publication_doi'].notna().sum() * 100):.1f}%</span></p>\\n\")\n",
    "        f.write(\"</div>\\n\")\n",
    "        \n",
    "        # Instructions\n",
    "        f.write(\"<h2>Instructions for Remediation</h2>\\n\")\n",
    "        f.write(\"<ol>\\n\")\n",
    "        f.write(\"<li>Click on the <strong>cleaned DOI links</strong> to verify if the articles exist and are accessible</li>\\n\")\n",
    "        f.write(\"<li>Search for the article on <a href='https://pubmed.ncbi.nlm.nih.gov/' target='_blank'>PubMed</a> using the cleaned DOI or title</li>\\n\")\n",
    "        f.write(\"<li>If found, note the PMID/PMCID in the remediation <strong>TSV file</strong></li>\\n\")\n",
    "        f.write(\"<li>Check if the original DOI has formatting issues (shown in gray below cleaned DOI)</li>\\n\")\n",
    "        f.write(\"<li>Some articles may not be indexed in PubMed/PMC yet (recently published)</li>\\n\")\n",
    "        f.write(\"</ol>\\n\")\n",
    "        \n",
    "        # Table\n",
    "        f.write(\"<h2>Failed Mappings Table</h2>\\n\")\n",
    "        f.write(\"<table>\\n\")\n",
    "        f.write(\"<tr>\\n\")\n",
    "        \n",
    "        # Table headers\n",
    "        display_headers = ['DOME ID', 'DOI (Cleaned)', 'Link', 'Title', 'Year', 'Journal']\n",
    "        \n",
    "        for header in display_headers:\n",
    "            f.write(f\"<th>{header}</th>\\n\")\n",
    "        f.write(\"</tr>\\n\")\n",
    "        \n",
    "        # Table rows\n",
    "        for idx, row in remediation_df.iterrows():\n",
    "            f.write(\"<tr>\\n\")\n",
    "            \n",
    "            # DOME ID\n",
    "            dome_id = row.get('shortid', 'N/A')\n",
    "            f.write(f\"<td>{dome_id}</td>\\n\")\n",
    "            \n",
    "            # DOI (show cleaned DOI with original below)\n",
    "            doi_cleaned = row.get('DOI_Cleaned', 'N/A')\n",
    "            doi_original = row.get('publication_doi', '')\n",
    "            f.write(f\"<td style='font-size: 0.9em;'>{doi_cleaned}\")\n",
    "            if pd.notna(doi_original) and str(doi_original) != str(doi_cleaned):\n",
    "                f.write(f\"<br><span class='original-doi'>Original: {doi_original}</span>\")\n",
    "            f.write(\"</td>\\n\")\n",
    "            \n",
    "            # DOI Link (using cleaned DOI)\n",
    "            doi_link = row.get('DOI_Link', '')\n",
    "            if pd.notna(doi_link):\n",
    "                f.write(f\"<td><a href='{doi_link}' target='_blank'>Open DOI</a></td>\\n\")\n",
    "            else:\n",
    "                f.write(\"<td>N/A</td>\\n\")\n",
    "            \n",
    "            # Title\n",
    "            title = row.get('publication_title', 'N/A')\n",
    "            if pd.notna(title) and len(str(title)) > 100:\n",
    "                title = str(title)[:100] + \"...\"\n",
    "            f.write(f\"<td>{title}</td>\\n\")\n",
    "            \n",
    "            # Year\n",
    "            year = row.get('publication_year', 'N/A')\n",
    "            f.write(f\"<td>{year}</td>\\n\")\n",
    "            \n",
    "            # Journal\n",
    "            journal = row.get('publication_journal', 'N/A')\n",
    "            if pd.notna(journal) and len(str(journal)) > 50:\n",
    "                journal = str(journal)[:50] + \"...\"\n",
    "            f.write(f\"<td>{journal}</td>\\n\")\n",
    "            \n",
    "            f.write(\"</tr>\\n\")\n",
    "        \n",
    "        f.write(\"</table>\\n\")\n",
    "        \n",
    "        # Footer\n",
    "        f.write(\"<div style='margin-top: 30px; padding: 20px; background-color: #ecf0f1; border-radius: 5px;'>\\n\")\n",
    "        f.write(\"<h2>Next Steps</h2>\\n\")\n",
    "        f.write(\"<p>1. Use the TSV file to track remediation progress</p>\\n\")\n",
    "        f.write(\"<p>2. Update 'Remediation_Status' column as you verify each entry</p>\\n\")\n",
    "        f.write(\"<p>3. Add manual PMID/PMCID values if found</p>\\n\")\n",
    "        f.write(\"<p>4. Re-import the remediated data for complete pipeline processing</p>\\n\")\n",
    "        f.write(\"<p><strong>Note:</strong> The 'DOI_Cleaned' column shows normalized DOIs (URLs removed). Use these for searches.</p>\\n\")\n",
    "        f.write(\"<p><strong>TSV Format:</strong> Tab-separated values preserve special characters and complex text better than CSV.</p>\\n\")\n",
    "        f.write(\"</div>\\n\")\n",
    "        \n",
    "        f.write(\"</body>\\n</html>\")\n",
    "    \n",
    "    print(f\"  ✓ HTML report saved: {html_path}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECTION 5: CREATE REMEDIATION INSTRUCTIONS\n",
    "    # ============================================================================\n",
    "    print(\"\\n5. Creating remediation instructions...\")\n",
    "    \n",
    "    instructions_path = os.path.join(remediation_folder, 'REMEDIATION_INSTRUCTIONS.txt')\n",
    "    \n",
    "    with open(instructions_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"DOME REGISTRY - FAILED DOI MAPPINGS REMEDIATION INSTRUCTIONS\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"OVERVIEW\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Total failed mappings: {total_failed}\\n\")\n",
    "        f.write(f\"These DOIs were present in DOME Registry but could not be mapped to\\n\")\n",
    "        f.write(f\"PMID/PMCID identifiers via NCBI E-Utilities API.\\n\\n\")\n",
    "        \n",
    "        f.write(\"POSSIBLE REASONS FOR FAILURE\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(\"1. Article not indexed in PubMed/PMC database\\n\")\n",
    "        f.write(\"2. DOI format error or typo in DOME Registry\\n\")\n",
    "        f.write(\"3. Recently published article (not yet indexed)\\n\")\n",
    "        f.write(\"4. Article from non-PubMed indexed journal\\n\")\n",
    "        f.write(\"5. Preprint or non-peer-reviewed article\\n\\n\")\n",
    "        \n",
    "        f.write(\"REMEDIATION STEPS\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(\"1. Open the HTML report for easy DOI link navigation\\n\")\n",
    "        f.write(f\"   File: {html_path}\\n\\n\")\n",
    "        \n",
    "        f.write(\"2. For each failed DOI:\\n\")\n",
    "        f.write(\"   a. Click the DOI link to verify article exists\\n\")\n",
    "        f.write(\"   b. Search PubMed using DOI or article title:\\n\")\n",
    "        f.write(\"      https://pubmed.ncbi.nlm.nih.gov/\\n\")\n",
    "        f.write(\"   c. If found, note the PMID (and PMCID if available)\\n\")\n",
    "        f.write(\"   d. Update the remediation TSV with:\\n\")\n",
    "        f.write(\"      - Manual_PMID column\\n\")\n",
    "        f.write(\"      - Manual_PMCID column (if available)\\n\")\n",
    "        f.write(\"      - Remediation_Status: 'RESOLVED' or 'NOT_IN_PUBMED'\\n\")\n",
    "        f.write(\"      - Remediation_Notes: Brief explanation\\n\\n\")\n",
    "        \n",
    "        f.write(\"3. Common remediation scenarios:\\n\\n\")\n",
    "        f.write(\"   SCENARIO A: Article found in PubMed\\n\")\n",
    "        f.write(\"   - Manual_PMID: <paste PMID>\\n\")\n",
    "        f.write(\"   - Manual_PMCID: <paste PMCID if available>\\n\")\n",
    "        f.write(\"   - Remediation_Status: RESOLVED\\n\")\n",
    "        f.write(\"   - Remediation_Notes: Found via manual search\\n\\n\")\n",
    "        \n",
    "        f.write(\"   SCENARIO B: Article not in PubMed\\n\")\n",
    "        f.write(\"   - Manual_PMID: (leave empty)\\n\")\n",
    "        f.write(\"   - Manual_PMCID: (leave empty)\\n\")\n",
    "        f.write(\"   - Remediation_Status: NOT_IN_PUBMED\\n\")\n",
    "        f.write(\"   - Remediation_Notes: Journal not indexed in PubMed\\n\\n\")\n",
    "        \n",
    "        f.write(\"   SCENARIO C: DOI error/broken\\n\")\n",
    "        f.write(\"   - Manual_PMID: (leave empty or add correct if found)\\n\")\n",
    "        f.write(\"   - Manual_PMCID: (leave empty or add correct if found)\\n\")\n",
    "        f.write(\"   - Remediation_Status: DOI_ERROR\\n\")\n",
    "        f.write(\"   - Remediation_Notes: DOI link broken, needs correction in DOME\\n\\n\")\n",
    "        \n",
    "        f.write(\"4. Save the updated TSV file\\n\\n\")\n",
    "        \n",
    "        f.write(\"5. Re-import remediated data (optional future step):\\n\")\n",
    "        f.write(\"   - Merge manual PMID/PMCID back into main dataset\\n\")\n",
    "        f.write(\"   - Re-run download blocks for newly identified PMCIDs\\n\\n\")\n",
    "        \n",
    "        f.write(\"FILES CREATED\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"1. TSV file: {remediation_tsv_path}\\n\")\n",
    "        f.write(f\"2. HTML report: {html_path}\\n\")\n",
    "        f.write(f\"3. This file: {instructions_path}\\n\\n\")\n",
    "        \n",
    "        f.write(\"NOTE ON TSV FORMAT\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(\"TSV (Tab-Separated Values) is used instead of CSV because:\\n\")\n",
    "        f.write(\"- Better handling of commas in text (titles, journal names)\\n\")\n",
    "        f.write(\"- Preserves special characters and unicode\\n\")\n",
    "        f.write(\"- More robust for scientific text and metadata\\n\")\n",
    "        f.write(\"- Can be opened in Excel, LibreOffice, or any text editor\\n\\n\")\n",
    "        \n",
    "    print(f\"  ✓ Instructions saved: {instructions_path}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECTION 6: SUMMARY STATISTICS\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REMEDIATION FILES SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Failed mappings identified: {total_failed}\")\n",
    "    print(f\"Remediation folder: {remediation_folder}/\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  1. {os.path.basename(remediation_tsv_path)}\")\n",
    "    print(f\"  2. {os.path.basename(html_path)}\")\n",
    "    print(f\"  3. {os.path.basename(instructions_path)}\")\n",
    "    print(\"\\nKey features:\")\n",
    "    print(f\"  - DOIs have been cleaned (URLs and prefixes removed)\")\n",
    "    print(f\"  - Original DOIs shown in HTML for comparison\")\n",
    "    print(f\"  - Clickable links use cleaned DOI format\")\n",
    "    print(f\"  - TSV format preserves complex text and special characters\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(f\"  - Open the HTML report to review failed DOIs with clickable links\")\n",
    "    print(f\"  - Follow instructions in REMEDIATION_INSTRUCTIONS.txt\")\n",
    "    print(f\"  - Update TSV file with manual PMID/PMCID findings\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBlock 9 complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
