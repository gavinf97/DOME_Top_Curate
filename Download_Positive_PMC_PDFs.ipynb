{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "import shutil\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Define output folders\n",
    "pdf_output_folder = 'Positive_PMC_PDFs'\n",
    "supp_output_folder = 'Positive_PMC_Supplementary'\n",
    "tsv_output_folder = 'Positive_PMC_TSV_Files'\n",
    "\n",
    "os.makedirs(pdf_output_folder, exist_ok=True)\n",
    "os.makedirs(supp_output_folder, exist_ok=True)\n",
    "os.makedirs(tsv_output_folder, exist_ok=True)\n",
    "\n",
    "print(\"Directories created/verified:\")\n",
    "print(f\"- {pdf_output_folder}\")\n",
    "print(f\"- {supp_output_folder}\")\n",
    "print(f\"- {tsv_output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Input Data\n",
    "# Load the input CSV files\n",
    "positive_entries_file = 'positive_entries.csv'\n",
    "top_pos_file = 'top_pos.csv'\n",
    "\n",
    "print(f\"Loading {positive_entries_file}...\")\n",
    "df_positive = pd.read_csv(positive_entries_file)\n",
    "print(f\"Loaded {len(df_positive)} rows.\")\n",
    "\n",
    "print(f\"Loading {top_pos_file}...\")\n",
    "df_top = pd.read_csv(top_pos_file)\n",
    "print(f\"Loaded {len(df_top)} rows.\")\n",
    "\n",
    "# Combine the dataframes\n",
    "df_combined = pd.concat([df_positive, df_top], ignore_index=True)\n",
    "\n",
    "# Remove duplicates based on PMCID\n",
    "df_combined = df_combined.drop_duplicates(subset=['PMCID'])\n",
    "\n",
    "# Filter for rows with valid PMCIDs\n",
    "df_combined = df_combined[df_combined['PMCID'].notna()]\n",
    "\n",
    "# Map columns to match the expected names in the adapted pipeline\n",
    "df_combined['mapped_pmcid'] = df_combined['PMCID']\n",
    "df_combined['mapped_pmid'] = df_combined['PMID']\n",
    "df_combined['publication_doi'] = df_combined['DOI']\n",
    "df_combined['article_title'] = df_combined['Title']\n",
    "df_combined['article_abstract'] = df_combined['Abstract']\n",
    "\n",
    "# Create a working TSV file to track progress\n",
    "output_tsv_file_name = os.path.join(tsv_output_folder, 'positive_entries_status.tsv')\n",
    "df_combined.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "\n",
    "print(f\"\\nCombined unique entries with PMCID: {len(df_combined)}\")\n",
    "print(f\"Working TSV saved to: {output_tsv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b46285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Download full text PDFs using PMCIDs from Europe PMC\n",
    "# Note: Europe PMC does not directly provide PDFs through REST API - we need to use alternative methods\n",
    "\n",
    "# Read in the working TSV\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Extract PMCIDs from the DataFrame\n",
    "pmcids = df['mapped_pmcid'].dropna().unique()\n",
    "\n",
    "# Use the defined output folder\n",
    "output_folder = pdf_output_folder\n",
    "\n",
    "# Track which PMCIDs need downloading (skip already downloaded)\n",
    "to_download_pmcid = []\n",
    "for pmcid in pmcids:\n",
    "    if os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "        print(f\"PDF for PMCID {pmcid} already downloaded.\")\n",
    "    else:\n",
    "        # print(f\"PDF for PMCID {pmcid} not yet downloaded.\") # Reduce verbosity\n",
    "        to_download_pmcid.append(pmcid)\n",
    "\n",
    "print(f\"\\nNeed to download {len(to_download_pmcid)} PDFs out of {len(pmcids)} total entries.\\n\")\n",
    "\n",
    "# Function to download full text PDF\n",
    "def download_pdfs(pmcids):\n",
    "    \"\"\"\n",
    "    Download PDFs from Europe PMC. \n",
    "    Note: Direct PDF downloads are not always available through Europe PMC REST API.\n",
    "    We'll try multiple approaches:\n",
    "    1. Try to get PDF link from article metadata\n",
    "    2. Download supplementary files if available\n",
    "    3. Construct publisher URLs where possible\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for idx, pmcid in enumerate(pmcids, 1):\n",
    "        print(f\"[{idx}/{len(pmcids)}] Processing {pmcid}...\")\n",
    "        \n",
    "        # Clean PMCID (remove 'PMC' prefix for some API calls)\n",
    "        clean_pmcid = pmcid.replace('PMC', '') if pmcid.startswith('PMC') else pmcid\n",
    "        \n",
    "        # Try Method 1: Get article metadata to find PDF link\n",
    "        try:\n",
    "            metadata_url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=PMCID:{pmcid}&resultType=core&format=json\"\n",
    "            metadata_response = requests.get(metadata_url, timeout=30)\n",
    "            \n",
    "            if metadata_response.status_code == 200:\n",
    "                metadata = metadata_response.json()\n",
    "                \n",
    "                if metadata.get('hitCount', 0) > 0:\n",
    "                    result = metadata['resultList']['result'][0]\n",
    "                    \n",
    "                    # Try to get PDF link from fullTextUrlList\n",
    "                    if 'fullTextUrlList' in result and result['fullTextUrlList']:\n",
    "                        for url_info in result['fullTextUrlList']['fullTextUrl']:\n",
    "                            if url_info.get('documentStyle') == 'pdf' or url_info.get('availabilityCode') == 'OA':\n",
    "                                pdf_url = url_info.get('url')\n",
    "                                \n",
    "                                if pdf_url and '.pdf' in pdf_url.lower():\n",
    "                                    # Try to download the PDF\n",
    "                                    pdf_response = requests.get(pdf_url, timeout=30, allow_redirects=True)\n",
    "                                    \n",
    "                                    if pdf_response.status_code == 200 and pdf_response.headers.get('Content-Type', '').startswith('application/pdf'):\n",
    "                                        output_file = os.path.join(output_folder, f\"{pmcid}_main.pdf\")\n",
    "                                        with open(output_file, 'wb') as file:\n",
    "                                            file.write(pdf_response.content)\n",
    "                                        print(f\"  ✓ Downloaded main PDF from publisher\")\n",
    "                                        success_count += 1\n",
    "                                        break\n",
    "                    \n",
    "                    # If no PDF found yet, try PMC OA service\n",
    "                    if not os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "                        # Try Europe PMC OA PDF service (different endpoint)\n",
    "                        pmc_oa_url = f\"https://europepmc.org/articles/{pmcid}?pdf=render\"\n",
    "                        pmc_response = requests.get(pmc_oa_url, timeout=30, allow_redirects=True)\n",
    "                        \n",
    "                        if pmc_response.status_code == 200 and len(pmc_response.content) > 1000:\n",
    "                            # Check if it's actually a PDF\n",
    "                            if pmc_response.content[:4] == b'%PDF':\n",
    "                                output_file = os.path.join(output_folder, f\"{pmcid}_main.pdf\")\n",
    "                                with open(output_file, 'wb') as file:\n",
    "                                    file.write(pmc_response.content)\n",
    "                                print(f\"  ✓ Downloaded main PDF from PMC OA service\")\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                print(f\"  ✗ Could not retrieve PDF (not openly available)\")\n",
    "                                fail_count += 1\n",
    "                        else:\n",
    "                            print(f\"  ✗ Could not retrieve PDF (status: {pmc_response.status_code})\")\n",
    "                            fail_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error downloading main PDF: {str(e)}\")\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Rate limiting - be respectful to the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Main PDFs successfully downloaded: {success_count}\")\n",
    "    print(f\"Main PDFs failed/not available: {fail_count}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Download PDFs for each PMCID that hasn't been downloaded yet\n",
    "if to_download_pmcid:\n",
    "    download_pdfs(to_download_pmcid)\n",
    "else:\n",
    "    print(\"All PDFs already downloaded. Skipping download step.\")\n",
    "\n",
    "# Update the TSV with download status\n",
    "print(\"Updating TSV with PDF download status...\")\n",
    "pdf_downloadable = []\n",
    "\n",
    "for pmcid in df['mapped_pmcid']:\n",
    "    if pd.notna(pmcid) and os.path.exists(f'{output_folder}/{pmcid}_main.pdf'):\n",
    "        pdf_downloadable.append('yes')\n",
    "    else:\n",
    "        pdf_downloadable.append('no')\n",
    "\n",
    "# Add the new column of download status to the DataFrame and save\n",
    "df['pdf_downloadable'] = pdf_downloadable\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Updated TSV with PDF download status saved to '{output_tsv_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download supplementary files using NCBI OA API and organize by PMCID\n",
    " \n",
    "# Read in the working TSV\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# Extract PMCIDs from the DataFrame\n",
    "if 'mapped_pmcid' in df.columns:\n",
    "    pmcids = df['mapped_pmcid'].dropna().unique()\n",
    "    print(f\"Found {len(pmcids)} PMCIDs to process for supplementary files\")\n",
    "else:\n",
    "    print(\"Error: 'mapped_pmcid' column not found in TSV.\")\n",
    "    pmcids = []\n",
    "\n",
    "# Define the output folder for supplementary files\n",
    "# supp_output_folder is already defined in Cell 1\n",
    "\n",
    "# Human-readable file extensions to keep - ONLY PDF as per request\n",
    "KEEP_EXTENSIONS = {'.pdf'}\n",
    "\n",
    "# Track which PMCIDs need downloading\n",
    "to_download_pmcids = []\n",
    "already_downloaded = 0\n",
    "\n",
    "for pmcid in pmcids:\n",
    "    pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "    # Check if folder exists and has files\n",
    "    if os.path.exists(pmcid_folder):\n",
    "        files = [f for f in os.listdir(pmcid_folder) if os.path.isfile(os.path.join(pmcid_folder, f))]\n",
    "        if files:\n",
    "            # print(f\"Supplementary files for {pmcid} already downloaded ({len(files)} files).\")\n",
    "            already_downloaded += 1\n",
    "        else:\n",
    "            # Folder exists but empty - re-download\n",
    "            to_download_pmcids.append(pmcid)\n",
    "    else:\n",
    "        to_download_pmcids.append(pmcid)\n",
    "\n",
    "print(f\"\\nAlready downloaded: {already_downloaded} PMCIDs\")\n",
    "print(f\"Need to download: {len(to_download_pmcids)} PMCIDs\\n\")\n",
    "\n",
    "# Function to download and extract supplementary files from NCBI OA\n",
    "def download_supplementary_files(pmcids):\n",
    "    \"\"\"\n",
    "    Download supplementary files from NCBI PMC Open Access FTP.\n",
    "    Uses NCBI OA API to find FTP location for each PMCID.\n",
    "    Extracts only PDF files.\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    total_files_extracted = 0\n",
    "    \n",
    "    for idx, pmcid in enumerate(pmcids, 1):\n",
    "        print(f\"[{idx}/{len(pmcids)}] Processing {pmcid}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create folder for this PMCID\n",
    "            pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "            os.makedirs(pmcid_folder, exist_ok=True)\n",
    "            \n",
    "            # Use NCBI OA API to find the exact FTP path\n",
    "            api_url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmcid}\"\n",
    "            \n",
    "            with urllib.request.urlopen(api_url, timeout=30) as response:\n",
    "                xml_data = response.read()\n",
    "            \n",
    "            # Parse the XML to find the FTP link (format=\"tgz\" contains all files)\n",
    "            root = ET.fromstring(xml_data)\n",
    "            link_element = root.find(\".//link[@format='tgz']\")\n",
    "            \n",
    "            if link_element is not None:\n",
    "                ftp_url = link_element.get(\"href\")\n",
    "                tar_filename = os.path.join(pmcid_folder, f\"{pmcid}.tar.gz\")\n",
    "                \n",
    "                print(f\"  Found FTP location: {ftp_url}\")\n",
    "                print(f\"  Downloading...\")\n",
    "                \n",
    "                # Download the .tar.gz file\n",
    "                urllib.request.urlretrieve(ftp_url, tar_filename)\n",
    "                \n",
    "                print(f\"  Extracting files...\")\n",
    "                \n",
    "                # Extract and filter files\n",
    "                files_kept = 0\n",
    "                with tarfile.open(tar_filename, \"r:gz\") as tar:\n",
    "                    for member in tar.getmembers():\n",
    "                        if member.isfile():\n",
    "                            # Get file extension\n",
    "                            _, ext = os.path.splitext(member.name.lower())\n",
    "                            \n",
    "                            # Keep only human-readable files\n",
    "                            if ext in KEEP_EXTENSIONS:\n",
    "                                # Extract with sanitized filename (remove directory structure)\n",
    "                                member.name = os.path.basename(member.name)\n",
    "                                tar.extract(member, path=pmcid_folder)\n",
    "                                files_kept += 1\n",
    "                                print(f\"    ✓ Extracted: {member.name}\")\n",
    "                            else:\n",
    "                                # print(f\"    ✗ Skipped (not PDF): {member.name}\")\n",
    "                                pass\n",
    "                \n",
    "                # Remove the .tar.gz file after extraction\n",
    "                os.remove(tar_filename)\n",
    "                \n",
    "                if files_kept > 0:\n",
    "                    print(f\"  ✓ Successfully extracted {files_kept} PDF file(s)\")\n",
    "                    success_count += 1\n",
    "                    total_files_extracted += files_kept\n",
    "                else:\n",
    "                    print(f\"  ⚠ No PDF files found in package\")\n",
    "                    # Remove empty folder\n",
    "                    try:\n",
    "                        os.rmdir(pmcid_folder)\n",
    "                    except:\n",
    "                        pass\n",
    "                    fail_count += 1\n",
    "                    \n",
    "            else:\n",
    "                print(f\"  ✗ No open access FTP link found (may be under copyright)\")\n",
    "                # Remove empty folder\n",
    "                try:\n",
    "                    os.rmdir(pmcid_folder)\n",
    "                except:\n",
    "                    pass\n",
    "                fail_count += 1\n",
    "                \n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"  ✗ HTTP Error: {e.code} - {e.reason}\")\n",
    "            fail_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {str(e)}\")\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Rate limiting - be respectful to the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUPPLEMENTARY FILES DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"PMCIDs with supplementary files downloaded: {success_count}\")\n",
    "    print(f\"PMCIDs without supplementary files/failed: {fail_count}\")\n",
    "    print(f\"Total PDF files extracted: {total_files_extracted}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Download supplementary files only for those that need it\n",
    "if to_download_pmcids:\n",
    "    download_supplementary_files(to_download_pmcids)\n",
    "else:\n",
    "    print(\"All supplementary files already downloaded. Skipping download step.\\n\")\n",
    "\n",
    "# Update TSV with supplementary files information\n",
    "print(\"Updating TSV with supplementary files information...\")\n",
    "\n",
    "supp_download_status = []\n",
    "supp_file_count = []\n",
    "supp_file_list = []\n",
    "\n",
    "for pmcid in df['mapped_pmcid']:\n",
    "    if pd.notna(pmcid):\n",
    "        pmcid_folder = os.path.join(supp_output_folder, pmcid)\n",
    "        \n",
    "        if os.path.exists(pmcid_folder):\n",
    "            # Count files in the folder\n",
    "            files = [f for f in os.listdir(pmcid_folder) if os.path.isfile(os.path.join(pmcid_folder, f))]\n",
    "            count = len(files)\n",
    "            \n",
    "            if count > 0:\n",
    "                supp_download_status.append('yes')\n",
    "                supp_file_count.append(count)\n",
    "                supp_file_list.append('; '.join(files))\n",
    "            else:\n",
    "                supp_download_status.append('no')\n",
    "                supp_file_count.append(0)\n",
    "                supp_file_list.append(None)\n",
    "        else:\n",
    "            supp_download_status.append('no')\n",
    "            supp_file_count.append(0)\n",
    "            supp_file_list.append(None)\n",
    "    else:\n",
    "        supp_download_status.append('no')\n",
    "        supp_file_count.append(0)\n",
    "        supp_file_list.append(None)\n",
    "\n",
    "# Add columns to DataFrame\n",
    "df['supplementary_downloadable'] = supp_download_status\n",
    "df['supplementary_file_count'] = supp_file_count\n",
    "df['supplementary_file_list'] = supp_file_list\n",
    "\n",
    "# Save updated TSV\n",
    "df.to_csv(output_tsv_file_name, sep='\\t', index=False)\n",
    "print(f\"✓ Updated TSV saved to '{output_tsv_file_name}'\")\n",
    "\n",
    "# Print statistics\n",
    "total_supp_files = sum(supp_file_count)\n",
    "entries_with_supp = sum(1 for status in supp_download_status if status == 'yes')\n",
    "\n",
    "print(f\"\\nSupplementary Files Statistics:\")\n",
    "print(f\"  Total supplementary files: {total_supp_files}\")\n",
    "print(f\"  Entries with supplementary files: {entries_with_supp}/{len(df)}\")\n",
    "if len(df) > 0:\n",
    "    print(f\"  Success rate: {entries_with_supp/len(df)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d358a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Clean and organize PDF files\n",
    "# - Integrates main PDFs into supplementary folders with smart replacement\n",
    "# - Cleans irrelevant files based on keyword heuristics\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLEANING AND ORGANIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_source_folder = pdf_output_folder\n",
    "supp_root_folder = supp_output_folder\n",
    "\n",
    "# Statistics tracking\n",
    "stats = {\n",
    "    'folders_created': 0,\n",
    "    'main_pdfs_moved': 0,\n",
    "    'replacements_exact': 0,\n",
    "    'replacements_diff_size': 0,\n",
    "    'irrelevant_removed': 0,\n",
    "    'files_kept': 0,\n",
    "    'prefixes_cleaned': 0\n",
    "}\n",
    "\n",
    "# Keywords for filtering supplementary files\n",
    "KEEP_KEYWORDS = [r's\\d+', r'sup', r'supp', r'supplementary', r'table', r'figure', r'data', r'appendix']\n",
    "REMOVE_KEYWORDS = [r'review', r'comment', r'response', r'revision', r'original', r'author', r'letter', r'correction']\n",
    "\n",
    "def is_irrelevant(filename):\n",
    "    \"\"\"Check if file should be removed based on keywords.\"\"\"\n",
    "    name_lower = filename.lower()\n",
    "    # If it matches remove keywords\n",
    "    for kw in REMOVE_KEYWORDS:\n",
    "        if re.search(kw, name_lower):\n",
    "            return True, kw\n",
    "    return False, None\n",
    "\n",
    "# 0. Clean up existing 'potential_duplicate_' prefixes\n",
    "if os.path.exists(supp_root_folder):\n",
    "    print(\"\\n0. Cleaning up 'potential_duplicate_' prefixes...\")\n",
    "    for root, dirs, files in os.walk(supp_root_folder):\n",
    "        for file in files:\n",
    "            if 'potential_duplicate_' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Remove all occurrences of the prefix\n",
    "                new_name = file.replace('potential_duplicate_', '')\n",
    "                new_path = os.path.join(root, new_name)\n",
    "                \n",
    "                try:\n",
    "                    if os.path.exists(new_path):\n",
    "                        # If target exists, remove the prefixed file (it's likely a duplicate or unwanted version)\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"  ✓ Removed prefixed file (target exists): {file}\")\n",
    "                    else:\n",
    "                        # Rename to clean name\n",
    "                        os.rename(file_path, new_path)\n",
    "                        print(f\"  ✓ Renamed {file} -> {new_name}\")\n",
    "                    stats['prefixes_cleaned'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error cleaning prefix for {file}: {e}\")\n",
    "\n",
    "# 1. Process Main PDFs\n",
    "if os.path.exists(pdf_source_folder):\n",
    "    print(\"\\n1. Integrating Main PDFs...\")\n",
    "    for filename in os.listdir(pdf_source_folder):\n",
    "        if filename.endswith('_main.pdf'):\n",
    "            pmcid = filename.replace('_main.pdf', '')\n",
    "            source_path = os.path.join(pdf_source_folder, filename)\n",
    "            dest_folder = os.path.join(supp_root_folder, pmcid)\n",
    "            dest_path = os.path.join(dest_folder, filename)\n",
    "            \n",
    "            # Create folder if it doesn't exist\n",
    "            if not os.path.exists(dest_folder):\n",
    "                os.makedirs(dest_folder, exist_ok=True)\n",
    "                stats['folders_created'] += 1\n",
    "            \n",
    "            # Check for existing files in destination to see if we need to replace/merge\n",
    "            # Exclude the target filename itself to avoid comparing/deleting the file we are about to write\n",
    "            existing_files = [f for f in os.listdir(dest_folder) if f.endswith('.pdf') and f != filename]\n",
    "            \n",
    "            src_size = os.path.getsize(source_path)\n",
    "            \n",
    "            # 1. Check for exact size matches among ANY existing PDFs and remove them\n",
    "            for existing_pdf in existing_files:\n",
    "                existing_path = os.path.join(dest_folder, existing_pdf)\n",
    "                try:\n",
    "                    if os.path.getsize(existing_path) == src_size:\n",
    "                        os.remove(existing_path)\n",
    "                        stats['replacements_exact'] += 1\n",
    "                        print(f\"  ✓ Removed identical file {existing_pdf} (size match)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error checking/removing {existing_pdf}: {e}\")\n",
    "            \n",
    "            # 2. Copy the main PDF (Overwrite if exists)\n",
    "            try:\n",
    "                shutil.copy2(source_path, dest_path)\n",
    "                stats['main_pdfs_moved'] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error copying main PDF: {e}\")\n",
    "\n",
    "# 2. Clean Irrelevant Files\n",
    "if os.path.exists(supp_root_folder):\n",
    "    print(\"\\n2. Cleaning Irrelevant Files...\")\n",
    "    for root, dirs, files in os.walk(supp_root_folder):\n",
    "        for file in files:\n",
    "            # Skip the main PDFs we just organized\n",
    "            if file.endswith('_main.pdf'):\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Check for removal keywords\n",
    "            should_remove, reason = is_irrelevant(file)\n",
    "            \n",
    "            if should_remove:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    stats['irrelevant_removed'] += 1\n",
    "                    print(f\"  ✓ Removed (matched '{reason}'): {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error removing {file}: {e}\")\n",
    "            else:\n",
    "                stats['files_kept'] += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Prefixes cleaned: {stats['prefixes_cleaned']}\")\n",
    "print(f\"New folders created for orphans: {stats['folders_created']}\")\n",
    "print(f\"Main PDFs integrated: {stats['main_pdfs_moved']}\")\n",
    "print(f\"Exact duplicate replacements: {stats['replacements_exact']}\")\n",
    "print(f\"Irrelevant files removed: {stats['irrelevant_removed']}\")\n",
    "print(f\"Supplementary files retained: {stats['files_kept']}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08359fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create comprehensive metadata analysis and visualizations\n",
    "# Adapted for Positive Entries\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING COMPREHENSIVE METADATA REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read in the final TSV file\n",
    "df = pd.read_csv(output_tsv_file_name, sep='\\t')\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: REGISTRY RETRIEVAL METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n1. Analyzing Registry retrieval...\")\n",
    "\n",
    "total_entries = len(df)\n",
    "entries_with_doi = df['publication_doi'].notna().sum()\n",
    "entries_without_doi = total_entries - entries_with_doi\n",
    "\n",
    "print(f\"  Total Entries: {total_entries}\")\n",
    "print(f\"  Entries with DOI: {entries_with_doi}\")\n",
    "print(f\"  Entries without DOI: {entries_without_doi}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: DOI-TO-PMCID MAPPING METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n2. Analyzing PMCID availability...\")\n",
    "\n",
    "entries_with_pmcid = df['mapped_pmcid'].notna().sum()\n",
    "entries_with_pmid = df['mapped_pmid'].notna().sum()\n",
    "entries_with_pmid_only = (df['mapped_pmid'].notna() & df['mapped_pmcid'].isna()).sum()\n",
    "\n",
    "# Calculate mapping success rate (In this context, we started with PMCIDs, so it should be 100% or close)\n",
    "mapping_success = entries_with_pmcid\n",
    "mapping_attempted = total_entries # We assume all entries were candidates\n",
    "mapping_success_rate = (mapping_success / mapping_attempted * 100) if mapping_attempted > 0 else 0\n",
    "\n",
    "print(f\"  Entries processed: {mapping_attempted}\")\n",
    "print(f\"  Entries with PMCID: {entries_with_pmcid} ({mapping_success_rate:.1f}%)\")\n",
    "print(f\"  Entries with PMID only: {entries_with_pmid_only}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: FULL TEXT PDF RETRIEVAL METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n3. Analyzing full text PDF retrieval...\")\n",
    "\n",
    "pdf_available = df['pdf_downloadable'].value_counts().get('yes', 0)\n",
    "pdf_not_available = df['pdf_downloadable'].value_counts().get('no', 0)\n",
    "pdf_success_rate = (pdf_available / entries_with_pmcid * 100) if entries_with_pmcid > 0 else 0\n",
    "\n",
    "print(f\"  PDFs successfully downloaded: {pdf_available} ({pdf_success_rate:.1f}% of PMCIDs)\")\n",
    "print(f\"  PDFs not available: {pdf_not_available}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: SUPPLEMENTARY FILES METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n4. Analyzing supplementary files retrieval...\")\n",
    "\n",
    "entries_with_supp = (df['supplementary_file_count'] > 0).sum()\n",
    "entries_without_supp = total_entries - entries_with_supp\n",
    "total_supp_files = int(df['supplementary_file_count'].sum())\n",
    "\n",
    "# Breakdown by number of supplementary files\n",
    "supp_file_distribution = df[df['supplementary_file_count'] > 0]['supplementary_file_count'].value_counts().sort_index()\n",
    "entries_with_1_file = supp_file_distribution.get(1, 0)\n",
    "entries_with_2_5_files = supp_file_distribution[(supp_file_distribution.index >= 2) & (supp_file_distribution.index <= 5)].sum()\n",
    "entries_with_6plus_files = supp_file_distribution[supp_file_distribution.index > 5].sum()\n",
    "\n",
    "avg_supp_per_entry = (total_supp_files / entries_with_supp) if entries_with_supp > 0 else 0\n",
    "supp_success_rate = (entries_with_supp / entries_with_pmcid * 100) if entries_with_pmcid > 0 else 0\n",
    "\n",
    "print(f\"  Entries with supplementary files: {entries_with_supp} ({supp_success_rate:.1f}% of PMCIDs)\")\n",
    "print(f\"  Total supplementary files downloaded: {total_supp_files}\")\n",
    "print(f\"  Average files per entry (with supp): {avg_supp_per_entry:.2f}\")\n",
    "print(f\"  Entries with 1 file: {entries_with_1_file}\")\n",
    "print(f\"  Entries with 2-5 files: {entries_with_2_5_files}\")\n",
    "print(f\"  Entries with 6+ files: {entries_with_6plus_files}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: TITLE/ABSTRACT ENRICHMENT METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n5. Analyzing title/abstract availability...\")\n",
    "\n",
    "title_abstract_available = (df['article_title'].notna() & df['article_abstract'].notna()).sum()\n",
    "title_only = (df['article_title'].notna() & df['article_abstract'].isna()).sum()\n",
    "abstract_only = (df['article_title'].isna() & df['article_abstract'].notna()).sum()\n",
    "title_abstract_missing = total_entries - title_abstract_available - title_only - abstract_only\n",
    "\n",
    "enrichment_success_rate = (title_abstract_available / entries_with_pmcid * 100) if entries_with_pmcid > 0 else 0\n",
    "\n",
    "print(f\"  Complete title & abstract: {title_abstract_available} ({enrichment_success_rate:.1f}% of PMCIDs)\")\n",
    "print(f\"  Title only: {title_only}\")\n",
    "print(f\"  Abstract only: {abstract_only}\")\n",
    "print(f\"  Neither available: {title_abstract_missing}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: OVERALL SUCCESS METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n6. Calculating overall pipeline success...\")\n",
    "\n",
    "# Full data retrieval success (PDF + Supp + Title/Abstract)\n",
    "full_success = ((df['pdf_downloadable'] == 'yes') & \n",
    "                (df['supplementary_file_count'] > 0) & \n",
    "                (df['article_title'].notna()) & \n",
    "                (df['article_abstract'].notna())).sum()\n",
    "\n",
    "# Partial success (PDF + Title/Abstract, no supp required)\n",
    "partial_success = ((df['pdf_downloadable'] == 'yes') & \n",
    "                   (df['article_title'].notna()) & \n",
    "                   (df['article_abstract'].notna())).sum()\n",
    "\n",
    "# At least PDF\n",
    "pdf_only_success = (df['pdf_downloadable'] == 'yes').sum()\n",
    "\n",
    "print(f\"  Full data retrieval (PDF + Supp + Title/Abstract): {full_success} ({full_success/total_entries*100:.1f}%)\")\n",
    "print(f\"  PDF + Title/Abstract (no supp required): {partial_success} ({partial_success/total_entries*100:.1f}%)\")\n",
    "print(f\"  At least PDF available: {pdf_only_success} ({pdf_only_success/total_entries*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: CREATE COMPREHENSIVE METADATA TSV\n",
    "# ============================================================================\n",
    "print(\"\\n7. Creating metadata TSV file...\")\n",
    "\n",
    "metadata = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        # Registry\n",
    "        'Total Entries',\n",
    "        'Entries with DOI',\n",
    "        'Entries without DOI',\n",
    "        '',\n",
    "        # PMCID Availability\n",
    "        'Entries with PMCID',\n",
    "        'Entries with PMID Only',\n",
    "        '',\n",
    "        # PDF Retrieval\n",
    "        'PDF - Successfully Downloaded',\n",
    "        'PDF - Not Available',\n",
    "        'PDF - Success Rate (% of PMCIDs)',\n",
    "        '',\n",
    "        # Supplementary Files\n",
    "        'Supplementary - Entries with Files',\n",
    "        'Supplementary - Total Files Count',\n",
    "        'Supplementary - Avg Files per Entry',\n",
    "        'Supplementary - Success Rate (% of PMCIDs)',\n",
    "        'Supplementary - Entries with 1 File',\n",
    "        'Supplementary - Entries with 2-5 Files',\n",
    "        'Supplementary - Entries with 6+ Files',\n",
    "        '',\n",
    "        # Title/Abstract\n",
    "        'Title/Abstract - Complete',\n",
    "        'Title/Abstract - Title Only',\n",
    "        'Title/Abstract - Abstract Only',\n",
    "        'Title/Abstract - Neither',\n",
    "        'Title/Abstract - Success Rate (% of PMCIDs)',\n",
    "        '',\n",
    "        # Overall Success\n",
    "        'Overall - Full Retrieval (PDF+Supp+Title/Abstract)',\n",
    "        'Overall - Partial Retrieval (PDF+Title/Abstract)',\n",
    "        'Overall - Minimum Retrieval (PDF Only)'\n",
    "    ],\n",
    "    'Count': [\n",
    "        # Registry\n",
    "        total_entries,\n",
    "        entries_with_doi,\n",
    "        entries_without_doi,\n",
    "        '',\n",
    "        # PMCID Availability\n",
    "        entries_with_pmcid,\n",
    "        entries_with_pmid_only,\n",
    "        '',\n",
    "        # PDF Retrieval\n",
    "        pdf_available,\n",
    "        pdf_not_available,\n",
    "        f\"{pdf_success_rate:.1f}\",\n",
    "        '',\n",
    "        # Supplementary Files\n",
    "        entries_with_supp,\n",
    "        total_supp_files,\n",
    "        f\"{avg_supp_per_entry:.2f}\",\n",
    "        f\"{supp_success_rate:.1f}\",\n",
    "        entries_with_1_file,\n",
    "        entries_with_2_5_files,\n",
    "        entries_with_6plus_files,\n",
    "        '',\n",
    "        # Title/Abstract\n",
    "        title_abstract_available,\n",
    "        title_only,\n",
    "        abstract_only,\n",
    "        title_abstract_missing,\n",
    "        f\"{enrichment_success_rate:.1f}\",\n",
    "        '',\n",
    "        # Overall Success\n",
    "        full_success,\n",
    "        partial_success,\n",
    "        pdf_only_success\n",
    "    ]\n",
    "})\n",
    "\n",
    "metadata_tsv_path = os.path.join(tsv_output_folder, 'Positive_Metadata_Complete.tsv')\n",
    "metadata.to_csv(metadata_tsv_path, sep='\\t', index=False)\n",
    "print(f\"  ✓ Metadata TSV saved to '{metadata_tsv_path}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: CREATE DETAILED TEXT SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n8. Creating detailed text summary...\")\n",
    "\n",
    "summary_text_path = os.path.join(tsv_output_folder, 'Positive_Metadata_Summary.txt')\n",
    "with open(summary_text_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"POSITIVE ENTRIES DATA RETRIEVAL - COMPREHENSIVE SUMMARY\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. REGISTRY RETRIEVAL\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Total entries processed: {total_entries}\\n\")\n",
    "    f.write(f\"Entries with DOI: {entries_with_doi} ({entries_with_doi/total_entries*100:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"2. PMCID AVAILABILITY\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Entries with PMCID: {entries_with_pmcid} ({mapping_success_rate:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"3. FULL TEXT PDF RETRIEVAL\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"PDFs successfully downloaded: {pdf_available}\\n\")\n",
    "    f.write(f\"Success rate: {pdf_success_rate:.1f}% of entries with PMCIDs\\n\")\n",
    "    \n",
    "    f.write(\"4. SUPPLEMENTARY FILES RETRIEVAL\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Entries with supplementary files: {entries_with_supp} ({supp_success_rate:.1f}% of PMCIDs)\\n\")\n",
    "    f.write(f\"Total supplementary files downloaded: {total_supp_files}\\n\")\n",
    "    \n",
    "    f.write(\"5. TITLE & ABSTRACT AVAILABILITY\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Complete title & abstract: {title_abstract_available} ({enrichment_success_rate:.1f}% of PMCIDs)\\n\")\n",
    "    \n",
    "    f.write(\"6. OVERALL PIPELINE SUCCESS\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Full data retrieval (PDF + Supp + Title/Abstract): {full_success} ({full_success/total_entries*100:.1f}%)\\n\")\n",
    "    f.write(f\"Partial retrieval (PDF + Title/Abstract): {partial_success} ({partial_success/total_entries*100:.1f}%)\")\n",
    "    f.write(f\"Minimum retrieval (PDF only): {pdf_only_success} ({pdf_only_success/total_entries*100:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"7. DATA LOCATIONS\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Main TSV file: {output_tsv_file_name}\\n\")\n",
    "    f.write(f\"Full text PDFs: {pdf_output_folder}/\\n\")\n",
    "    f.write(f\"Supplementary files: {supp_output_folder}/\\n\")\n",
    "    f.write(f\"Metadata files: {tsv_output_folder}/\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"END OF SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"  ✓ Summary text saved to '{summary_text_path}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: CREATE COMPREHENSIVE VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n9. Creating comprehensive visualizations...\")\n",
    "\n",
    "# Create a larger figure with multiple subplots\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Overall Pipeline Success\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "pipeline_stages = ['Total\\nEntries', 'With\\nPMCID', 'PDF\\nDownloaded', 'Full\\nRetrieval']\n",
    "pipeline_counts = [total_entries, entries_with_pmcid, pdf_available, full_success]\n",
    "colors_pipeline = ['#1f77b4', '#ff7f0e', '#d62728', '#9467bd']\n",
    "bars1 = ax1.bar(pipeline_stages, pipeline_counts, color=colors_pipeline, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Pipeline Success Funnel', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: PMCID Availability (Simplified)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "mapping_labels = ['With\\nPMCID', 'No\\nPMCID']\n",
    "mapping_values = [entries_with_pmcid, total_entries - entries_with_pmcid]\n",
    "colors_mapping = ['#2ca02c', '#d62728']\n",
    "wedges2, texts2, autotexts2 = ax2.pie(mapping_values, labels=mapping_labels, autopct='%1.1f%%',\n",
    "                                        colors=colors_mapping, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax2.set_title('PMCID Availability', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: PDF Availability\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "pdf_labels = ['Available', 'Not Available']\n",
    "pdf_values = [pdf_available, pdf_not_available]\n",
    "colors_pdf = ['green', 'red']\n",
    "bars3 = ax3.bar(pdf_labels, pdf_values, color=colors_pdf, alpha=0.7, edgecolor='black')\n",
    "ax3.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Full Text PDF Availability', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Supplementary Files Distribution\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "supp_labels = ['1 File', '2-5 Files', '6+ Files', 'No Files']\n",
    "supp_values = [entries_with_1_file, entries_with_2_5_files, \n",
    "               entries_with_6plus_files, entries_without_supp]\n",
    "colors_supp = ['#8c564b', '#e377c2', '#7f7f7f', '#bcbd22']\n",
    "wedges4, texts4, autotexts4 = ax4.pie(supp_values, labels=supp_labels, autopct='%1.1f%%',\n",
    "                                        colors=colors_supp, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax4.set_title('Supplementary Files\\nDistribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 5: Title/Abstract Enrichment\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "enrichment_labels = ['Complete', 'Title Only', 'Abstract Only', 'Missing']\n",
    "enrichment_values = [title_abstract_available, title_only, abstract_only, title_abstract_missing]\n",
    "colors_enrich = ['#2ca02c', '#ff7f0e', '#1f77b4', '#d62728']\n",
    "bars5 = ax5.bar(enrichment_labels, enrichment_values, color=colors_enrich, alpha=0.7, edgecolor='black')\n",
    "ax5.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax5.set_title('Title/Abstract Availability', fontsize=14, fontweight='bold')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "for bar in bars5:\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 6: Overall Success Levels\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "success_labels = ['Full\\nRetrieval', 'Partial\\nRetrieval', 'PDF\\nOnly']\n",
    "success_values = [full_success, partial_success - full_success, pdf_only_success - partial_success]\n",
    "colors_success = ['#2ca02c', '#ff7f0e', '#d62728']\n",
    "bars6 = ax6.bar(success_labels, success_values, color=colors_success, alpha=0.7, edgecolor='black')\n",
    "ax6.set_ylabel('Number of Entries', fontsize=12, fontweight='bold')\n",
    "ax6.set_title('Overall Retrieval Success Levels', fontsize=14, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "for bar in bars6:\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add overall title\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "fig.suptitle(f'Positive Entries Data Retrieval - Comprehensive Analysis ({current_date})', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "\n",
    "# Save the comprehensive plot\n",
    "plot_image_path = os.path.join(tsv_output_folder, 'Positive_Metadata_Complete_Analysis.png')\n",
    "plt.savefig(plot_image_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"  ✓ Comprehensive charts saved to '{plot_image_path}'\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY PRINTOUT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POSITIVE ENTRIES DATA RETRIEVAL - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total entries: {total_entries}\")\n",
    "print(f\"PDF retrieval success: {pdf_success_rate:.1f}%\")\n",
    "print(f\"Supplementary files retrieved: {entries_with_supp} entries ({total_supp_files} files)\")\n",
    "print(f\"Title/Abstract availability: {enrichment_success_rate:.1f}%\")\n",
    "print(f\"\\nFull data retrieval: {full_success} entries ({full_success/total_entries*100:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAll metadata files saved to: {tsv_output_folder}/\")\n",
    "print(f\"  - {metadata_tsv_path}\")\n",
    "print(f\"  - {summary_text_path}\")\n",
    "print(f\"  - {plot_image_path}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
