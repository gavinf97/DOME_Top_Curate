{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902ce18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b97ebf81",
   "metadata": {},
   "source": [
    "# CSV Randomiser\n",
    "\n",
    "This notebook randomizes the order of entries in a CSV file while preserving the header row and original file.\n",
    "\n",
    "**Features:**\n",
    "- Reads CSV file and preserves header\n",
    "- Randomly shuffles data rows\n",
    "- Outputs to new file with 'random_' prefix\n",
    "- Maintains original file integrity\n",
    "- Progress reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdee500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d2992",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your input file path and configuration options here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704b60bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed (results will vary each run)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_FILE = 'Dataset/top20_entries_across_journals.csv'\n",
    "SEED = None  # Set to a number for reproducible randomization, None for truly random\n",
    "\n",
    "# Set random seed if specified\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    print(f\"Random seed set to: {SEED}\")\n",
    "else:\n",
    "    print(\"Using random seed (results will vary each run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e464f",
   "metadata": {},
   "source": [
    "## Randomization Function\n",
    "\n",
    "Main function that handles the CSV randomization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9507f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_csv(input_file, seed=None):\n",
    "    \"\"\"\n",
    "    Randomize the order of rows in a CSV file while preserving the header.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file\n",
    "        seed (int, optional): Random seed for reproducible results\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the output file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if input file exists\n",
    "        if not os.path.exists(input_file):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "        \n",
    "        print(f\"Reading CSV file: {input_file}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(input_file)\n",
    "        original_rows = len(df)\n",
    "        \n",
    "        print(f\"Original file contains {original_rows} rows (excluding header)\")\n",
    "        \n",
    "        # Create a copy and shuffle the rows\n",
    "        df_shuffled = df.copy()\n",
    "        df_shuffled = df_shuffled.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        \n",
    "        # Generate output filename\n",
    "        directory = os.path.dirname(input_file)\n",
    "        filename = os.path.basename(input_file)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        \n",
    "        if directory:\n",
    "            output_file = os.path.join(directory, f\"random_{name}{ext}\")\n",
    "        else:\n",
    "            output_file = f\"random_{name}{ext}\"\n",
    "        \n",
    "        # Save the shuffled data\n",
    "        df_shuffled.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"Randomized file saved as: {output_file}\")\n",
    "        print(f\"Successfully shuffled {original_rows} rows\")\n",
    "        \n",
    "        # Verify the output\n",
    "        df_verify = pd.read_csv(output_file)\n",
    "        if len(df_verify) == original_rows:\n",
    "            print(\"✓ Verification passed: Row count matches original\")\n",
    "        else:\n",
    "            print(\"⚠ Warning: Row count mismatch!\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c562d9",
   "metadata": {},
   "source": [
    "## Execute Randomization\n",
    "\n",
    "Run the randomization process on the specified CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df53aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CSV RANDOMIZER\n",
      "============================================================\n",
      "Timestamp: 2025-11-10 15:12:17\n",
      "\n",
      "Reading CSV file: Dataset/top20_entries_across_journals.csv\n",
      "Original file contains 104056 rows (excluding header)\n",
      "Randomized file saved as: Dataset/random_top20_entries_across_journals.csv\n",
      "Successfully shuffled 104056 rows\n",
      "✓ Verification passed: Row count matches original\n",
      "\n",
      "============================================================\n",
      "RANDOMIZATION COMPLETE\n",
      "============================================================\n",
      "Input file:  Dataset/top20_entries_across_journals.csv\n",
      "Output file: Dataset/random_top20_entries_across_journals.csv\n",
      "\n",
      "Original file has been preserved.\n"
     ]
    }
   ],
   "source": [
    "# Execute the randomization\n",
    "print(\"=\" * 60)\n",
    "print(\"CSV RANDOMIZER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "output_file = randomize_csv(INPUT_FILE, SEED)\n",
    "\n",
    "if output_file:\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RANDOMIZATION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input file:  {INPUT_FILE}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print()\n",
    "    print(\"Original file has been preserved.\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RANDOMIZATION FAILED\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd1fc9",
   "metadata": {},
   "source": [
    "## Verification and Statistics\n",
    "\n",
    "Optional verification and comparison between original and randomized files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02d83b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FILE COMPARISON\n",
      "----------------------------------------\n",
      "Original rows:    104056\n",
      "Randomized rows:  104056\n",
      "Columns match:    True\n",
      "Data preserved:   True\n",
      "✓ All data successfully preserved and randomized\n",
      "\n",
      "First 3 rows of original file:\n",
      "                             DOI        PMID        PMCID    Year  Citation_Count                                                                                                                                                                                           Title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Abstract                                     Journal                                                                                         Authors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          matches  match_count  total_weight  match_percentage                            Top20_In_Journal\n",
      "0  10.1097/corr.0000000000003442  40106382.0          NaN  2025.0               0                                                              Development of Machine Learning-based Algorithms to Predict the 2- and 5-year Risk of TKA After Tibial Plateau Fracture Treatment.  <h4>Background</h4>When faced with a severe intraarticular injury like a tibial plateau fracture, patients count on surgeons to make an accurate estimation of prognosis. Unfortunately, there are few tools available that enable precise, personalized prognosis estimation tailored to each patient's unique circumstances, including their individual and fracture-specific characteristics. In this study, we developed and validated a clinical prediction model using machine-learning algorithms for the 2- and 5-year risk of TKA after tibia plateau fractures.<h4>Questions/purposes</h4>Can machine learning-based probability calculators estimate the probability of 2- and 5-year risk of conversion to TKA in patients with a tibial plateau fracture?<h4>Methods</h4>A multicenter, cross-sectional study was performed in six hospitals in patients treated for a tibial plateau fracture between 2003 to 2019. In total, 2057 patients were eligible for inclusion and were sent informed consent and a questionnaire to inquire whether they underwent conversion to TKA. For 56% (1160 of 2057), status of conversion to TKA was accounted for at a minimum of 2 years, and 53% (1082 of 2057) were accounted for at a minimum of 5 years. The mean follow-up among responders was 6 ± 4 years after injury. An analysis of nonresponders found that responders were slightly older than nonresponders (53 ± 16 years versus 51 ± 17 years; p = 0.001), they were more often women (68% [788 of 1160] versus 58% [523 of 897]; p = 0.001), they were treated nonoperatively less often (30% [346 of 1160] versus 43% [387 of 897]; p = 0.001), and they had larger fracture gaps (6.4 ± 6.3 mm versus 4.2 ± 5.2 mm; p < 0.001) and step-offs (6.3 ± 5.7 mm versus 4.5 ± 4.7 mm; p < 0.001). AO Foundation/Orthopaedic Trauma Association (AO/OTA) fracture classification did not differ between nonresponders and responders (B1 11% versus 15%, B2 16% versus 19%, B3 45% versus 39%, C2 6% versus 8%, C3 22% versus 17%; p = 0.26). A total of 70% (814 of 1160) of patients were treated with open reduction and internal fixation, whereas 30% (346 of 1160) of patients were treated nonoperatively with a cast. Most fractures (80% [930 of 1160]) were AO/OTA type B fractures, and 20% (230 of 1160) were type C. Of these patients, 7% (79 of 1160) and 10% (109 of 1082) underwent conversion to a TKA at 2- and 5-year follow-up, respectively. Patient characteristics were retrieved from electronic patient records, and imaging data were shared with the initiating center from which fracture characteristics were determined. Obtained features derived from follow-up questionnaires, electronic patient records, and radiographic assessments were eligible for development of the prediction model. The first step consisted of data cleaning and included simple type formatting and standardization of numerical columns. Subsequent feature selection consisted of a review of the published evidence and expert opinion. This was followed by bivariate analysis of the identified features. The features for the models included: age, gender, BMI, AO/OTA fracture classification, fracture displacement (gap, step-off), medial proximal tibial alignment, and posterior proximal tibial alignment. The data set was used to train three models: logistic regression, random forest, and XGBoost. Logistic regression models linear relationships, random forest handles nonlinear complexities with decision trees, and XGBoost excels with sequential error correction and regularization. The models were tested using a sixfold validation approach by training the model on data from five (of six) respective medical centers and validating it against the remaining center that was left out for training. Performance was assessed by the area under the receiver operating characteristic curve (AUC), which measures a model's ability to distinguish between classes. AUC varies between 0 and 1, with values closer to 1 indicating better performance. To ensure robust and reliable results, we used bootstrapping as a resampling technique. In addition, calibration curves were plotted, and calibration was assessed with the calibration slope and intercept. The calibration plot compares the estimated probabilities with the observed probabilities for the primary outcome. Calibration slope evaluates alignment between predicted probabilities and observed outcomes (1 = perfect, < 1 = overfit, > 1 = underfit). Calibration intercept indicates bias (0 = perfect, negative = underestimation, positive = overestimation). Last, the Brier score, measuring the mean squared error of predicted probabilities (0 = perfect), was calculated.<h4>Results</h4>There were no differences among the models in terms of sensitivity and specificity; the AUCs for each overlapped broadly and ranged from 0.76 to 0.83. Calibration was most optimal in logistic regression for both 2- and 5-year models, with slopes of 0.82 (random forest 0.60, XGBoost 0.26) and 0.95 (random forest 0.85, XGBoost 0.48) and intercepts of 0.01 for both (random forest 0.01 to 0.02; XGBoost 0.05 to 0.07). Brier score was similar between models varying between 0.06 to 0.09. Given that its performance metrics were highest, we chose the logistic regression algorithm as the final prediction model. The web application providing the prediction tool is freely available and can be accessed through: https://3dtrauma.shinyapps.io/tka_prediction/.<h4>Conclusion</h4>In this study, a personalized risk assessment tool was developed to support clinical decision-making and patient counseling. Our findings demonstrate that machine-learning algorithms, particularly logistic regression, can provide accurate and reliable predictions of TKA conversion at 2 and 5 years after a tibial plateau fracture. In addition, it provides a useful prognostic tool for surgeons who perform fracture surgery that can be used quickly and easily with patients in the clinic or emergency department once it complies with medical device regulations. External validation is needed to assess performance in other institutions and countries; to account for patient and surgeon preferences, resources, and cultures; and to further strengthen its clinical applicability.<h4>Level of evidence</h4>Level III, therapeutic study.  Clinical orthopaedics and related research  Assink N, Gonzalez-Perrino MP, Santana-Trejo R, Doornberg JN, Hoekstra H, Kraeima J, IJpma FFA  [{'term': 'learning', 'column': 'Abstract', 'weight': 1}, {'term': 'data', 'column': 'Abstract', 'weight': 1}, {'term': 'prediction', 'column': 'Abstract', 'weight': 1}, {'term': 'machine', 'column': 'Abstract', 'weight': 1}, {'term': 'model', 'column': 'Abstract', 'weight': 1}, {'term': 'methods', 'column': 'Abstract', 'weight': 1}, {'term': 'models', 'column': 'Abstract', 'weight': 1}, {'term': 'method', 'column': 'Abstract', 'weight': 1}, {'term': 'features', 'column': 'Abstract', 'weight': 1}, {'term': 'analysis', 'column': 'Abstract', 'weight': 1}, {'term': 'classification', 'column': 'Abstract', 'weight': 1}, {'term': 'performance', 'column': 'Abstract', 'weight': 1}, {'term': 'results', 'column': 'Abstract', 'weight': 1}, {'term': 'predict', 'column': 'Abstract', 'weight': 1}, {'term': 'feature', 'column': 'Abstract', 'weight': 1}, {'term': 'high', 'column': 'Abstract', 'weight': 1}, {'term': 'identified', 'column': 'Abstract', 'weight': 1}, {'term': 'specific', 'column': 'Abstract', 'weight': 1}, {'term': 'validation', 'column': 'Abstract', 'weight': 1}, {'term': 'algorithm', 'column': 'Abstract', 'weight': 1}, {'term': 'predictions', 'column': 'Abstract', 'weight': 1}, {'term': 'training', 'column': 'Abstract', 'weight': 1}, {'term': 'algorithms', 'column': 'Abstract', 'weight': 1}, {'term': 'accurate', 'column': 'Abstract', 'weight': 1}, {'term': 'developed', 'column': 'Abstract', 'weight': 1}, {'term': 'tool', 'column': 'Abstract', 'weight': 1}, {'term': 'selection', 'column': 'Abstract', 'weight': 1}, {'term': 'development', 'column': 'Abstract', 'weight': 1}, {'term': 'specificity', 'column': 'Abstract', 'weight': 1}, {'term': 'cross', 'column': 'Abstract', 'weight': 1}, {'term': 'auc', 'column': 'Abstract', 'weight': 1}, {'term': 'tools', 'column': 'Abstract', 'weight': 1}, {'term': 'regression', 'column': 'Abstract', 'weight': 1}, {'term': 'graph', 'column': 'Abstract', 'weight': 1}, {'term': 'random', 'column': 'Abstract', 'weight': 1}, {'term': 'performed', 'column': 'Abstract', 'weight': 1}, {'term': 'negative', 'column': 'Abstract', 'weight': 1}, {'term': 'sensitivity', 'column': 'Abstract', 'weight': 1}, {'term': 'support', 'column': 'Abstract', 'weight': 1}, {'term': 'predicted', 'column': 'Abstract', 'weight': 1}, {'term': 'test', 'column': 'Abstract', 'weight': 1}, {'term': 'positive', 'column': 'Abstract', 'weight': 1}, {'term': 'value', 'column': 'Abstract', 'weight': 1}, {'term': 'forest', 'column': 'Abstract', 'weight': 1}, {'term': 'optimal', 'column': 'Abstract', 'weight': 1}, {'term': 'score', 'column': 'Abstract', 'weight': 1}, {'term': 'develop', 'column': 'Abstract', 'weight': 1}, {'term': 'values', 'column': 'Abstract', 'weight': 1}, {'term': 'range', 'column': 'Abstract', 'weight': 1}, {'term': 'validated', 'column': 'Abstract', 'weight': 1}, {'term': 'tested', 'column': 'Abstract', 'weight': 1}, {'term': 'train', 'column': 'Abstract', 'weight': 1}, {'term': 'robust', 'column': 'Abstract', 'weight': 1}, {'term': 'association', 'column': 'Abstract', 'weight': 1}, {'term': 'evaluate', 'column': 'Abstract', 'weight': 1}, {'term': 'metrics', 'column': 'Abstract', 'weight': 1}, {'term': 'total', 'column': 'Abstract', 'weight': 1}, {'term': 'reliable', 'column': 'Abstract', 'weight': 1}, {'term': 'rf', 'column': 'Abstract', 'weight': 1}, {'term': 'class', 'column': 'Abstract', 'weight': 1}, {'term': 'mean', 'column': 'Abstract', 'weight': 1}, {'term': 'linear', 'column': 'Abstract', 'weight': 1}, {'term': 'curve', 'column': 'Abstract', 'weight': 1}, {'term': 'receiver', 'column': 'Abstract', 'weight': 1}, {'term': 'technique', 'column': 'Abstract', 'weight': 1}, {'term': 'prognosis', 'column': 'Abstract', 'weight': 1}, {'term': 'resource', 'column': 'Abstract', 'weight': 1}, {'term': 'operating', 'column': 'Abstract', 'weight': 1}, {'term': 'observed', 'column': 'Abstract', 'weight': 1}, {'term': 'dl', 'column': 'Abstract', 'weight': 1}, {'term': 'learning', 'column': 'Title', 'weight': 1}, {'term': 'machine', 'column': 'Title', 'weight': 1}, {'term': 'predict', 'column': 'Title', 'weight': 1}, {'term': 'algorithm', 'column': 'Title', 'weight': 1}, {'term': 'algorithms', 'column': 'Title', 'weight': 1}, {'term': 'development', 'column': 'Title', 'weight': 1}, {'term': 'develop', 'column': 'Title', 'weight': 1}]           77            77         23.475610  Clinical orthopaedics and related research\n",
      "1  10.1097/corr.0000000000001251  32282466.0   PMC7310396  2020.0              18                                                                                  Can Predictive Modeling Tools Identify Patients at High Risk of Prolonged Opioid Use After ACL Reconstruction?                                                                                                                                                      <h4>Background</h4>Machine-learning methods such as the Bayesian belief network, random forest, gradient boosting machine, and decision trees have been used to develop decision-support tools in other clinical settings. Opioid abuse is a problem among civilians and military service members, and it is difficult to anticipate which patients are at risk for prolonged opioid use.<h4>Questions/purposes</h4>(1) To build a cross-validated model that predicts risk of prolonged opioid use after a specific orthopaedic procedure (ACL reconstruction), (2) To describe the relationships between prognostic and outcome variables, and (3) To determine the clinical utility of a predictive model using a decision curve analysis (as measured by our predictive system's ability to effectively identify high-risk patients and allow for preventative measures to be taken to ensure a successful procedure process).<h4>Methods</h4>We used the Military Analysis and Reporting Tool (M2) to search the Military Health System Data Repository for all patients undergoing arthroscopically assisted ACL reconstruction (Current Procedure Terminology code 29888) from January 2012 through December 2015 with a minimum of 90 days postoperative follow-up. In total, 10,919 patients met the inclusion criteria, most of whom were young men on active duty. We obtained complete opioid prescription filling histories from the Military Health System Data Repository's pharmacy records. We extracted data including patient demographics, military characteristics, and pharmacy data. A total of 3.3% of the data was missing. To curate and impute all missing variables, we used a random forest algorithm. We shuffled and split the data into 80% training and 20% hold-out sets, balanced by outcome variable (Outcome90Days). Next, the training set was further split into training and validation sets. Each model was built on the training data set, tuned with the validation set as applicable, and finally tested on the separate hold-out dataset. We chose four predictive models to develop, at the end choosing the best-fit model for implementation. Logistic regression, random forest, Bayesian belief network, and gradient boosting machine models were the four chosen models based on type of analysis (classification). Each were trained to estimate the likelihood of prolonged opioid use, defined as any opioid prescription filled more than 90 days after anterior cruciate reconstruction. After this, we tested the models on our holdout set and performed an area under the curve analysis concordance statistic, calculated the Brier score, and performed a decision curve analysis for validation. Then, we chose the method that produced the most suitable analysis results and, consequently, predictive power across the three calculations. Based on the calculations, the gradient boosting machine model was selected for future implementation. We systematically selected features and tuned the gradient boosting machine to produce a working predictive model. We performed area under the curve, Brier, and decision curve analysis calculations for the final model to test its viability and gain an understanding of whether it is possible to predict prolonged opioid use.<h4>Results</h4>Four predictive models were successfully developed using gradient boosting machine, logistic regression, Bayesian belief network, and random forest methods. After applying the Boruta algorithm for feature selection based on a 100-tree random forest algorithm, features were narrowed to a final seven features. The most influential features with a positive association with prolonged opioid use are preoperative morphine equivalents (yes), particular pharmacy ordering sites locations, shorter deployment time, and younger age. Those observed to have a negative association with prolonged opioid use are particular pharmacy ordering sites locations, preoperative morphine equivalents (no), longer deployment, race (American Indian or Alaskan native) and rank (junior enlisted).On internal validation, the models showed accuracy for predicting prolonged opioid use with AUC greater than our benchmark cutoff 0.70; random forest were 0.76 (95% confidence interval 0.73 to 0.79), 0.76 (95% CI 0.73 to 0.78), 0.73 (95% CI 0.71 to 0.76), and 0.72 (95% CI 0.69 to 0.75), respectively. Although the results from logistic regression and gradient boosting machines were very similar, only one model can be used in implementation. Based on our calculation of the Brier score, area under the curve, and decision curve analysis, we chose the gradient boosting machine as the final model. After selecting features and tuning the chosen gradient boosting machine, we saw an incremental improvement in our implementation model; the final model is accurate, with a Brier score of 0.10 (95% CI 0.09 to 0.11) and area under the curve of 0.77 (95% CI 0.75 to 0.80). It also shows the best clinical utility in a decision curve analysis.<h4>Conclusions</h4>These scores support our claim that it is possible to predict which patients are at risk of prolonged opioid use, as seen by the appropriate range of hold-out analysis calculations. Current opioid guidelines recommend preoperative identification of at-risk patients, but available tools for this purpose are crude, largely focusing on identifying the presence (but not relative contributions) of various risk factors and screening for depression. The power of this model is that it will permit the development of a true clinical decision-support tool, which risk-stratifies individual patients with a single numerical score that is easily understandable to both patient and surgeon. Probabilistic models provide insight into how clinical factors are conditionally related. Not only will this gradient boosting machine be used to help understand factors contributing to opiate misuse after ACL reconstruction, but also it will allow orthopaedic surgeons to identify at-risk patients before surgery and offer increased support and monitoring to prevent opioid abuse and dependency.<h4>Level of evidence</h4>Level III, therapeutic study.  Clinical orthopaedics and related research                           Anderson AB, Grazal CF, Balazs GC, Potter BK, Dickens JF, Forsberg JA                                                                       [{'term': 'learning', 'column': 'Abstract', 'weight': 1}, {'term': 'data', 'column': 'Abstract', 'weight': 1}, {'term': 'machine', 'column': 'Abstract', 'weight': 1}, {'term': 'model', 'column': 'Abstract', 'weight': 1}, {'term': 'methods', 'column': 'Abstract', 'weight': 1}, {'term': 'models', 'column': 'Abstract', 'weight': 1}, {'term': 'method', 'column': 'Abstract', 'weight': 1}, {'term': 'features', 'column': 'Abstract', 'weight': 1}, {'term': 'analysis', 'column': 'Abstract', 'weight': 1}, {'term': 'classification', 'column': 'Abstract', 'weight': 1}, {'term': 'network', 'column': 'Abstract', 'weight': 1}, {'term': 'results', 'column': 'Abstract', 'weight': 1}, {'term': 'accuracy', 'column': 'Abstract', 'weight': 1}, {'term': 'predict', 'column': 'Abstract', 'weight': 1}, {'term': 'feature', 'column': 'Abstract', 'weight': 1}, {'term': 'high', 'column': 'Abstract', 'weight': 1}, {'term': 'identify', 'column': 'Abstract', 'weight': 1}, {'term': 'identification', 'column': 'Abstract', 'weight': 1}, {'term': 'predicting', 'column': 'Abstract', 'weight': 1}, {'term': 'specific', 'column': 'Abstract', 'weight': 1}, {'term': 'validation', 'column': 'Abstract', 'weight': 1}, {'term': 'algorithm', 'column': 'Abstract', 'weight': 1}, {'term': 'training', 'column': 'Abstract', 'weight': 1}, {'term': 'accurate', 'column': 'Abstract', 'weight': 1}, {'term': 'developed', 'column': 'Abstract', 'weight': 1}, {'term': 'tool', 'column': 'Abstract', 'weight': 1}, {'term': 'selection', 'column': 'Abstract', 'weight': 1}, {'term': 'development', 'column': 'Abstract', 'weight': 1}, {'term': 'cross', 'column': 'Abstract', 'weight': 1}, {'term': 'auc', 'column': 'Abstract', 'weight': 1}, {'term': 'tools', 'column': 'Abstract', 'weight': 1}, {'term': 'trained', 'column': 'Abstract', 'weight': 1}, {'term': 'dataset', 'column': 'Abstract', 'weight': 1}, {'term': 'regression', 'column': 'Abstract', 'weight': 1}, {'term': 'identifying', 'column': 'Abstract', 'weight': 1}, {'term': 'predictive', 'column': 'Abstract', 'weight': 1}, {'term': 'graph', 'column': 'Abstract', 'weight': 1}, {'term': 'random', 'column': 'Abstract', 'weight': 1}, {'term': 'performed', 'column': 'Abstract', 'weight': 1}, {'term': 'negative', 'column': 'Abstract', 'weight': 1}, {'term': 'best', 'column': 'Abstract', 'weight': 1}, {'term': 'support', 'column': 'Abstract', 'weight': 1}, {'term': 'improve', 'column': 'Abstract', 'weight': 1}, {'term': 'sets', 'column': 'Abstract', 'weight': 1}, {'term': 'effective', 'column': 'Abstract', 'weight': 1}, {'term': 'test', 'column': 'Abstract', 'weight': 1}, {'term': 'positive', 'column': 'Abstract', 'weight': 1}, {'term': 'forest', 'column': 'Abstract', 'weight': 1}, {'term': 'score', 'column': 'Abstract', 'weight': 1}, {'term': 'develop', 'column': 'Abstract', 'weight': 1}, {'term': 'selected', 'column': 'Abstract', 'weight': 1}, {'term': 'range', 'column': 'Abstract', 'weight': 1}, {'term': 'validated', 'column': 'Abstract', 'weight': 1}, {'term': 'tested', 'column': 'Abstract', 'weight': 1}, {'term': 'effectively', 'column': 'Abstract', 'weight': 1}, {'term': 'train', 'column': 'Abstract', 'weight': 1}, {'term': 'association', 'column': 'Abstract', 'weight': 1}, {'term': 'measured', 'column': 'Abstract', 'weight': 1}, {'term': 'total', 'column': 'Abstract', 'weight': 1}, {'term': 'code', 'column': 'Abstract', 'weight': 1}, {'term': 'rf', 'column': 'Abstract', 'weight': 1}, {'term': 'extract', 'column': 'Abstract', 'weight': 1}, {'term': 'class', 'column': 'Abstract', 'weight': 1}, {'term': 'scores', 'column': 'Abstract', 'weight': 1}, {'term': 'successfully', 'column': 'Abstract', 'weight': 1}, {'term': 'implementation', 'column': 'Abstract', 'weight': 1}, {'term': 'curve', 'column': 'Abstract', 'weight': 1}, {'term': 'observed', 'column': 'Abstract', 'weight': 1}, {'term': 'model', 'column': 'Title', 'weight': 1}, {'term': 'predict', 'column': 'Title', 'weight': 1}, {'term': 'high', 'column': 'Title', 'weight': 1}, {'term': 'identify', 'column': 'Title', 'weight': 1}, {'term': 'tool', 'column': 'Title', 'weight': 1}, {'term': 'tools', 'column': 'Title', 'weight': 1}, {'term': 'predictive', 'column': 'Title', 'weight': 1}, {'term': 'modeling', 'column': 'Title', 'weight': 1}]           76            76         23.170732  Clinical orthopaedics and related research\n",
      "2  10.1097/corr.0000000000002771  37615504.0  PMC10566917  2023.0               2  Development and Validation of a Convolutional Neural Network Model to Predict a Pathologic Fracture in the Proximal Femur Using Abdomen and Pelvis CT Images of Patients With Advanced Cancer.                                                                                                                                          <h4>Background</h4>Improvement in survival in patients with advanced cancer is accompanied by an increased probability of bone metastasis and related pathologic fractures (especially in the proximal femur). The few systems proposed and used to diagnose impending fractures owing to metastasis and to ultimately prevent future fractures have practical limitations; thus, novel screening tools are essential. A CT scan of the abdomen and pelvis is a standard modality for staging and follow-up in patients with cancer, and radiologic assessments of the proximal femur are possible with CT-based digitally reconstructed radiographs. Deep-learning models, such as convolutional neural networks (CNNs), may be able to predict pathologic fractures from digitally reconstructed radiographs, but to our knowledge, they have not been tested for this application.<h4>Questions/purposes</h4>(1) How accurate is a CNN model for predicting a pathologic fracture in a proximal femur with metastasis using digitally reconstructed radiographs of the abdomen and pelvis CT images in patients with advanced cancer? (2) Do CNN models perform better than clinicians with varying backgrounds and experience levels in predicting a pathologic fracture on abdomen and pelvis CT images without any knowledge of the patients' histories, except for metastasis in the proximal femur?<h4>Methods</h4>A total of 392 patients received radiation treatment of the proximal femur at three hospitals from January 2011 to December 2021. The patients had 2945 CT scans of the abdomen and pelvis for systemic evaluation and follow-up in relation to their primary cancer. In 33% of the CT scans (974), it was impossible to identify whether a pathologic fracture developed within 3 months after each CT image was acquired, and these were excluded. Finally, 1971 cases with a mean age of 59 ± 12 years were included in this study. Pathologic fractures developed within 3 months after CT in 3% (60 of 1971) of cases. A total of 47% (936 of 1971) were women. Sixty cases had an established pathologic fracture within 3 months after each CT scan, and another group of 1911 cases had no established pathologic fracture within 3 months after CT scan. The mean age of the cases in the former and latter groups was 64 ± 11 years and 59 ± 12 years, respectively, and 32% (19 of 60) and 53% (1016 of 1911) of cases, respectively, were female. Digitally reconstructed radiographs were generated with perspective projections of three-dimensional CT volumes onto two-dimensional planes. Then, 1557 images from one hospital were used for a training set. To verify that the deep-learning models could consistently operate even in hospitals with a different medical environment, 414 images from other hospitals were used for external validation. The number of images in the groups with and without a pathologic fracture within 3 months after each CT scan increased from 1911 to 22,932 and from 60 to 720, respectively, using data augmentation methods that are known to be an effective way to boost the performance of deep-learning models. Three CNNs (VGG16, ResNet50, and DenseNet121) were fine-tuned using digitally reconstructed radiographs. For performance measures, the area under the receiver operating characteristic curve, accuracy, sensitivity, specificity, precision, and F1 score were determined. The area under the receiver operating characteristic curve was used to evaluate three CNN models mainly, and the optimal accuracy, sensitivity, and specificity were calculated using the Youden J statistic. Accuracy refers to the proportion of fractures in the groups with and without a pathologic fracture within 3 months after each CT scan that were accurately predicted by the CNN model. Sensitivity and specificity represent the proportion of accurately predicted fractures among those with and without a pathologic fracture within 3 months after each CT scan, respectively. Precision is a measure of how few false-positives the model produces. The F1 score is a harmonic mean of sensitivity and precision, which have a tradeoff relationship. Gradient-weighted class activation mapping images were created to check whether the CNN model correctly focused on potential pathologic fracture regions. The CNN model with the best performance was compared with the performance of clinicians.<h4>Results</h4>DenseNet121 showed the best performance in identifying pathologic fractures; the area under the receiver operating characteristic curve for DenseNet121 was larger than those for VGG16 (0.77 ± 0.07 [95% CI 0.75 to 0.79] versus 0.71 ± 0.08 [95% CI 0.69 to 0.73]; p = 0.001) and ResNet50 (0.77 ± 0.07 [95% CI 0.75 to 0.79] versus 0.72 ± 0.09 [95% CI 0.69 to 0.74]; p = 0.001). Specifically, DenseNet121 scored the highest in sensitivity (0.22 ± 0.07 [95% CI 0.20 to 0.24]), precision (0.72 ± 0.19 [95% CI 0.67 to 0.77]), and F1 score (0.34 ± 0.10 [95% CI 0.31 to 0.37]), and it focused accurately on the region with the expected pathologic fracture. Further, DenseNet121 was less likely than clinicians to mispredict cases in which there was no pathologic fracture than cases in which there was a fracture; the performance of DenseNet121 was better than clinician performance in terms of specificity (0.98 ± 0.01 [95% CI 0.98 to 0.99] versus 0.86 ± 0.09 [95% CI 0.81 to 0.91]; p = 0.01), precision (0.72 ± 0.19 [95% CI 0.67 to 0.77] versus 0.11 ± 0.10 [95% CI 0.05 to 0.17]; p = 0.0001), and F1 score (0.34 ± 0.10 [95% CI 0.31 to 0.37] versus 0.17 ± 0.15 [95% CI 0.08 to 0.26]; p = 0.0001).<h4>Conclusion</h4>CNN models may be able to accurately predict impending pathologic fractures from digitally reconstructed radiographs of the abdomen and pelvis CT images that clinicians may not anticipate; this can assist medical, radiation, and orthopaedic oncologists clinically. To achieve better performance, ensemble-learning models using knowledge of the patients' histories should be developed and validated. The code for our model is publicly available online at https://github.com/taehoonko/CNN_path_fx_prediction .<h4>Level of evidence</h4>Level III, diagnostic study.  Clinical orthopaedics and related research                                         Joo MW, Ko T, Kim MS, Lee YS, Shin SH, Chung YG, Lee HK                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'term': 'learning', 'column': 'Abstract', 'weight': 1}, {'term': 'data', 'column': 'Abstract', 'weight': 1}, {'term': 'prediction', 'column': 'Abstract', 'weight': 1}, {'term': 'model', 'column': 'Abstract', 'weight': 1}, {'term': 'methods', 'column': 'Abstract', 'weight': 1}, {'term': 'models', 'column': 'Abstract', 'weight': 1}, {'term': 'method', 'column': 'Abstract', 'weight': 1}, {'term': 'performance', 'column': 'Abstract', 'weight': 1}, {'term': 'network', 'column': 'Abstract', 'weight': 1}, {'term': 'results', 'column': 'Abstract', 'weight': 1}, {'term': 'accuracy', 'column': 'Abstract', 'weight': 1}, {'term': 'predict', 'column': 'Abstract', 'weight': 1}, {'term': 'neural', 'column': 'Abstract', 'weight': 1}, {'term': 'deep', 'column': 'Abstract', 'weight': 1}, {'term': 'high', 'column': 'Abstract', 'weight': 1}, {'term': 'identify', 'column': 'Abstract', 'weight': 1}, {'term': 'networks', 'column': 'Abstract', 'weight': 1}, {'term': 'predicting', 'column': 'Abstract', 'weight': 1}, {'term': 'specific', 'column': 'Abstract', 'weight': 1}, {'term': 'validation', 'column': 'Abstract', 'weight': 1}, {'term': 'training', 'column': 'Abstract', 'weight': 1}, {'term': 'accurate', 'column': 'Abstract', 'weight': 1}, {'term': 'developed', 'column': 'Abstract', 'weight': 1}, {'term': 'tool', 'column': 'Abstract', 'weight': 1}, {'term': 'specificity', 'column': 'Abstract', 'weight': 1}, {'term': 'compared', 'column': 'Abstract', 'weight': 1}, {'term': 'tools', 'column': 'Abstract', 'weight': 1}, {'term': 'identifying', 'column': 'Abstract', 'weight': 1}, {'term': 'graph', 'column': 'Abstract', 'weight': 1}, {'term': 'best', 'column': 'Abstract', 'weight': 1}, {'term': 'sensitivity', 'column': 'Abstract', 'weight': 1}, {'term': 'evaluation', 'column': 'Abstract', 'weight': 1}, {'term': 'generated', 'column': 'Abstract', 'weight': 1}, {'term': 'improve', 'column': 'Abstract', 'weight': 1}, {'term': 'predicted', 'column': 'Abstract', 'weight': 1}, {'term': 'effective', 'column': 'Abstract', 'weight': 1}, {'term': 'ensemble', 'column': 'Abstract', 'weight': 1}, {'term': 'test', 'column': 'Abstract', 'weight': 1}, {'term': 'positive', 'column': 'Abstract', 'weight': 1}, {'term': 'optimal', 'column': 'Abstract', 'weight': 1}, {'term': 'score', 'column': 'Abstract', 'weight': 1}, {'term': 'develop', 'column': 'Abstract', 'weight': 1}, {'term': 'precision', 'column': 'Abstract', 'weight': 1}, {'term': 'github', 'column': 'Abstract', 'weight': 1}, {'term': 'diagnostic', 'column': 'Abstract', 'weight': 1}, {'term': 'validated', 'column': 'Abstract', 'weight': 1}, {'term': 'tested', 'column': 'Abstract', 'weight': 1}, {'term': 'accurately', 'column': 'Abstract', 'weight': 1}, {'term': 'train', 'column': 'Abstract', 'weight': 1}, {'term': 'evaluate', 'column': 'Abstract', 'weight': 1}, {'term': 'total', 'column': 'Abstract', 'weight': 1}, {'term': 'code', 'column': 'Abstract', 'weight': 1}, {'term': 'rf', 'column': 'Abstract', 'weight': 1}, {'term': 'dimensional', 'column': 'Abstract', 'weight': 1}, {'term': 'class', 'column': 'Abstract', 'weight': 1}, {'term': 'mean', 'column': 'Abstract', 'weight': 1}, {'term': 'curve', 'column': 'Abstract', 'weight': 1}, {'term': 'receiver', 'column': 'Abstract', 'weight': 1}, {'term': 'convolutional', 'column': 'Abstract', 'weight': 1}, {'term': 'operating', 'column': 'Abstract', 'weight': 1}, {'term': 'model', 'column': 'Title', 'weight': 1}, {'term': 'network', 'column': 'Title', 'weight': 1}, {'term': 'predict', 'column': 'Title', 'weight': 1}, {'term': 'neural', 'column': 'Title', 'weight': 1}, {'term': 'validation', 'column': 'Title', 'weight': 1}, {'term': 'development', 'column': 'Title', 'weight': 1}, {'term': 'develop', 'column': 'Title', 'weight': 1}, {'term': 'convolutional', 'column': 'Title', 'weight': 1}]           68            68         20.731707  Clinical orthopaedics and related research\n",
      "\n",
      "First 3 rows of randomized file:\n",
      "                            DOI        PMID       PMCID    Year  Citation_Count                                                                                                                                                          Title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Abstract                                                                                                                                                                 Journal                                                                                                           Authors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   matches  match_count  total_weight  match_percentage                                                                                                                                                        Top20_In_Journal\n",
      "0           10.1155/2011/578903  22084690.0  PMC3200215  2011.0               9  Proteomic Characterization of Cerebrospinal Fluid from Ataxia-Telangiectasia (A-T) Patients Using a LC/MS-Based Label-Free Protein Quantification Technology.                                                                                                                                                                                                                                                                                                                                                                                                                                                            Cerebrospinal fluid (CSF) has been used for biomarker discovery of neurodegenerative diseases in humans since biological changes in the brain can be seen in this biofluid. Inactivation of A-T-mutated protein (ATM), a multifunctional protein kinase, is responsible for A-T, yet biochemical studies have not succeeded in conclusively identifying the molecular mechanism(s) underlying the neurodegeneration seen in A-T patients or the proteins that can be used as biomarkers for neurologic assessment of A-T or as potential therapeutic targets. In this study, we applied a high-throughput LC/MS-based label-free protein quantification technology to quantitatively characterize the proteins in CSF samples in order to identify differentially expressed proteins that can serve as potential biomarker candidates for A-T. Among 204 identified CSF proteins with high peptide-identification confidence, thirteen showed significant protein expression changes. Bioinformatic analysis revealed that these 13 proteins are either involved in neurodegenerative disorders or cancer. Future molecular and functional characterization of these proteins would provide more insights into the potential therapeutic targets for the treatment of A-T and the biomarkers that can be used to monitor or predict A-T disease progression. Clinical validation studies are required before any of these proteins can be developed into clinically useful biomarkers.                                                                                                                                     International journal of proteomics  Dzieciatkowska M, Qi G, You J, Bemis KG, Sahm H, Lederman HM, Crawford TO, Gelbert LM, Rothblum-Oviatt C, Wang M                                            [{'term': 'analysis', 'column': 'Abstract', 'weight': 1}, {'term': 'predict', 'column': 'Abstract', 'weight': 1}, {'term': 'high', 'column': 'Abstract', 'weight': 1}, {'term': 'function', 'column': 'Abstract', 'weight': 1}, {'term': 'identify', 'column': 'Abstract', 'weight': 1}, {'term': 'identified', 'column': 'Abstract', 'weight': 1}, {'term': 'identification', 'column': 'Abstract', 'weight': 1}, {'term': 'validation', 'column': 'Abstract', 'weight': 1}, {'term': 'developed', 'column': 'Abstract', 'weight': 1}, {'term': 'identifying', 'column': 'Abstract', 'weight': 1}, {'term': 'applied', 'column': 'Abstract', 'weight': 1}, {'term': 'develop', 'column': 'Abstract', 'weight': 1}, {'term': 'throughput', 'column': 'Abstract', 'weight': 1}]           13            13          3.963415                                                                                                                                     International journal of proteomics\n",
      "1   10.1007/978-3-319-46840-2_6  28042620.0  PMC5198779  2016.0               1                                                                                                 Summarizing Simulation Results using Causally-relevant States.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     As increasingly large-scale multiagent simulations are being implemented, new methods are becoming necessary to make sense of the results of these simulations. Even concisely summarizing the results of a given simulation run is a challenge. Here we pose this as the problem of simulation summarization: how to extract the causally-relevant descriptions of the trajectories of the agents in the simulation. We present a simple algorithm to compress agent trajectories through state space by identifying the state transitions which are relevant to determining the distribution of outcomes at the end of the simulation. We present a toy-example to illustrate the working of the algorithm, and then apply it to a complex simulation of a major disaster in an urban area.  Multi-agent-based simulation ... : International Workshop, MABS ... : revised and invited papers. International Symposium on Military Applications of Blast Simulation                                                                                     Parikh N, Marathe M, Swarup S                                                                                                                                                                                                                                                                                                                                                       [{'term': 'methods', 'column': 'Abstract', 'weight': 1}, {'term': 'method', 'column': 'Abstract', 'weight': 1}, {'term': 'results', 'column': 'Abstract', 'weight': 1}, {'term': 'identify', 'column': 'Abstract', 'weight': 1}, {'term': 'algorithm', 'column': 'Abstract', 'weight': 1}, {'term': 'identifying', 'column': 'Abstract', 'weight': 1}, {'term': 'extract', 'column': 'Abstract', 'weight': 1}, {'term': 'results', 'column': 'Title', 'weight': 1}]            8             8          2.439024  Multi-agent-based simulation ... : International Workshop, MABS ... : revised and invited papers. International Symposium on Military Applications of Blast Simulation\n",
      "2  10.1097/ee9.0000000000000056  31538137.0  PMC6693982  2019.0              18                                        Environmental noise and sleep and mental health outcomes in a nationally representative sample of urban US adolescents.  <h4>Background</h4>Environmental noise has been linked to negative health outcomes, like poor sleep, poor mental health, and cardiovascular disease, and likely accounts for more than 1 million disability-adjusted life years annually in Western Europe. Adolescence may be a particularly sensitive period for noise exposure due to an increased need for sleep, failure to meet sleep guidelines, and increased risk for first onset of some mental health disorders. However, the potential health effects of living in high-noise environments have not been studied in US adolescents, rarely in European adolescents, and mental health outcomes studied have not corresponded to diagnoses from the Diagnostic and Statistical Manual of Mental Disorders (DSM).<h4>Methods</h4>Using a US-based nationally representative survey of urban adolescents (N = 4,508), we estimated associations of day-night average sound levels exceeding the US Environmental Protection Agency's 55 decibel limit with sleep outcomes and lifetime mental health DSM diagnoses. We implemented doubly robust targeted minimum loss-based estimation coupled with propensity score matching to account for numerous potential adolescent, household, and environmental confounders.<h4>Results</h4>Living in a high- versus low-noise Census block group was associated with later bedtimes on weeknights (0.48 hours, 95% confidence interval [CI] = -0.15, 1.12) and weekend nights (0.65 hours, 95% CI = 0.37, 0.93), but not with total hours slept. Associations between living in a high- versus low-noise Census block group and mental disorders were mixed, with wide CIs, and not robust to sensitivity analyses.<h4>Conclusions</h4>We find evidence for an association between residence in a high-noise area and later bedtimes among urban adolescents but no consistent evidence of such an association with mental health disorders.                                                                                                                          Environmental epidemiology (Philadelphia, Pa.)                                     Rudolph KE, Shev A, Paksarian D, Merikangas KR, Mennitt DJ, James P, Casey JA  [{'term': 'methods', 'column': 'Abstract', 'weight': 1}, {'term': 'method', 'column': 'Abstract', 'weight': 1}, {'term': 'results', 'column': 'Abstract', 'weight': 1}, {'term': 'high', 'column': 'Abstract', 'weight': 1}, {'term': 'negative', 'column': 'Abstract', 'weight': 1}, {'term': 'sensitivity', 'column': 'Abstract', 'weight': 1}, {'term': 'score', 'column': 'Abstract', 'weight': 1}, {'term': 'statistical', 'column': 'Abstract', 'weight': 1}, {'term': 'average', 'column': 'Abstract', 'weight': 1}, {'term': 'diagnostic', 'column': 'Abstract', 'weight': 1}, {'term': 'robust', 'column': 'Abstract', 'weight': 1}, {'term': 'association', 'column': 'Abstract', 'weight': 1}, {'term': 'total', 'column': 'Abstract', 'weight': 1}, {'term': 'analyses', 'column': 'Abstract', 'weight': 1}]           14            14          4.268293                                                                                                                          Environmental epidemiology (Philadelphia, Pa.)\n"
     ]
    }
   ],
   "source": [
    "# Verification function\n",
    "def compare_files(original_file, randomized_file):\n",
    "    \"\"\"\n",
    "    Compare original and randomized files to ensure data integrity.\n",
    "    \n",
    "    Args:\n",
    "        original_file (str): Path to original CSV file\n",
    "        randomized_file (str): Path to randomized CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read both files\n",
    "        df_original = pd.read_csv(original_file)\n",
    "        df_randomized = pd.read_csv(randomized_file)\n",
    "        \n",
    "        print(\"FILE COMPARISON\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Original rows:    {len(df_original)}\")\n",
    "        print(f\"Randomized rows:  {len(df_randomized)}\")\n",
    "        print(f\"Columns match:    {list(df_original.columns) == list(df_randomized.columns)}\")\n",
    "        \n",
    "        # Check if all data is preserved (just reordered)\n",
    "        original_sorted = df_original.sort_values(by=df_original.columns.tolist()).reset_index(drop=True)\n",
    "        randomized_sorted = df_randomized.sort_values(by=df_randomized.columns.tolist()).reset_index(drop=True)\n",
    "        \n",
    "        data_preserved = original_sorted.equals(randomized_sorted)\n",
    "        print(f\"Data preserved:   {data_preserved}\")\n",
    "        \n",
    "        if data_preserved:\n",
    "            print(\"✓ All data successfully preserved and randomized\")\n",
    "        else:\n",
    "            print(\"⚠ Warning: Data may have been lost or corrupted\")\n",
    "            \n",
    "        # Show first few rows of each file for visual comparison\n",
    "        print(f\"\\nFirst 3 rows of original file:\")\n",
    "        print(df_original.head(3).to_string())\n",
    "        \n",
    "        print(f\"\\nFirst 3 rows of randomized file:\")\n",
    "        print(df_randomized.head(3).to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Comparison failed: {str(e)}\")\n",
    "\n",
    "# Run comparison if randomization was successful\n",
    "if output_file and os.path.exists(output_file):\n",
    "    print()\n",
    "    compare_files(INPUT_FILE, output_file)\n",
    "else:\n",
    "    print(\"Cannot perform comparison - randomization was not successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b80979",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Additional utility functions for file management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ad899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def list_random_files(directory=\"Dataset\"):\n",
    "    \"\"\"List all files with 'random_' prefix in the specified directory.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Directory not found: {directory}\")\n",
    "            return\n",
    "        \n",
    "        random_files = [f for f in os.listdir(directory) if f.startswith('random_')]\n",
    "        \n",
    "        if random_files:\n",
    "            print(f\"Random files found in {directory}:\")\n",
    "            for i, file in enumerate(random_files, 1):\n",
    "                file_path = os.path.join(directory, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                print(f\"  {i}. {file} ({size} bytes, modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "        else:\n",
    "            print(f\"No random files found in {directory}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {str(e)}\")\n",
    "\n",
    "def clean_random_files(directory=\"Dataset\", confirm=True):\n",
    "    \"\"\"Remove all files with 'random_' prefix (use with caution!).\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Directory not found: {directory}\")\n",
    "            return\n",
    "        \n",
    "        random_files = [f for f in os.listdir(directory) if f.startswith('random_')]\n",
    "        \n",
    "        if not random_files:\n",
    "            print(f\"No random files found in {directory}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(random_files)} random files:\")\n",
    "        for file in random_files:\n",
    "            print(f\"  - {file}\")\n",
    "        \n",
    "        if confirm:\n",
    "            response = input(\"\\nAre you sure you want to delete these files? (yes/no): \")\n",
    "            if response.lower() != 'yes':\n",
    "                print(\"Operation cancelled.\")\n",
    "                return\n",
    "        \n",
    "        deleted_count = 0\n",
    "        for file in random_files:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            os.remove(file_path)\n",
    "            deleted_count += 1\n",
    "            print(f\"Deleted: {file}\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully deleted {deleted_count} files.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning files: {str(e)}\")\n",
    "\n",
    "# Uncomment to list existing random files\n",
    "# list_random_files()\n",
    "\n",
    "# Uncomment to clean up random files (BE CAREFUL!)\n",
    "# clean_random_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
